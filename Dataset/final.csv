ID,Topic,Document Title,Authors,Author Affiliations,Publication Year,Abstract,PDF Link,Author Keywords,IEEE Terms,INSPEC Controlled Terms,INSPEC Non-Controlled Terms,Article Citation Count
1,Traceability,Semantically Enhanced Software Traceability Using Deep Learning Techniques,J. Guo; J. Cheng; J. Cleland-Huang,"University of Notre Dame, IN, USA; University of Notre Dame, IN, USA; University of Notre Dame, IN, USA",2017,"In most safety-critical domains the need for traceability is prescribed by certifying bodies. Trace links are generally created among requirements, design, source code, test cases and other artifacts, however, creating such links manually is time consuming and error prone. Automated solutions use information retrieval and machine learning techniques to generate trace links, however, current techniques fail to understand semantics of the software artifacts or to integrate domain knowledge into the tracing process and therefore tend to deliver imprecise and inaccurate results. In this paper, we present a solution that uses deep learning to incorporate requirements artifact semantics and domain knowledge into the tracing solution. We propose a tracing network architecture that utilizes Word Embedding and Recurrent Neural Network (RNN) models to generate trace links. Word embedding learns word vectors that represent knowledge of the domain corpus and RNN uses these word vectors to learn the sentence semantics of requirements artifacts. We trained 360 different configurations of the tracing network using existing trace links in the Positive Train Control domain and identified the Bidirectional Gated Recurrent Unit (BI-GRU) as the best model for the tracing task. BI-GRU significantly out-performed state-of-the-art tracing methods including the Vector Space Model and Latent Semantic Indexing.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985645,Traceability;Deep Learning;Recurrent Neural Network;Semantic Representation,Semantics;Machine learning;Recurrent neural networks;Standards;Training;Natural language processing;Software,information retrieval;knowledge representation;learning (artificial intelligence);program diagnostics;program testing;programming language semantics;recurrent neural nets;safety-critical software;source code (software);vectors,semantically enhanced software traceability;deep learning techniques;safety-critical domains;source code;test cases;information retrieval technique;machine learning technique;trace link generation;requirements artifact semantics;domain knowledge;tracing network architecture;word embedding model;recurrent neural network model;word vectors;knowledge representation;domain corpus;sentence semantics;requirements artifacts;positive train control domain;bidirectional gated recurrent unit;BI-GRU;vector space model;latent semantic indexing,147
2,Traceability,Can Latent Topics in Source Code Predict Missing Architectural Tactics?,R. Gopalakrishnan; P. Sharma; M. Mirakhorli; M. Galster,"Rochester Institute of Technology, USA; Rochester Institute of Technology, USA; Rochester Institute of Technology, USA; University of Canterbury, New Zealand",2017,"Architectural tactics such as heartbeat, resource pooling, and scheduling provide solutions to satisfy reliability, security, performance, and other critical characteristics of a software system. Current design practices advocate rigorous up-front analysis of the system's quality concerns to identify tactics and where in the code they should be used. In this paper, we explore a bottom-up approach to recommend architectural tactics based on latent topics discovered in the source code of projects. We present a recommender system developed by building predictor models which capture relationships between topical concepts in source code and the use of specific architectural tactics in that code. Based on an extensive analysis of over 116,000 open source systems, we identify significant correlations between latent topics in source code and the usage of architectural tactics. We use this information to construct a predictor for generating tactic recommendations. Our approach is validated through a series of experiments which demonstrate the ability to generate package-level tactic recommendations. We provide further validation via two large-scale studies of Apache Hive and Hadoop to illustrate that our recommender system predicts tactics that are actually implemented by developers in later releases.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985646,Architectural design and implementation;tactic recommender;emergent design,Recommender systems;Inference algorithms;Training;Machine learning algorithms;Security;Software reliability,public domain software;recommender systems;software architecture;software quality;software reliability;source code (software),source code;missing architectural tactics prediction;software system;system quality analysis;bottom-up approach;latent topics;recommender system;predictor models;open source systems;package-level tactic recommendation generation;Apache Hive;Hadoop,8
3,Documentation,Analyzing APIs Documentation and Code to Detect Directive Defects,Y. Zhou; R. Gu; T. Chen; Z. Huang; S. Panichella; H. Gall,"College of Computer Science, Nanjing University of Aeronautics and Astronautics, Nanjing, China; College of Computer Science, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Computer Science, Middlesex University, London, UK; College of Computer Science, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Department of Informatics, University of Zurich, Switzerland; Department of Informatics, University of Zurich, Switzerland",2017,"Application Programming Interface (API) documents represent one of the most important references for API users. However, it is frequently reported that the documentation is inconsistent with the source code and deviates from the API itself. Such inconsistencies in the documents inevitably confuse the API users hampering considerably their API comprehension and the quality of software built from such APIs. In this paper, we propose an automated approach to detect defects of API documents by leveraging techniques from program comprehension and natural language processing. Particularly, we focus on the directives of the API documents which are related to parameter constraints and exception throwing declarations. A first-order logic based constraint solver is employed to detect such defects based on the obtained analysis results. We evaluate our approach on parts of well documented JDK 1.8 APIs. Experiment results show that, out of around 2000 API usage constraints, our approach can detect 1158 defective document directives, with a precision rate of 81.6%, and a recall rate of 82.0%, which demonstrates its practical feasibility.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985647,API documentation;static analysis;natural language processing,Documentation;Natural language processing;Null value;Software;Data mining;Feature extraction;Computer science,application program interfaces;formal logic;natural language processing;program diagnostics;software maintenance;software quality;system documentation,API documentation analysis;directive defect detection;API code analysis;application programming interface documents;API comprehension;software quality;natural language processing;program comprehension;first-order logic based constraint solver;parameter constraints;exception throwing declarations;JDK 1.8 API;API usage constraints;defective document directives;precision rate;recall rate,66
4,Documentation,An Unsupervised Approach for Discovering Relevant Tutorial Fragments for APIs,H. Jiang; J. Zhang; Z. Ren; T. Zhang,"State Key Laboratory of Software Engineering, Wuhan University, Wuhan, China; School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China; College of Computer Science and Technology, Harbin Engineering University, Harbin, China",2017,"Developers increasingly rely on API tutorials to facilitate software development. However, it remains a challenging task for them to discover relevant API tutorial fragments explaining unfamiliar APIs. Existing supervised approaches suffer from the heavy burden of manually preparing corpus-specific annotated data and features. In this study, we propose a novel unsupervised approach, namely Fragment Recommender for APIs with PageRank and Topic model (FRAPT). FRAPT can well address two main challenges lying in the task and effectively determine relevant tutorial fragments for APIs. In FRAPT, a Fragment Parser is proposed to identify APIs in tutorial fragments and replace ambiguous pronouns and variables with related ontologies and API names, so as to address the pronoun and variable resolution challenge. Then, a Fragment Filter employs a set of non-explanatory detection rules to remove non-explanatory fragments, thus address the non-explanatory fragment identification challenge. Finally, two correlation scores are achieved and aggregated to determine relevant fragments for APIs, by applying both topic model and PageRank algorithm to the retained fragments. Extensive experiments over two publicly open tutorial corpora show that, FRAPT improves the state-of-the-art approach by 8.77% and 12.32% respectively in terms of F-Measure. The effectiveness of key components of FRAPT is also validated.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985648,Application Programming Interface;PageRank Algorithm;Topic Model;Unsupervised Approaches,Tutorials;Correlation;Ontologies;Programming;Manuals;Software;Filtering algorithms,application program interfaces;computer aided instruction;computer science education;grammars;recommender systems;unsupervised learning,API tutorials;unsupervised approach;Fragment Recommender for APIs with PageRank and Topic model;FRAPT;fragment parser;fragment filter;nonexplanatory fragment identification;variable resolution challenge;pronoun challenge;F-Measure;application programming interfaces,43
5,Documentation,Detecting User Story Information in Developer-Client Conversations to Generate Extractive Summaries,P. Rodeghero; S. Jiang; A. Armaly; C. McMillan,"Department of Computer Science and Engineering, University of Notre Dame, IN, USA; Department of Computer Science and Engineering, University of Notre Dame, IN, USA; Department of Computer Science and Engineering, University of Notre Dame, IN, USA; Department of Computer Science and Engineering, University of Notre Dame, IN, USA",2017,"User stories are descriptions of functionality that a software user needs. They play an important role in determining which software requirements and bug fixes should be handled and in what order. Developers elicit user stories through meetings with customers. But user story elicitation is complex, and involves many passes to accommodate shifting and unclear customer needs. The result is that developers must take detailed notes during meetings or risk missing important information. Ideally, developers would be freed of the need to take notes themselves, and instead speak naturally with their customers. This paper is a step towards that ideal. We present a technique for automatically extracting information relevant to user stories from recorded conversations between customers and developers. We perform a qualitative study to demonstrate that user story information exists in these conversations in a sufficient quantity to extract automatically. From this, we found that roughly 10.2% of these conversations contained user story information. Then, we test our technique in a quantitative study to determine the degree to which our technique can extract user story information. In our experiment, our process obtained about 70.8% precision and 18.3% recall on the information.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985649,developer communication;user story generation;software engineering;productivity;transcripts,Data mining;Software;Computer bugs;Electronic mail;Software algorithms;Software engineering;Algorithm design and analysis,software engineering,user story information detection;developer-client conversations;extractive summaries;software functionality,40
6,Refactoring,Clone Refactoring with Lambda Expressions,N. Tsantalis; D. Mazinanian; S. Rostami,"Computer Science and Software Engineering, Concordia University, Montreal, Canada; Computer Science and Software Engineering, Concordia University, Montreal, Canada; Computer Science and Software Engineering, Concordia University, Montreal, Canada",2017,"Lambda expressions have been introduced in Java 8 to support functional programming and enable behavior parameterization by passing functions as parameters to methods. The majority of software clones (duplicated code) are known to have behavioral differences (i.e., Type-2 and Type-3 clones). However, to the best of our knowledge, there is no previous work to investigate the utility of Lambda expressions for parameterizing such behavioral differences in clones. In this paper, we propose a technique that examines the applicability of Lambda expressions for the refactoring of clones with behavioral differences. Moreover, we empirically investigate the applicability and characteristics of the Lambda expressions introduced to refactor a large dataset of clones. Our findings show that Lambda expressions enable the refactoring of a significant portion of clones that could not be refactored by any other means.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985650,Refactoring;Code duplication;Lambda expressions,Cloning;Data mining;Java;Europe;Testing;Open source software,functional programming;Java;lambda calculus;software maintenance,clone refactoring;lambda expressions;Java 8;functional programming;behavior parameterization;software clones;duplicated code;behavioral differences;Type-2 clones;Type-3 clones,34
7,Refactoring,Characterizing and Detecting Anti-Patterns in the Logging Code,B. Chen; Z. M. Jiang,"Software Construction, AnaLytics and Evaluation (SCALE) Lab, York University, Toronto, Canada; Software Construction, AnaLytics and Evaluation (SCALE) Lab, York University, Toronto, Canada",2017,"Snippets of logging code are output statements (e.g., LOG.info or System.out.println) that developers insert into a software system. Although more logging code can provide more execution context of the system's behavior during runtime, it is undesirable to instrument the system with too much logging code due to maintenance overhead. Furthermore, excessive logging may cause unexpected side-effects like performance slow-down or high disk I/O bandwidth. Recent studies show that there are no well-defined coding guidelines for performing effective logging. Previous research on the logging code mainly tackles the problems of where-to-log and what-to-log. There are very few works trying to address the problem of how-to-log (developing and maintaining high-quality logging code). In this paper, we study the problem of how-to-log by characterizing and detecting the anti-patterns in the logging code. As the majority of the logging code is evolved together with the feature code, the remaining set of logging code changes usually contains the fixes to the anti-patterns. We have manually examined 352 pairs of independently changed logging code snippets from three well-maintenance open source systems: ActiveMQ, Hadoop and Maven. Our analysis has resulted in six different anti-patterns in the logging code. To demonstrate the value of our findings, we have encoded these anti-patterns into a static code analysis tool, LCAnalyzer. Case studies show that LCAnalyzer has an average recall of 95% and precision of 60% and can be used to automatically detect previously unknown anti-patterns in the source code. To gather feedback, we have filed 64 representative instances of the logging code anti-patterns from the most recent releases of ten open source software systems. Among them, 46 instances (72%) have already been accepted by their developers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985651,anti-patterns;logging code;logging practices;empirical studies;software maintenance,Tools;Open source software;Software systems;Runtime;Computer crashes;Data mining,data handling;parallel processing;program diagnostics;public domain software;software maintenance;source code (software),anti-pattern detection;anti-pattern characterization;system behavior;maintenance overhead;high disk I/O bandwidth;logging code snippets;ActiveMQ;Hadoop;Maven;static code analysis tool;LCAnalyzer;source code;open source software systems;how-to-log problem,68
8,Refactoring,Automated Refactoring of Legacy Java Software to Default Methods,R. Khatchadourian; H. Masuhara,City University of New York; Tokyo Institute of Technology,2017,"Java 8 default methods, which allow interfaces to contain (instance) method implementations, are useful for the skeletal implementation software design pattern. However, it is not easy to transform existing software to exploit default methods as it requires analyzing complex type hierarchies, resolving multiple implementation inheritance issues, reconciling differences between class and interface methods, and analyzing tie-breakers (dispatch precedence) with overriding class methods to preserve type-correctness and confirm semantics preservation. In this paper, we present an efficient, fully-automated, type constraint-based refactoring approach that assists developers in taking advantage of enhanced interfaces for their legacy Java software. The approach features an extensive rule set that covers various corner-cases where default methods cannot be used. To demonstrate applicability, we implemented our approach as an Eclipse plug-in and applied it to 19 real-world Java projects, as well as submitted pull requests to popular GitHub repositories. The indication is that it is useful in migrating skeletal implementation methods to interfaces as default methods, sheds light onto the pattern's usage, and provides insight to language designers on how this new construct applies to existing software.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985652,refactoring;java;interfaces;default methods,Java;Software;Semantics;Concrete;Face;Printing;Syntactics,Java;software maintenance,automated refactoring;legacy Java software;software design pattern;fully-automated type constraint-based refactoring approach;Eclipse plug-in;GitHub repositories,9
9,Recommendation Systems,Supporting Software Developers with a Holistic Recommender System,L. Ponzanelli; S. Scalabrino; G. Bavota; A. Mocci; R. Oliveto; M. Di Penta; M. Lanza,"UniversitÃ della Svizzera italiana (USI), Switzerland; University of Molise, Italy; UniversitÃ della Svizzera italiana (USI), Switzerland; UniversitÃ della Svizzera italiana (USI), Switzerland; University of Molise, Italy; University of Sannio, Italy; UniversitÃ della Svizzera italiana (USI), Switzerland",2017,"The promise of recommender systems is to provide intelligent support to developers during their programming tasks. Such support ranges from suggesting program entities to taking into account pertinent Q&A pages. However, current recommender systems limit the context analysis to change history and developers' activities in the IDE, without considering what a developer has already consulted or perused, e.g., by performing searches from the Web browser. Given the faceted nature of many programming tasks, and the incompleteness of the information provided by a single artifact, several heterogeneous resources are required to obtain the broader picture needed by a developer to accomplish a task. We present Libra, a holistic recommender system. It supports the process of searching and navigating the information needed by constructing a holistic meta-information model of the resources perused by a developer, analyzing their semantic relationships, and augmenting the web browser with a dedicated interactive navigation chart. The quantitative and qualitative evaluation of Libra provides evidence that a holistic analysis of a developer's information context can indeed offer comprehensive and contextualized support to information navigation and retrieval during software development.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985653,Mining unstructured data;Recommender systems,Browsers;Uniform resource locators;Recommender systems;Navigation;Web search;User interfaces;Web pages,information analysis;information retrieval;interactive programming;online front-ends;recommender systems;software engineering,software developers;holistic recommender system;intelligent support;programming tasks;program entities;pertinent Q&A pages;heterogeneous resources;Libra;information search;information navigation;holistic meta-information model;Web browser;interactive navigation chart;information context analysis;information retrieval;software development,33
10,Recommendation Systems,Recommending and Localizing Change Requests for Mobile Apps Based on User Reviews,F. Palomba; P. Salza; A. Ciurumelea; S. Panichella; H. Gall; F. Ferrucci; A. De Lucia,"Delft University of Technology, The Netherlands; University of Salerno, Italy; University of Zurich, Switzerland; University of Zurich, Switzerland; University of Zurich, Switzerland; University of Salerno, Italy; University of Salerno, Italy",2017,"Researchers have proposed several approaches to extract information from user reviews useful for maintaining and evolving mobile apps. However, most of them just perform automatic classification of user reviews according to specific keywords (e.g., bugs, features). Moreover, they do not provide any support for linking user feedback to the source code components to be changed, thus requiring a manual, time-consuming, and error-prone task. In this paper, we introduce CHANGEADVISOR, a novel approach that analyzes the structure, semantics, and sentiments of sentences contained in user reviews to extract useful (user) feedback from maintenance perspectives and recommend to developers changes to software artifacts. It relies on natural language processing and clustering algorithms to group user reviews around similar user needs and suggestions for change. Then, it involves textual based heuristics to determine the code artifacts that need to be maintained according to the recommended software changes. The quantitative and qualitative studies carried out on 44,683 user reviews of 10 open source mobile apps and their original developers showed a high accuracy of CHANGEADVISOR in (i) clustering similar user change requests and (ii) identifying the code components impacted by the suggested changes. Moreover, the obtained results show that ChangeAdvisor is more accurate than a baseline approach for linking user feedback clusters to the source code in terms of both precision (+47%) and recall (+38%).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985654,Mobile Apps;Mining User Reviews;Natural Language Processing;Impact Analysis,Computer bugs;Mobile communication;Joining processes;Tools;Maintenance engineering;Software;Feature extraction,mobile computing;natural language processing;source code (software),textual based heuristics;CHANGEADVISOR;user feedback;source code components;mobile apps,97
11,Recommendation Systems,Machine Learning-Based Detection of Open Source License Exceptions,C. Vendome; M. Linares-VÃ¡squez; G. Bavota; M. Di Penta; D. German; D. Poshyvanyk,"College of William and Mary, Williamsburg, VA, USA; Universidad de los Andes, BogotÃ¡, Colombia; UniversitÃ della Svizzera italiana (USI), Lugano, Switzerland; University of Sannio, Benevento, Italy; University of Victoria, BC, Canada; College of William and Mary, Williamsburg, VA, US",2017,"From a legal perspective, software licenses govern the redistribution, reuse, and modification of software as both source and binary code. Free and Open Source Software (FOSS) licenses vary in the degree to which they are permissive or restrictive in allowing redistribution or modification under licenses different from the original one(s). In certain cases, developers may modify the license by appending to it an exception to specifically allow reuse or modification under a particular condition. These exceptions are an important factor to consider for license compliance analysis since they modify the standard (and widely understood) terms of the original license. In this work, we first perform a large-scale empirical study on the change history of over 51K FOSS systems aimed at quantitatively investigating the prevalence of known license exceptions and identifying new ones. Subsequently, we performed a study on the detection of license exceptions by relying on machine learning. We evaluated the license exception classification with four different supervised learners and sensitivity analysis. Finally, we present a categorization of license exceptions and explain their implications.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985655,Software Licenses;Empirical Studies;Classifiers,Software engineering,learning (artificial intelligence);pattern classification;public domain software;software reusability;source code (software),sensitivity analysis;supervised learners;license exception classification;license compliance analysis;FOSS licenses;free and open source software;source code;binary code;software reuse;software redistribution;software modification;software licenses;open source license exceptions;machine learning-based detection,16
12,Software Process,Software Development Waste,T. Sedano; P. Ralph; C. PÃ©raire,"Carnegie Mellon University; University of British Columbia, Vancouver, BC, Canada; Carnegie Mellon University Electrical and Computer Engineering, CA, USA",2017,"Context: Since software development is a complex socio-technical activity that involves coordinating different disciplines and skill sets, it provides ample opportunities for waste to emerge. Waste is any activity that produces no value for the customer or user. Objective: The purpose of this paper is to identify and describe different types of waste in software development. Method: Following Constructivist Grounded Theory, we conducted a two-year five-month participant-observation study of eight software development projects at Pivotal, a software development consultancy. We also interviewed 33 software engineers, interaction designers, and product managers, and analyzed one year of retrospection topics. We iterated between analysis and theoretical sampling until achieving theoretical saturation. Results: This paper introduces the first empirical waste taxonomy. It identifies nine wastes and explores their causes, underlying tensions, and overall relationship to the waste taxonomy found in Lean Software Development. Limitations: Grounded Theory does not support statistical generalization. While the proposed taxonomy appears widely applicable, organizations with different software development cultures may experience different waste types. Conclusion: Software development projects manifest nine types of waste: building the wrong feature or product, mismanaging the backlog, rework, unnecessarily complex solutions, extraneous cognitive load, psychological distress, waiting/multitasking, knowledge loss, and ineffective communication.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985656,Software engineering waste;Extreme Programming;Lean Software Development,Software;Interviews;Manufacturing;Taxonomy;Programming;Production systems,software engineering,software development waste;complex socio-technical activity;constructivist grounded theory;software development consultancy;Pivotal;lean software development,52
13,Software Process,Becoming Agile: A Grounded Theory of Agile Transitions in Practice,R. Hoda; J. Noble,"Department of Electrical and Computer Engineering, The University of Auckland, Auckland, New Zealand; School of Engineering and Computer Science, Victoria University of Wellington, Wellington, New Zealand",2017,"Agile adoption is typically understood as a one-off organizational process involving a staged selection of agile development practices. This view of agility fails to explain the differences in the pace and effectiveness of individual teams transitioning to agile development. Based on a Grounded Theory study of 31 agile practitioners drawn from 18 teams across five countries, we present a grounded theory of becoming agile as a network of on-going transitions across five dimensions: software development practices, team practices, management approach, reflective practices, and culture. The unique position of a software team through this network, and their pace of progress along the five dimensions, explains why individual agile teams present distinct manifestations of agility and unique transition experiences. The theory expands the current understanding of agility as a holistic and complex network of on-going multidimensional transitions, and will help software teams, their managers, and organizations better navigate their individual agile journeys.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985657,agile software development;transition;selforganizing;teams;management;culture;theory;grounded theory,Software;Interviews;Telecommunications;Banking;Encoding;Finance,cultural aspects;software development management;software prototyping;team working,agile development practices;grounded theory;software development practices;team practices;management approach;reflective practices;culture;software team,57
14,Software Process,From Diversity by Numbers to Diversity as Process: Supporting Inclusiveness in Software Development Teams with Brainstorming,A. Filippova; E. Trainer; J. D. Herbsleb,"Institute for Software Research, Carnegie Mellon University, Pittsburgh, PA, USA; Institute for Software Research, Carnegie Mellon University, Pittsburgh, PA, USA; Institute for Software Research, Carnegie Mellon University, Pittsburgh, PA, USA",2017,"Negative experiences in diverse software development teams have the potential to turn off minority participants from future team-based software development activity. We examine the use of brainstorming as one concrete team processes that may be used to improve the satisfaction of minority developers when working in a group. Situating our study in time-intensive hackathon-like environments where engagement of all team members is particularly crucial, we use a combination of survey and interview data to test our propositions. We find that brainstorming strategies are particularly effective for team members who identify as minorities, and support satisfaction with both the process and outcomes of teamwork through different mechanisms.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985658,Diversity;hackathons;teamwork;brainstorming;satisfaction;software engineering management,Software;Teamwork;Software engineering;Concrete;Cultural differences;Organizations,software engineering,software development;time-intensive hackathon-like environments;brainstorming,38
15,Studies of Software Developers,Classifying Developers into Core and Peripheral: An Empirical Study on Count and Network Metrics,M. Joblin; S. Apel; C. Hunsen; W. Mauerer,"Siemens AG, Erlangen, Germany; University of Passau, Passau, Germany; University of Passau, Passau, Germany; Siemens AG, OTH Regensburg, Munich / Regensburg, Germany",2017,"Knowledge about the roles developers play in a software project is crucial to understanding the project's collaborative dynamics. In practice, developers are often classified according to the dichotomy of core and peripheral roles. Typically, count-based operationalizations, which rely on simple counts of individual developer activities (e.g., number of commits), are used for this purpose, but there is concern regarding their validity and ability to elicit meaningful insights. To shed light on this issue, we investigate whether count-based operationalizations of developer roles produce consistent results, and we validate them with respect to developers' perceptions by surveying 166 developers. Improving over the state of the art, we propose a relational perspective on developer roles, using fine-grained developer networks modeling the organizational structure, and by examining developer roles in terms of developers' positions and stability within the developer network. In a study of 10 substantial open-source projects, we found that the primary difference between the count-based and our proposed network-based core-peripheral operationalizations is that the network-based ones agree more with developer perception than count-based ones. Furthermore, we demonstrate that a relational perspective can reveal further meaningful insights, such as that core developers exhibit high positional stability, upper positions in the hierarchy, and high levels of coordination with other core developers, which confirms assumptions of previous work.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985659,Developer roles;developer networks;classification;mining software repositories,Measurement;Collaboration;Stability analysis;Open source software;Systems architecture;Computer bugs,software engineering,software project;count-based operationalizations;fine-grained developer network;open-source projects;network-based core-peripheral operationalizations,44
16,Studies of Software Developers,Decoding the Representation of Code in the Brain: An fMRI Study of Code Review and Expertise,B. Floyd; T. Santander; W. Weimer,University of Virginia; University of Virginia; University of Virginia,2017,"Subjective judgments in software engineering tasks are of critical importance but can be difficult to study with conventional means. Medical imaging techniques hold the promise of relating cognition to physical activities and brain structures. In a controlled experiment involving 29 participants, we examine code comprehension, code review and prose review using functional magnetic resonance imaging. We find that the neural representations of programming languages vs. natural languages are distinct. We can classify which task a participant is undertaking based solely on brain activity (balanced accuracy 79%, p <; 0.001). Further, we find that the same set of brain regions distinguish between code and prose (near-perfect correlation, r = 0.99, p <; 0.001). Finally, we find that task distinctions are modulated by expertise, such that greater skill predicts a less differentiated neural representation (r = -0.44, p = 0.016) indicating that more skilled participants treat code and prose more similarly at a neural activation level.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985660,medical imaging;code comprehension;prose review,Biomedical imaging;Software engineering;Brain;Software;Computer science;Natural languages;Tools,biomedical MRI;medical image processing;software engineering,fMRI study;software engineering tasks;medical imaging techniques;functional magnetic resonance imaging;code comprehension;code review;prose review;neural representations;programming languages;natural languages,71
17,Studies of Software Developers,"Understanding the Impressions, Motivations, and Barriers of One Time Code Contributors to FLOSS Projects: A Survey",A. Lee; J. C. Carver; A. Bosu,"Computer Science Department, University of Alabama, Tuscaloosa, AL, USA; Computer Science Department, University of Alabama, Tuscaloosa, AL, USA; Department of Computer Science, Southern Illinois University, Carbondale, IL, USA",2017,"Successful Free/Libre Open Source Software (FLOSS) projects must attract and retain high-quality talent. Researchers have invested considerable effort in the study of core and peripheral FLOSS developers. To this point, one critical subset of developers that have not been studied are One-Time code Contributors (OTC) - those that have had exactly one patch accepted. To understand why OTCs have not contributed another patch and provide guidance to FLOSS projects on retaining OTCs, this study seeks to understand the impressions, motivations, and barriers experienced by OTCs. We conducted an online survey of OTCs from 23 popular FLOSS projects. Based on the 184 responses received, we observed that OTCs generally have positive impressions of their FLOSS project and are driven by a variety of motivations. Most OTCs primarily made contributions to fix bugs that impeded their work and did not plan on becoming long term contributors. Furthermore, OTCs encounter a number of barriers that prevent them from continuing to contribute to the project. Based on our findings, there are some concrete actions FLOSS projects can take to increase the chances of converting OTCs into long-term contributors.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985661,FLOSS;Open source;OSS;Newcomers;One Time Contributors;Survey;Qualitative Research,Computer bugs;Electronic mail;Encoding;Computer science;Open source software;Concrete;Tools,project management;public domain software;software engineering,one time code contributors;FLOSS projects;free-Libre open source software project;OTC,70
18,Search-based software engineering,Search-Driven String Constraint Solving for Vulnerability Detection,J. ThomÃ©; L. K. Shar; D. Bianculli; L. Briand,"SnT Centre, University of Luxembourg; SnT Centre, University of Luxembourg; SnT Centre, University of Luxembourg; SnT Centre, University of Luxembourg",2017,"Constraint solving is an essential technique for detecting vulnerabilities in programs, since it can reason about input sanitization and validation operations performed on user inputs. However, real-world programs typically contain complex string operations that challenge vulnerability detection. State-of-the-art string constraint solvers support only a limited set of string operations and fail when they encounter an unsupported one, this leads to limited effectiveness in finding vulnerabilities. In this paper we propose a search-driven constraint solving technique that complements the support for complex string operations provided by any existing string constraint solver. Our technique uses a hybrid constraint solving procedure based on the Ant Colony Optimization meta-heuristic. The idea is to execute it as a fallback mechanism, only when a solver encounters a constraint containing an operation that it does not support. We have implemented the proposed search-driven constraint solving technique in the ACO-Solver tool, which we have evaluated in the context of injection and XSS vulnerability detection for Java Web applications. We have assessed the benefits and costs of combining the proposed technique with two state-of-the-art constraint solvers (Z3-str2 and CVC4). The experimental results, based on a benchmark with 104 constraints derived from nine realistic Web applications, show that our approach, when combined in a state-of-the-art solver, significantly improves the number of detected vulnerabilities (from 4.7% to 71.9% for Z3-str2, from 85.9% to 100.0% for CVC4), and solves several cases on which the solver fails when used stand-alone (46 more solved cases for Z3-str2, and 11 more for CVC4), while still keeping the execution time affordable in practice.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985662,vulnerability detection;string constraint solving;search-based software engineering,Java;Libraries;Security;Ant colony optimization;Automata;Search problems;Standards,ant colony optimisation;Internet;Java;program verification;search problems,input sanitization;program validation operations;complex string operations;search-driven string constraint solving technique;hybrid constraint solving procedure;ant colony optimization metaheuristic;fallback mechanism;ACO-solver tool;XSS vulnerability detection;Java Web applications,23
19,Search-based software engineering,A Guided Genetic Algorithm for Automated Crash Reproduction,M. Soltani; A. Panichella; A. van Deursen,"Delft University of Technology, The Netherlands; SnT Centre, University of Luxembourg, Luxembourg; Delft University of Technology, The Netherlands",2017,"To reduce the effort developers have to make for crash debugging, researchers have proposed several solutions for automatic failure reproduction. Recent advances proposed the use of symbolic execution, mutation analysis, and directed model checking as underling techniques for post-failure analysis of crash stack traces. However, existing approaches still cannot reproduce many real-world crashes due to such limitations as environment dependencies, path explosion, and time complexity. To address these challenges, we present EvoCrash, a post-failure approach which uses a novel Guided Genetic Algorithm (GGA) to cope with the large search space characterizing real-world software programs. Our empirical study on three open-source systems shows that EvoCrash can replicate 41 (82%) of real-world crashes, 34 (89%) of which are useful reproductions for debugging purposes, outperforming the state-of-the-art in crash replication.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985663,Search-Based Software Testing;Genetic Algorithms;Automated Crash Reproduction,Tools;Genetic algorithms;Software;Model checking;Computer bugs;Debugging,genetic algorithms;program debugging,guided genetic algorithm;automated crash reproduction;crash debugging;automatic failure reproduction;EvoCrash;post-failure approach;GGA,31
20,Search-based software engineering,Stochastic Optimization of Program Obfuscation,H. Liu; C. Sun; Z. Su; Y. Jiang; M. Gu; J. Sun,"School of Software, Tsinghua University, Beijing, China; University of California, Davis, USA; University of California, Davis, USA; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China; School of Software, Tsinghua University, Beijing, China",2017,"Program obfuscation is a common practice in software development to obscure source code or binary code, in order to prevent humans from understanding the purpose or logic of software. It protects intellectual property and deters malicious attacks. While tremendous efforts have been devoted to the development of various obfuscation techniques, we have relatively little knowledge on how to most effectively use them together. The biggest challenge lies in identifying the most effective combination of obfuscation techniques. This paper presents a unified framework to optimize program obfuscation. Given an input program P and a set T of obfuscation transformations, our technique can automatically identify a sequence seq = ã€ˆt1, t2, ..., tnã€‰ (âˆ€i âˆˆ [1, n]. ti âˆˆ T), such that applying ti in order on P yields the optimal obfuscation performance. We model the process of searching for seq as a mathematical optimization problem. The key technical contributions of this paper are: (1) an obscurity language model to assess obfuscation effectiveness/optimality, and (2) a guided stochastic algorithm based on Markov chain Monte Carlo methods to search for the optimal solution seq. We have realized the framework in a tool Closure* for JavaScript, and evaluated it on 25 most starred JavaScript projects on GitHub (19K lines of code). Our machinery study shows that Closure* outperforms the well-known Google Closure Compiler by defending 26% of the attacks initiated by JSNice. Our human study also reveals that Closure* is practical and can reduce the human attack success rate by 30%.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985664,program obfuscation;obscurity language model;markov chain monte carlo methods,Optimization;Mathematical model;Reactive power;Markov processes;Google;Lenses;Software,Java;Markov processes;Monte Carlo methods;software engineering,stochastic optimization;program obfuscation;software development;source code;binary code;mathematical optimization problem;guided stochastic algorithm;obscurity language model;Markov chain Monte Carlo methods;JavaScript,17
21,Web Applications,ZenIDS: Introspective Intrusion Detection for PHP Applications,B. Hawkins; B. Demsky,"University of California, Irvine; University of California, Irvine",2017,"Since its first appearance more than 20 years ago, PHP has steadily increased in popularity, and has become the foundation of the Internet's most popular content management systems (CMS). Of the world's 1 million most visited websites, nearly half use a CMS, and WordPress alone claims 25% market share of all websites. While their easy-to-use templates and components have greatly simplified the work of developing high quality websites, it comes at the cost of software vulnerabilities that are inevitable in such large and rapidly evolving frameworks. Intrusion Detection Systems (IDS) are often used to protect Internet-facing applications, but conventional techniques struggle to keep up with the fast pace of development in today's web applications. Rapid changes to application interfaces increase the workload of maintaining an IDS whitelist, yet the broad attack surface of a web application makes for a similarly verbose blacklist. We developed ZenIDS to dynamically learn the trusted execution paths of an application during a short online training period and report execution anomalies as potential intrusions. We implement ZenIDS as a PHP extension supported by 8 hooks instrumented in the PHP interpreter. Our experiments demonstrate its effectiveness monitoring live web traffic for one year to 3 large PHP applications, detecting malicious requests with a false positive rate of less than .01% after training on fewer than 4,000 requests. ZenIDS excludes the vast majority of deployed PHP code from the whitelist because it is never used for valid requests-yet could potentially be exploited by a remote adversary. We observe 5% performance overhead (or less) for our applications vs. an optimized vanilla LAMP stack.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985665,,Training;Monitoring;Authentication;Intrusion detection;Internet;Content management,authoring languages;security of data;system monitoring,PHP applications;ZENIDS;PHP extension;PHP interpreter;malicious requests detection;live Web traffic monitoring;vanilla LAMP stack;introspective intrusion detection;intrusion detection systems,5
22,Web Applications,Statically Checking Web API Requests in JavaScript,E. Wittern; A. T. T. Ying; Y. Zheng; J. Dolby; J. A. Laredo,"IBM T. J. Watson Research Center, NY, USA; IBM T. J. Watson Research Center, NY, USA; IBM T. J. Watson Research Center, NY, USA; IBM T. J. Watson Research Center, NY, USA; IBM T. J. Watson Research Center, NY, USA",2017,"Many JavaScript applications perform HTTP requests to web APIs, relying on the request URL, HTTP method, and request data to be constructed correctly by string operations. Traditional compile-time error checking, such as calling a non-existent method in Java, are not available for checking whether such requests comply with the requirements of a web API. In this paper, we propose an approach to statically check web API requests in JavaScript. Our approach first extracts a request's URL string, HTTP method, and the corresponding request data using an inter-procedural string analysis, and then checks whether the request conforms to given web API specifications. We evaluated our approach by checking whether web API requests in JavaScript files mined from GitHub are consistent or inconsistent with publicly available API specifications. From the 6575 requests in scope, our approach determined whether the request's URL and HTTP method was consistent or inconsistent with web API specifications with a precision of 96.0%. Our approach also correctly determined whether extracted request data was consistent or inconsistent with the data requirements with a precision of 87.9% for payload data and 99.9% for query data. In a systematic analysis of the inconsistent cases, we found that many of them were due to errors in the client code. The here proposed checker can be integrated with code editors or with continuous integration tools to warn programmers about code containing potentially erroneous requests.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985666,Static analysis;JavaScript;Web APIs,Uniform resource locators;Data mining;Reactive power;Payloads;Tools;Media;Writing,application program interfaces;Java;program diagnostics,Web API requests;JavaScript;HTTP requests;compile-time error checking;URL string;interprocedural string analysis;GitHub,24
23,Web Applications,On Cross-Stack Configuration Errors,M. Sayagh; N. Kerzazi; B. Adams,"Polytechnique Montreal; ENSIAS, Univ Mohammed V, Rabat; Polytechnique Montreal",2017,"Today's web applications are deployed on powerful software stacks such as MEAN (JavaScript) or LAMP (PHP), which consist of multiple layers such as an operating system, web server, database, execution engine and application framework, each of which provide resources to the layer just above it. These powerful software stacks unfortunately are plagued by so-called cross-stack configuration errors (CsCEs), where a higher layer in the stack suddenly starts to behave incorrectly or even crash due to incorrect configuration choices in lower layers. Due to differences in programming languages and lack of explicit links between configuration options of different layers, sysadmins and developers have a hard time identifying the cause of a CsCE, which is why this paper (1) performs a qualitative analysis of 1,082 configuration errors to understand the impact, effort and complexity of dealing with CsCEs, then (2) proposes a modular approach that plugs existing source code analysis (slicing) techniques, in order to recommend the culprit configuration option. Empirical evaluation of this approach on 36 real CsCEs of the top 3 LAMP stack layers shows that our approach reports the misconfigured option with an average rank of 2.18 for 32 of the CsCEs, and takes only few minutes, making it practically useful.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985667,Software Configuration;Software Stack;Qualitative Study;Multi-layer Systems;Empirical Study;Slicing;PHP,Databases;Web servers;Testing;Debugging;Operating systems,Internet;program slicing;source code (software),Web applications;software stacks;cross-stack configuration errors;programming languages;CsCE;qualitative analysis;configuration errors;source code analysis;source code slicing;culprit configuration option;LAMP stack layers,20
24,Concurrency,Efficient Detection of Thread Safety Violations via Coverage-Guided Generation of Concurrent Tests,A. Choudhary; S. Lu; M. Pradel,"Department of Computer Science, TU Darmstadt, Germany; Department of Computer Science, TU Darmstadt, Germany; Department of Computer Science, TU Darmstadt, Germany",2017,"As writing concurrent programs is challenging, developers often rely on thread-safe classes, which encapsulate most synchronization issues. Testing such classes is crucial to ensure the correctness of concurrent programs. An effective approach to uncover otherwise missed concurrency bugs is to automatically generate concurrent tests. Existing approaches either create tests randomly, which is inefficient, build on a computationally expensive analysis of potential concurrency bugs exposed by sequential tests, or focus on exposing a particular kind of concurrency bugs, such as atomicity violations. This paper presents CovCon, a coverage-guided approach to generate concurrent tests. The key idea is to measure how often pairs of methods have already been executed concurrently and to focus the test generation on infrequently or not at all covered pairs of methods. The approach is independent of any particular bug pattern, allowing it to find arbitrary concurrency bugs, and is computationally inexpensive, allowing it to generate many tests in short time. We apply CovCon to 18 thread-safe Java classes, and it detects concurrency bugs in 17 of them. Compared to five state of the art approaches, CovCon detects more bugs than any other approach while requiring less time. Specifically, our approach finds bugs faster in 38 of 47 cases, with speedups of at least 4x for 22 of 47 cases.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985668,test generation;coverage;concurrency,Instruction sets;Computer bugs;Concurrent computing;Testing;Synchronization;Schedules,concurrency (computers);Java;program debugging;program testing;synchronisation,thread safety violation detection;coverage-guided generation;concurrent tests;concurrent program writing;synchronization;missed concurrency bugs;sequential tests;CovCon;bug pattern;thread-safe Java classes;test generation,30
25,Concurrency,RClassify: Classifying Race Conditions in Web Applications via Deterministic Replay,L. Zhang; C. Wang,"Virginia Tech, Blacksburg, VA, USA; University of Southern California, Los Angeles, CA, USA",2017,"Race conditions are common in web applications but are difficult to diagnose and repair. Although there exist tools for detecting races in web applications, they all report a large number of false positives. That is, the races they report are either bogus, meaning they can never occur in practice, or benign, meaning they do not lead to erroneous behaviors. Since manually diagnosing them is tedious and error prone, reporting these race warnings to developers would be counter-productive. We propose a platform-agnostic, deterministic replay-based method for identifying not only the real but also the truly harmful race conditions. It relies on executing each pair of racing events in two different orders and assessing their impact on the program state: we say a race is harmful only if (1) both of the two executions arefeasible and (2) they lead to different program states. We have evaluated our evidence-based classification method on a large set of real websites from Fortune-500 companies and demonstrated that it significantly outperforms all state-of-the-art techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985669,Race condition;web application;JavaScript;deterministic replay;program repair,Tools;HTML;Browsers;Companies;Robustness;Benchmark testing;Standards,Internet;pattern classification,RClassify;race condition classification;Web applications;platform-agnostic deterministic replay-based method;program state;evidence-based classification method;Web sites,15
26,Concurrency,Repairing Event Race Errors by Controlling Nondeterminism,C. Q. Adamsen; A. MÃ¸ller; R. Karim; M. Sridharan; F. Tip; K. Sen,"Aarhus University, Aarhus, Denmark; Aarhus University, Aarhus, Denmark; Samsung Research America, Mountain View, CA, USA; Samsung Research America, Mountain View, CA, USA; Northeastern University, Boston, MA, USA; EECS Department, UC Berkeley, CA, USA",2017,"Modern web applications are written in an event-driven style, in which event handlers execute asynchronously in response to user or system events. The nondeterminism arising from this programming style can lead to pernicious errors. Recent work focuses on detecting event races and classifying them as harmful or harmless. However, since modifying the source code to prevent harmful races can be a difficult and error-prone task, it may be preferable to steer away from the bad executions. In this paper, we present a technique for automated repair of event race errors in JavaScript web applications. Our approach relies on an event controller that restricts event handler scheduling in the browser according to a specified repair policy, by intercepting and carefully postponing or discarding selected events. We have implemented the technique in a tool called EventRaceCommander, which relies entirely on source code instrumentation, and evaluated it by repairing more than 100 event race errors that occur in the web applications from the largest 20 of the Fortune 500 companies. Our results show that application-independent repair policies usually suffice to repair event race errors without excessive negative impact on performance or user experience, though application-specific repair policies that target specific event races are sometimes desirable.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985670,JavaScript;event-driven programming;automated repair,Maintenance engineering;Thumb;Tools;Browsers;Instruments;Programming;Schedules,high level languages;Internet;object-oriented programming;online front-ends;software maintenance,event race error repair;JavaScript Web applications;event handler scheduling;browser;EventRaceCommander;source code instrumentation;application-independent repair policies;application-specific repair policies,26
27,Mobile Application Security,Making Malory Behave Maliciously: Targeted Fuzzing of Android Execution Environments,S. Rasthofer; S. Arzt; S. Triller; M. Pradel,"Fraunhofer SIT, TU Darmstadt, Germany; Fraunhofer SIT, TU Darmstadt, Germany; Fraunhofer SIT, Germany; Department of Computer Science, TU Darmstadt, Germany",2017,"Android applications, or apps, provide useful features to end-users, but many apps also contain malicious behavior. Modern malware makes understanding such behavior challenging by behaving maliciously only under particular conditions. For example, a malware app may check whether it runs on a real device and not an emulator, in a particular country, and alongside a specific target app, such as a vulnerable banking app. To observe the malicious behavior, a security analyst must find out and emulate all these app-specific constraints. This paper presents FuzzDroid, a framework for automatically generating an Android execution environment where an app exposes its malicious behavior. The key idea is to combine an extensible set of static and dynamic analyses through a search-based algorithm that steers the app toward a configurable target location. On recent malware, the approach reaches the target location in 75% of the apps. In total, we reach 240 code locations within an average time of only one minute. To reach these code locations, FuzzDroid generates 106 different environments, too many for a human analyst to create manually.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985671,,Malware;Smart phones;Heuristic algorithms;Algorithm design and analysis;Mobile communication;Security;Instruments,invasive software;smart phones,malory;Android execution environment targeted fuzzing;Android applications;malicious behavior;malware;security analyst;app-specific constraints;FuzzDroid;static analysis;dynamic analysis;search-based algorithm;code locations,32
28,Mobile Application Security,A SEALANT for Inter-App Security Holes in Android,Y. K. Lee; J. Y. Bang; G. Safi; A. Shahbazian; Y. Zhao; N. Medvidovic,"Computer Science Department, University of Southern California, Los Angeles, California, USA; Kakao Corporation Seongnam, Gyeonggi, Korea; Computer Science Department, University of Southern California, Los Angeles, California, USA; Computer Science Department, University of Southern California, Los Angeles, California, USA; Computer Science Department, University of Southern California, Los Angeles, California, USA; Computer Science Department, University of Southern California, Los Angeles, California, USA",2017,"Android's communication model has a major security weakness: malicious apps can manipulate other apps into performing unintended operations and can steal end-user data, while appearing ordinary and harmless. This paper presents SEALANT, a technique that combines static analysis of app code, which infers vulnerable communication channels, with runtime monitoring of inter-app communication through those channels, which helps to prevent attacks. SEALANT's extensive evaluation demonstrates that (1) it detects and blocks inter-app attacks with high accuracy in a corpus of over 1,100 real-world apps, (2) it suffers from fewer false alarms than existing techniques in several representative scenarios, (3) its performance overhead is negligible, and (4) end-users do not find it challenging to adopt.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985672,Android;Security;Inter-app vulnerability,Sealing materials;Androids;Humanoid robots;Runtime;Security;Monitoring;Focusing,Android (operating system);program diagnostics;security of data,SEALANT technique;inter-app security holes;Android communication model;static analysis;app code;vulnerable communication channels;inter-app communication runtime monitoring,30
29,Mobile Application Security,"An Efficient, Robust, and Scalable Approach for Analyzing Interacting Android Apps",Y. Tsutano; S. Bachala; W. Srisa-An; G. Rothermel; J. Dinh,"Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE, USA; Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE, USA; Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE, USA; Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE, USA; Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, NE, USA",2017,"When multiple apps on an Android platform interact, faults and security vulnerabilities can occur. Software engineers need to be able to analyze interacting apps to detect such problems. Current approaches for performing such analyses, however, do not scale to the numbers of apps that may need to be considered, and thus, are impractical for application to real-world scenarios. In this paper, we introduce JITANA, a program analysis framework designed to analyze multiple Android apps simultaneously. By using a classloader-based approach instead of a compiler-based approach such as SOOT, JITANA is able to simultaneously analyze large numbers of interacting apps, perform on-demand analysis of large libraries, and effectively analyze dynamically generated code. Empirical studies of JITANA show that it is substantially more efficient than a state-of-the-art approach, and that it can effectively and efficiently analyze complex apps including Facebook, Pokemon Go, and Pandora that the state-of-the-art approach cannot handle.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985673,Android;program analysis;inter-app communication,Androids;Humanoid robots;Tools;Security;Java;Libraries;Robustness,Android (operating system);mobile computing;program compilers,Android Apps;JITANA;program analysis;classloader-based approach;SOOT;Facebook;Pokemon Go;Pandora,15
30,Mobile Application Development,LibD: Scalable and Precise Third-Party Library Detection in Android Markets,M. Li; W. Wang; P. Wang; S. Wang; D. Wu; J. Liu; R. Xue; W. Huo,"School of CyberSpace Security, University of Chinese Academy of Sciences, China; Key Laboratory of Network Assessment Technology, Chinese Academy of Sciences, China; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA; College of Information Sciences and Technology, The Pennsylvania State University, University Park, PA, USA; School of CyberSpace Security, University of Chinese Academy of Sciences, China; School of CyberSpace Security, University of Chinese Academy of Sciences, China; School of CyberSpace Security, University of Chinese Academy of Sciences, China",2017,"With the thriving of the mobile app markets, third-party libraries are pervasively integrated in the Android applications. Third-party libraries provide functionality such as advertisements, location services, and social networking services, making multi-functional app development much more productive. However, the spread of vulnerable or harmful third-party libraries may also hurt the entire mobile ecosystem, leading to various security problems. The Android platform suffers severely from such problems due to the way its ecosystem is constructed and maintained. Therefore, third-party Android library identification has emerged as an important problem which is the basis of many security applications such as repackaging detection and malware analysis. According to our investigation, existing work on Android library detection still requires improvement in many aspects, including accuracy and obfuscation resilience. In response to these limitations, we propose a novel approach to identifying third-party Android libraries. Our method utilizes the internal code dependencies of an Android app to detect and classify library candidates. Different from most previous methods which classify detected library candidates based on similarity comparison, our method is based on feature hashing and can better handle code whose package and method names are obfuscated. Based on this approach, we have developed a prototypical tool called LibD and evaluated it with an update-to-date and large-scale dataset. Our experimental results on 1,427,395 apps show that compared to existing tools, LibD can better handle multi-package third-party libraries in the presence of name-based obfuscation, leading to significantly improved precision without the loss of scalability.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985674,Android;third-party library;software mining,Libraries;Androids;Humanoid robots;Tools;Mobile communication;Security;Java,Android (operating system);mobile computing;pattern classification;security of data;software libraries;software tools,LibD tool;precise third-party library detection;Android markets;mobile app markets;security problems;mobile ecosystem;third-party Android library identification;malware analysis;repackaging detection;internal code dependencies;library candidate classification;feature hashing;multipackage third-party libraries;name-based obfuscation,100
31,Mobile Application Development,Analysis and Testing of Notifications in Android Wear Applications,H. Zhang; A. Rountev,"Ohio State University, Columbus, OH, USA; Ohio State University, Columbus, OH, USA",2017,"Android Wear (AW) is Google's platform for developing applications for wearable devices. Our goal is to make a first step toward a foundation for analysis and testing of AW apps. We focus on a core feature of such apps: notifications issued by a handheld device (e.g., a smartphone) and displayed on a wearable device (e.g., a smartwatch). We first define a formal semantics of AW notifications in order to capture the core features and behavior of the notification mechanism. Next, we describe a constraint-based static analysis to build a model of this run-time behavior. We then use this model to develop a novel testing tool for AW apps. The tool contains a testing framework together with components to support AW-specific coverage criteria and to automate the generation of GUI events on the wearable. These contributions advance the state of the art in the increasingly important area of software for wearable devices.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985675,,Semantics;Testing;Androids;Humanoid robots;Handheld computers;Software;Smart phones,Android (operating system);graphical user interfaces;program diagnostics;program testing;software tools;wearable computers,notification testing;notification analysis;Android wear applications;Google platform;wearable devices;AW apps testing;AW apps analysis;handheld device;constraint-based static analysis;AW-specific coverage criteria;GUI event generation,17
32,Mobile Application Development,Adaptive Unpacking of Android Apps,L. Xue; X. Luo; L. Yu; S. Wang; D. Wu,"Department of Computing, The Hong Kong Polytechnic University; Department of Computing, The Hong Kong Polytechnic University; Department of Computing, The Hong Kong Polytechnic University; Department of Computing, The Hong Kong Polytechnic University; College of Information Sciences and Technology, The Pennsylvania State University",2017,"More and more app developers use the packing services (or packers) to prevent attackers from reverse engineering and modifying the executable (or Dex files) of their apps. At the same time, malware authors also use the packers to hide the malicious component and evade the signature-based detection. Although there are a few recent studies on unpacking Android apps, it has been shown that the evolving packers can easily circumvent them because they are not adaptive to the changes of packers. In this paper, we propose a novel adaptive approach and develop a new system, named PackerGrind, to unpack Android apps. We also evaluate PackerGrind with real packed apps, and the results show that PackerGrind can successfully reveal the packers' protection mechanisms and recover the Dex files with low overhead, showing that our approach can effectively handle the evolution of packers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985676,Dynamic Analysis;App Unpacking,Androids;Humanoid robots;Monitoring;Subspace constraints;Data collection;Runtime;Loading,Android (operating system);invasive software;reverse engineering,adaptive unpacking;Android apps;signature-based detection;reverse engineering;PackerGrind;malware authors,52
33,Debugging,Performance Diagnosis for Inefficient Loops,L. Song; S. Lu,"Fireeye, Inc.; University of Chicago",2017,"Writing efficient software is difficult. Design and implementation defects can cause severe performance degradation. Unfortunately, existing performance diagnosis techniques like profilers are still preliminary. They can locate code regions that consume resources, but not the ones that waste resources. In this paper, we first design a root-cause and fix-strategy taxonomy for inefficient loops, one of the most common performance problems in the field. We then design a static-dynamic hybrid analysis tool, LDoctor, to provide accurate performance diagnosis for loops. We further use sampling techniques to lower the run-time overhead without degrading the accuracy or latency of LDoctor diagnosis. Evaluation using real-world performance problems shows that LDoctor can provide better coverage and accuracy than existing techniques, with low overhead.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985677,performance diagnosis;debugging;loop inefficiency,Tools;Computer bugs;Taxonomy;Software;Redundancy;Anodes;Debugging,program diagnostics;software engineering,software performance degradation;root-cause and fix-strategy taxonomy;LDoctor diagnosis;static-dynamic hybrid analysis tool,29
34,Debugging,How Do Developers Fix Cross-Project Correlated Bugs? A Case Study on the GitHub Scientific Python Ecosystem,W. Ma; L. Chen; X. Zhang; Y. Zhou; B. Xu,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; Department of Computer Science, Purdue University, USA; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",2017,"GitHub, a popular social-software-development platform, has fostered a variety of software ecosystems where projects depend on one another and practitioners interact with each other. Projects within an ecosystem often have complex inter-dependencies that impose new challenges in bug reporting and fixing. In this paper, we conduct an empirical study on cross-project correlated bugs, i.e., causally related bugs reported to different projects, focusing on two aspects: 1) how developers track the root causes across projects, and 2) how the downstream developers coordinate to deal with upstream bugs. Through manual inspection of bug reports collected from the scientific Python ecosystem and an online survey with developers, this study reveals the common practices of developers and the various factors in fixing cross-project bugs. These findings provide implications for future software bug analysis in the scope of ecosystem, as well as shed light on the requirements of issue trackers for such bugs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985678,GitHub ecosystems;cross-project correlated bugs;root causes tracking;coordinate,Computer bugs;Ecosystems;Software;Collaboration;Maintenance engineering;Tools;Software engineering,configuration management;object-oriented languages;program debugging;project management;software development management,cross-project correlated bugs;GitHub scientific Python ecosystem;upstream bugs;bug reports,38
35,Debugging,Feedback-Based Debugging,Y. Lin; J. Sun; Y. Xue; Y. Liu; J. Dong,"School of Computing, National University of Singapore, Singapore; Singapore University of Technology and Design, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore; School of Computer Engineering, Nanyang Technological University, Singapore; School of Computing, National University of Singapore, Singapore",2017,"Software debugging has long been regarded as a time and effort consuming task. In the process of debugging, developers usually need to manually inspect many program steps to see whether they deviate from their intended behaviors. Given that intended behaviors usually exist nowhere but in human mind, the automation of debugging turns out to be extremely hard, if not impossible. In this work, we propose a feedback-based debugging approach, which (1) builds on light-weight human feedbacks on a buggy program and (2) regards the feedbacks as partial program specification to infer suspicious steps of the buggy execution. Given a buggy program, we record its execution trace and allow developers to provide light-weight feedback on trace steps. Based on the feedbacks, we recommend suspicious steps on the trace. Moreover, our approach can further learn and approximate bug-free paths, which helps reduce required feedbacks to expedite the debugging process. We conduct an experiment to evaluate our approach with simulated feedbacks on 3409 mutated bugs across 3 open source projects. The results show that our feedback-based approach can detect 92.8% of the bugs and 65% of the detected bugs require less than 20 feedbacks. In addition, we implement our proof-of-concept tool, Microbat, and conduct a user study involving 16 participants on 3 debugging tasks. The results show that, compared to the participants using the baseline tool, Whyline, the ones using Microbat can spend on average 55.8% less time to locate the bugs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985679,debugging;feedback;slicing;path pattern;approximation,Computer bugs;Debugging;Tools;Reactive power;Inspection;Software debugging;Software engineering,formal specification;program debugging,feedback-based debugging;human feedbacks;buggy program;partial program specification;buggy execution;execution trace;light-weight feedback;Microbat;Whyline,29
36,Program Synthesis and Repair,Learning Syntactic Program Transformations from Examples,R. Rolim; G. Soares; L. D'Antoni; O. Polozov; S. Gulwani; R. Gheyi; R. Suzuki; B. Hartmann,"UFCG, Brazil; UFCG, Brazil; University of Wisconsin-Madison, USA; University of Washington, USA; Microsoft, USA; UFCG, Brazil; University of Colorado Boulder, USA; UC Berkeley, USA",2017,"Automatic program transformation tools can be valuable for programmers to help them with refactoring tasks, and for Computer Science students in the form of tutoring systems that suggest repairs to programming assignments. However, manually creating catalogs of transformations is complex and time-consuming. In this paper, we present REFAZER, a technique for automatically learning program transformations. REFAZER builds on the observation that code edits performed by developers can be used as input-output examples for learning program transformations. Example edits may share the same structure but involve different variables and subexpressions, which must be generalized in a transformation at the right level of abstraction. To learn transformations, REFAZER leverages state-of-the-art programming-by-example methodology using the following key components: (a) a novel domain-specific language (DSL) for describing program transformations, (b) domain-specific deductive algorithms for efficiently synthesizing transformations in the DSL, and (c) functions for ranking the synthesized transformations. We instantiate and evaluate REFAZER in two domains. First, given examples of code edits used by students to fix incorrect programming assignment submissions, we learn program transformations that can fix other students' submissions with similar faults. In our evaluation conducted on 4 programming tasks performed by 720 students, our technique helped to fix incorrect submissions for 87% of the students. In the second domain, we use repetitive code edits applied by developers to the same project to synthesize a program transformation that applies these edits to other locations in the code. In our evaluation conducted on 56 scenarios of repetitive edits taken from three large C# open-source projects, REFAZER learns the intended program transformation in 84% of the cases using only 2.9 examples on average.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985680,Program transformation;program synthesis;tutoring systems;refactoring,DSL;Programming profession;Tools;C# languages;Pattern matching;Open source software,automatic programming;program processors,syntactic program transformations learning;REFAZER;programming-by-example methodology;domain-specific language;domain-specific deductive algorithms;DSL;code edits;C# open-source projects,129
37,Program Synthesis and Repair,Precise Condition Synthesis for Program Repair,Y. Xiong; J. Wang; R. Yan; J. Zhang; S. Han; G. Huang; L. Zhang,"EECS, Peking University, Beijing, China; EECS, Peking University, Beijing, China; SISE, University of Electronic Science and Technology of China, Chengdu, China; EECS, Peking University, Beijing, China; Microsoft Research, Beijing, China; EECS, Peking University, Beijing, China; EECS, Peking University, Beijing, China",2017,"Due to the difficulty of repairing defect, many research efforts have been devoted into automatic defect repair. Given a buggy program that fails some test cases, a typical automatic repair technique tries to modify the program to make all tests pass. However, since the test suites in real world projects are usually insufficient, aiming at passing the test suites often leads to incorrect patches. This problem is known as weak test suites or overfitting. In this paper we aim to produce precise patches, that is, any patch we produce has a relatively high probability to be correct. More concretely, we focus on condition synthesis, which was shown to be able to repair more than half of the defects in existing approaches. Our key insight is threefold. First, it is important to know what variables in a local context should be used in an ""if"" condition, and we propose a sorting method based on the dependency relations between variables. Second, we observe that the API document can be used to guide the repair process, and propose document analysis technique to further filter the variables. Third, it is important to know what predicates should be performed on the set of variables, and we propose to mine a set of frequently used predicates in similar contexts from existing projects. Based on the insight, we develop a novel program repair system, ACS, that could generate precise conditions at faulty locations. Furthermore, given the generated conditions are very precise, we can perform a repair operation that is previously deemed to be too overfitting: directly returning the test oracle to repair the defect. Using our approach, we successfully repaired 18 defects on four projects of Defects4J, which is the largest number of fully automatically repaired defects reported on the dataset so far. More importantly, the precision of our approach in the evaluation is 78.3%, which is significantly higher than previous approaches, which are usually less than 40%.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985681,,Maintenance engineering;Text analysis;Benchmark testing;Java;Input variables;Software engineering;Software,application program interfaces;document handling;fault location;program testing;software maintenance;sorting,precise condition synthesis;program repair system;automatic defect repair;weak test suites;sorting method;dependency relations;document analysis technique;API document;ACS;faulty locations;repair operation;Defects4J,166
38,Program Synthesis and Repair,Heuristically Matching Solution Spaces of Arithmetic Formulas to Efficiently Reuse Solutions,A. Aquino; G. Denaro; M. PezzÃ¨,"UniversitÃ della Svizzera italiana (USI), Lugano, Switzerland; University of Milano-Bicocca, Milano, Italy; UniversitÃ della Svizzera italiana (USI), Lugano, Switzerland",2017,"Many symbolic program analysis techniques rely on SMT solvers to verify properties of programs. Despite the remarkable progress made in the development of such tools, SMT solvers still represent a main bottleneck to the scalability of these techniques. Recent approaches tackle this bottleneck by reusing solutions of formulas that recur during program analysis, thus reducing the number of queries to SMT solvers. Current approaches only reuse solutions across formulas that are equivalent to, contained in or implied by other formulas, as identified through a set of predefined rules, and cannot reuse solutions across formulas that differ in their structure, even if they share some potentially reusable solutions. In this paper, we propose a novel approach that can reuse solutions across formulas that share at least one solution, regardless of their structural resemblance. Our approach exploits a novel heuristic to efficiently identify solutions computed for previously solved formulas and most likely shared by new formulas. The results of an empirical evaluation of our approach on two different logics show that our approach can identify on average more reuse opportunities and is markedly faster than competing approaches.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985682,SMT-based program analysis;symbolic execution;SMT solvers;solution reuse,Scalability;Terminology;Heuristic algorithms;Software engineering;Tools;Operating systems;Mission critical systems,computability;program diagnostics,heuristically matching solution spaces;arithmetic formulas;symbolic program analysis techniques;SMT solvers;structural resemblance,8
39,Mining Software Repositories,Exploring API Embedding for API Usages and Applications,T. D. Nguyen; A. T. Nguyen; H. D. Phan; T. N. Nguyen,"Electrical and Computer Engineering Department, Iowa State University; Electrical and Computer Engineering Department, Iowa State University; Electrical and Computer Engineering Department, Iowa State University; Computer Science Department, University of Texas, Dallas",2017,"Word2Vec is a class of neural network models that as being trainedfrom a large corpus of texts, they can produce for each unique word acorresponding vector in a continuous space in which linguisticcontexts of words can be observed. In this work, we study thecharacteristics of Word2Vec vectors, called API2VEC or API embeddings, for the API elements within the API sequences in source code. Ourempirical study shows that the close proximity of the API2VEC vectorsfor API elements reflects the similar usage contexts containing thesurrounding APIs of those API elements. Moreover, API2VEC can captureseveral similar semantic relations between API elements in API usagesvia vector offsets. We demonstrate the usefulness of API2VEC vectorsfor API elements in three applications. First, we build a tool thatmines the pairs of API elements that share the same usage relationsamong them. The other applications are in the code migrationdomain. We develop API2API, a tool to automatically learn the APImappings between Java and C# using a characteristic of the API2VECvectors for API elements in the two languages: semantic relationsamong API elements in their usages are observed in the two vectorspaces for the two languages as similar geometric arrangements amongtheir API2VEC vectors. Our empirical evaluation shows that API2APIrelatively improves 22.6% and 40.1% top-1 and top-5 accuracy over astate-of-the-art mining approach for API mappings. Finally, as anotherapplication in code migration, we are able to migrate equivalent APIusages from Java to C# with up to 90.6% recall and 87.2% precision.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985683,Word2Vec;API embedding;API usages;migration,Java;C# languages;Tools;Semantics;Neural networks;Syntactics,application program interfaces;data mining;geometry;Java;neural nets;source code (software);text analysis;vectors,API embedding;API usages;API applications;Word2Vec vectors;neural network models;text corpus;linguistic words contexts;API2VEC;API elements;API sequences;source code;semantic relations;vector offsets;code migration domain;API mappings;Java;C#;geometric arrangements;code migration,89
40,Mining Software Repositories,Unsupervised Software-Specific Morphological Forms Inference from Informal Discussions,C. Chen; Z. Xing; X. Wang,"School of Computer Science and Engineering, Nanyang Technological University, Singapore; Research School of Computer Science, Australian National University, Australia; School of Computer Science and Engineering, Nanyang Technological University, Singapore",2017,"Informal discussions on social platforms (e.g., Stack Overflow) accumulates a large body of programming knowledge in natural language text. Natural language process (NLP) techniques can be exploited to harvest this knowledge base for software engineering tasks. To make an effective use of NLP techniques, consistent vocabulary is essential. Unfortunately, the same concepts are often intentionally or accidentally mentioned in many different morphological forms in informal discussions, such as abbreviations, synonyms and misspellings. Existing techniques to deal with such morphological forms are either designed for general English or predominantly rely on domain-specific lexical rules. A thesaurus of software-specific terms and commonly-used morphological forms is desirable for normalizing software engineering text, but very difficult to build manually. In this work, we propose an automatic approach to build such a thesaurus. Our approach identifies software-specific terms by contrasting software-specific and general corpuses, and infers morphological forms of software-specific terms by combining distributed word semantics, domain-specific lexical rules and transformations, and graph analysis of morphological relations. We evaluate the coverage and accuracy of the resulting thesaurus against community-curated lists of software-specific terms, abbreviations and synonyms. We also manually examine the correctness of the identified abbreviations and synonyms in our thesaurus. We demonstrate the usefulness of our thesaurus in a case study of normalizing questions from Stack Overflow and CodeProject.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985684,abbreviation;synonym;morphological form;word embedding;Stack Overflow,Software engineering;Encyclopedias;Electronic publishing;Internet;Thesauri;Dictionaries,natural language processing;software engineering;text analysis;thesauri,unsupervised software-specific morphological forms inference;informal discussions;natural language process;software engineering tasks;NLP techniques;software-specific terms;thesaurus;domain-specific lexical rules,44
41,Program Analysis 1,SPAIN: Security Patch Analysis for Binaries towards Understanding the Pain and Pills,Z. Xu; B. Chen; M. Chandramohan; Y. Liu; F. Song,"School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Information Science and Technology, ShanghaiTech University, China",2017,"Software vulnerability is one of the major threats to software security. Once discovered, vulnerabilities are often fixed by applying security patches. In that sense, security patches carry valuable information about vulnerabilities, which could be used to discover, understand and fix (similar) vulnerabilities. However, most existing patch analysis approaches work at the source code level, while binary-level patch analysis often heavily relies on a lot of human efforts and expertise. Even worse, some vulnerabilities may be secretly patched without applying CVE numbers, or only the patched binary programs are available while the patches are not publicly released. These practices greatly hinder patch analysis and vulnerability analysis. In this paper, we propose a scalable binary-level patch analysis framework, named SPAIN, which can automatically identify security patches and summarize patch patterns and their corresponding vulnerability patterns. Specifically, given the original and patched versions of a binary program, we locate the patched functions and identify the changed traces (i.e., a sequence of basic blocks) that may contain security or non-security patches. Then we identify security patches through a semantic analysis of these traces and summarize the patterns through a taint analysis on the patched functions. The summarized patterns can be used to search similar patches or vulnerabilities in binary programs. Our experimental results on several real-world projects have shown that: i) SPAIN identified security patches with high accuracy and high scalability, ii) SPAIN summarized 5 patch patterns and their corresponding vulnerability patterns for 5 vulnerability types, and iii) SPAIN discovered security patches that were not documented, and discovered 3 zero-day vulnerabilities.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985685,,Security;Semantics;Software;Tools;Scalability;Registers;Portable document format,security of data;software engineering,SPAIN;security patch analysis;software vulnerability;software security;scalable binary-level patch analysis framework;binary programs,68
42,Program Analysis 1,Travioli: A Dynamic Analysis for Detecting Data-Structure Traversals,R. Padhye; K. Sen,"EECS Department, University of California, Berkeley, USA; EECS Department, University of California, Berkeley, USA",2017,"Traversal is one of the most fundamental operations on data structures, in which an algorithm systematically visits some or all of the data items of a data structure. We propose a dynamic analysis technique, called Travioli, for detecting data-structure traversals. We introduce the concept of acyclic execution contexts, which enables precise detection of traversals of arrays and linked data structures such as lists and trees in the presence of both loops and recursion. We describe how the information reported by Travioli can be used for visualizing data-structure traversals, manually generating performance regression tests, and for discovering performance bugs caused by redundant traversals. We evaluate Travioli on five real-world JavaScript programs. In our experiments, Travioli produced fewer than 4% false positives. We were able to construct performance tests for 93.75% of the reported true traversals. Travioli also found two asymptotic performance bugs in widely used JavaScript frameworks D3 and express.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985686,,Reactive power;Arrays;Computer bugs;Lenses;Data visualization;Performance analysis,data structures;Java;system monitoring,dynamic analysis;data-structure traversals;acyclic execution contexts;real-world JavaScript programs;Travioli;regression tests;performance bugs,8
43,Program Analysis 1,ProEva: Runtime Proactive Performance Evaluation Based on Continuous-Time Markov Chains,G. Su; T. Chen; Y. Feng; D. S. Rosenblum,"School of Computing and Information Technology, University of Wollongong; Department of Computer Science, Middlesex University London; Faculty of Engineering and Information Technology, University of Technology Sydney; School of Computing, National University of Singapore",2017,"Software systems, especially service-based software systems, need to guarantee runtime performance. If their performance is degraded, some reconfiguration countermeasures should be taken. However, there is usually some latency before the countermeasures take effect. It is thus important not only to monitor the current system status passively but also to predict its future performance proactively. Continuous-time Markov chains (CTMCs) are suitable models to analyze time-bounded performance metrics (e.g., how likely a performance degradation may occur within some future period). One challenge to harness CTMCs is the measurement of model parameters (i.e., transition rates) in CTMCs at runtime. As these parameters may be updated by the system or environment frequently, it is difficult for the model builder to provide precise parameter values. In this paper, we present a framework called ProEva, which extends the conventional technique of time-bounded CTMC model checking by admitting imprecise, interval-valued estimates for transition rates. The core method of ProEva computes asymptotic expressions and bounds for the imprecise model checking output. We also present an evaluation of accuracy and computational overhead for ProEva.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985687,continuous-time Markov chain;imprecise parameters;performance;Quality-of-Service,Software engineering,formal verification;Markov processes;software performance evaluation,ProEva;runtime proactive performance evaluation;continuous-time Markov chains;service-based software systems;time-bounded CTMC model checking,9
44,Program Analysis 2,Glacier: Transitive Class Immutability for Java,M. Coblenz; W. Nelson; J. Aldrich; B. Myers; J. Sunshine,"School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Hampton University, Hampton, VA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA",2017,"Though immutability has been long-proposed as a way to prevent bugs in software, little is known about how to make immutability support in programming languages effective for software engineers. We designed a new formalism that extends Java to support transitive class immutability, the form of immutability for which there is the strongest empirical support, and implemented that formalism in a tool called Glacier. We applied Glacier successfully to two real-world systems. We also compared Glacier to Java's final in a user study of twenty participants. We found that even after being given instructions on how to express immutability with final, participants who used final were unable to express immutability correctly, whereas almost all participants who used Glacier succeeded. We also asked participants to make specific changes to immutable classes and found that participants who used final all incorrectly mutated immutable state, whereas almost all of the participants who used Glacier succeeded. Glacier represents a promising approach to enforcing immutability in Java and provides a model for enforcement in other languages.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985688,immutability;programming language usability;empirical studies of programmers,Java;Computer bugs;Runtime;Usability;Tools,Java;program debugging;software tools,Glacier tool;Java;transitive class immutability;software bugs prevention;programming languages,20
45,Program Analysis 2,Challenges for Static Analysis of Java Reflection - Literature Review and Empirical Study,D. Landman; A. Serebrenik; J. J. Vinju,"Centrum Wiskunde & Informatica, Amsterdam, The Netherlands; Eindhoven University of Technology, Eindhoven, The Netherlands; Eindhoven University of Technology, Eindhoven, The Netherlands",2017,"The behavior of software that uses the Java Reflection API is fundamentally hard to predict by analyzing code. Only recent static analysis approaches can resolve reflection under unsound yet pragmatic assumptions. We survey what approaches exist and what their limitations are. We then analyze how real-world Java code uses the Reflection API, and how many Java projects contain code challenging state-of-the-art static analysis. Using a systematic literature review we collected and categorized all known methods of statically approximating reflective Java code. Next to this we constructed a representative corpus of Java systems and collected descriptive statistics of the usage of the Reflection API. We then applied an analysis on the abstract syntax trees of all source code to count code idioms which go beyond the limitation boundaries of static analysis approaches. The resulting data answers the research questions. The corpus, the tool and the results are openly available. We conclude that the need for unsound assumptions to resolve reflection is widely supported. In our corpus, reflection can not be ignored for 78% of the projects. Common challenges for analysis tools such as non-exceptional exceptions, programmatic filtering meta objects, semantics of collections, and dynamic proxies, widely occur in the corpus. For Java software engineers prioritizing on robustness, we list tactics to obtain more easy to analyze reflection code, and for static analysis tool builders we provide a list of opportunities to have significant impact on real Java code.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985689,Java;Reflection;Static Analysis;Systematic Literature Review;Empirical Study,Java;Tools;Bibliographies;Grammar;Software;Systematics;Semantics,application program interfaces;computational linguistics;Java;program diagnostics;public domain software;software tools;source code (software);trees (mathematics),static analysis tool;Java Reflection API;literature review;software behavior;real-world Java code analysis;Java projects;reflective Java code;Java systems;collected descriptive statistics;abstract syntax trees;source code;code idioms;nonexceptional exceptions;programmatic filtering meta objects;dynamic proxies;collections semantics;reflection code analysis,58
46,Program Analysis 2,Machine-Learning-Guided Selectively Unsound Static Analysis,K. Heo; H. Oh; K. Yi,"Seoul National University, Seoul, Korea; Korea University, Seoul, Korea; Seoul National University, Seoul, Korea",2017,"We present a machine-learning-based technique for selectively applying unsoundness in static analysis. Existing bug-finding static analyzers are unsound in order to be precise and scalable in practice. However, they are uniformly unsound and hence at the risk of missing a large amount of real bugs. By being sound, we can improve the detectability of the analyzer but it often suffers from a large number of false alarms. Our approach aims to strike a balance between these two approaches by selectively allowing unsoundness only when it is likely to reduce false alarms, while retaining true alarms. We use an anomaly-detection technique to learn such harmless unsoundness. We implemented our technique in two static analyzers for full C. One is for a taint analysis for detecting format-string vulnerabilities, and the other is for an interval analysis for buffer-overflow detection. The experimental results show that our approach significantly improves the recall of the original unsound analysis without sacrificing the precision.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985690,Static Analysis;Machine Learning;Bug-finding,Computer bugs;Libraries;Benchmark testing;Tools;Software engineering;Scalability;Support vector machines,learning (artificial intelligence);program diagnostics,machine-learning-guided selectively unsound static Analysis;bug-finding static analyzers;anomaly-detection technique;taint analysis;interval analysis;buffer-overflow detection,29
47,"Security, Safety, and Privacy",How Good Is a Security Policy against Real Breaches? A HIPAA Case Study,Ã–. Kafali; J. Jones; M. Petruso; L. Williams; M. P. Singh,"Department of Computer Science, North Carolina State University, Raleigh, NC, USA; College of Arts and Sciences, Elon University, Elon, NC, USA; Department of Computer Science, Appalachian State University, Boone, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA",2017,"Policy design is an important part of software development. As security breaches increase in variety, designing a security policy that addresses all potential breaches becomes a nontrivial task. A complete security policy would specify rules to prevent breaches. Systematically determining which, if any, policy clause has been violated by a reported breach is a means for identifying gaps in a policy. Our research goal is to help analysts measure the gaps between security policies and reported breaches by developing a systematic process based on semantic reasoning. We propose SEMAVER, a framework for determining coverage of breaches by policies via comparison of individual policy clauses and breach descriptions. We represent a security policy as a set of norms. Norms (commitments, authorizations, and prohibitions) describe expected behaviors of users, and formalize who is accountable to whom and for what. A breach corresponds to a norm violation. We develop a semantic similarity metric for pairwise comparison between the norm that represents a policy clause and the norm that has been violated by a reported breach. We use the US Health Insurance Portability and Accountability Act (HIPAA) as a case study. Our investigation of a subset of the breaches reported by the US Department of Health and Human Services (HHS) reveals the gaps between HIPAA and reported breaches, leading to a coverage of 65%. Additionally, our classification of the 1,577 HHS breaches shows that 44% of the breaches are accidental misuses and 56% are malicious misuses. We find that HIPAA's gaps regarding accidental misuses are significantly larger than its gaps regarding malicious misuses.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985691,Security and privacy breaches;social norms;breach ontology;semantic similarity,Ontologies;Security;Semantics;Medical services;Cognition;Measurement;Taxonomy,inference mechanisms;security of data;software engineering,HIPAA;security policies;security breaches;semantic reasoning;SEMAVER;norm violation;semantic similarity metric;US Health Insurance Portability and Accountability Act;US Department of Health and Human Services;HHS;accidental misuses;malicious misuses;software development,16
48,"Security, Safety, and Privacy",Adaptive Coverage and Operational Profile-Based Testing for Reliability Improvement,A. Bertolino; B. Miranda; R. Pietrantuono; S. Russo,"ISTI - CNR, Pisa, Italy; ISTI - CNR, Pisa, Italy; UniversitÃ degli Studi di Napoli Federico II, Napoli, Italy; UniversitÃ degli Studi di Napoli Federico II, Napoli, Italy",2017,"We introduce covrel, an adaptive software testing approach based on the combined use of operational profile and coverage spectrum, with the ultimate goal of improving the delivered reliability of the program under test. Operational profile-based testing is a black-box technique that selects test cases having the largest impact on failure probability in operation, as such, it is considered well suited when reliability is a major concern. Program spectrum is a characterization of a program's behavior in terms of the code entities (e.g., branches, statements, functions) that are covered as the program executes. The driving idea of covrel is to complement operational profile information with white-box coverage measures based on count spectra, so as to dynamically select the most effective test cases for reliability improvement. In particular, we bias operational profile-based test selection towards those entities covered less frequently. We assess the approach by experiments with 18 versions from 4 subjects commonly used in software testing research, comparing results with traditional operational and coverage testing. Results show that exploiting operational and coverage data in a combined adaptive way actually pays in terms of reliability improvement, with covrel overcoming conventional operational testing in more than 80% of the cases.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985692,Testing;Reliability;Operational profile;Program count spectrum;Operational coverage;Test case selection,Software reliability;Resource management;Software testing;Reliability engineering;Optimization,probability;program testing;software reliability,adaptive coverage testing;operational profile-based testing;reliability improvement;adaptive software testing approach;operational profile;coverage spectrum;program-under-test;black-box technique;failure probability;program spectrum;program behavior;code entities;white-box coverage measures;count spectra;operational profile-based test selection;operational data;coverage data;covrel,14
49,Development Tools and Frameworks,RADAR: A Lightweight Tool for Requirements and Architecture Decision Analysis,S. A. Busari; E. Letier,"Department of Computer Science, University College London, London, United Kingdom; Department of Computer Science, University College London, London, United Kingdom",2017,"Uncertainty and conflicting stakeholders' objectives make many requirements and architecture decisions particularly hard. Quantitative probabilistic models allow software architects to analyse such decisions using stochastic simulation and multi-objective optimisation, but the difficulty of elaborating the models is an obstacle to the wider adoption of such techniques. To reduce this obstacle, this paper presents a novel modelling language and analysis tool, called RADAR, intended to facilitate requirements and architecture decision analysis. The language has relations to quantitative AND/OR goal models used in requirements engineering and to feature models used in software product lines. However, it simplifies such models to a minimum set of language constructs essential for decision analysis. The paper presents RADAR's modelling language, automated support for decision analysis, and evaluates its application to four real-world examples.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985693,Decision Analysis;Requirements Engineering;Software Architecture;Goal Modelling;Monte-Carlo Simulation;Multi-Objective Optimisation;Search-Based Software Engineering;Expected Value of Information,Radar;Computer architecture;Analytical models;Mathematical model;Optimization;Decision analysis;Software,software architecture;software product lines;systems analysis,modelling language;RADAR;architecture decision analysis;requirements decision analysis;quantitative AND/OR goal models;requirements engineering;software product lines;Requirements and Architecture Decision Analyser,9
50,Development Tools and Frameworks,PEoPL: Projectional Editing of Product Lines,B. Behringer; J. Palz; T. Berger,"HTW saar, University of Luxembourg, Luxembourg, Germany; HTW saar, Germany; Chalmers, University of Gothenburg, Sweden",2017,"The features of a software product line - a portfolio of system variants - can be realized using various implementation techniques (a. k. a., variability mechanisms). Each technique represents the software artifacts of features differently, typically classified into annotative (e.g., C preprocessor) and modular representations (e.g., feature modules), each with distinct advantages and disadvantages. Annotative representations are easy to realize, but annotations clutter source code and hinder program comprehension. Modular representations support comprehension, but are difficult to realize. Most importantly, to engineer feature artifacts, developers need to choose one representation and adhere to it for evolving and maintaining the same artifacts. We present PEoPL, an approach to combine the advantages of annotative and modular representations. When engineering a feature artifact, developers can choose the most-suited representation and even use different representations in parallel. PEoPL relies on separating a product line into an internal and external representation, the latter by providing editable projections used by the developers. We contribute a programming-language-independent internal representation of variability, five editable projections reflecting different variability representations, a supporting IDE, and a tailoring of PEoPL to Java. We evaluate PEoPL's expressiveness, scalability, and flexibility in eight Java-based product lines, finding that all can be realized, that projections are feasible, and that variant computation is fast (<;45ms on average for our largest subject Berkeley DB).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985694,,Latches;Java;Switches;Visualization;Bars;Software;Clutter,Java;software product lines,PEoPL;projectional editing of product lines;software product line;software artifacts;modular representations;programming-language-independent internal representation;Java-based product lines,19
51,Development Tools and Frameworks,Do Developers Read Compiler Error Messages?,T. Barik; J. Smith; K. Lubick; E. Holmes; J. Feng; E. Murphy-Hill; C. Parnin,"Department of Computer Science, North Carolina State University, Raleigh, North Carolina, USA; Department of Computer Science, North Carolina State University, Raleigh, North Carolina, USA; Department of Computer Science, North Carolina State University, Raleigh, North Carolina, USA; Department of Psychology, Washington and Lee University, Lexington, Virginia, USA; Department of Psychology, North Carolina State University, Raleigh, North Carolina, USA; Department of Computer Science, North Carolina State University, Raleigh, North Carolina, USA; Department of Computer Science, North Carolina State University, Raleigh, North Carolina, USA",2017,"In integrated development environments, developers receive compiler error messages through a variety of textual and visual mechanisms, such as popups and wavy red underlines. Although error messages are the primary means of communicating defects to developers, researchers have a limited understanding on how developers actually use these messages to resolve defects. To understand how developers use error messages, we conducted an eye tracking study with 56 participants from undergraduate and graduate software engineering courses at our university. The participants attempted to resolve common, yet problematic defects in a Java code base within the Eclipse development environment. We found that: 1) participants read error messages and the difficulty of reading these messages is comparable to the difficulty of reading source code, 2) difficulty reading error messages significantly predicts participants' task performance, and 3) participants allocate a substantial portion of their total task to reading error messages (13%-25%). The results of our study offer empirical justification for the need to improve compiler error messages for developers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985695,compiler errors;eye tracking;integrated development environments;programmer comprehension;reading;visual attention,Gaze tracking;Libraries;Java;Software engineering;Visualization;Google;Navigation,Java;program compilers,compiler error messages;software engineering courses;Java code base;Eclipse development environment;source code,48
52,Development Tools and Frameworks,A General Framework for Dynamic Stub Injection,M. Christakis; P. Emmisberger; P. Godefroid; P. MÃ¼ller,"Microsoft Research, Redmond, WA, USA; Dept. of Computer Science, ETH Zurich, Switzerland; Microsoft Research, Redmond, WA, USA; Dept. of Computer Science, ETH Zurich, Switzerland",2017,"Stub testing is a standard technique to simulate the behavior of dependencies of an application under test such as the file system. Even though existing frameworks automate the actual stub injection, testers typically have to implement manually where and when to inject stubs, in addition to the stub behavior. This paper presents a novel framework that reduces this effort. The framework provides a domain specific language to describe stub injection strategies and stub behaviors via declarative rules, as well as a tool that automatically injects stubs dynamically into binary code according to these rules. Both the domain specific language and the injection are language independent, which enables the reuse of stubs and injection strategies across applications. We implemented this framework for both unmanaged (assembly) and managed (.NET) code and used it to perform fault injection for twelve large applications, which revealed numerous crashes and bugs in error handling code. We also show how to prioritize the analysis of test failures based on a comparison of the effectiveness of stub injection rules across applications.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985696,,Instruments;DSL;Computer bugs;Testing;Runtime;Debugging,binary codes;program diagnostics;program testing;specification languages,stub testing;fault injection;binary code;declarative rules;stub injection strategies;domain specific language,2
53,Testing and Debugging,"An Empirical Study on Mutation, Statement and Branch Coverage Fault Revelation That Avoids the Unreliable Clean Program Assumption",T. T. Chekam; M. Papadakis; Y. Le Traon; M. Harman,"Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; University College London, Facebook, London, UK",2017,"Many studies suggest using coverage concepts, such as branch coverage, as the starting point of testing, while others as the most prominent test quality indicator. Yet the relationship between coverage and fault-revelation remains unknown, yielding uncertainty and controversy. Most previous studies rely on the Clean Program Assumption, that a test suite will obtain similar coverage for both faulty and fixed ('clean') program versions. This assumption may appear intuitive, especially for bugs that denote small semantic deviations. However, we present evidence that the Clean Program Assumption does not always hold, thereby raising a critical threat to the validity of previous results. We then conducted a study using a robust experimental methodology that avoids this threat to validity, from which our primary finding is that strong mutation testing has the highest fault revelation of four widely-used criteria. Our findings also revealed that fault revelation starts to increase significantly only once relatively high levels of coverage are attained.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985697,Mutation testing;test effectiveness;code coverage;real faults;test adequacy,Testing;Java;Correlation;Robustness;Standards;Tools,program testing;software fault tolerance;software quality,branch coverage fault revelation;unreliable clean program assumption avoidance;test quality indicator;semantic deviations;mutation testing,79
54,Testing and Debugging,Evaluating and Improving Fault Localization,S. Pearson; J. Campos; R. Just; G. Fraser; R. Abreu; M. D. Ernst; D. Pang; B. Keller,"U. of Washington, USA; U. of Sheffield, UK; U. of Massachusetts, USA; U. of Sheffield, UK; Palo Alto Research Center, USA; U. of Washington, USA; U. of Washington, USA; U. of Washington, USA",2017,"Most fault localization techniques take as input a faulty program, and produce as output a ranked list of suspicious code locations at which the program may be defective. When researchers propose a new fault localization technique, they typically evaluate it on programs with known faults. The technique is scored based on where in its output list the defective code appears. This enables the comparison of multiple fault localization techniques to determine which one is better. Previous research has evaluated fault localization techniques using artificial faults, generated either by mutation tools or manually. In other words, previous research has determined which fault localization techniques are best at finding artificial faults. However, it is not known which fault localization techniques are best at finding real faults. It is not obvious that the answer is the same, given previous work showing that artificial faults have both similarities to and differences from real faults. We performed a replication study to evaluate 10 claims in the literature that compared fault localization techniques (from the spectrum-based and mutation-based families). We used 2995 artificial faults in 6 real-world programs. Our results support 7 of the previous claims as statistically significant, but only 3 as having non-negligible effect sizes. Then, we evaluated the same 10 claims, using 310 real faults from the 6 programs. Every previous result was refuted or was statistically and practically insignificant. Our experiments show that artificial faults are not useful for predicting which fault localization techniques perform best on real faults. In light of these results, we identified a design space that includes many previously-studied fault localization techniques as well as hundreds of new techniques. We experimentally determined which factors in the design space are most important, using an overall set of 395 real faults. Then, we extended this design space with new techniques. Several of our novel techniques outperform all existing techniques, notably in terms of ranking defective code in the top-5 or top-10 reports.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985698,,Maintenance engineering;Tools;Debugging;Java;Computer bugs;Focusing;Manuals,program testing;source code (software),fault localization techniques;faulty program;suspicious code locations;defective code;replication study;spectrum-based families;mutation-based families;design space,221
55,Testing 1,Syntactic and Semantic Differencing for Combinatorial Models of Test Designs,R. Tzoref-Brill; S. Maoz,"School of Computer Science, IBM Research, Israel; School of Computer Science, Tel Aviv University",2017,"Combinatorial test design (CTD) is an effective test design technique, considered to be a testing best practice. CTD provides automatic test plan generation, but it requires a manual definition of the test space in the form of a combinatorial model. As the system under test evolves, e.g., due to iterative development processes and bug fixing, so does the test space, and thus, in the context of CTD, evolution translates into frequent manual model definition updates. Manually reasoning about the differences between versions of real-world models following such updates is infeasible due to their complexity and size. Moreover, representing the differences is challenging. In this work, we propose a first syntactic and semantic differencing technique for combinatorial models of test designs. We define a concise and canonical representation for differences between two models, and suggest a scalable algorithm for automatically computing and presenting it. We use our differencing technique to analyze the evolution of 42 real-world industrial models, demonstrating its applicability and scalability. Further, a user study with 16 CTD practitioners shows that comprehension of differences between real-world combinatorial model versions is challenging and that our differencing tool significantly improves the performance of less experienced practitioners. The analysis and user study provide evidence for the potential usefulness of our differencing approach. Our work advances the state-of-the-art in CTD with better capabilities for change comprehension and management.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985699,,Computational modeling;Semantics;Tools;Syntactics;Analytical models;Data models;Binary decision diagrams,program testing,combinatorial test design;automatic test plan generation;CTD practitioners,7
56,Testing 1,Balancing Soundness and Efficiency for Practical Testing of Configurable Systems,S. Souto; M. D'Amorim; R. Gheyi,"State University of ParaÃ­ba, ParaÃ­ba, Brazil; Federal University of Pernambuco, Pernambuco, Brazil; Federal University of Campina Grande, ParaÃ­ba, Brazil",2017,"Testing configurable systems is important and challenging due to the enormous space of configurations where errors can hide. Existing approaches to test these systems are often costly or unreliable. This paper proposes S-SPLat, a technique that combines heuristic sampling with symbolic search to obtain both breadth and depth in the exploration of the configuration space. S-SPLat builds on SPLat, our previously developed technique, that explores all reachable configurations from tests. In contrast to its predecessor, S-SPLat sacrifices soundness in favor of efficiency. We evaluated our technique on eight software product lines of various sizes and on a large configurable system â€“ GCC. Considering the results for GCC, S-SPLat was able to reproduce all five bugs that we previously found in a previous study with SPLat but much faster and it was able to find two new bugs in a recent release of GCC. Results suggest that it is preferable to use a combination of simple heuristics to drive the symbolic search as opposed to a single heuristic. S-SPLat and our experimental infrastructure are publicly available.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985700,sampling;testing;configuration,Testing;Computer bugs;Software product lines;Complexity theory;Reliability;Space exploration,program testing;software product lines,configurable system testing;S-SPLat technique;heuristic sampling;symbolic search;configuration space;software product lines;GCC,23
57,Testing 1,Automatic Text Input Generation for Mobile Testing,P. Liu; X. Zhang; M. Pistoia; Y. Zheng; M. Marques; L. Zeng,"IBM T. J. Watson Research Center, Yorktown Heights, New York, USA; Purdue University, West Lafayette, Indiana, USA; IBM T. J. Watson Research Center, Yorktown Heights, New York, USA; IBM T. J. Watson Research Center, Yorktown Heights, New York, USA; IBM T. J. Watson Research Center, Yorktown Heights, New York, USA; Purdue University, West Lafayette, Indiana, USA",2017,"Many designs have been proposed to improve the automated mobile testing. Despite these improvements, providing appropriate text inputs remains a prominent obstacle, which hinders the large-scale adoption of automated testing approaches. The key challenge is how to automatically produce the most relevant text in a use case context. For example, a valid website address should be entered in the address bar of a mobile browser app to continue the testing of the app, a singer's name should be entered in the search bar of a music recommendation app. Without the proper text inputs, the testing would get stuck. We propose a novel deep learning based approach to address the challenge, which reduces the problem to a minimization problem. Another challenge is how to make the approach generally applicable to both the trained apps and the untrained apps. We leverage the Word2Vec model to address the challenge. We have built our approaches as a tool and evaluated it with 50 iOS mobile apps including Firefox and Wikipedia. The results show that our approach significantly outperforms existing automatic text input generation methods.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985701,,Testing;Mobile communication;Neurons;Biological neural networks;Predictive models;Training;Context modeling,learning (artificial intelligence);mobile computing;program testing;text analysis,Wikipedia;Firefox;iOS mobile apps;Word2Vec model;minimization problem;deep learning based approach;music recommendation app;mobile browser app;Web site address;automated mobile testing;automatic text input generation method,30
58,Testing 1,A Test-Suite Diagnosability Metric for Spectrum-Based Fault Localization Approaches,A. Perez; R. Abreu; A. van Deursen,"Palo Alto Research Center, USA; Palo Alto Research Center, USA; Delft University of Technology, The Netherlands",2017,"Current metrics for assessing the adequacy of a test-suite plainly focus on the number of components (be it lines, branches, paths) covered by the suite, but do not explicitly check how the tests actually exercise these components and whether they provide enough information so that spectrum-based fault localization techniques can perform accurate fault isolation. We propose a metric, called DDU, aimed at complementing adequacy measurements by quantifying a test-suite's diagnosability, i.e., the effectiveness of applying spectrum-based fault localization to pinpoint faults in the code in the event of test failures. Our aim is to increase the value generated by creating thorough test-suites, so they are not only regarded as error detection mechanisms but also as effective diagnostic aids that help widely-used fault-localization techniques to accurately pinpoint the location of bugs in the system. Our experiments show that optimizing a test suite with respect to DDU yields a 34% gain in spectrum-based fault localization report accuracy when compared to the standard branch-coverage metric.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985702,Testing;Coverage;Diagnosability,Cognition;Density measurement;Software;Computer bugs;Gain measurement;Measurement uncertainty,error detection;fault diagnosis;program debugging;program testing,test-suite diagnosability metric;spectrum-based fault localization techniques;fault isolation;DDU;error detection mechanisms;bugs;branch-coverage metric;density-diversity-uniqueness,42
59,Testing 2,Automated Transplantation and Differential Testing for Clones,T. Zhang; M. Kim,"University of California, Los Angeles; University of California, Los Angeles",2017,"Code clones are common in software. When applying similar edits to clones, developers often find it difficult to examine the runtime behavior of clones. The problem is exacerbated when some clones are tested, while their counterparts are not. To reuse tests for similar but not identical clones, Grafter transplants one clone to its counterpart by (1) identifying variations in identifier names, types, and method call targets, (2) resolving compilation errors caused by such variations through code transformation, and (3) inserting stub code to transfer input data and intermediate output values for examination. To help developers examine behavioral differences between clones, Grafter supports fine-grained differential testing at both the test outcome level and the intermediate program state level. In our evaluation on three open source projects, Grafter successfully reuses tests in 94% of clone pairs without inducing build errors, demonstrating its automated code transplantation capability. To examine the robustness of G RAFTER, we systematically inject faults using a mutation testing tool, Major, and detect behavioral differences induced by seeded faults. Compared with a static cloning bug finder, Grafter detects 31% more mutants using the test-level comparison and almost 2X more using the state-level comparison. This result indicates that Grafter should effectively complement static cloning bug finders.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985703,Test Reuse;Code Transplantation;Differential Testing;Code Clones,Cloning;Computer bugs;Testing;Runtime;Software;Safety;Java,program diagnostics;program testing;public domain software;software reusability,automated transplantation testing;automated differential testing;code clones;runtime behavior;Grafter;variation identification;identifier names;targets;compilation errors;code transformation;stub code insertion;input data transfer;intermediate output value transfer;test outcome level;intermediate program state level;open source projects;test reuse;clone pairs;mutation testing tool;Major;fault injection;behavioral differences detection;test-level comparison;state-level comparison;static cloning bug finders,25
60,Testing 2,Code Defenders: Crowdsourcing Effective Tests and Subtle Mutants with a Mutation Testing Game,J. M. Rojas; T. D. White; B. S. Clegg; G. Fraser,"Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom; Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom; Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom; Department of Computer Science, The University of Sheffield, Sheffield, United Kingdom",2017,"Writing good software tests is difficult and not every developer's favorite occupation. Mutation testing aims to help by seeding artificial faults (mutants) that good tests should identify, and test generation tools help by providing automatically generated tests. However, mutation tools tend to produce huge numbers of mutants, many of which are trivial, redundant, or semantically equivalent to the original program, automated test generation tools tend to produce tests that achieve good code coverage, but are otherwise weak and have no clear purpose. In this paper, we present an approach based on gamification and crowdsourcing to produce better software tests and mutants: The Code Defenders web-based game lets teams of players compete over a program, where attackers try to create subtle mutants, which the defenders try to counter by writing strong tests. Experiments in controlled and crowdsourced scenarios reveal that writing tests as part of the game is more enjoyable, and that playing Code Defenders results in stronger test suites and mutants than those produced by automated tools.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985704,gamification;crowdsourcing;software testing;mutation testing,Games;Tools;Testing;Crowdsourcing;Writing;Software;Computer bugs,computer games;crowdsourcing;Internet;program testing,Code Defenders;mutation testing game;gamification;crowdsourcing;software tests;Web-based game,28
61,Testing 2,Optimizing Test Placement for Module-Level Regression Testing,A. Shi; S. Thummalapenta; S. K. Lahiri; N. Bjorner; J. Czerwonka,"University of Illinois, Urbana-Champaign, USA; Microsoft Corporation, USA; Microsoft Research, USA; Microsoft Research, USA; Microsoft Corporation, USA",2017,"Modern build systems help increase developer productivity by performing incremental building and testing. These build systems view a software project as a group of interdependent modules and perform regression test selection at the module level. However, many large software projects have imprecise dependency graphs that lead to wasteful test executions. If a test belongs to a module that has more dependencies than the actual dependencies of the test, then it is executed unnecessarily whenever a code change impacts those additional dependencies. In this paper, we formulate the problem of wasteful test executions due to suboptimal placement of tests in modules. We propose a greedy algorithm to reduce the number of test executions by suggesting test movements while considering historical build information and actual dependencies of tests. We have implemented our technique, called TestOptimizer, on top of CloudBuild, the build system developed within Microsoft over the last few years. We have evaluated the technique on five large proprietary projects. Our results show that the suggested test movements can lead to a reduction of 21.66 million test executions (17.09%) across all our subject projects. We received encouraging feedback from the developers of these projects; they accepted and intend to implement â‰ˆ80% of our reported suggestions.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985705,regression test selection;module-level regression testing;build system,Software;Testing;Buildings;Greedy algorithms;Metadata;Productivity;Google,greedy algorithms;program testing;regression analysis,test placement optimization;module-level regression testing;software project;dependency graphs;greedy algorithm;TestOptimizer;CloudBuild,22
62,Testing 2,Learning to Prioritize Test Programs for Compiler Testing,J. Chen; Y. Bai; D. Hao; Y. Xiong; H. Zhang; B. Xie,"EECS, Peking University, Beijing, China; EECS, Peking University, Beijing, China; EECS, Peking University, Beijing, China; EECS, Peking University, Beijing, China; The University of Newcastle, NSW, Australia; EECS, Peking University, Beijing, China",2017,"Compiler testing is a crucial way of guaranteeing the reliability of compilers (and software systems in general). Many techniques have been proposed to facilitate automated compiler testing. These techniques rely on a large number of test programs (which are test inputs of compilers) generated by some test-generation tools (e.g., CSmith). However, these compiler testing techniques have serious efficiency problems as they usually take a long period of time to find compiler bugs. To accelerate compiler testing, it is desirable to prioritize the generated test programs so that the test programs that are more likely to trigger compiler bugs are executed earlier. In this paper, we propose the idea of learning to test, which learns the characteristics of bug-revealing test programs from previous test programs that triggered bugs. Based on the idea of learning to test, we propose LET, an approach to prioritizing test programs for compiler testing acceleration. LET consists of a learning process and a scheduling process. In the learning process, LET identifies a set of features of test programs, trains a capability model to predict the probability of a new test program for triggering compiler bugs and a time model to predict the execution time of a test program. In the scheduling process, LET prioritizes new test programs according to their bug-revealing probabilities in unit time, which is calculated based on the two trained models. Our extensive experiments show that LET significantly accelerates compiler testing. In particular, LET reduces more than 50% of the testing time in 24.64% of the cases, and reduces between 25% and 50% of the testing time in 36.23% of the cases.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985706,,Computer bugs;Testing;Program processors;Life estimation;Feature extraction;Training;Predictive models,learning (artificial intelligence);program compilers;program testing;scheduling;software reliability,compiler reliability;software systems;automated compiler testing;test-generation tools;compiler bugs;bug-revealing test programs;LET;learning process;scheduling process;time model;bug-revealing probabilities,40
63,Defect Prediction,What Causes My Test Alarm? Automatic Cause Analysis for Test Alarms in System and Integration Testing,H. Jiang; X. Li; Z. Yang; J. Xuan,"State Key Lab of Software Engineering, Wuhan University, Wuhan, China; School of Software, Dalian University of Technology, Dalian, China; Western Michigan University, Kalamazoo, MI, USA; State Key Lab of Software Engineering, Wuhan University, Wuhan, China",2017,"Driven by new software development processes and testing in clouds, system and integration testing nowadays tends to produce enormous number of alarms. Such test alarms lay an almost unbearable burden on software testing engineers who have to manually analyze the causes of these alarms. The causes are critical because they decide which stakeholders are responsible to fix the bugs detected during the testing. In this paper, we present a novel approach that aims to relieve the burden by automating the procedure. Our approach, called Cause Analysis Model, exploits information retrieval techniques to efficiently infer test alarm causes based on test logs. We have developed a prototype and evaluated our tool on two industrial datasets with more than 14,000 test alarms. Experiments on the two datasets show that our tool achieves an accuracy of 58.3% and 65.8%, respectively, which outperforms the baseline algorithms by up to 13.3%. Our algorithm is also extremely efficient, spending about 0.1s per cause analysis. Due to the attractive experimental results, our industrial partner, a leading information and communication technology company in the world, has deployed the tool and it achieves an average accuracy of 72% after two months of running, nearly three times more accurate than a previous strategy based on regular expressions.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985707,software testing;system and integration testing;test alarm analysis;multiclass classification,Software;Computer bugs;Product codes;Instruments;Software testing;Analytical models,information retrieval;program diagnostics;program testing,automatic cause analysis;system and integration testing;test alarms;cause analysis model;information retrieval techniques;test logs;SIT,43
64,Formal Methods,Symbolic Model Extraction for Web Application Verification,I. Bocic; T. Bultan,"Department of Computer Science, University of California, Santa Barbara, USA; Department of Computer Science, University of California, Santa Barbara, USA",2017,"Modern web applications use complex data models and access control rules which lead to data integrity and access control errors. One approach to find such errors is to use formal verification techniques. However, as a first step, most formal verification techniques require extraction of a formal model which is a difficult problem in itself due to dynamic features of modern languages, and it is typically done either manually, or using ad hoc techniques. In this paper, we present a technique called symbolic model extraction for extracting formal data models from web applications. The key ideas of symbolic model extraction are 1) to use the source language interpreter for model extraction, which enables us to handle dynamic features of the language, 2) to use code instrumentation so that execution of each instrumented piece of code returns the formal model that corresponds to that piece of code, 3) to instrument the code dynamically so that the models of methods that are created at runtime can also be extracted, and 4) to execute both sides of branches during instrumented execution so that all program behaviors can be covered in a single instrumented execution. We implemented the symbolic model extraction technique for the Rails framework and used it to extract data and access control models from web applications. Our experiments demonstrate that symbolic model extraction is scalable and extracts formal models that are precise enough to find bugs in real-world applications without reporting too many false positives.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985708,Formal Verification;Model Extraction;Web Applications,Instruments;Feature extraction;Data models;Data mining;Rails;Runtime;Load modeling,authorisation;data integrity;formal verification,symbolic model extraction technique;program behaviors;source language interpreter;formal verification techniques;access control rules;complex data models;Web application verification,4
65,Formal Methods,UML Diagram Refinement (Focusing on Class-and Use Case Diagrams),D. Faitelson; S. Tyszberowicz,"Software Engineering Department, Afeka Tel Aviv Academic College of Engineering, Israel; Academic College of Tel Aviv, School of Computer Science, Yaffo, Israel",2017,"Large and complicated UML models are not useful, because they are difficult to understand. This problem can be solved by using several diagrams of the same system at different levels of abstraction. Unfortunately, UML does not define an explicit set of rules for ensuring that diagrams at different levels of abstraction are consistent. We define such a set of rules, that we call diagram refinement. Diagram refinement is intuitive, and applicable to several kinds of UML diagrams (mostly to structural diagrams but also to use case diagrams), yet it rests on a solid mathematical basis-the theory of graph homomorphisms. We illustrate its usefulness with a series of examples.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985709,Refinement;UML;Class diagram;Use case diagram;Graph homomorphism;Design patterns,Unified modeling language;Concrete;Lattices;Semantics;Software engineering;Electronic mail;Mathematical model,graph theory;Unified Modeling Language,UML diagram refinement;graph homomorphisms,12
66,Software Evolution,Fuzzy Fine-Grained Code-History Analysis,F. Servant; J. A. Jones,"Virginia Tech; University of California, Irvine",2017,"Existing software-history techniques represent source-code evolution as an absolute and unambiguous mapping of lines of code in prior revisions to lines of code in subsequent revisions. However, the true evolutionary lineage of a line of code is often complex, subjective, and ambiguous. As such, existing techniques are predisposed to, both, overestimate and underestimate true evolution lineage. In this paper, we seek to address these issues by providing a more expressive model of code evolution, the fuzzy history graph, by representing code lineage as a continuous (i.e., fuzzy) metric rather than a discrete (i.e., absolute) one. Using this more descriptive model, we additionally provide a novel multi-revision code-history analysis - fuzzy history slicing. In our experiments over three real-world software systems, we found that the fuzzy history graph provides a tunable balance of precision and recall, and an overall improved accuracy over existing code-evolution models. Furthermore, we found that the use of such a fuzzy model of history provided improved accuracy for code-history analysis tasks.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985710,software engineering;computer aided software engineering;software maintenance;reasoning about programs,History;Analytical models;Solids;Computer bugs;Measurement;Computational modeling;Cloning,fuzzy set theory;graph theory;program slicing;software maintenance,fuzzy fine-grained code-history analysis;code lineage;novel multirevision code-history analysis;fuzzy history graph;fuzzy history slicing,9
67,Software Evolution,To Type or Not to Type: Quantifying Detectable Bugs in JavaScript,Z. Gao; C. Bird; E. T. Barr,"University College London, London, UK; Microsoft Research, Redmond, USA; University College London, London, UK",2017,"JavaScript is growing explosively and is now used in large mature projects even outside the web domain. JavaScript is also a dynamically typed language for which static type systems, notably Facebook's Flow and Microsoft's TypeScript, have been written. What benefits do these static type systems provide? Leveraging JavaScript project histories, we select a fixed bug and check out the code just prior to the fix. We manually add type annotations to the buggy code and test whether Flow and TypeScript report an error on the buggy code, thereby possibly prompting a developer to fix the bug before its public release. We then report the proportion of bugs on which these type systems reported an error. Evaluating static type systems against public bugs, which have survived testing and review, is conservative: it understates their effectiveness at detecting bugs during private development, not to mention their other benefits such as facilitating code search/completion and serving as documentation. Despite this uneven playing field, our central finding is that both static type systems find an important percentage of public bugs: both Flow 0.30 and TypeScript 2.0 successfully detect 15%!.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985711,JavaScript;static type systems;Flow;TypeScript;mining software repositories,Computer bugs;History;Software;Surgery;Facebook;Measurement uncertainty;Documentation,Java;program debugging,JavaScript;TypeScript;static type systems;bug detection;Web domain,46
68,Software Evolution,The Evolution of Continuous Experimentation in Software Product Development: From Data to a Data-Driven Organization at Scale,A. Fabijan; P. Dmitriev; H. H. Olsson; J. Bosch,"Faculty of Technology and Society, MalmÃ¶ University, MalmÃ¶, Sweden; Microsoft, Analysis & Experimentation Microsoft, Redmond, WA, USA; Faculty of Technology and Society, MalmÃ¶ University, MalmÃ¶, Sweden; Dep. of Computer Science & Eng., Chalmers University of Technology, GÃ¶teborg, Sweden",2017,"Software development companies are increasingly aiming to become data-driven by trying to continuously experiment with the products used by their customers. Although familiar with the competitive edge that the A/B testing technology delivers, they seldom succeed in evolving and adopting the methodology. In this paper, and based on an exhaustive and collaborative case study research in a large software-intense company with highly developed experimentation culture, we present the evolution process of moving from ad-hoc customer data analysis towards continuous controlled experimentation at scale. Our main contribution is the ""Experimentation Evolution Model"" in which we detail three phases of evolution: technical, organizational and business evolution. With our contribution, we aim to provide guidance to practitioners on how to develop and scale continuous experimentation in software organizations with the purpose of becoming data-driven at scale.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7985712,A/B testing;continuous experimentation;data science;customer feedback;continuous product innovation;Experimentation Evolution Model;product value;Experiment Owner,Software engineering,organisational aspects;product development;software engineering,continuous experimentation;software product development;data-driven organization;A/B testing technology;software-intense company;ad-hoc customer data analysis;experimentation evolution model;business evolution;organizational evolution;technical evolution;software organizations,90
69,Software Repair 1,Context-Aware Patch Generation for Better Automated Program Repair,M. Wen; J. Chen; R. Wu; D. Hao; S. -C. Cheung,"Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; EECS, Peking University, Beijing, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; EECS, Peking University, Beijing, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",2018,"The effectiveness of search-based automated program repair is limited in the number of correct patches that can be successfully generated. There are two causes of such limitation. First, the search space does not contain the correct patch. Second, the search space is huge and therefore the correct patch cannot be generated (ie correct patches are either generated after incorrect plausible ones or not generated within the time budget). To increase the likelihood of including the correct patches in the search space, we propose to work at a fine granularity in terms of AST nodes. This, however, will further enlarge the search space, increasing the challenge to find the correct patches. We address the challenge by devising a strategy to prioritize the candidate patches based on their likelihood of being correct. Specifically, we study the use of AST nodes' context information to estimate the likelihood. In this paper, we propose CapGen, a context-aware patch generation technique. The novelty which allows CapGen to produce more correct patches lies in three aspects: (1) The fine-granularity design enables it to find more correct fixing ingredients; (2) The context-aware prioritization of mutation operators enables it to constrain the search space; (3) Three context-aware models enable it to rank correct patches at high positions before incorrect plausible ones. We evaluate CapGen on Defects4J and compare it with the state-of-the-art program repair techniques. Our evaluation shows that CapGen outperforms and complements existing techniques. CapGen achieves a high precision of 84.00% and can prioritize the correct patches before 98.78% of the incorrect plausible ones.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453055,Context-Aware;Automated Program Repair;Patch Prioritization,Computer bugs;Maintenance engineering;Search problems;Explosions;Context modeling;Software;Benchmark testing,maximum likelihood estimation;program diagnostics;search problems;ubiquitous computing,search-based automated program repair;context-aware patch generation technique;CapGen;likelihood estimation;AST nodes context information,89
70,Software Repair 1,Towards Practical Program Repair with On-demand Candidate Generation,J. Hua; M. Zhang; K. Wang; S. Khurshid,"University of Texas at Austin, Austin, TX, US; University of Texas at Austin, Austin, TX, US; University of Texas at Austin, Austin, TX, US; University of Texas at Austin, Austin, TX, US",2018,"Effective program repair techniques, which modify faulty programs to fix them with respect to given test suites, can substantially reduce the cost of manual debugging. A common repair approach is to iteratively first generate candidate programs with possible bug fixes and then validate them against the given tests until a candidate that passes all the tests is found. While this approach is conceptually simple, due to the potentially high number of candidates that need to first be generated and then be compiled and tested, existing repair techniques that embody this approach have relatively low effectiveness, especially for faults at a fine granularity. To tackle this limitation, we introduce a novel repair technique, SketchFix, which generates candidate fixes on demand (as needed) during the test execution. Instead of iteratively re-compiling and re-executing each actual candidate program, SketchFix translates faulty programs to sketches, i.e., partial programs with ""holes"", and compiles each sketch once which may represent thousands of concrete candidates. With the insight that the space of candidates can be reduced substantially by utilizing the runtime behaviors of the tests, SketchFix lazily initializes the candidates of the sketches while validating them against the test execution. We experimentally evaluate SketchFix on the Defects4J benchmark and the experimental results show that SketchFix works particularly well in repairing bugs with expression manipulation at the AST node-level granularity compared to other program repair techniques. Specifically, SketchFix correctly fixes 19 out of 357 defects in 23 minutes on average using the default setting. In addition, SketchFix finds the first repair with 1.6% of re-compilations (#compiled sketches/#candidates) and 3.0% of re-executions out of all repair candidates.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453056,debugging;program repair;program synthesis;lazy initialization;execution driven pruning,Maintenance engineering;Computer bugs;Runtime;Syntactics;Debugging;Java,program compilers;program debugging;program diagnostics;program testing;software fault tolerance;software maintenance,effective program repair techniques;faulty programs;manual debugging;common repair approach;candidate programs;possible bug fixes;relatively low effectiveness;novel repair technique;SketchFix;candidate fixes;test execution;iteratively re-compiling;re-executing each actual candidate program;partial programs;concrete candidates;re-compilations;repair candidates;practical program repair;on-demand candidate generation;#compiled sketches-#candidates,39
71,Software Repair 1,[Journal First] A Correlation Study Between Automated Program Repair and Test-Suite Metrics,J. Yi; S. H. Tan; S. Mechtaev; M. BÃ¶hme; A. Roychoudhury,NA; NA; NA; NA; NA,2018,"Automated program repair has attracted attention due to its potential to reduce debugging cost. Prior works show the feasibility of automated repair, and the research focus is gradually shifting towards the quality of generated patches. One promising direction is to control the quality of generated patches by controlling the quality of test-suites used. In this paper, we investigate the question: """"Can traditional test-suite metrics used in software testing be used for automated program repair?"""". We empirically investigate the effectiveness of test-suite metrics (statement / branch coverage and mutation score) in controlling the reliability of repairs (the likelihood that repairs cause regressions). We conduct the largest-scale experiments to date with real-world software, and perform the first correlation study between test-suite metrics and the reliability of generated repairs. Our results show that by increasing test-suite metrics, the reliability of repairs tend to increase. Particularly, such trend is most strongly observed in statement coverage. This implies that traditional test-suite metrics used in software testing can also be used to improve the reliability of repairs in program repair.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453057,Code Coverage;Program Repair;Mutation Testing,Maintenance engineering;Measurement;Correlation;Software reliability;Software;Software testing,program debugging;program testing;software maintenance,regressions;test-suite metrics;automated repair;generated repairs;correlation study;automated program repair;software testing;test-suites;generated patches,2
72,Software Repair 1,[Journal First] Do Automated Program Repair Techniques Repair Hard and Important Bugs?,M. Motwani; S. Sankaranarayanan; R. Just; Y. Brun,"University of Massachusetts Amherst, Amherst, MA, US; University of Massachusetts Amherst, Amherst, MA, US; University of Massachusetts Amherst, Amherst, MA, US; University of Massachusetts Amherst, Amherst, MA, US",2018,"The full version of this article is: Manish Motwani, Sandhya Sankaranarayanan, Rene Just, and Yuriy Brun, ""Do Automated Program Repair Techniques Repair Hard and Important Bugs?"" in Empirical Software Engineering, http://dx.doi.org/10.1007/s10664-017-9550-0.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453058,Automated program repair;Repairability,Maintenance engineering;Java;Computer bugs;Correlation;Software engineering;Complexity theory;Benchmark testing,program debugging;software maintenance,repair techniques;important bugs;automated program,1
73,Apps and App Stores 1,Software Protection on the Go: A Large-Scale Empirical Study on Mobile App Obfuscation,P. Wang; Q. Bao; L. Wang; S. Wang; Z. Chen; T. Wei; D. Wu,NA; NA; NA; NA; NA; NA; NA,2018,"The prosperity of smartphone markets has raised new concerns about software security on mobile platforms, leading to a growing demand for effective software obfuscation techniques. Due to various differences between the mobile and desktop ecosystems, obfuscation faces both technical and non-technical challenges when applied to mobile software. Although there have been quite a few software security solution providers launching their mobile app obfuscation services, it is yet unclear how real-world mobile developers perform obfuscation as part of their software engineering practices. Our research takes a first step to systematically studying the deployment of software obfuscation techniques in mobile software development. With the help of an automated but coarse-grained method, we computed the likelihood of an app being obfuscated for over a million app samples crawled from Apple App Store. We then inspected the top 6600 instances and managed to identify 601 obfuscated versions of 539 iOS apps. By analyzing this sample set with extensive manual effort, we made various observations that reveal the status quo of mobile obfuscation in the real world, providing insights into understanding and improving the situation of software protection on mobile platforms.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453059,obfuscation;reverse engineering;mobile app;empirical study,Software;Security;Androids;Humanoid robots;Libraries;Software protection;Manuals,computer crime;security of data;smart phones;software engineering,software protection;large-scale empirical study;smartphone markets;mobile platforms;effective software obfuscation techniques;mobile ecosystems;desktop ecosystems;nontechnical challenges;software security solution providers;mobile app obfuscation services;real-world mobile developers;mobile obfuscation;Apple App Store;million app samples;mobile software development;software engineering practices,6
74,Apps and App Stores 1,GUILeak: Tracing Privacy Policy Claims on User Input Data for Android Applications,X. Wang; X. Qin; M. Bokaei Hosseini; R. Slavin; T. D. Breaux; J. Niu,"Univ. of Texas at San Antonio, San Antonio, TX, USA; Univ. of Texas at San Antonio, San Antonio, TX, USA; Univ. of Texas at San Antonio, San Antonio, TX, USA; Univ. of Texas at San Antonio, San Antonio, TX, USA; Carnegie Mellon Univ., Pittsburgh, PA, USA; Univ. of Texas at San Antonio, San Antonio, TX, USA",2018,"The Android mobile platform supports billions of devices across more than 190 countries around the world. This popularity coupled with user data collection by Android apps has made privacy protection a well-known challenge in the Android ecosystem. In practice, app producers provide privacy policies disclosing what information is collected and processed by the app. However, it is difficult to trace such claims to the corresponding app code to verify whether the implementation is consistent with the policy. Existing approaches for privacy policy alignment focus on information directly accessed through the Android platform (e.g., location and device ID), but are unable to handle user input, a major source of private information. In this paper, we propose a novel approach that automatically detects privacy leaks of user-entered data for a given Android app and determines whether such leakage may violate the app's privacy policy claims. For evaluation, we applied our approach to 120 popular apps from three privacy-relevant app categories: finance, health, and dating. The results show that our approach was able to detect 21 strong violations and 18 weak violations from the studied apps.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453060,Mobile privacy policy;Android application;User input,Graphical user interfaces;Ontologies;Privacy;Data privacy;Androids;Humanoid robots;Layout,data privacy;mobile computing,privacy protection;Android ecosystem;app producers;privacy policies;private information;privacy-relevant app categories;privacy policy claims;android applications;Android mobile platform;user data collection;Android apps;app code;privacy leaks detection;privacy policy alignment,17
75,Apps and App Stores 1,Online App Review Analysis for Identifying Emerging Issues,C. Gao; J. Zeng; M. R. Lyu; I. King,"Chinese University of Hong Kong, New Territories, HK; Chinese University of Hong Kong, New Territories, HK; Chinese University of Hong Kong, New Territories, HK; Chinese University of Hong Kong, New Territories, HK",2018,"Detecting emerging issues (e.g., new bugs) timely and precisely is crucial for developers to update their apps. App reviews provide an opportunity to proactively collect user complaints and promptly improve apps' user experience, in terms of bug fixing and feature refinement. However, the tremendous quantities of reviews and noise words (e.g., misspelled words) increase the difficulties in accurately identifying newly-appearing app issues. In this paper, we propose a novel and automated framework IDEA, which aims to IDentify Emerging App issues effectively based on online review analysis. We evaluate IDEA on six popular apps from Google Play and Apple's App Store, employing the official app changelogs as our ground truth. Experiment results demonstrate the effectiveness of IDEA in identifying emerging app issues. Feedback from engineers and product managers shows that 88.9% of them think that the identified issues can facilitate app development in practice. Moreover, we have successfully applied IDEA to several products of Tencent, which serve hundreds of millions of users.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453061,app reviews;online analysis;emerging issues,Computer bugs;Meteorology;Facebook;Google;Semantics;Software engineering,program debugging;program testing;smart phones,noise words;emerging app issues;IDEA;popular apps;official app changelogs;app development;online App review analysis;App reviews;Apple App Store,39
76,Apps and App Stores 1,EARMO: An Energy-Aware Refactoring Approach for Mobile Apps,R. Morales; R. Saborido; F. Khomh; F. Chicano; G. Antoniol,NA; NA; NA; NA; NA,2018,"With millions of smartphones sold every year, the development of mobile apps has grown substantially. The battery power limitation of mobile devices has push developers and researchers to search for methods to improve the energy efficiency of mobile apps. We propose a multiobjective refactoring approach to automatically improve the architecture of mobile apps, while controlling for energy efficiency. In this extended abstract we briefly summarize our work.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453062,Refactoring;Anti-patterns;Mobile apps;Energy consumption,Energy efficiency;Software engineering;Batteries;Energy measurement;Mobile handsets;Energy consumption;Software,C language;energy conservation;smart phones;software maintenance,multiobjective refactoring approach;mobile apps;energy efficiency;energy-aware refactoring approach;mobile devices;push developers,2
77,Software Evolution and Maintenance 1,Neuro-Symbolic Program Corrector for Introductory Programming Assignments,S. Bhatia; P. Kohli; R. Singh,"Netaji Subhas Institute of Technology, New Delhi, Delhi, IN; Google Deepmind, London, UK; Microsoft Research, Redmond, WA, US",2018,"Automatic correction of programs is a challenging problem with numerous real world applications in security, verification, and education. One application that is becoming increasingly important is the correction of student submissions in online courses for providing feedback. Most existing program repair techniques analyze Abstract Syntax Trees (ASTs) of programs, which are unfortunately unavailable for programs with syntax errors. In this paper, we propose a novel Neuro-symbolic approach that combines neural networks with constraint-based reasoning. Specifically, our method first uses a Recurrent Neural Network (RNN) to perform syntax repairs for the buggy programs; subsequently, the resulting syntactically-fixed programs are repaired using constraint-based techniques to ensure functional correctness. The RNNs are trained using a corpus of syntactically correct submissions for a given programming assignment, and are then queried to fix syntax errors in an incorrect programming submission by replacing or inserting the predicted tokens at the error location. We evaluate our technique on a dataset comprising of over 14,500 student submissions with syntax errors. Our method is able to repair syntax errors in 60% (8689) of submissions, and finds functionally correct repairs for 23.8% (3455) submissions.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453063,Neural Program Correction;Automated Feedback Generation;Neural guided search,Syntactics;Maintenance engineering;Programming;Recurrent neural networks;Prediction algorithms;Semantics,computer aided instruction;educational courses;error handling;program debugging;program diagnostics;programming;recurrent neural nets;trees (mathematics),program repair techniques;neuro-symbolic program corrector;introductory programming assignment;automatic program correction;Abstract Syntax Trees;online courses;error location;syntax errors;constraint-based techniques;syntactically-fixed programs;buggy programs;syntax repairs;Recurrent Neural Network;constraint-based reasoning,13
78,Software Evolution and Maintenance 1,Automated Localization for Unreproducible Builds,Z. Ren; H. Jiang; J. Xuan; Z. Yang,"Dalian University of Technology, Dalian, Liaoning, CN; Dalian University of Technology, Dalian, Liaoning, CN; Wuhan University, Wuhan, Hubei, CN; Western Michigan University, Kalamazoo, MI, US",2018,"Reproducibility is the ability of recreating identical binaries under pre-defined build environments. Due to the need of quality assurance and the benefit of better detecting attacks against build environments, the practice of reproducible builds has gained popularity in many open-source software repositories such as Debian and Bitcoin. However, identifying the unreproducible issues remains a labour intensive and time consuming challenge, because of the lacking of information to guide the search and the diversity of the causes that may lead to the unreproducible binaries. In this paper we propose an automated framework called RepLoc to localize the problematic files for unreproducible builds. RepLoc features a query augmentation component that utilizes the information extracted from the build logs, and a heuristic rule-based filtering component that narrows the search scope. By integrating the two components with a weighted file ranking module, RepLoc is able to automatically produce a ranked list of files that are helpful in locating the problematic files for the unreproducible builds. We have implemented a prototype and conducted extensive experiments over 671 real-world unreproducible Debian packages in four different categories. By considering the topmost ranked file only, RepLoc achieves an accuracy rate of 47.09%. If we expand our examination to the top ten ranked files in the list produced by RepLoc, the accuracy rate becomes 79.28%. Considering that there are hundreds of source code, scripts, Makefiles, etc., in a package, RepLoc significantly reduces the scope of localizing problematic files. Moreover, with the help of RepLoc, we successfully identified and fixed six new unreproducible packages from Debian and Guix.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453064,Unreproducible Build;Localization;Software Maintenance,Software;Task analysis;Feature extraction;Filtering;Hafnium;Software engineering;Computer science,program diagnostics;public domain software;query processing;security of data;software maintenance,detecting attacks;open-source software repositories;unreproducible binaries;problematic files;query augmentation component;build logs;heuristic rule-based filtering component;search scope;weighted file ranking module;ranked list;topmost ranked file;automated localization;unreproducible builds;pre-defined build environments;quality assurance;Bitcoin;Guix;RepLoc features;unreproducible Debian packages,12
79,Software Evolution and Maintenance 1,Enlightened Debugging,X. Li; S. Zhu; M. dâ€™Amorim; A. Orso,"Georgia Institute of Technology, Atlanta, GA, US; Georgia Institute of Technology, Atlanta, GA, US; Universidade Federal de Pernambuco, Recife, PE, BR; Georgia Institute of Technology, Atlanta, GA, US",2018,"Numerous automated techniques have been proposed to reduce the cost of software debugging, a notoriously time-consuming and human-intensive activity. Among these techniques, Statistical Fault Localization (SFL) is particularly popular. One issue with SFL is that it is based on strong, often unrealistic assumptions on how developers behave when debugging. To address this problem, we propose Enlighten, an interactive, feedback-driven fault localization technique. Given a failing test, Enlighten (1) leverages SFL and dynamic dependence analysis to identify suspicious method invocations and corresponding data values, (2) presents the developer with a query about the most suspicious invocation expressed in terms of inputs and outputs, (3) encodes the developer feedback on the correctness of individual data values as extra program specifications, and (4) repeats these steps until the fault is found. We evaluated Enlighten in two ways. First, we applied Enlighten to 1,807 real and seeded faults in 3 open source programs using an automated oracle as a simulated user; for over 96% of these faults, Enlighten required less than 10 interactions with the simulated user to localize the fault, and a sensitivity analysis showed that the results were robust to erroneous responses. Second, we performed an actual user study on 4 faults with 24 participants and found that participants who used Enlighten performed significantly better than those not using our tool, in terms of both number of faults localized and time needed to localize the faults.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453065,debugging;fault localization;dynamic analysis,Debugging;Performance analysis;Tools;Generators;Calculators;Software debugging;Task analysis,fault diagnosis;program debugging;program testing;software fault tolerance,enlightened debugging;numerous automated techniques;software debugging;notoriously time-consuming;human-intensive activity;unrealistic assumptions;feedback-driven fault localization technique;Enlighten leverages SFL;dynamic dependence analysis;developer feedback;individual data values;extra program specifications;automated oracle;sensitivity analysis;open source programs;statistical fault localization,14
80,Software Evolution and Maintenance 1,[Journal First] Experiences and Challenges in Building a Data Intensive System for Data Migration,M. Scavuzzo; E. Di Nitto; D. Ardagna,"Politecnico di Milano, Milano, Lombardia, IT; Politecnico di Milano, Milano, Lombardia, IT; Politecnico di Milano, Milano, Lombardia, IT",2018,"Recent analyses[2, 4, 5] report that many sectors of our economy and society are more and more guided by data-driven decision processes (e.g., health care, public administrations, etc.). As such, Data Intensive (DI) applications are becoming more and more important and critical. They must be fault-tolerant, they should scale with the amount of data, and be able to elastically leverage additional resources as and when these last ones are provided [3]. Moreover, they should be able to avoid data drops introduced in case of sudden overloads and should offer some Quality of Service (QoS) guarantees. Ensuring all these properties is, per se, a challenge, but it becomes even more difficult for DI applications, given the large amount of data to be managed and the significant level of parallelism required for its components. Even if today some technological frameworks are available for the development of such applications (for instance, think of Spark, Storm, Flink), we still lack solid software engineering approaches to support their development and, in particular, to ensure that they offer the required properties in terms of availability, throughput, data loss, etc. In fact, at the time of writing, identifying the right solution can require several rounds of experiments and the adoption of many different technologies. This implies the need for highly skilled persons and the execution of experiments with large data sets and a large number of resources, and, consequently, a significant amount of time and budget. To experiment with currently available approaches, we performed an action research experiment focusing on developing- testing-reengineering a specific DI application, Hegira4Cloud, that migrates data between widely used NoSQL databases, including so-called Database as a Service (DaaS), as well as on-premise databases. This is a representative DI system because it has to handle large volumes of data with different structures and has to guarantee that some important characteristics, in terms of data types and transactional properties, are preserved. Also, it poses stringent requirements in terms of correctness, high performance, fault tolerance, and fast and effective recovery. In our action research, we discovered that the literature offered some high level design guidelines for DI applications, as well as some tools to support modelling and QoS analysis/simulation of complex architectures, however the available tools were not yet. suitable to support DI systems. Moreover, we realized that the available big data frameworks we could have used were not flexible enough to cope with all possible application-specific aspects of our system. Hence, to achieve the desired level of performance, fault tolerance and recovery, we had to adopt a time-consuming, experiment-based approach [1, 6], which, in our case, consisted of three iterations: (1) the design and implementation of a Mediation Data Model capable of managing data extracted from different databases, together with a first monholitical prototype of Hegira4Cloud; (2) the improvement of performance of our prototype when managing and transferring huge amounts of data; (3) the introduction of fault-tolerant data extraction and management mechanisms, which are independent from the targeted databases. Among the others, an important issue that has forced us to reiterate in the development of Hegira4Cloud concerned the DaaS we interfaced with. In particular these DaaS, which are well-known services with a large number of users: (1) were missing detailed information regarding the behaviour of their APIs; (2) did not offer a predictable service; (3) were suffering of random downtimes not correlated with the datasets we were experimenting with. In this journal first presentation, we describe our experience and the issues we encountered that led to some important decisions during the software design and engineering process. Also, we analyse the state of the art of software design and verification tools and approaches in the light of our experience, and identify weaknesses, alternative design approaches and open challenges that could generate new research in these areas. More details can be found in the journal publication.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453066,Data intensive applications;Experiment driven action research;Big data;Data migration,Big Data;Databases;Quality of service;Tools;Fault tolerance;Fault tolerant systems;Software,Big Data;cloud computing;data analysis;data models;formal verification;NoSQL databases;program testing;quality of service;software quality,Hegira4Cloud;fault-tolerant data extraction;software design;verification tools;Data Intensive system;data loss;Mediation Data Model;software engineering;QoS analysis;big data frameworks;NoSQL databases;data migration;Quality of Service;developing- testing-reengineering;Database as a Service,
81,Human and Social Aspects of Computing 1,Sentiment Analysis for Software Engineering: How Far Can We Go?,B. Lin; F. Zampetti; G. Bavota; M. Di Penta; M. Lanza; R. Oliveto,"Universita della Svizzera Italiana, Lugano, TI, CH; Universita degli Studi del Sannio, Benevento, Campania, IT; Universita della Svizzera Italiana, Lugano, TI, CH; Universita degli Studi del Sannio, Benevento, Campania, IT; Universita della Svizzera Italiana, Lugano, TI, CH; Universita degli Studi del Molise, Campobasso, Molise, IT",2018,"Sentiment analysis has been applied to various software engineering (SE) tasks, such as evaluating app reviews or analyzing developers' emotions in commit messages. Studies indicate that sentiment analysis tools provide unreliable results when used out-of-the-box, since they are not designed to process SE datasets. The silver bullet for a successful application of sentiment analysis tools to SE datasets might be their customization to the specific usage context. We describe our experience in building a software library recommender exploiting crowdsourced opinions mined from Stack Overflow (e.g., what is the sentiment of developers about the usability of a library). To reach our goal, we retrained-on a set of 40k manually labeled sentences/words extracted from Stack Overflow-a state-of-the-art sentiment analysis tool exploiting deep learning. Despite such an effort- and time-consuming training process, the results were negative. We changed our focus and performed a thorough investigation of the accuracy of these tools on a variety of SE datasets. Our results should warn the research community about the strong limitations of current sentiment analysis tools.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453067,sentiment analysis;software engineering;NLP,Sentiment analysis;Tools;Software engineering;Software;Task analysis;Training;Motion pictures,data analysis;data mining;learning (artificial intelligence);sentiment analysis;social networking (online);software engineering;software libraries,sentiment analysis tools;app reviews evaluation;commit messages;out-of-the-box;silver bullet;crowdsourced opinions mined;stack overflow;deep learning;time-consuming training process;effort training process;research community;developers emotions analyzing;software engineering tasks;SE datasets;software library recommender,55
82,Human and Social Aspects of Computing 1,Identifying Features in Forks,S. Zhou; S. Stanciulescu; O. LeÃŸenich; Y. Xiong; A. Wasowski; C. KÃ¤stner,NA; NA; NA; NA; NA; NA,2018,"Fork-based development has been widely used both in open source communities and in industry, because it gives developers flexibility to modify their own fork without affecting others. Unfortunately, this mechanism has downsides: When the number of forks becomes large, it is difficult for developers to get or maintain an overview of activities in the forks. Current tools provide little help. We introduce INFOX, an approach to automatically identify non-merged features in forks and to generate an overview of active forks in a project. The approach clusters cohesive code fragments using code and network-analysis techniques and uses information-retrieval techniques to label clusters with keywords. The clustering is effective, with 90% accuracy on a set of known features. In addition, a human-subject evaluation shows that INFOX can provide actionable insight for developers of forks.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453068,Fork-based development;Github;Community detection;Information retrieval;overview of forks;transparency,Feature extraction;Computer bugs;Navigation;History;Tools;Visualization;Google,feature extraction;information retrieval;pattern clustering,identifying features;fork-based development;open source communities;INFOX;nonmerged features;clusters cohesive code fragments;network-analysis techniques;information-retrieval techniques;human-subject evaluation,22
83,Human and Social Aspects of Computing 1,Roles and Impacts of Hands-on Software Architects in Five Industrial Case Studies,I. Rehman; M. Mirakhorli; M. Nagappan; A. Aralbay Uulu; M. Thornton,NA; NA; NA; NA; NA,2018,"Whether software architects should also code is an enduring question. In order to satisfy performance, security, reliability and other quality concerns, architects need to compare and carefully choose a combination of architectural patterns, styles or tactics. Then later in the development cycle, these architectural choices must be implemented completely and correctly so there will not be any drift from envisioned design. In this paper, we use data analytics-based techniques to study five large-scale software systems, examining the impact and the role of software architects who write code on software quality. Our quantitative study is augmented with a follow-up interview of architects. This paper provides empirical evidence for supporting the pragmatic opinions that architects should write code. Our analysis shows that implementing architectural tactics is more complex than delivering functionality, tactics are more error prone than software functionalities, and the architects tend to introduce fewer bugs into the implementation of architectural tactics compared to the developers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453069,Architect;coding;tactics;architecture savvy,Software engineering,data analysis;software architecture;software quality,architectural choices;large-scale software systems;software architects;software quality;quantitative study;architectural tactics;software functionalities,2
84,Human and Social Aspects of Computing 1,[Journal First] Sentiment Polarity Detection for Software Development,F. Calefato; F. Lanubile; F. Maiorano; N. Novielli,"Consiglio Nazionale delle Ricerche Area di Ricerca di Napoli 1, Napoli, Campania, IT; Consiglio Nazionale delle Ricerche Area di Ricerca di Napoli 1, Napoli, Campania, IT; Consiglio Nazionale delle Ricerche Area di Ricerca di Napoli 1, Napoli, Campania, IT; Consiglio Nazionale delle Ricerche Area di Ricerca di Napoli 1, Napoli, Campania, IT",2018,"The role of sentiment analysis is increasingly emerging to study software developers' emotions by mining crowd-generated content within software repositories and information sources. With a few notable exceptions, empirical software engineering studies have exploited off-the-shelf sentiment analysis tools. However, such tools have been trained on non-technical domains and general-purpose social media, thus resulting in misclassifications of technical jargon and problem reports. In particular, Jongeling et al. show how the choice of the sentiment analysis tool may impact the conclusion validity of empirical studies because not only these tools do not agree with human annotation of developers' communication channels, but they also disagree among themselves. Our goal is to move beyond the limitations of off-the-shelf sentiment analysis tools when applied in the software engineering domain. Accordingly, we present Senti4SD, a sentiment polarity classifier for software developers' communication channels. Senti4SD exploits a suite of lexicon-based, keyword-based, and semantic features for appropriately dealing with the domain-dependent use of a lexicon. We built a Distributional Semantic Model (DSM) to derive the semantic features exploited by Senti4SD. Specifically, we ran word2vec on a collection of over 20 million documents from Stack Overflow, thus obtaining word vectors that are representative of developers' communication style. The classifier is trained and validated using a gold standard of 4,423 Stack Overflow posts, including questions, answers, and comments, which were manually annotated for sentiment polarity. We release the full lab package, which includes both the gold standard and the emotion annotation guidelines, to ease the execution of replications as well as new studies on emotion awareness in software engineering. To inform future research on word embedding for text categorization and information retrieval in software engineering, the replication kit also includes the DSM. Results. The contribution of the lexicon-based, keyword-based, and semantic features is assessed by our empirical evaluation leveraging different feature settings. With respect to SentiStrength, a mainstream off-the-shelf tool that we use as a baseline, Senti4SD reduces the misclassifications of neutral and positive posts as emotionally negative. Furthermore, we provide empirical evidence of better performance also in presence of a minimal set of training documents.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453070,Sentiment Analysis;Communication Channels;Stack Overflow;Word Embedding;Social Software Engineering,Software;Software engineering;Collaboration;Programming,data mining;information retrieval;pattern classification;sentiment analysis;social networking (online);software engineering,keyword-based features;sentiment polarity detection;empirical software engineering;distributional semantic model;DSM;stack overflow;emotion annotation guidelines;text categorization;information retrieval;replication kit;sentistrength;word embedding;software developers emotions;crowd-generated content mining;lexicon-based features;software developers communication channels;word vectors;sentiment polarity classifier;software engineering domain;problem reports;technical jargon;general-purpose social media;nontechnical domains;off-the-shelf sentiment analysis tools;information sources;software repositories;software development;Senti4SD;semantic features,4
85,Software Repair 2,Semantic Program Repair Using a Reference Implementation,S. Mechtaev; M. -D. Nguyen; Y. Noller; L. Grunske; A. Roychoudhury,National University of Singapore; National University of Singapore; Humboldt University of Berlin; Humboldt University of Berlin; National University of Singapore,2018,"Automated program repair has been studied via the use of techniques involving search, semantic analysis and artificial intelligence. Most of these techniques rely on tests as the correctness criteria, which causes the test overfitting problem. Although various approaches such as learning from code corpus have been proposed to address this problem, they are unable to guarantee that the generated patches generalize beyond the given tests. This work studies automated repair of errors using a reference implementation. The reference implementation is symbolically analyzed to automatically infer a specification of the intended behavior. This specification is then used to synthesize a patch that enforces conditional equivalence of the patched and the reference programs. The use of the reference implementation as an implicit correctness criterion alleviates overfitting in test-based repair. Besides, since we generate patches by semantic analysis, the reference program may have a substantially different implementation from the patched program, which distinguishes our approach from existing techniques for regression repair like Relifix. Our experiments in repairing the embedded Linux Busybox with GNU Coreutils as reference (and vice-versa) revealed that the proposed approach scales to real-world programs and enables the generation of more correct patches.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453071,Debugging;Program repair;Verification,Maintenance engineering;Software;Semantics;Forestry;Instruments;Signal processing algorithms;Scalability,embedded systems;learning (artificial intelligence);Linux;program diagnostics;program testing;public domain software;regression analysis;software maintenance,semantic program repair;reference implementation;automated program repair;semantic analysis;test overfitting problem;reference program;test-based repair;patched program;regression repair;GNU Coreutils;Linux Busybox;code corpus learning,24
86,Software Repair 2,Automated Repair of Mobile Friendly Problems in Web Pages,S. Mahajan; N. Abolhassani; P. McMinn; W. G. J. Halfond,"University of Southern California, Los Angeles, CA, US; University of Southern California, Los Angeles, CA, US; The University of Sheffield, Sheffield, Sheffield, GB; University of Southern California, Los Angeles, CA, US",2018,"Mobile devices have become a primary means of accessing the Internet. Unfortunately, many websites are not designed to be mobile friendly. This results in problems such as unreadable text, cluttered navigation, and content over owing a device's viewport; all of which can lead to a frustrating and poor user experience. Existing techniques are limited in helping developers repair these mobile friendly problems. To address this limitation of prior work, we designed a novel automated approach for repairing mobile friendly problems in web pages. Our empirical evaluation showed that our approach was able to successfully resolve mobile friendly problems in 95% of the evaluation subjects. In a user study, participants preferred our repaired versions of the subjects and also considered the repaired pages to be more readable than the originals.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453072,Mobile Friendly Problems;automated repair;web apps,Web pages;Maintenance engineering;Mobile handsets;Cascading style sheets;Layout;Navigation;Organizations,computer displays;Internet;mobile computing;Web sites,mobile devices;mobile friendly problems;Web pages;Internet;Websites;cluttered navigation;user experience,12
87,Software Repair 2,Static Automated Program Repair for Heap Properties,R. van Tonder; C. Le Goues,NA; NA,2018,"Static analysis tools have demonstrated effectiveness at finding bugs in real world code. Such tools are increasingly widely adopted to improve software quality in practice. Automated Program Repair (APR) has the potential to further cut down on the cost of improving software quality. However, there is a disconnect between these effective bug-finding tools and APR. Recent advances in APR rely on test cases, making them inapplicable to newly discovered bugs or bugs difficult to test for deterministically (like memory leaks). Additionally, the quality of patches generated to satisfy a test suite is a key challenge. We address these challenges by adapting advances in practical static analysis and verification techniques to enable a new technique that finds and then accurately fixes real bugs without test cases. We present a new automated program repair technique using Separation Logic. At a high-level, our technique reasons over semantic effects of existing program fragments to fix faults related to general pointer safety properties: resource leaks, memory leaks, and null dereferences. The procedure automatically translates identified fragments into source-level patches, and verifies patch correctness with respect to reported faults. In this work we conduct the largest study of automatically fixing undiscovered bugs in real-world code to date. We demonstrate our approach by correctly fixing 55 bugs, including 11 previously undiscovered bugs, in 11 real-world projects.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453073,Automated Program Repair;Separation Logic,Computer bugs;Maintenance engineering;Tools;Static analysis;Semantics;Software;Safety,formal verification;program debugging;program diagnostics;program testing;program verification;software quality;software tools,bug-finding tools;static analysis;real-world code;undiscovered bugs;source-level patches;resource leaks;general pointer safety properties;program fragments;semantic effects;automated program repair technique;verification techniques;test suite;memory leaks;software quality;static analysis tools;heap properties;static automated Program Repair,19
88,Software Repair 2,[Journal First] Overfitting in Semantics-Based Automated Program Repair,X. D. Le; F. Thung; D. Lo; C. Le Goues,NA; NA; NA; NA,2018,"The primary goal of Automated Program Repair (APR) is to automatically fix buggy software, to reduce the manual bug-fix burden that presently rests on human developers. Existing APR techniques can be generally divided into two families: semantics-vs. heuristics-based. Semantics-based APR uses symbolic execution and test suites to extract semantic constraints, and uses program synthesis to synthesize repairs that satisfy the extracted constraints. Heuristic-based APR generates large populations of repair candidates via source manipulation, and searches for the best among them. Both families largely rely on a primary assumption that a program is correctly patched if the generated patch leads the program to pass all provided test cases. Patch correctness is thus an especially pressing concern. A repair technique may generate overfitting patches, which lead a program to pass all existing test cases, but fails to generalize beyond them. In this work, we revisit the overfitting problem with a focus on semantics-based APR techniques, complementing previous studies of the overfitting problem in heuristics-based APR. We perform our study using IntroClass and Codeflaws benchmarks, two datasets well-suited for assessing repair quality, to systematically characterize and understand the nature of overfitting in semantics-based APR. We find that similar to heuristics-based APR, overfitting also occurs in semantics-based APR in various different ways.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453074,Automated Program Repair;Program Synthesis;Symbolic Execution;Patch Overfitting,Maintenance engineering;Semantics;Benchmark testing;Software engineering;Tools;Engines;Sociology,program debugging;program diagnostics;program testing;software fault tolerance;software maintenance,symbolic execution;test suites;program synthesis;overfitting patches;semantics-based APR techniques;semantics-based Automated Program Repair;buggy software;patch correctness;IntroClass;repair quality assessment;Codeflaws,9
89,Apps and App Stores 2,[Journal First] Studying the Dialogue Between Users and Developers of Free Apps in the Google Play Store,S. Hassan; C. Tantithamthavorn; C. -P. Bezemer; A. E. Hassan,"Queen's University Faculty of Health Sciences, Kingston, ON, CA; The University of Adelaide, Adelaide, SA, AU; Queen's University Faculty of Health Sciences, Kingston, ON, CA; Queen's University Faculty of Health Sciences, Kingston, ON, CA",2018,"The popularity of mobile apps continues to grow over the past few years. Mobile app stores, such as the Google Play Store and Apple's App Store provide a unique user feedback mechanism to app developers through app reviews. In the Google Play Store (and most recently in the Apple App Store), developers are able to respond to such user feedback. Over the past years, mobile app reviews have been studied excessively by researchers. However, much of prior work (including our own prior work) incorrectly assumes that reviews are static in nature and that users never update their reviews. In a recent study, we started analyzing the dynamic nature of the review-response mechanism. Our previous study showed that responding to a review often has a positive effect on the rating that is given by the user to an app. In this paper [1], we revisit our prior finding in more depth by studying 4.5 million reviews with 126,686 responses of 2,328 top free-to-download apps in the Google Play Store. One of the major findings of our paper is that the assumption that reviews are static is incorrect. In particular, we find that developers and users in some cases use this response mechanism as a rudimentary user support tool, where dialogues emerge between users and developers through updated reviews and responses. Even though the messages are often simple, we find instances of as many as ten user-developer back-and-forth messages that occur via the response mechanism. Using a mixed-effect model, we identify that the likelihood of a developer responding to a review increases as the review rating gets lower or as the review content gets longer. In addition, we identify four patterns of developers: 1) developers who primarily respond to only negative reviews, 2) developers who primarily respond to negative reviews or to reviews based on their content, 3) developers who primarily respond to reviews which are posted shortly after the latest release of their app, and 4) developers who primarily respond to reviews which are posted long after the latest release of their app. We perform a qualitative analysis of developer responses to understand what drives developers to respond to a review. We manually analyzed a statistically representative random sample of 347 reviews with responses of the top ten apps with the highest number of developer responses. We identify seven drivers that make a developer respond to a review, of which the most important ones are to thank the users for using the app and to ask the user for more details about the reported issue. Our findings show that it can be worthwhile for app owners to respond to reviews, as responding may lead to an increase in the given rating. In addition, our findings show that studying the dialogue between users and developers provides valuable insights that can lead to improvements in the app store and the user support process. The main contributions of this paper are as follows: (1) Our paper is the first work to demonstrate the dynamic nature of reviews. (2) Furthermore, we are the first to demonstrate a peculiar use of the app-review platforms as a user support medium. (3) In addition, our work is the first work to deeply explore developer responses in a systematic manner. (4) Finally, our classification of developer-responses highlights the value of providing canned or even automated responses in next generation app-review platforms.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453075,Google Play Store;User-developer dialogue;Developer response;Mixed-effect model;Android mobile apps,Google;Software engineering;Software;Computer science;Australia;Tools;Systematics,consumer behaviour;mobile computing,free-to-download apps;review-response mechanism;mobile app reviews;Apple App Store;app developers;unique user feedback mechanism;mobile app stores;Google Play Store;free apps;developer-responses;app-review platforms;app owners;developer responses;negative reviews;review content;review rating;review increases;developer responding;user-developer;updated reviews;rudimentary user support tool,
90,Apps and App Stores 2,Automated Reporting of GUI Design Violations for Mobile Apps,K. Moran; B. Li; C. Bernal-CÃ¡rdenas; D. Jelf; D. Poshyvanyk,"College of William and Mary, Williamsburg, VA, US; College of William and Mary, Williamsburg, VA, US; College of William and Mary, Williamsburg, VA, US; College of William and Mary, Williamsburg, VA, US; College of William and Mary, Williamsburg, VA, US",2018,"The inception of a mobile app often takes form of a mock-up of the Graphical User Interface (GUI), represented as a static image delineating the proper layout and style of GUI widgets that satisfy requirements. Following this initial mock-up, the design artifacts are then handed off to developers whose goal is to accurately implement these GUIs and the desired functionality in code. Given the sizable abstraction gap between mock-ups and code, developers often introduce mistakes related to the GUI that can negatively impact an app's success in highly competitive marketplaces. Moreover, such mistakes are common in the evolutionary context of rapidly changing apps. This leads to the time-consuming and laborious task of design teams verifying that each screen of an app was implemented according to intended design specifications. This paper introduces a novel, automated approach for verifying whether the GUI of a mobile app was implemented according to its intended design. Our approach resolves GUI-related information from both implemented apps and mock-ups and uses computer vision techniques to identify common errors in the implementations of mobile GUIs. We implemented this approach for Android in a tool called GVT and carried out both a controlled empirical evaluation with open-source apps as well as an industrial evaluation with designers and developers from Huawei. The results show that GVT solves an important, difficult, and highly practical problem with remarkable efficiency and accuracy and is both useful and scalable from the point of view of industrial designers and developers. The tool is currently used by over one-thousand industrial designers and developers at Huawei to improve the quality of their mobile apps.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453076,GUI;design;Mobile Apps;Android;Computer Vision,Graphical user interfaces;Tools;Androids;Humanoid robots;Visualization;Layout;Computer vision,graphical user interfaces;mobile computing,GUI design violations;mobile app;design artifacts;mock-ups;intended design specifications;open-source apps;mobile GUI,26
91,Apps and App Stores 2,Leveraging Program Analysis to Reduce User-Perceived Latency in Mobile Applications,Y. Zhao; M. Schmitt Laser; Y. Lyu; N. Medvidovic,"University of Southern California, Los Angeles, CA, US; Pontificia Universidade Catolica do Rio Grande do Sul, Porto Alegre, RS, BR; University of Southern California, Los Angeles, CA, US; University of Southern California, Los Angeles, CA, US",2018,"Reducing network latency in mobile applications is an effective way of improving the mobile user experience and has tangible economic benefits. This paper presents PALOMA, a novel client-centric technique for reducing the network latency by prefetching HTTP requests in Android apps. Our work leverages string analysis and callback control-flow analysis to automatically instrument apps using PALOMA's rigorous formulation of scenarios that address ""what"" and ""when"" to prefetch. PALOMA has been shown to incur significant runtime savings (several hundred milliseconds per prefetchable HTTP request), both when applied on a reusable evaluation benchmark we have developed and on real applications.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453077,program analysis;prefetch;network latency;mobile applications,Prefetching;Uniform resource locators;Androids;Humanoid robots;Servers;Runtime;Mobile applications,mobile computing;program diagnostics;storage management,automatically instrument apps;prefetchable HTTP request;program analysis;user-perceived latency;mobile applications;mobile user experience;tangible economic benefits;novel client-centric technique;HTTP requests;Android apps,10
92,Apps and App Stores 2,Repairing Crashes in Android Apps,S. H. Tan; Z. Dong; X. Gao; A. Roychoudhury,"Southern Univ. of Sci. & Technol., Shenzhen, China; National University of Singapore, Singapore, SG; National University of Singapore, Singapore, SG; National University of Singapore, Singapore, SG",2018,"Android apps are omnipresent, and frequently suffer from crashes - leading to poor user experience and economic loss. Past work focused on automated test generation to detect crashes in Android apps. However, automated repair of crashes has not been studied. In this paper, we propose the first approach to automatically repair Android apps, specifically we propose a technique for fixing crashes in Android apps. Unlike most test-based repair approaches, we do not need a test-suite; instead a single failing test is meticulously analyzed for crash locations and reasons behind these crashes. Our approach hinges on a careful empirical study which seeks to establish common root-causes for crashes in Android apps, and then distills the remedy of these root-causes in the form of eight generic transformation operators. These operators are applied using a search-based repair framework embodied in our repair tool Droix. We also prepare a benchmark DroixBench capturing reproducible crashes in Android apps. Our evaluation of Droix on DroixBench reveals that the automatically produced patches are often syntactically identical to the human patch, and on some rare occasion even better than the human patch (in terms of avoiding regressions). These results confirm our intuition that our proposed transformations form a sufficient set of operators to patch crashes in Android.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453078,Automated repair;Android apps;Crash;SBSE,Computer crashes;Androids;Humanoid robots;Maintenance engineering;Mobile applications;Transistors;Testing,Android (operating system);program testing,Android apps;test-based repair approaches;benchmark DroixBench capturing reproducible crashes,36
93,Regression Testing,Hybrid Regression Test Selection,L. Zhang,"University of Texas at Dallas, Richardson, TX, US",2018,"Regression testing is crucial but can be extremely costly. Regression Test Selection (RTS) aims to reduce regression testing cost by only selecting and running the tests that may be affected by code changes. To date, various RTS techniques analyzing at different granularities (e.g., at the basic-block, method, and file levels) have been proposed. RTS techniques working on finer granularities may be more precise in selecting tests, while techniques working on coarser granularities may have lower overhead. According to a recent study, RTS at the file level (FRTS) can have less overall testing time compared with a finer grained technique at the method level, and represents state-of-the-art RTS. In this paper, we present the first hybrid RTS approach, HyRTS, that analyzes at multiple granularities to combine the strengths of traditional RTS techniques at different granularities. We implemented the basic HyRTS technique by combining the method and file granularity RTS. The experimental results on 2707 revisions of 32 projects, totalling over 124 Million LoC, demonstrate that HyRTS outperforms state-of-the-art FRTS significantly in terms of selected test ratio and the offline testing time. We also studied the impacts of each type of method-level changes, and further designed two new HyRTS variants based on the study results. Our additional experiments show that transforming instance method additions/deletions into file-level changes produces an even more effective HyRTS variant that can significantly outperform FRTS in both offline and online testing time.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453079,Test selection;Regression testing;Dynamic analysis;Empirical study,Testing;Tools;Java;Runtime;Open source software,program testing;regression analysis,hybrid RTS approach;granularity RTS;selected test ratio;offline testing time;method-level changes;instance method additions/deletions;file-level changes;online testing time;code changes;RTS techniques;hybrid regression test selection;regression testing cost reduction;HyRTS technique;FRTS,42
94,Regression Testing,Fine-Grained Test Minimization,A. Vahabzadeh; A. Stocco; A. Mesbah,"The University of British Columbia Faculty of Medicine, Vancouver, BC, CA; The University of British Columbia Faculty of Medicine, Vancouver, BC, CA; The University of British Columbia Faculty of Medicine, Vancouver, BC, CA",2018,"As a software system evolves, its test suite can accumulate redundancies over time. Test minimization aims at removing redundant test cases. However, current techniques remove whole test cases from the test suite using test adequacy criteria, such as code coverage. This has two limitations, namely (1) by removing a whole test case the corresponding test assertions are also lost, which can inhibit test suite effectiveness, (2) the issue of partly redundant test cases, i.e., tests with redundant test statements, is ignored. We propose a novel approach for fine-grained test case minimization. Our analysis is based on the inference of a test suite model that enables automated test reorganization within test cases. It enables removing redundancies at the test statement level, while preserving the coverage and test assertions of the test suite. We evaluated our approach, implemented in a tool called Testler, on the test suites of 15 open source projects. Our analysis shows that over 4,639 (24%) of the tests in these test suites are partly redundant, with over 11,819 redundant test statements in total. Our results show that Testler removes 43% of the redundant test statements, reducing the number of partly redundant tests by 52%. As a result, test suite execution time is reduced by up to 37% (20% on average), while maintaining the original statement coverage, branch coverage, test assertions, and fault detection capability.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453080,test minization;test reduction;test redundancy;test model,Production;Minimization;Redundancy;Analytical models;Computational modeling;Software engineering;Software systems,program testing,test minimization;test adequacy criteria;corresponding test assertions;test suite effectiveness;partly redundant test cases;fine-grained test case minimization;test suite model;test reorganization;test statement level;partly redundant tests;test suite execution time;redundant test statements,4
95,Regression Testing,FAST Approaches to Scalable Similarity-Based Test Case Prioritization,B. Miranda; E. Cruciani; R. Verdecchia; A. Bertolino,"Federal University of Pernambuco Recife, Brazil; Gran Sasso Science Institute, Lâ€™Aquila, Italy; Gran Sasso Science Institute, Lâ€™Aquila, Italy; ISTI - CNR, Pisa, Italy",2018,"Many test case prioritization criteria have been proposed for speeding up fault detection. Among them, similarity-based approaches give priority to the test cases that are the most dissimilar from those already selected. However, the proposed criteria do not scale up to handle the many thousands or even some millions test suite sizes of modern industrial systems and simple heuristics are used instead. We introduce the FAST family of test case prioritization techniques that radically changes this landscape by borrowing algorithms commonly exploited in the big data domain to find similar items. FAST techniques provide scalable similarity-based test case prioritization in both white-box and black-box fashion. The results from experimentation on real world C and Java subjects show that the fastest members of the family outperform other black-box approaches in efficiency with no significant impact on effectiveness, and also outperform white-box approaches, including greedy ones, if preparation time is not counted. A simulation study of scalability shows that one FAST technique can prioritize a million test cases in less than 20 minutes.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453081,locality sensitive hashing;minhashing;scalability;similarity;software testing;test case prioritization,Scalability;History;Fault detection;Big Data;Software testing;Data mining,fault diagnosis;greedy algorithms;Java;program testing,test case prioritization criteria;similarity-based approaches;test case prioritization techniques;FAST technique;scalable similarity-based test case prioritization;black-box fashion;black-box approaches;white-box approaches;FAST approaches,24
96,Regression Testing,Towards Refactoring-Aware Regression Test Selection,K. Wang; C. Zhu; A. Celik; J. Kim; D. Batory; M. Gligoric,"Iona College, The University of Texas at Austin; Iona College, The University of Texas at Austin; Iona College, The University of Texas at Austin; Iona College, The University of Texas at Austin; Iona College, The University of Texas at Austin; Iona College, The University of Texas at Austin",2018,"Regression testing checks that recent project changes do not break previously working functionality. Although important, regression testing is costly when changes are frequent. Regression test selection (RTS) optimizes regression testing by running only tests whose results might be affected by a change. Traditionally, RTS collects dependencies (e.g., on files) for each test and skips the tests, at a new project revision, whose dependencies did not change. Existing RTS techniques do not differentiate behavior-preserving transformations (i.e., refactorings) from other code changes. As a result, tests are run more frequently than necessary. We present the first step towards a refactoring-aware RTS technique, dubbed Reks, which skips tests affected only by behavior-preserving changes. Reks defines rules to update the test dependencies without running the tests. To ensure that Reks does not hide any bug introduced by the refactoring engines, we integrate Reks only in the pre-submit testing phase, which happens on the developers' machines. We evaluate Reks by measuring the savings in the testing effort. Specifically, we reproduce 100 refactoring tasks performed by developers of 37 projects on GitHub. Our results show that Reks would not run, on average, 33% of available tests (that would be run by a refactoring-unaware RTS technique). Additionally, we systematically run 27 refactoring types on ten projects. The results, based on 74,160 refactoring tasks, show that Reks would not run, on average, 16% of tests (max: 97% and SD: 24%). Finally, our results show that the Reks update rules are efficient.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453082,Regression test selection;Behavior-preserving changes;Reks,Testing;Computer bugs;Task analysis;Engines;Software;Google;Tools,program testing;regression analysis;software maintenance,Reks update rules;refactoring-aware regression test selection;regression testing checks;recent project changes;RTS techniques;code changes;refactoring-aware RTS technique;dubbed Reks;behavior-preserving changes;test dependencies;refactoring engines;testing effort;refactoring-unaware RTS technique;refactoring tasks,12
97,Open-Source Systems,Inheritance Usage Patterns in Open-Source Systems,J. Stevenson; M. Wood,"University of Strathclyde, Glasgow, Glasgow, GB; University of Strathclyde, Glasgow, Glasgow, GB",2018,"This research investigates how object-oriented inheritance is actually used in practice. The aim is to close the gap between inheritance guidance and inheritance practice. It is based on detailed analyses of 2440 inheritance hierarchies drawn from 14 open-source systems. The original contributions made by this paper concern pragmatic assessment of inheritance hierarchy design quality. The findings show that inheritance is very widely used but that most of the usage patterns that occur in practice are simple in structure. They are so simple that they may not require much inheritance-specific design consideration. On the other hand, the majority of classes defined using inheritance actually appear within a relatively small number of large, complex hierarchies. While some of these large hierarchies appear to have a consistent structure, often based on a problem domain model or a design pattern, others do not. Another contribution is that the quality of hierarchies, especially the large problematic ones, may be assessed in practice based on size, shape, and the definition and invocation of novel methods - all properties that can be detected automatically.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453083,Object-oriented;inheritance;open source;empirical;design guidance,Software engineering;Gold,inheritance;object-oriented methods;public domain software,inheritance hierarchies;complex hierarchies;inheritance-specific design consideration;inheritance hierarchy design quality;14 open-source systems;inheritance guidance;object-oriented inheritance;inheritance usage patterns,
98,Open-Source Systems,Almost There: A Study on Quasi-Contributors in Open-Source Software Projects,I. Steinmacher; G. Pinto; I. S. Wiese; M. A. Gerosa,NA; NA; NA; NA,2018,"Recent studies suggest that well-known OSS projects struggle to find the needed workforce to continue evolving-in part because external developers fail to overcome their first contribution barriers. In this paper, we investigate how and why quasi-contributors (external developers who did not succeed in getting their contributions accepted to an OSS project) fail. To achieve our goal, we collected data from 21 popular, non-trivial GitHub projects, identified quasi-contributors, and analyzed their pull-requests. In addition, we conducted surveys with quasi-contributors, and projects' integrators, to understand their perceptions about nonacceptance.We found 10,099 quasi-contributors - about 70% of the total actual contributors - that submitted 12,367 non-accepted pull-requests. In five projects, we found more quasi-contributors than actual contributors. About one-third of the developers who took our survey disagreed with the nonacceptance, and around 30% declared the nonacceptance demotivated or prevented them from placing another pull-request. The main reasons for pull-request nonacceptance from the quasi-contributors' perspective were ""superseded/duplicated pull-request"" and ""mismatch between developer's and team's vision/opinion."" A manual analysis of a representative sample of 263 pull-requests corroborated with this finding. We also found reasons related to the relationship with the community and lack of experience or commitment from the quasi-contributors. This empirical study is particularly relevant to those interested in fostering developers' participation and retention in OSS communities.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453084,pull-requests;quasi contributors;newcomers;open source software,Open source software;Computer languages;Software engineering;Face;Analytical models;Encoding,project management;public domain software;software management,OSS project;open-source software projects;manual analysis;quasicontributors;GitHub projects,31
99,Open-Source Systems,[Journal First] Analyzing a Decade of Linux System Calls,M. Bagherzadeh; N. Kahani; C. -P. Bezemer; A. E. Hassan; J. Dingel; J. R. Cordy,"Queen's University Faculty of Health Sciences, Kingston, ON, CA; Queen's University Faculty of Health Sciences, Kingston, ON, CA; Queen's University Faculty of Health Sciences, Kingston, ON, CA; Queen's University Faculty of Health Sciences, Kingston, ON, CA; Queen's University Faculty of Health Sciences, Kingston, ON, CA; Queen's University Faculty of Health Sciences, Kingston, ON, CA",2018,"Over the past 25 years, thousands of developers have contributed more than 18 million lines of code (LOC) to the Linux kernel. As the Linux kernel forms the central part of various operating systems that are used by millions of users, the kernel must be continuously adapted to the changing demands and expectations of these users. The Linux kernel provides its services to an application through system calls. The combined set of all system calls forms the essential Application Programming Interface (API) through which an application interacts with the kernel. In this paper, we conduct an empirical study of 8,770 changes that were made to Linux system calls during the last decade (i.e., from April 2005 to December 2014). In particular, we study the size of the changes, and we manually identify the type of changes and bug fixes that were made. Our analysis provides an overview of the evolution of the Linux system calls over the last decade. We find that there was a considerable amount of technical debt in the kernel, that was addressed by adding a number of sibling calls (i.e., 26% of all system calls). In addition, we find that by far, the ptraceand signal handling system calls are the most challenging to maintain. Our study can be used by developers who want to improve the design and ensure the successful evolution of their own kernel APIs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453085,Linux kernel;System calls;API evolution;Software evolution,Linux;Kernel;Computer bugs;Software engineering;Maintenance engineering;Testing,application program interfaces;Linux;operating system kernels;program debugging,essential application programming interface;kernel API;sibling calls;operating systems;Linux kernel forms;Linux system calls,
100,Open-Source Systems,To Distribute or Not to Distribute? Why Licensing Bugs Matter,C. Vendome; D. German; M. Di Penta; G. Bavota; M. Linares-VÃ¡squez; D. Poshyvanyk,"Coll. of William & Mary Williamsburg, Williamsburg, VA, USA; University of Victoria, Victoria, BC, CA; Universita degli Studi del Sannio, Benevento, Campania, IT; Universita della Svizzera Italiana, Lugano, TI, CH; Universidad de los Andes, Bogota, CO; Coll. of William & Mary Williamsburg, Williamsburg, VA, USA",2018,"Software licenses dictate how source code or binaries can be modified, reused, and redistributed. In the case of open source projects, software licenses generally fit into two main categories, permissive and restrictive, depending on the degree to which they allow redistribution or modification under licenses different from the original one(s). Developers and organizations can also modify existing licenses, creating custom licenses with specific permissive/restrictive terms. Having such a variety of software licenses can create confusion among software developers, and can easily result in the introduction of licensing bugs, not necessarily limited to well-known license incompatibilities. In this work, we report a study aimed at characterizing licensing bugs by (i) building a catalog categorizing the types of licensing bugs developers and other stakeholders face, and (ii) understanding the implications licensing bugs have on the software projects they affect. The presented study is the result of the manual analysis of 1,200 discussions related to licensing bugs carried out in issue trackers and in five legal mailing lists of open source communities. Our findings uncover new types of licensing bugs not addressed in prior literature, and a detailed assessment of their implications.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453086,Software Licenses;Empirical Studies;Open Source Practices,Licenses;Computer bugs;Software;Law;Guidelines;Stakeholders,program debugging;project management;public domain software;software engineering,software projects;software licenses;custom licenses;software developers;license incompatibilities;licensing bug developers,3
101,Test Generation,Augusto: Exploiting Popular Functionalities for the Generation of Semantic GUI Tests with Oracles,L. Mariani; M. PezzÃ¨; D. Zuddas,NA; NA; NA,2018,"Testing software applications by interacting with their graphical user interface (GUI) is an expensive and complex process. Current automatic test case generation techniques implement explorative approaches that, although producing useful test cases, have a limited capability of covering semantically relevant interactions, thus frequently missing important testing scenarios. These techniques typically interact with the available widgets following the structure of the GUI, without any guess about the functions that are executed. In this paper we propose Augusto, a test case generation technique that exploits a built-in knowledge of the semantics associated with popular and well-known functionalities, such as CRUD operations, to automatically generate effective test cases with automated functional oracles. Empirical results indicate that Augusto can reveal faults that cannot be revealed with state of the art techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453087,GUI testing;automatic test case generation;semantics;oracles,Graphical user interfaces;Semantics;Testing;Software;Authentication;Pattern matching;Adaptation models,graphical user interfaces;program testing,useful test cases;semantically relevant interactions;important testing scenarios;Augusto;test case generation technique;semantics;effective test cases;automated functional oracles;popular functionalities;semantic GUI tests;software applications;graphical user interface;expensive process;complex process;current automatic test case generation techniques,10
102,Test Generation,Towards Optimal Concolic Testing,X. Wang; J. Sun; Z. Chen; P. Zhang; J. Wang; Y. Lin,NA; NA; NA; NA; NA; NA,2018,"Concolic testing integrates concrete execution (e.g., random testing) and symbolic execution for test case generation. It is shown to be more cost-effective than random testing or symbolic execution sometimes. A concolic testing strategy is a function which decides when to apply random testing or symbolic execution, and if it is the latter case, which program path to symbolically execute. Many heuristics-based strategies have been proposed. It is still an open problem what is the optimal concolic testing strategy. In this work, we make two contributions towards solving this problem. First, we show the optimal strategy can be defined based on the probability of program paths and the cost of constraint solving. The problem of identifying the optimal strategy is then reduced to a model checking problem of Markov Decision Processes with Costs. Secondly, in view of the complexity in identifying the optimal strategy, we design a greedy algorithm for approximating the optimal strategy. We conduct two sets of experiments. One is based on randomly generated models and the other is based on a set of C programs. The results show that existing heuristics have much room to improve and our greedy algorithm often outperforms existing heuristics.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453088,Concolic Testing;Markov Chain;Test Case Generation,Greedy algorithms;Cost accounting;Sun;Model checking;Probabilistic logic;Systematics,greedy algorithms;Markov processes;program diagnostics;program testing,concrete execution;random testing;test case generation;symbolic execution;program path;heuristics-based strategies;optimal concolic testing strategy;optimal strategy;optimal concolic testing,20
103,Test Generation,DeepTest: Automated Testing of Deep-Neural-Network-Driven Autonomous Cars,Y. Tian; K. Pei; S. Jana; B. Ray,University of Virginia; Columbia University; Columbia University; University of Virginia,2018,"Recent advances in Deep Neural Networks (DNNs) have led to the development of DNN-driven autonomous cars that, using sensors like camera, LiDAR, etc., can drive without any human intervention. Most major manufacturers including Tesla, GM, Ford, BMW, and Waymo/Google are working on building and testing different types of autonomous vehicles. The lawmakers of several US states including California, Texas, and New York have passed new legislation to fast-track the process of testing and deployment of autonomous vehicles on their roads. However, despite their spectacular progress, DNNs, just like traditional software, often demonstrate incorrect or unexpected corner-case behaviors that can lead to potentially fatal collisions. Several such real-world accidents involving autonomous cars have already happened including one which resulted in a fatality. Most existing testing techniques for DNN-driven vehicles are heavily dependent on the manual collection of test data under different driving conditions which become prohibitively expensive as the number of test conditions increases. In this paper, we design, implement, and evaluate DeepTest, a systematic testing tool for automatically detecting erroneous behaviors of DNN-driven vehicles that can potentially lead to fatal crashes. First, our tool is designed to automatically generated test cases leveraging real-world changes in driving conditions like rain, fog, lighting conditions, etc. DeepTest systematically explore different parts of the DNN logic by generating test inputs that maximize the numbers of activated neurons. DeepTest found thousands of erroneous behaviors under different realistic driving conditions (e.g., blurring, rain, fog, etc.) many of which lead to potentially fatal crashes in three top performing DNNs in the Udacity self-driving car challenge.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453089,deep learning;testing;self-driving cars;deep neural networks;autonomous vehicle;neuron coverage,Neurons;Autonomous automobiles;Software;Testing;Automobiles;Sensors;Computer architecture,automobiles;intelligent transportation systems;neural nets;program testing;road accidents;road safety,DeepTest;automated testing;deep-neural-network-driven autonomous cars;recent advances;DNN-driven autonomous cars;human intervention;autonomous vehicles;US states;potentially fatal collisions;DNN-driven vehicles;test conditions increases;systematic testing tool;erroneous behaviors;automatically generated test cases;DNN logic;test inputs;potentially fatal crashes;Udacity self-driving car challenge;deep neural networks;driving conditions;realistic driving conditions;Waymo-Google,359
104,Test Generation,Precise Concolic Unit Testing of C Programs Using Extended Units and Symbolic Alarm Filtering,Y. Kim; Y. Choi; M. Kim,"Korea Advanced Institute of Science and Technology, Daejeon, Daejeon, KR; Kyungpook National University, Daegu, Daegu, KR; Korea Advanced Institute of Science and Technology, Daejeon, Daejeon, KR",2018,"Automated unit testing reduces manual effort to write unit test drivers/stubs and generate unit test inputs. However, automatically generated unit test drivers/stubs raise false alarms because they often over-approximate real contexts of a target function f and allow infeasible executions off. To solve this problem, we have developed a concolic unit testing technique CONBRIO. To provide realistic context to f, it constructs an extended unit of f that consists of f and closely relevant functions to f. Also, CONBRIO filters out a false alarm by checking feasibility of a corresponding symbolic execution path with regard to f's symbolic calling contexts obtained by combining symbolic execution paths of f's closely related predecessor functions. In the experiments on the crash bugs of 15 real-world C programs, CONBRIO shows both high bug detection ability (i.e. 91.0% of the target bugs detected) and high precision (i.e. a true to false alarm ratio is 1:4.5). Also, CONBRIO detects 14 new bugs in 9 target C programs studied in papers on crash bug detection techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453090,automated unit testing;unit test driver/stub generation;dynamic symbolic execution;false alarm reduction,Testing;Computer bugs;Filtering;Manuals;Software engineering;Space exploration,C language;program debugging;program diagnostics;program verification,symbolic alarm filtering;automated unit testing;manual effort;unit test inputs;automatically generated unit test drivers/stubs;over-approximate real contexts;infeasible executions;technique CONBRIO;closely relevant functions;corresponding symbolic execution path;symbolic calling contexts;symbolic execution paths;predecessor functions;false alarm ratio;crash bug detection techniques;C program precise concolic unit testing,3
105,Program Reduction Techniques,Spatio-Temporal Context Reduction: A Pointer-Analysis-Based Static Approach for Detecting Use-After-Free Vulnerabilities,H. Yan; Y. Sui; S. Chen; J. Xue,"University of New South Wales, Sydney, NSW, AU; University of Technology Sydney, Sydney, NSW, AU; Commonwealth Scientific and Industrial Research Organisation, Canberra, ACT, AU; University of New South Wales, Sydney, NSW, AU",2018,"Zero-day Use-After-Free (UAF) vulnerabilities are increasingly popular and highly dangerous, but few mitigations exist. We introduce a new pointer-analysis-based static analysis, CRed, for finding UAF bugs in multi-MLOC C source code efficiently and effectively. Cred achieves this by making three advances: (i) a spatio-temporal context reduction technique for scaling down soundly and precisely the exponential number of contexts that would otherwise be considered at a pair of free and use sites, (ii) a multi-stage analysis for filtering out false alarms efficiently, and (iii) a path-sensitive demand-driven approach for finding the points-to information required. We have implemented CRed in LLVM-3.8.0 and compared it with four different state-of-the-art static tools: CBMC (model checking), Clang (abstract interpretation), Coccinelle (pattern matching), and Supa (pointer analysis) using all the C test cases in Juliet Test Suite (JTS) and 10 open-source C applications. For the ground-truth validated with JTS, CRed detects all the 138 known UAF bugs as CBMC and Supa do while Clang and Coccinelle miss some bugs, with no false alarms from any tool. For practicality validated with the 10 applications (totaling 3+ MLOC), CRed reports 132 warnings including 85 bugs in 7.6 hours while the existing tools are either unscalable by terminating within 3 days only for one application (CBMC) or impractical by finding virtually no bugs (Clang and Coccinelle) or issuing an excessive number of false alarms (Supa).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453091,use after free;program analysis;bug detection,Computer bugs;Tools;Correlation;Australia;Pattern matching;Software;Static analysis,C language;pattern matching;program compilers;program debugging;program diagnostics;program verification,Coccinelle;false alarms;pointer-analysis-based static approach;zero-day Use-After-Free vulnerabilities;pointer-analysis-based static analysis;finding UAF bugs;multiMLOC C source code;spatio-temporal context reduction technique;multistage analysis;path-sensitive demand-driven approach;CBMC;Supa;Juliet Test Suite;open-source C applications;UAF bugs,18
106,Program Reduction Techniques,Program Splicing,Y. Lu; S. Chaudhuri; C. Jermaine; D. Melski,"Rice University, Houston, TX, US; Rice University, Houston, TX, US; Rice University, Houston, TX, US; Grammatech Inc., Ithaca, NY, USA",2018,"We introduce program splicing, a programming methodology that aims to automate the work ow of copying, pasting, and modifying code available online. Here, the programmer starts by writing a ""draft"" that mixes un nished code, natural language comments, and correctness requirements. A program synthesizer that interacts with a large, searchable database of program snippets is used to automatically complete the draft into a program that meets the re-quirements. The synthesis process happens in two stages. First, the synthesizer identi es a small number of programs in the database that are relevant to the synthesis task. Next it uses an enumerative search to systematically ll the draft with expressions and statements from these relevant programs. The resulting program is returned to the programmer, who can modify it and possibly invoke additional rounds of synthesis. We present an implementation of program splicing, called Splicer, for the Java programming language. Splicer uses a corpus of over 3.5 million procedures from an open-source software repository. Our evaluation uses the system in a suite of everyday programming tasks, and includes a comparison with a state-of-the-art competing approach as well as a user study. The results point to the broad scope and scalability of program splicing and indicate that the approach can signi cantly boost programmer productivity.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453092,Big Data;Program Synthesis,Splicing;Task analysis;Databases;Synthesizers;Programming;Java;Software engineering,Java;public domain software;software engineering,program splicing;programming methodology;program synthesizer;program snippets;relevant programs;resulting program;Java programming language;everyday programming tasks,
107,Program Reduction Techniques,Chopped Symbolic Execution,D. Trabish; A. Mattavelli; N. Rinetzky; C. Cadar,"Tel Aviv University, Tel Aviv, IL; Tel Aviv University, Israel; Imperial College London, United Kingdom; Tel Aviv University, Israel",2018,"Symbolic execution is a powerful program analysis technique that systematically explores multiple program paths. However, despite important technical advances, symbolic execution often struggles to reach deep parts of the code due to the well-known path explosion problem and constraint solving limitations. In this paper, we propose chopped symbolic execution, a novel form of symbolic execution that allows users to specify uninter-esting parts of the code to exclude during the analysis, thus only targeting the exploration to paths of importance. However, the excluded parts are not summarily ignored, as this may lead to both false positives and false negatives. Instead, they are executed lazily, when their effect may be observable by code under anal-ysis. Chopped symbolic execution leverages various on-demand static analyses at runtime to automatically exclude code fragments while resolving their side effects, thus avoiding expensive manual annotations and imprecision. Our preliminary results show that the approach can effectively improve the effectiveness of symbolic execution in several different scenarios, including failure reproduction and test suite augmentation.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453093,Symbolic execution;Static analysis;Program slicing,Explosions;Engines;Computer bugs;Static analysis;Runtime;Software,program diagnostics;program testing;program verification,multiple program paths;chopped symbolic execution leverages;powerful program analysis technique;constraint solving limitations,24
108,Program Reduction Techniques,Perses: Syntax-Guided Program Reduction,C. Sun; Y. Li; Q. Zhang; T. Gu; Z. Su,"University of California Davis, Davis, CA, US; University of California Davis, Davis, CA, US; University of California Davis, Davis, CA, US; University of California Davis, Davis, CA, US; University of California Davis, Davis, CA, US",2018,"Given a program P that exhibits a certain property Ïˆ (e.g., a C program that crashes GCC when it is being compiled), the goal of program reduction is to minimize P to a smaller variant P? that still exhibits the same property, i.e., Ïˆ(P'). Program reduction is important and widely demanded for testing and debugging. For example, all compiler/interpreter development projects need effective program reduction to minimize failure-inducing test programs to ease debugging. However, state-of-the-art program reduction techniques - notably Delta Debugging (DD), Hierarchical Delta Debugging (HDD), and C-Reduce - do not perform well in terms of speed (reduction time) and quality (size of reduced programs), or are highly customized for certain languages and thus lack generality. This paper presents Perses, a novel framework for effective, efficient, and general program reduction. The key insight is to exploit, in a general manner, the formal syntax of the programs under reduction and ensure that each reduction step considers only smaller, syntactically valid variants to avoid futile efforts on syntactically invalid variants. Our framework supports not only deletion (as for DD and HDD), but also general, effective program transformations. We have designed and implemented Perses, and evaluated it for two language settings: C and Java. Our evaluation results on 20 C programs triggering bugs in GCC and Clang demonstrate Perses's strong practicality compared to the state-of-the-art: (1) smaller size - Perses's results are respectively 2% and 45% in size of those from DD and HDD; and (2) shorter reduction time - Perses takes 23% and 47% time taken by DD and HDD respectively. Even when compared to the highly customized and optimized C-Reduce for C/C++, Perses takes only 38-60% reduction time.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453094,program reduction;delta debugging;debugging,Grammar;Debugging;Syntactics;Computer bugs;Program processors;Sun;Java,Java;program compilers;program debugging;program testing,DD;HDD;shorter reduction time - Perses;syntax-guided program reduction;C program;effective program reduction;failure-inducing test programs;reduced programs;general program reduction;reduction step;general program transformations;effective program transformations;compiler-interpreter development projects;program reduction techniques;hierarchical Delta debugging;efficient program reduction,13
109,"Security, Privacy and Trust 1",Secure Coding Practices in Java: Challenges and Vulnerabilities,N. Meng; S. Nagy; D. Yao; W. Zhuang; G. Arango-Argoty,"Virginia Polytechnic Institute and State University, Blacksburg, VA, US; Virginia Polytechnic Institute and State University, Blacksburg, VA, US; Virginia Polytechnic Institute and State University, Blacksburg, VA, US; Virginia Polytechnic Institute and State University, Blacksburg, VA, US; Virginia Polytechnic Institute and State University, Blacksburg, VA, US",2018,"The Java platform and its third-party libraries provide useful features to facilitate secure coding. However, misusing them can cost developers time and effort, as well as introduce security vulnerabilities in software. We conducted an empirical study on StackOverflow posts, aiming to understand developers' concerns on Java secure coding, their programming obstacles, and insecure coding practices. We observed a wide adoption of the authentication and authorization features provided by Spring Security - a third-party framework designed to secure enterprise applications. We found that programming challenges are usually related to APIs or libraries, including the complicated cross-language data handling of cryptography APIs, and the complex Java-based or XML-based approaches to configure Spring Security. In addition, we reported multiple security vulnerabilities in the suggested code of accepted answers on the StackOverflow forum. The vulnerabilities included disabling the default protection against Cross-Site Request Forgery (CSRF) attacks, breaking SSL/TLS security through bypassing certificate validation, and using insecure cryptographic hash functions. Our findings reveal the insufficiency of secure coding assistance and documentation, as well as the huge gap between security theory and coding practices.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453095,Secure coding;Spring security;CSRF;SSL/TLS;certificate validation;cryptographic hash functions;authentication;authorization;StackOverflow;cryptography,Java;Cryptography;Encoding;Libraries;Programming;Authentication,application program interfaces;authorisation;computer crime;cryptography;Java;XML,secure coding practices;Java platform;third-party libraries;developers time;empirical study;StackOverflow posts;Java secure coding;programming obstacles;insecure coding practices;authorization features;third-party framework;programming challenges;complicated cross-language data handling;complex Java-based;XML-based approaches;cross-site request forgery attacks;Spring security;cryptography API;SSL-TLS security;secure coding assistance;StackOverflow forum;multiple security vulnerabilities,43
110,"Security, Privacy and Trust 1",EnMobile: Entity-Based Characterization and Analysis of Mobile Malware,W. Yang; M. Prasad; T. Xie,"NA; Fujitsu Labs. of America, Sunnyvale, CA, USA; NA",2018,"Modern mobile malware tend to conduct their malicious exploits through sophisticated patterns of interactions that involve multiple entities, e.g., the mobile platform, human users, and network locations. Such malware often evade the detection by existing approaches due to their limited expressiveness and accuracy in characterizing and detecting these malware. To address these issues, in this paper, we recognize entities in the environment of an app, the app's interactions with such entities, and the provenance of these interactions, i.e., the intent and ownership of each interaction, as the key to comprehensively characterizing modern mobile apps, and mobile malware in particular. With this insight, we propose a novel approach named EnMobile including a new entity-based characterization of mobile-app behaviors, and corresponding static analyses, to accurately characterize an app's interactions with entities. We implement EnMobile and provide a practical application of EnMobile in a signature-based scheme for detecting mobile malware. We evaluate EnMobile on a set of 6614 apps consisting of malware from Genome and Drebin along with benign apps from Google Play. Our results show that EnMobile detects malware with substantially higher precision and recall than four state-of-the-art approaches, namely Apposcopy, Drebin, MUDFLOW, and AppContext.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453096,Program Analysis;Mobile Security,Malware;Servers;Security;Databases;Static analysis;Feature extraction,invasive software;mobile computing;program diagnostics,mobile-app behaviors;EnMobile;benign apps;entity-based characterization;modern mobile malware;mobile platform;modern mobile apps,7
111,"Security, Privacy and Trust 1",[Journal First] Model Comprehension for Security Risk Assessment: An Empirical Comparison of Tabular vs. Graphical Representations,K. Labunets; F. Massacci; F. Paci; S. Marczak; F. M. de Oliveira,"Technische Universiteit Delft, Delft, Zuid-Holland, NL; Universita degli Studi di Trento, Trento, Trentino-Alto Adige, IT; University of Southampton, Southampton, Hampshire, GB; Pontificia Universidade Catolica do Rio Grande do Sul, Porto Alegre, RS, BR; Pontificia Universidade Catolica do Rio Grande do Sul, Porto Alegre, RS, BR",2018,"Context: Tabular and graphical representations are used to communicate security risk assessments for IT systems. However, there is no consensus on which type of representation better supports the comprehension of risks (such as the relationships between threats, vulnerabilities and security controls). Vessey's cognitive fit theory predicts that graphs should be better because they capture spatial relationships. Method: We report the results of two studies performed in two countries with 69 and 83 participants respectively, in which we assessed the effectiveness of tabular and graphical representations concerning the extraction of correct information about security risks. Results: Participants who applied tabular risk models gave more precise and complete answers to the comprehension questions when requested to find simple and complex information about threats, vulnerabilities, or other elements of the risk models. Conclusions: Our findings can be explained by Vessey's cognitive fit theory as tabular models implicitly capture elementary linear spatial relationships. Interest for ICSE: It is almost taken for granted in Software Engineering that graphical-, diagram-based models are ""the"" way to go (e.g., the SE Body of Knowledge). This paper provides some experimental-based doubts that this might not always be the case. It will provide an interesting debate that might ripple to traditional requirements and design notations outside security.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453097,Empirical Study;Security Risk Assessment;Risk Modeling;Comprehensibility;Cognitive Fit,Security;Task analysis;Unified modeling language;Software engineering;Risk management;Data mining;Complexity theory,risk analysis;risk management;security of data;software engineering;solid modelling,comprehension questions;Vessey's cognitive fit theory;elementary linear spatial relationships;diagram-based models;security risk assessment;graphical representations;security controls;security risks;tabular risk models;model comprehension;Software Engineering,
112,"Security, Privacy and Trust 1",[Journal First] Privacy by Designers: Software Developers' Privacy Mindset,I. Hadar; T. Hasson; O. Ayalon; E. Toch; M. Birnhack; S. Sherman; A. Balissa,"University of Haifa, Haifa, Haifa, IL; University of Haifa, Haifa, Haifa, IL; Tel Aviv University, Tel Aviv, IL; Tel Aviv University, Tel Aviv, IL; Tel Aviv University, Tel Aviv, IL; University of Haifa, Haifa, Haifa, IL; Tel Aviv University, Tel Aviv, IL",2018,"Privacy by design (PbD) is a policy measure that guides software developers to apply inherent solutions to achieve better privacy protection. For PbD to be a viable option, it is important to understand developers' perceptions, interpretation and practices as to informational privacy (or data protection). To this end, we conducted in-depth interviews with 27 developers from different domains, who practice software design. Grounded analysis of the data revealed an interplay between several different forces affecting the way in which developers handle privacy concerns. Borrowing the schema of Social Cognitive Theory (SCT), we classified and analyzed the cognitive, organizational and behavioral factors that play a role in developers' privacy decision making. Our findings indicate that developers use the vocabulary of data security to approach privacy challenges, and that this vocabulary limits their perceptions of privacy mainly to third-party threats coming from outside of the organization; that organizational privacy climate is a powerful means for organizations to guide developers toward particular practices of privacy; and that software architectural patterns frame privacy solutions that are used throughout the development process, possibly explaining developers' preference of policy-based solutions to architectural solutions. Further, we show, through the use of the SCT schema for framing the findings of this study, how a theoretical model of the factors that influence developers' privacy practices can be conceptualized and used as a guide for future research toward effective implementation of PbD.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453098,Data protection;Privacy;Privacy by design;Qualitative research;Grounded analysis;Social cognitive theory;Organizational climate,Privacy;Data privacy;Meteorology;Software;Information systems;Vocabulary;Software engineering,cognition;data privacy;decision making;security of data;software architecture,decision making;privacy by design;organizational privacy;data protection;software development process;software architectural patterns;informational privacy;privacy protection;PbD;policy-based solutions;data security;behavioral factors;Social Cognitive Theory;software design,2
113,Empirical Software Engineering,Does the Propagation of Artifact Changes Across Tasks Reflect Work Dependencies?,C. Mayr-Dorn; A. Egyed,"Johannes Kepler Universitat Linz, Linz, AT; Johannes Kepler Universitat Linz, Linz, AT",2018,"Developers commonly define tasks to help coordinate software development efforts--whether they be feature implementation, refactoring, or bug fixes. Developers establish links between tasks to express implicit dependencies that needs explicit handling--dependencies that often require the developers responsible for a given task to assess how changes in a linked task affect their own work and vice versa (i.e., change propagation). While seemingly useful, it is unknown if change propagation indeed coincides with task links. No study has investigated to what extent change propagation actually occurs between task pairs and whether it is able to serve as a metric for characterizing the underlying task dependency. In this paper, we study the temporal relationship between developer reading and changing of source code in relationship to task links. We identify seven situations that explain the varying correlation of change propagation with linked task pairs and find six motifs describing when change propagation occurs between non-linked task pairs. Our paper demonstrates that task links are indeed useful for recommending which artifacts to monitor for changes, which developers to involve in a task, or which tasks to inspect.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453099,task links;change propagation;bugzilla;mylyn;empirical study,Licenses,project management;software engineering;software maintenance,implicit dependencies;task links;extent change propagation;underlying task dependency;linked task pairs;nonlinked task pairs;software development efforts;explicit handling,2
114,Empirical Software Engineering,Large-Scale Analysis of Framework-Specific Exceptions in Android Apps,L. Fan; T. Su; S. Chen; G. Meng; Y. Liu; L. Xu; G. Pu; Z. Su,"East China Normal University, Shanghai, CN; Nanyang Technological University, Singapore, Singapore, SG; East China Normal University, Shanghai, CN; Institute of Information Engineering Chinese Academy of Sciences, Beijing, Beijing, CN; Nanyang Technological University, Singapore, Singapore, SG; East China Normal University, Shanghai, CN; East China Normal University, Shanghai, CN; University of California Davis, Davis, CA, US",2018,"Mobile apps have become ubiquitous. For app developers, it is a key priority to ensure their apps' correctness and reliability. However, many apps still suffer from occasional to frequent crashes, weakening their competitive edge. Large-scale, deep analyses of the characteristics of real-world app crashes can provide useful insights to guide developers, or help improve testing and analysis tools. However, such studies do not exist - this paper fills this gap. Over a four-month long effort, we have collected 16,245 unique exception traces from 2,486 open-source Android apps, and observed that framework-specific exceptions account for the majority of these crashes. We then extensively investigated the 8,243 framework-specific exceptions (which took six person-months): (1) identifying their characteristics (e.g., manifestation locations, common fault categories), (2) evaluating their manifestation via state-of-the-art bug detection techniques, and (3) reviewing their fixes. Besides the insights they provide, these findings motivate and enable follow-up research on mobile apps, such as bug detection, fault localization and patch generation. In addition, to demonstrate the utility of our findings, we have optimized Stoat, a dynamic testing tool, and implemented ExLocator, an exception localization tool, for Android apps. Stoat is able to quickly uncover three previously-unknown, confirmed/fixed crashes in Gmail and Google+; ExLocator is capable of precisely locating the root causes of identified exceptions in real-world apps. Our substantial dataset is made publicly available to share with and benefit the community.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453100,Empirical study;mobile app bugs;testing;static analysis,Testing;Androids;Humanoid robots;Computer bugs;Tools;Google,Android (operating system);dynamic testing;mobile computing;program debugging;program testing;public domain software,unique exception;open-source Android apps;framework-specific exceptions;bug detection techniques;real-world apps;identified exceptions;confirmed/fixed crashes;exception localization tool;dynamic testing tool;mobile apps;four-month long effort;analysis tools;real-world app crashes;frequent crashes;app developers,46
115,Empirical Software Engineering,[Journal First] Effect Sizes and their Variance for AB/BA Crossover Design Studies,L. Madeyski; B. Kitchenham,"Fac. of Comput. Sci. & Manage., Wroclaw Univ. of Sci. & Technol., Wroclaw, Poland; Keele University, Keele, Staffordshire, GB",2018,"We addressed the issues related to repeated measures experimental design such as an AB/BA crossover design (where each participant uses each method) that have been neither discussed nor addressed in the software engineering literature. Firstly, there are potentially two different standardized mean difference effect sizes that can be calculated, depending on whether the mean difference is standardized by the pooled within groups variance or the within-participants variance. Hence, we provided equations for non-standardized and standardized effect sizes and explained the need for two different types of standardized effect size, one for the repeated measures and one that would be equivalent to an independent groups design. Secondly, as for any estimated parameters and also for the purposes of undertaking meta-analysis, it is necessary to calculate the variance of the standardized mean difference effect sizes (which is not the same as the variance of the study). Hence, we provided formulas for the small sample size effect size variance and the medium sample size approximation to the effect size variance, for both types of standardized effect size. We also presented the model underlying the AB/BA crossover design and provided two examples (an empirical analysis of the real data set by Scanniello, as well as simulated data) to demonstrate how to construct the two standardized mean difference effect sizes and their variances, both from standard descriptive statistics and from the outputs provided by the linear mixed model package lme4 in R. A conclusion is that crossover designs should be considered (instead of between groups design) only if: Â· previous research has suggested that Ï is greater than zero and preferably greater than 0.25; Â· there is either strong theoretical argument, or empirical evidence from a well-powered study, that the period by technique interaction is negligible. Summarizing, our journal first paper [3]: (1) Presents the formulas needed to calculate both non-standardized and standardized mean difference effect sizes for AB/BA crossover designs (see Section 4 and 5 of our paper [3]). (2) Presents the formulas needed to estimate the variances of the non-standardized and standardized effect sizes which in the later cases need to be appropriate for the small to medium sample sizes commonly used in software engineering crossover designs (see Section 5 of our paper [3]). (3) Explains how to calculate the effect sizes and their variances both from the descriptive statistics that should be reported and from the raw data (see Section 6 of our paper [3]). It is worth mentioning that we based our formulas on our own corrections to the formulas presented earlier by Curtin et al. [1]. Our corrections for the variances of standardized weighted mean difference of an AB/BA cross-over trial were accepted by the author of the original formulas (Curtin), submitted jointly as a letter to Editor of Statistics in Medicine to assure the widespread (also beyond the software engineering domain) adoption of the corrected formulas, and accepted [2]. We proposed an alternative formulation of the standardized effect size for individual difference effects that is comparable with the standardized effect size commonly used for pretest/posttest studies. We also corrected the small sample size and moderate sample size variances reported by Curtin et al. for both the individual difference effect size and the standardized effect size comparable to independent groups trials, showing the derivation of the formulas from the variance of at-variable. Using these results, researchers can now correctly calculate standardized effect size variances, allowing the calculation of confidence intervals for AB/BA cross-over trials, which in turn provides a direct link to null hypothesis testing and supports meta-analysis. Meta-analysts can now validly aggregate together results from independent groups, pretest/posttest and AB/BA cross-over trials. Last but not least, the presented contributions allow corrections of previously reported results.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453101,Empirical software engineering;Meta-analysis;Effect size,Software engineering;Size measurement;Analytical models;Data models;Standards;Clinical trials;Computer science,medical computing;software engineering;statistical analysis,standardized mean difference effect sizes;standardized weighted mean difference;sample size effect size variance;medium sample size approximation;difference effect size;AB-BA crossover design;software engineering literature;meta-analysis;standard descriptive statistics;linear mixed model package;null hypothesis testing;posttest studies;pretest studies;software engineering crossover designs,1
116,Empirical Software Engineering,A Large-Scale Empirical Study on the Effects of Code Obfuscations on Android Apps and Anti-Malware Products,M. Hammad; J. Garcia; S. Malek,"Department of Informatics, University of California, Irvine Irvine, California, USA; Department of Informatics, University of California, Irvine Irvine, California, USA; Department of Informatics, University of California, Irvine Irvine, California, USA",2018,"The Android platform has been the dominant mobile platform in recent years resulting in millions of apps and security threats against those apps. Anti-malware products aim to protect smartphone users from these threats, especially from malicious apps. However, malware authors use code obfuscation on their apps to evade detection by anti-malware products. To assess the effects of code obfuscation on Android apps and anti-malware products, we have conducted a large-scale empirical study that evaluates the effectiveness of the top anti-malware products against various obfuscation tools and strategies. To that end, we have obfuscated 3,000 benign apps and 3,000 malicious apps and generated 73,362 obfuscated apps using 29 obfuscation strategies from 7 open-source, academic, and commercial obfuscation tools. The findings of our study indicate that (1) code obfuscation significantly impacts Android anti-malware products; (2) the majority of anti-malware products are severely impacted by even trivial obfuscations; (3) in general, combined obfuscation strategies do not successfully evade anti-malware products more than individual strategies; (4) the detection of anti-malware products depend not only on the applied obfuscation strategy but also on the leveraged obfuscation tool; (5) anti-malware products are slow to adopt signatures of malicious apps; and (6) code obfuscation often results in changes to an app's semantic behaviors.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453102,Android;empirical study;Security;Code Obfuscation;Anti malware products,Androids;Humanoid robots;Tools;Malware;Cryptography;Reflection;Java,Android (operating system);invasive software;smart phones,code obfuscation effects;code obfuscation effects;smartphone users;Android anti-malware products;Android apps,19
117,Empirical Software Engineering,[Journal First] An Empirical Study on the Interplay Between Semantic Coupling and Co-change of Software Classes,N. Ajienka; A. Capiluppi; S. Counsell,"Edge Hill University, Ormskirk, Lancashire, GB; Brunel University, Uxbridge, Middlesex, GB; Brunel University, Uxbridge, Middlesex, GB",2018,"The evolution of software systems is an inevitable process which has to be managed effectively to enhance software quality. Change impact analysis (CIA) is a technique that identifies impact sets, i.e., the set of classes that require correction as a result of a change made to a class or artefact. These sets can also be considered as ripple effects and typically non-local: changes propagate to different parts of a system. Two classes are considered logically coupled if they have co-changed in the past; past research has shown that the precision of CIA techniques increases if logical and semantic coupling (i.e., the extent to which the lexical content of two classes is related) are both considered. However, the relationship between semantic and logical coupling of software artefacts has not been extensively studied and no dependencies established between these two types of coupling. Are two often co-changed artefacts also strongly connected from a semantic point of view? Are two semantically similar artefacts bound to co-change in the future? Answering those questions would help increase the precision of CIA. It would also help software maintainers to focus on a smaller subset of artefacts more likely to co-evolve in the future. This study investigated the relationship between semantic and logical coupling. Using Chi-squared statistical tests, we identified similarities in semantic coupling using class corpora and class identifiers. We then computed Spearman's rank correlation between semantic and logical coupling metrics for class pairs to detect whether semantic and logical relationships co-varied in OO software. Finally, we investigated the overlap between semantic and logical relationships by identifying the proportion of classes linked through both coupling types. Our empirical study and results were based on seventy-nine open-source software projects. Results showed that: (a) measuring the semantic similarity of classes by using their identifiers is computationally efficient; (b) using identifier-based coupling can be used interchangeably with semantic similarity based on their corpora, albeit not always; (c) no correlation between the strengths of semantic and change coupling was found. Finally, (d) a directional relationship between the two was identified; 70% of semantic dependencies are linked through change coupling but not vice versa. Based on our findings, we conclude that identifying more efficient methods of semantic coupling computation as well as a directional relationship between semantic and change dependencies could help to improve CIA methods that integrate semantic coupling information. This may also help to reveal implicit dependencies not captured by static source code analysis.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453103,Semantic coupling;co-change;Java software;metrics,Semantics;Couplings;Software;Computer science;Measurement;Software engineering;Correlation,object-oriented programming;software maintenance;software metrics;software quality;statistical analysis;statistical testing,directional relationship;semantic dependencies;semantic coupling computation;semantic coupling information;software classes;software systems;software quality;change impact analysis;CIA;software artefacts;semantically similar artefacts;software maintainers;class corpora;class identifiers;semantic coupling metrics;logical coupling metrics;semantic relationships;logical relationships;OO software;coupling types;open-source software projects;semantic similarity;identifier-based coupling,
118,Test Improvement,DeFlaker: Automatically Detecting Flaky Tests,J. Bell; O. Legunsen; M. Hilton; L. Eloussi; T. Yung; D. Marinov,"George Mason University, Fairfax, VA, US; University of Illinois at Urbana-Champaign, Urbana, IL, US; Carnegie Mellon University, Pittsburgh, PA, US; University of Illinois at Urbana-Champaign, Urbana, IL, US; University of Illinois at Urbana-Champaign, Urbana, IL, US; University of Illinois at Urbana-Champaign, Urbana, IL, US",2018,"Developers often run tests to check that their latest changes to a code repository did not break any previously working functionality. Ideally, any new test failures would indicate regressions caused by the latest changes. However, some test failures may not be due to the latest changes but due to non-determinism in the tests, popularly called flaky tests. The typical way to detect flaky tests is to rerun failing tests repeatedly. Unfortunately, rerunning failing tests can be costly and can slow down the development cycle. We present the first extensive evaluation of rerunning failing tests and propose a new technique, called DeFlaker, that detects if a test failure is due to a flaky test without rerunning and with very low runtime overhead. DeFlaker monitors the coverage of latest code changes and marks as flaky any newly failing test that did not execute any of the changes. We deployed DeFlaker live, in the build process of 96 Java projects on TravisCI, and found 87 previously unknown flaky tests in 10 of these projects. We also ran experiments on project histories, where DeFlaker detected 1,874 flaky tests from 4,846 failures, with a low false alarm rate (1.5%). DeFlaker had a higher recall (95.5% vs. 23%) of confirmed flaky tests than Maven's default flaky test detector.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453104,software testing;flaky tests;code coverage,Monitoring;Testing;Java;Software;Tools;Syntactics;Detectors,Java;program testing;public domain software;regression analysis,DeFlaker;test failure;unknown flaky tests,50
119,Test Improvement,DetReduce: Minimizing Android GUI Test Suites for Regression Testing,W. Choi; K. Sen; G. Necul; W. Wang,"University of California Berkeley, Berkeley, CA, US; University of California Berkeley, Berkeley, CA, US; University of California Berkeley, Berkeley, CA, US; University of Illinois System, Urbana, IL, US",2018,"In recent years, several automated GUI testing techniques for Android apps have been proposed. These tools have been shown to be effective in achieving good test coverage and in finding bugs without human intervention. Being automated, these tools typically run for a long time (say, for several hours), either until they saturate test coverage or until a testing time budget expires. Thus, these automated tools are not good at generating concise regression test suites that could be used for testing in incremental development of the apps and in regression testing. We propose a heuristic technique that helps create a small regression test suite for an Android app from a large test suite generated by an automated Android GUI testing tool. The key insight behind our technique is that if we can identify and remove some common forms of redundancies introduced by existing automated GUI testing tools, then we can drastically lower the time required to minimize a GUI test suite. We have implemented our algorithm in a prototype tool called DetReduce. We applied DetReduce to several Android apps and found that DetReduce reduces a test-suite by an average factor of16.9Ã— in size and14.7Ã— in running time. We also found that for a test suite generated by running SwiftHand and a randomized test generation algorithm for 8 hours, DetReduce minimizes the test suite in an average of 14.6 hours.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453105,Android;GUI;Test minimization,Graphical user interfaces;Testing;Tools;Androids;Humanoid robots;Redundancy;Computer bugs,Android (operating system);graphical user interfaces;program testing,Android GUI test suites;randomized test generation algorithm;GUI test suite;testing tools;automated Android GUI testing tool;regression test suite;concise regression test suites;automated tools;testing time budget;Android app;automated GUI testing techniques;regression testing;DetReduce,10
120,Test Improvement,Time to Clean Your Test Objectives,M. Marcozzi; S. Bardin; N. Kosmatov; M. Papadakis; V. Prevosto; L. Correnson,"Imperial College London, London, London, GB; Commissariat a l'energie atomique et aux energies alternatives Siege administratif, Gif-sur-Yvette, ÃƒÅ½le-de-France, FR; Commissariat a l'energie atomique et aux energies alternatives Siege administratif, Gif-sur-Yvette, ÃƒÅ½le-de-France, FR; Universite du Luxembourg, Luxembourg, LU; Commissariat a l'energie atomique et aux energies alternatives Siege administratif, Gif-sur-Yvette, ÃƒÅ½le-de-France, FR; Commissariat a l'energie atomique et aux energies alternatives Siege administratif, Gif-sur-Yvette, ÃƒÅ½le-de-France, FR",2018,"Testing is the primary approach for detecting software defects. A major challenge faced by testers lies in crafting efficient test suites, able to detect a maximum number of bugs with manageable effort. To do so, they rely on coverage criteria, which define some precise test objectives to be covered. However, many common criteria specify a significant number of objectives that occur to be infeasible or redundant in practice, like covering dead code or semantically equal mutants. Such objectives are well-known to be harmful to the design of test suites, impacting both the efficiency and precision of the tester's effort. This work introduces a sound and scalable technique to prune out a significant part of the infeasible and redundant objectives produced by a panel of white-box criteria. In a nutshell, we reduce this task to proving the validity of logical assertions in the code under test. The technique is implemented in a tool that relies on weakest-precondition calculus and SMT solving for proving the assertions. The tool is built on top of the Frama-C verification platform, which we carefully tune for our specific scalability needs. The experiments reveal that the pruning capabilities of the tool can reduce the number of targeted test objectives in a program by up to 27% and scale to real programs of 200K lines, making it possible to automate a painstaking part of their current testing process.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453106,Coverage Criteria;Infeasible Objectives;Redundant Objectives,Tools;Software safety;Security;Software testing;Scalability,program diagnostics;program testing;program verification,significant number;dead code;semantically equal mutants;scalable technique;infeasible objectives;redundant objectives;white-box criteria;logical assertions;targeted test objectives;current testing process;primary approach;detecting software defects;efficient test suites;coverage criteria;precise test objectives,5
121,Test Improvement,Prioritizing Browser Environments for Web Application Test Execution,J. -H. Kwon; I. -Y. Ko; G. Rothermel,"Korea Advanced Institute of Science and Technology, Daejeon, Daejeon, KR; Korea Advanced Institute of Science and Technology, Daejeon, Daejeon, KR; University of Nebraska-Lincoln, Lincoln, NE, US",2018,"When testing client-side web applications, it is important to consider different web-browser environments. Different properties of these environments such as web-browser types and underlying platforms may cause a web application to exhibit different types of failures. As web applications evolve, they must be regression tested across these different environments. Because there are many environments to consider this process can be expensive, resulting in delayed feedback about failures in applications. In this work, we propose six techniques for providing a developer with faster feedback on failures when regression testing web applications across different web-browser environments. Our techniques draw on methods used in test case prioritization; however, in our case we prioritize web-browser environments, based on information on recent and frequent failures. We evaluated our approach using four non-trivial and popular open-source web applications. Our results show that our techniques outperform two baseline methods, namely, no ordering and random ordering, in terms of the cost-effectiveness. The improvement rates ranged from -12.24% to 39.05% for no ordering, and from -0.04% to 45.85% for random ordering.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453107,Web application testing;Regression testing;Browser environments,Browsers;Testing;History;Schedules;Optimal scheduling;Operating systems;Production,Internet;online front-ends;program testing,test case prioritization;application failures;testing client-side Web application;open-source Web applications;Web application test execution;Web-browser environments;regression testing Web applications;Web-browser types;feedback delay,1
122,Empirical Studies of Code,[Journal First] An Empirical Study of Early Access Games on the Steam Platform,D. Lin; C. -P. Bezemer; A. E. Hassan,"Queen's University Faculty of Health Sciences, Kingston, ON, CA; Queen's University Faculty of Health Sciences, Kingston, ON, CA; Queen's University Faculty of Health Sciences, Kingston, ON, CA",2018,"""Early access"" is a release strategy for software that allows consumers to purchase an unfinished version of the software. In turn, consumers can influence the software development process by giving developers early feedback. This early access model has become increasingly popular through digital distribution platforms, such as Steam which is the most popular distribution platform for games. The plethora of options offered by Steam to communicate between developers and game players contribute to the popularity of the early access model. The early access model made a name for itself through several successful games, such as the DayZ game. The multiplayer survival-based game reached 400,000 sales during its first week as an early access game. However, the benefits of the early access model have been questioned as well. For instance, the Spacebase DF-9 game abandoned the early access stage unexpectedly, disappointing many players of the game. Shortly after abandoning the early access stage and terminating the development, twelve employees were laid off including the programmer and project lead. In this paper, we conduct an empirical study on 1,182 Early Access Games (EAGs) on the Steam platform to understand the characteristics, advantages and limitations of the early access model. We find that 15% of the games on Steam make use of the early access model, with the most popular EAG having as many as 29 million owners. 88% of the EAGs are classified by their developers as so-called ""indie"" games, indicating that most EAGs are developed by individual developers or small studios. We study the interaction between players and developers of EAGs and the Steam platform. We observe that on the one hand, developers update their games more frequently in the early access stage. On the other hand, the percentage of players that review a game during its early access stage is lower than the percentage of players that review the game after it leaves the early access stage. However, the average rating of the reviews is much higher during the early access stage, suggesting that players are more tolerant of imperfections in the early access stage. The positive review rate does not correlate with the length or the game update frequency of the early access stage. In addition, we discuss several learned lessons from the failure of an early access game. The main learned lesson from this failure is that the communication between the game developer and the players of the EAG is crucial. Players enjoy getting involved in the development of an early access game and they get emotionally involved in the decision-making about the game. Based on our findings, we suggest game developers to use the early access model as a method for eliciting early feedback and more positive reviews to attract additional new players. In addition, our findings suggest that developers can determine their release schedule without worrying about the length of the early access stage and the game update frequency during the early access stage.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453108,early access games;computer games;Steam,Games;Software;Software engineering;Decision making;Schedules;Lead;Computational modeling,computer games;sales management;software engineering,game developer;game players;Steam platform;software development process;consumers;digital distribution platforms;DayZ game;multiplayer survival-based game;Spacebase DF-9 game;employees;programmer;project lead;decision-making;EAG;early access stage;early access game;early access model,
123,Empirical Studies of Code,[Journal First] Correctness Attraction: A Study of Stability of Software Behavior Under Runtime Perturbation,B. Danglot; P. Preux; B. Baudry; M. Monperrus,"Inria, Le Chesnay, ÃƒÅ½le-de-France, FR; Univ. Lille, Villeneuve-d'Ascq, France; Inria Centre de Recherche Rennes Bretagne Atlantique, Rennes, Bretagne, FR; Kungliga Tekniska Hogskolan, Stockholm, SE",2018,"Can the execution of software be perturbed without breaking the correctness of the output? In this paper, we devise a protocol to answer this question from a novel perspective. In an experimental study, we observe that many perturbations do not break the correctness in ten subject programs. We call this phenomenon ""correctness attraction"". The uniqueness of this protocol is that it considers a systematic exploration of the perturbation space as well as perfect oracles to determine the correctness of the output. To this extent, our findings on the stability of software under execution perturbations have a level of validity that has never been reported before in the scarce related work. A qualitative manual analysis enables us to set up the first taxonomy ever of the reasons behind correctness attraction.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453109,perturbation analysis;software correctness;empirical study,Perturbation methods;Software;Protocols;Stability analysis;Runtime;Systematics,program diagnostics;program testing,execution perturbations;perturbation space;phenomenoncorrectness attraction;subject programs;runtime perturbation;software behavior,
124,Empirical Studies of Code,[Journal First] On the Diffuseness and the Impact on Maintainability of Code Smells: A Large Scale Empirical Investigation,F. Palomba; G. Bavota; M. Di Penta; F. Fasano; R. Oliveto; A. De Lucia,"Universitat Zurich, Zurich, ZH, CH; Universita della Svizzera Italiana, Lugano, TI, CH; Universita degli Studi del Sannio, Benevento, Campania, IT; Universita degli Studi del Molise, Campobasso, Molise, IT; Universita degli Studi del Molise, Campobasso, Molise, IT; Universita degli Studi di Salerno, Fisciano, Campania, IT",2018,"Code smells are symptoms of poor design and implementation choices that may hinder code comprehensibility and maintainability. Despite the effort devoted by the research community in studying code smells, the extent to which code smells in software systems affect software maintainability remains still unclear. In this paper we present a large scale empirical investigation on the diffuseness of code smells and their impact on code change- and fault-proneness. The study was conducted across a total of 395 releases of 30 open source projects and considering 17,350 manually validated instances of 13 different code smell types. The results show that smells characterized by long and/or complex code (e.g., Complex Class) are highly diffused, and that smelly classes have a higher change- and fault-proneness than smell-free classes.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453110,code smells;empirical studies;mining software repositories,Software engineering;History;Detectors;Maintenance engineering;Software systems;Correlation,public domain software;software maintenance,software maintainability;code fault-proneness;code change-proneness;open source projects;code comprehensibility;scale empirical investigation;code smells;smell-free classes;complex code,2
125,Empirical Studies of Code,Accurate and Efficient Refactoring Detection in Commit History,N. Tsantalis; M. Mansouri; L. Eshkevari; D. Mazinanian; D. Dig,"Concordia University, Montreal, Quebec, Canada; Concordia University, Montreal, Quebec, Canada; Concordia University, Montreal, Quebec, Canada; Concordia University, Montreal, Quebec, Canada; Oregon State University, Corvallis, Oregon, USA",2018,"Refactoring detection algorithms have been crucial to a variety of applications: (i) empirical studies about the evolution of code, tests, and faults, (ii) tools for library API migration, (iii) improving the comprehension of changes and code reviews, etc. However, recent research has questioned the accuracy of the state-of-the-art refactoring detection tools, which poses threats to the reliability of their application. Moreover, previous refactoring detection tools are very sensitive to user-provided similarity thresholds, which further reduces their practical accuracy. In addition, their requirement to build the project versions/revisions under analysis makes them inapplicable in many real-world scenarios. To reinvigorate a previously fruitful line of research that has stifled, we designed, implemented, and evaluated RMiner, a technique that overcomes the above limitations. At the heart of RMiner is an AST-based statement matching algorithm that determines refactoring candidates without requiring user-defined thresholds. To empirically evaluate RMiner, we created the most comprehensive oracle to date that uses triangulation to create a dataset with considerably reduced bias, representing 3,188 refactorings from 185 open-source projects. Using this oracle, we found that RMiner has a precision of 98% and recall of 87%, which is a significant improvement over the previous state-of-the-art.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453111,Refactoring;Commit;Git;Abstract Syntax Tree;Oracle;Accuracy,Tools;History;Open source software;Software systems;Java;Software engineering,application program interfaces;data mining;Java;public domain software;software libraries;software maintenance;software quality,comprehensive oracle;RMiner;commit history;detection algorithms;empirical studies;library API migration;code reviews;user-provided similarity thresholds;AST-based statement matching algorithm;user-defined thresholds;refactoring detection tools,85
126,"Security, Privacy and Trust 2",ENTRUST: Engineering Trustworthy Self-Adaptive Software with Dynamic Assurance Cases,R. Calinescu; D. Weyns; S. Gerasimou; M. U. Iftikhar; I. Habli; T. Kelly,"University of York, York, North Yorkshire, GB; Katholieke Universiteit Leuven, Leuven, Flanders, BE; University of York, York, North Yorkshire, GB; Linneuniversitet, Kalmar, SE; University of York, York, North Yorkshire, GB; University of York, York, North Yorkshire, GB",2018,"Software systems are increasingly expected to cope with variable workloads, component failures and other uncertainties through self-adaptation. As such, self-adaptive software has been the subject of intense research over the past decade. Our work focuses on the use of self-adaptive software in applications with strict functional and non-functional requirements. These applications need compelling assurances that the software continues to meet its requirements while it reconfigures its architecture and parameters at runtime. To address this need, we introduce an end-to-end methodology for the ENgineering of TRUstworthy Self-adaptive sofTware (ENTRUST).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453112,self adaptive software systems;software engineering methodology;assurance evidence;assurance cases,Computer science;Runtime;Software systems;Standards;Safety,formal specification;software architecture;trusted computing,engineering trustworthy self-adaptive software;dynamic assurance cases;software systems;nonfunctional requirements;self-adaptive software,2
127,"Security, Privacy and Trust 2","The Good, the Bad and the Ugly: A Study of Security Decisions in a Cyber-Physical Systems Game",S. Frey; A. Rashid; P. Anthonysamy; M. Pinto-Albuquerque; S. A. Naqvi,"University of Southampton, Southampton, Hampshire, GB; University of Bristol, Bristol, Bristol, GB; Google, Switzerland; Inst. Univ. de Lisboa, Lisbon, Portugal; Lancaster University, Lancaster, Lancashire, GB",2018,"Motivation: The security of any system is a direct consequence of stakeholders' decisions regarding security requirements and their relative prioritisation. Such decisions are taken with varying degrees of expertise in security. In some organisations - particularly those with resources - these are the preserve of computer (or information) security teams. In others - typically smaller organisations - the computing services team may be charged with the responsibility. Often managers have a role to play as guardians of business targets and goals. Be it common workplace practices or strategic decision making, security decisions underpin not only the initial security requirements and their prioritisation but also the adaptation and evolution of these requirements as new business or security contexts arise. However, little is currently understood about how these various demographics approach cyber security decisions and the strategies and approaches that underpin those decisions. What are the typical decision patterns, if any, the consequences of such patterns and their impact (positive or negative) on the security of the system in question? Nor is there any substantial understanding of how the strategies and decision patterns of these different groups contrast. Is security expertise necessarily an advantage when making security decisions in a given context? Answers to these questions are key to understanding the ""how"" and ""why"" behind security decision processes. The Game: In this talk [1], we present a tabletop game - Decisions and Disruptions (D-D) [2] - as a means to investigate these very questions. The game tasks a group of players with managing the security of a small utility company while facing a variety of threats. The game provides a requirements sandbox in which players can experiment with threats, learn about decision making and its consequences, and reflect on their own perception of risk. The game is intentionally kept short - 2 hours - and simple enough to be played without prior training. A cyber-physical infrastructure, depicted through a Lego(R) board, makes the game easy to understand and accessible to players from varying backgrounds and security expertise, without being too trivial a setting for security experts. Key insights: We played D-D with 43 players divided into homogeneous groups (group sizes of 2-6 players): 4 groups of security experts, 4 groups of non-technical managers and 4 groups of general computer scientists. Such observations should, of course, not be generalised, however, the substantial sample size enables in-depth qualitative analysis. Our analysis reveals a number of novel insights regarding security decisions of our three demographics: - Strategies: Security experts had a strong interest in advanced technological solutions and tended to neglect intelligence gathering, to their own detriment: some security expert teams achieved poor results in the game. Managers, too, were technology-driven and focused on data protection while neglecting human factors more than other groups. Computer scientists tended to balance human factors and intelligence gathering with technical solutions, and achieved the best results of the three demographics. - Decision Processes: Technical experience significantly changes the way players think. Teams with little technical experience had shallow, intuition-driven discussions with few concrete arguments. Technical teams, and the most experienced in particular, had much richer debates, driven by concrete scenarios, anecdotes from experience, and procedural thinking. Security experts showed a high confidence in their decisions - despite some of them having bad consequences - while the other groups tended to doubt their own skills - even when they were playing good games. - Patterns: A number of characteristic plays could be identified, some good (balance between priorities, open-mindedness, and adapting strategies based on inputs that challenge one's pre-conceptions), some bad (excessive focus on particular issues, confidence in charismatic leaders), some ugly (""tunnel vision"" syndrome by over-confident players). These patterns are documented and discussed in the full paper - showing the virtue of the positive ones, discouraging the negative ones, and inviting the readers to do their own introspection. Conclusion: D-D complements existing work on gamification as a means to improve security awareness, education, and training. Beyond the analysis of the security decisions of the three demographics, there is a definite educational and awareness-raising aspect to D-D (as noted consistently by players in all our subject groups). Game boxes will be brought to the conference for demonstration purposes, and the audience will be invited to experiment with D-D themselves, make their own decisions, and reflect on their own perception of security.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453113,security decisions;security requirements;game;decision patterns,Games;Cyber-physical systems;Computer security;Software engineering;Privacy;Google,computer games;cyber-physical systems;decision making;game theory;human factors;organisational aspects;security of data,decision making;tabletop game;stakeholders decisions;organisations;computer security teams;demographics approach;decisions and disruptions;D-D;sandbox requirements;qualitative analysis;human factors;gamification;security expert teams;players;cyber security decisions;cyber-physical systems game,2
128,"Security, Privacy and Trust 2","[Journal First] Lightweight, Obfuscation-Resilient Detection and Family Identification of Android Malware",J. Garcia; M. Hammad; S. Malek,"University of California Irvine, Irvine, CA, US; University of California Irvine, Irvine, CA, US; University of California Irvine, Irvine, CA, US",2018,"The number of malicious Android apps has been and continues to increase rapidly. These malware can damage or alter other files or settings, install additional applications, obfuscate their behaviors, propagate quickly, and so on. To identify and handle such malware, a security analyst can significantly benefit from identifying the family to which a malicious app belongs rather than only detecting if an app is malicious. To address these challenges, we present a novel machine learning-based Android malware detection and family-identification approach, RevealDroid, that operates without the need to perform complex program analyses or extract large sets of features. RevealDroid's selected features leverage categorized Android API usage, reflection-based features, and features from native binaries of apps. We assess RevealDroid for accuracy, efficiency, and obfuscation resilience using a large dataset consisting of more than 54,000 malicious and benign apps. Our experiments show that RevealDroid achieves an accuracy of 98% in detection of malware and an accuracy of 95% in determination of their families. We further demonstrate RevealDroid's superiority against state-of-the-art approaches.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453114,obfuscation;machine learning;lightweight;native code;reflection;Android malware,Malware;Feature extraction;Androids;Humanoid robots;Resilience;Machine learning;Reflection,Android (operating system);application program interfaces;invasive software;learning (artificial intelligence);mobile computing;program diagnostics,obfuscation-resilient detection;malicious Android apps;propagate quickly;security analyst;malicious app;Android malware detection;family-identification approach;Android API usage;reflection-based features;obfuscation resilience;complex program analysis;RevealDroid selected feature,1
129,"Security, Privacy and Trust 2",[Journal First] Are Vulnerabilities Discovered and Resolved Like Other Defects?,P. Morrison; R. Pandita; X. Xiao; R. Chillarege; L. Williams,"North Carolina State University, Raleigh, NC, US; Phase Change Software, Golden, CO, USA; Case Western Reserve University, Cleveland, OH, US; Chillarege Inc., Raleigh, NC, USA; North Carolina State University, Raleigh, NC, US",2018,"Software defect data has long been used to drive software development process improvement. If security defects (i.e., vulnerabilities) are discovered and resolved by different software development practices than non-security defects, the knowledge of that distinction could be applied to drive process improvement. The goal of this research is to support technical leaders in making security-specific software development process improvements by analyzing the differences between the discovery and resolution of defects versus that of vulnerabilities. We extend Orthogonal Defect Classification (ODC), a scheme for classifying software defects to support software development process improvement, to study process-related differences between vulnerabilities and defects, creating ODC + Vulnerabilities (ODC+V). We applied ODC+V to classify 583 vulnerabilities and 583 defects across 133 releases of three open-source projects (Firefox, phpMyAdmin, and Chrome). Compared with defects, vulnerabilities are found later in the development cycle and are more likely to be resolved through changes to conditional logic. In Firefox, vulnerabilities are resolved 33% more quickly than defects. From a process improvement perspective, these results indicate opportunities may exist for more efficient vulnerability detection and resolution. We found ODC+V's property of associating vulnerability and defect discovery and resolution events with their software development process contexts helpful for gaining insight into three open source software projects. The addition of the SecurityImpact attribute, in particular, brought visibility into when threat types are discovered during the development process. We would expect use of ODC+V (and of base ODC) periodically over time to be helpful for steering software development projects toward their quality assurance goals.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453115,metrics;security,Random access memory;Software engineering;Software measurement;Open source software;Security;Drives,pattern classification;project management;public domain software;quality assurance;security of data;software development management,software development projects;orthogonal defect classification;ODC;vulnerability detection;security-specific software development process;Firefox;phpMyAdmin;Chrome;open source software projects;defect discovery;nonsecurity defects;security defects,
130,Communities and Ecosystems,How Modern News Aggregators Help Development Communities Shape and Share Knowledge,M. Aniche; C. Treude; I. Steinmacher; I. Wiese; G. Pinto; M. Storey; M. A. Gerosa,NA; NA; NA; NA; NA; NA; NA,2018,"Many developers rely on modern news aggregator sites such as reddit and hn to stay up to date with the latest technological developments and trends. In order to understand what motivates developers to contribute, what kind of content is shared, and how knowledge is shaped by the community, we interviewed and surveyed developers that participate on the reddit programming subreddit and we analyzed a sample of posts on both reddit and hn. We learned what kind of content is shared in these websites and developer motivations for posting, sharing, discussing, evaluating, and aggregating knowledge on these aggregators, while revealing challenges developers face in terms of how content and participant behavior is moderated. Our insights aim to improve the practices developers follow when using news aggregators, as well as guide tool makers on how to improve their tools. Our findings are also relevant to researchers that study developer communities of practice.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453116,News aggregators;development communities;knowledge sharing;social computing,Computer hacking;Interviews;Software;Programming;Communication channels;Europe;Shape,social networking (online),modern news aggregator sites;reddit programming subreddit;developer motivations;development communities;Websites;participant behavior,18
131,Communities and Ecosystems,Adding Sparkle to Social Coding: An Empirical Study of Repository Badges in the npm Ecosystem,A. Trockman; S. Zhou; C. KÃ¤stner; B. Vasilescu,"University of Evansville, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA",2018,"In fast-paced, reuse-heavy, and distributed software development, the transparency provided by social coding platforms like GitHub is essential to decision making. Developers infer the quality of projects using visible cues, known as signals, collected from personal profile and repository pages. We report on a large-scale, mixed-methods empirical study of npm packages that explores the emerging phenomenon of repository badges, with which maintainers signal underlying qualities about their projects to contributors and users. We investigate which qualities maintainers intend to signal and how well badges correlate with those qualities. After surveying developers, mining 294,941 repositories, and applying statistical modeling and time-series analyses, we find that non-trivial badges, which display the build status, test coverage, and up-to-dateness of dependencies, are mostly reliable signals, correlating with more tests, better pull requests, and fresher dependencies. Displaying such badges correlates with best practices, but the effects do not always persist.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453117,repository badge;signaling theory;regression discontinuity design;mining software repositories;dependency manager,Reliability;Encoding;Ecosystems;Open source software;Tools;Best practices,,,5
132,Communities and Ecosystems,"""Was My Contribution Fairly Reviewed?"" A Framework to Study the Perception of Fairness in Modern Code Reviews",D. M. German; G. Robles; G. Poo-CaamaÃ±o; X. Yang; H. Iida; K. Inoue,"University of Victoria, Victoria, BC, CA; Universidad Rey Juan Carlos, Spain; University of Victoria, Canada; Osaka University, Japan; Nara Institute of Technology, Japan; Osaka University, Japan",2018,"Modern code reviews improve the quality of software products. Although modern code reviews rely heavily on human interactions, little is known regarding whether they are performed fairly. Fairness plays a role in any process where decisions that affect others are made. When a system is perceived to be unfair, it affects negatively the productivity and motivation of its participants. In this paper, using fairness theory we create a framework that describes how fairness affects modern code reviews. To demonstrate its applicability, and the importance of fairness in code reviews, we conducted an em-pirical study that asked developers of a large industrial open source ecosystem (OpenStack) what their perceptions are regarding fairness in their code reviewing process. Our study shows that, in general, the code review process in OpenStack is perceived as fair; however, a significant portion of respondents perceive it as unfair. We also show that the variability in the way they prioritize code reviews signals a lack of consistency and the existence of bias (potentially increasing the perception of unfairness). The contributions of this paper are: (1) we propose a framework-based on fairness theory-for studying and managing social behaviour in modern code reviews, (2) we provide support for the framework through the results of a case study on a large industrial-backed open source project, (3) we present evidence that fairness is an issue in the code review process of a large open source ecosystem, and, (4) we present a set of guidelines for practitioners to address unfairness in modern code reviews.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453118,Fairness;Software Development;Code Reviews;Open source software;human and social aspects;transparency,Process control;Organizations;Software reviews;Standards organizations;Guidelines;Ecosystems,organisational aspects;public domain software;software quality,modern code reviews;code review process;code review signals;software product quality;industrial open source ecosystem;OpenStack,8
133,Communities and Ecosystems,Collaborative Model-Driven Software Engineering: A Classification Framework and a Research Map [Extended Abstract],D. Di Ruscio; M. Franzago; I. Malavolta; H. Muccini,"DISIM Dept., Univ. of L'Aquila, L'Aquila, Italy; DISIM Dept., Univ. of L'Aquila, L'Aquila, Italy; DISIM Dept., Univ. of L'Aquila, L'Aquila, Italy; Vrije Universiteit Amsterdam, Amsterdam, Noord-Holland, NL",2018,"This proposal is about a study we recently published in the IEEE Transaction of Software Engineering journal [4]. Context: Collaborative software engineering (CoSE) deals with methods, processes and tools for enhancing collaboration, communication, and co-ordination (3C) among team members. CoSE can be employed to conceive different kinds of artifacts during the development and evolution of software systems. For instance, when focusing on software design, multiple stakeholders with different expertise and responsibility collaborate on the system design. Model-Driven Software Engineering (MDSE) provides suitable techniques and tools for specifying, manipulating, and analyzing modeling artifacts including metamodels, models, and transformations. Collaborative MDSE consists of methods or techniques in which multiple stakeholders manage, collaborate, and are aware of each others' work on a set of shared models. A collaborative MDSE approach is composed of three main complementary dimensions: (i) a model management infrastructure for managing the life cycle of the models, (ii) a set of collaboration means for allowing involved stakeholders to work on the modelling artifacts collaboratively, and (iii) a set of communication means for allowing involved stakeholders to exchange, share, and communicate information within the team. Collaborative MDSE is attracting several research efforts from different research areas (e.g., model-driven engineering, global software engineering, etc.), resulting in a variegated scientific body of knowledge on the topic. Objective: In this study we aim at identifying, classifying, and understanding existing collaborative MDSE approaches. More specifically, our goal is to assess (i) the key characteristics of collaborative MDSE approaches (e.g., model editing environments, model versioning mechanisms, model repositories, support for communication and decision making), (ii) their faced challenges and limitations, and (iii) the interest of researchers in collaborative MDSE approaches over time and their focus on the three dimensions of collaborative MDSE. Method: In order to achieve this, we designed and conducted a systematic mapping study on collaborative MDSE. Starting from over 3,000 potentially relevant studies, we applied a rigorous selection procedure resulting in 106 selected papers, further clustered into 48 primary studies, along a time span of nineteen years. A suitable classification framework has been empirically defined and rigorously applied for extracting key information from each selected study. We collated, summarized, and analyzed extracted data by applying scientifically sound data synthesis techniques. Results: In addition to a number of specific insights, our analysis revealed the following key findings: (i) there is a growing scientific interest on collaborative MDSE in the last years; (ii) multi-view modeling, validation support, reuse, and branching are more rarely covered with respect to other aspects about collaborative MDSE; (iii) different primary studies focus differently on individual dimensions of collaborative MDSE (i.e., model management, collaboration, and communication); (iv) most approaches are language-specific, with a prominence of UML-based approaches; (v) few approaches support the interplay between synchronous and asynchronous collaboration. Conclusion: This study gives a solid foundation for a thorough identification and comparison of existing and future approaches for collaborative MDSE. Those results can be used by both researchers and practitioners for identifying existing research/technical gaps to attack, better scoping their own contributions to the field, or better understanding or refining existing ones.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453119,Collaborative MDSE;CoMDSE;C MDSE;model driven engineering;collaborative software engineering;CoSE;systematic mapping study,Software engineering;Analytical models;Unified modeling language;Stakeholders;Collaborative software;Tools,decision making;groupware;software engineering;software maintenance;Unified Modeling Language,collaborative model-driven software engineering;CoSE;UML-based approach;decision making;understanding existing collaborative MDSE approaches;model-driven engineering,
134,Testing 1,[Journal First] ChangeLocator: Locate Crash-Inducing Changes Based on Crash Reports,R. Wu; M. Wen; S. -C. Cheung; H. Zhang,"Hong Kong University of Science and Technology, Kowloon, HK; Hong Kong University of Science and Technology, Kowloon, HK; Hong Kong University of Science and Technology, Kowloon, HK; The University of Newcastle, Callaghan, NSW, AU",2018,"Software crashes are severe manifestations of software bugs. Debugging crashing bugs is tedious and time-consuming. Understanding software changes that induce a crashing bug can provide useful contextual information for bug fixing and is highly demanded by developers. Locating the bug inducing changes is also useful for automatic program repair, since it narrows down the root causes and reduces the search space of bug fix location. However, currently there are no systematic studies on locating the software changes to a source code repository that induce a crashing bug reflected by a bucket of crash reports. To tackle this problem, we first conducted an empirical study on characterizing the bug inducing changes for crashing bugs (denoted as crashinducing changes). We also propose ChangeLocator, a method to automatically locate crash-inducing changes for a given bucket of crash reports. We base our approach on a learning model that uses features originated from our empirical study and train the model using the data from the historical fixed crashes. We evaluated ChangeLocator with six release versions of Netbeans project. The results show that it can locate the crash-inducing changes for 44.7%, 68.5%, and 74.5% of the bugs by examining only top 1, 5 and 10 changes in the recommended list, respectively. It significantly outperforms the existing state-of-the-art approach.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453120,crash-inducing change;software crash;crash stack;bug localization,Computer bugs;Software;Software engineering;Data models;Computer science;Maintenance engineering,program debugging;software maintenance,historical fixed crashes;crashinducing changes;bug fix location;bug inducing changes;crashing bug;software bugs;software crashes;crash reports;crash-inducing changes,1
135,Testing 1,Are Mutation Scores Correlated with Real Fault Detection? A Large Scale Empirical Study on the Relationship Between Mutants and Real Faults,M. Papadakis; D. Shin; S. Yoo; D. Bae,NA; NA; NA; NA,2018,"Empirical validation of software testing studies is increasingly relying on mutants. This practice is motivated by the strong correlation between mutant scores and real fault detection that is reported in the literature. In contrast, our study shows that correlations are the results of the confounding effects of the test suite size. In particular, we investigate the relation between two independent variables, mutation score and test suite size, with one dependent variable the detection of (real) faults. We use two data sets, CoreBench and De-fects4J, with large C and Java programs and real faults and provide evidence that all correlations between mutation scores and real fault detection are weak when controlling for test suite size. We also found that both independent variables significantly influence the dependent one, with significantly better fits, but overall with relative low prediction power. By measuring the fault detection capability of the top ranked, according to mutation score, test suites (opposed to randomly selected test suites of the same size), we found that achieving higher mutation scores improves significantly the fault detection. Taken together, our data suggest that mutants provide good guidance for improving the fault detection of test suites, but their correlation with fault detection are weak.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453121,mutation testing;real faults;test suite effectiveness,Fault detection;Correlation;Java;Measurement;Software testing;Software engineering,Java;program testing,mutation score;randomly selected test suites;higher mutation scores;mutation scores correlated;real fault detection;scale empirical study;empirical validation;software testing studies;mutant scores;test suite size;dependent variable the detection;fault detection capability,16
136,Testing 1,Efficient Sampling of SAT Solutions for Testing,R. Dutra; K. Laeufer; J. Bachrach; K. Sen,"University of California Berkeley, Berkeley, CA, US; University of California Berkeley, Berkeley, CA, US; University of California Berkeley, Berkeley, CA, US; University of California Berkeley, Berkeley, CA, US",2018,"In software and hardware testing, generating multiple inputs which satisfy a given set of constraints is an important problem with applications in fuzz testing and stimulus generation. However, it is a challenge to perform the sampling efficiently, while generating a diverse set of inputs which satisfy the constraints. We developed a new algorithm QuickSampler which requires a small number of solver calls to produce millions of samples which satisfy the constraints with high probability. We evaluate QuickSampler on large real-world benchmarks and show that it can produce unique valid solutions orders of magnitude faster than other state-of-the-art sampling tools, with a distribution which is reasonably close to uniform in practice.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453122,sampling;stimulus generation;constraint-based testing;constrained random verification,Hardware;Software;Fuzzing;Benchmark testing;Software engineering;Tools,computability;probability;program testing,quicksampler;sampling tools;unique valid solutions;fuzz testing;multiple inputs;hardware testing;SAT solutions;solver calls;stimulus generation,16
137,Testing 1,[Journal First] Are Fix-Inducing Changes a Moving Target?: A Longitudinal Case Study of Just-in-Time Defect Prediction,S. McIntosh; Y. Kamei,"McGill University, Montreal, QC, CA; Kyushu Daigaku, Fukuoka, JP",2018,"Just-In-Time (JIT) models identify fix-inducing code changes. JIT models are trained using techniques that assume that past fix-inducing changes are similar to future ones. However, this assumption may not hold, e.g., as system complexity tends to accrue, expertise may become more important as systems age. In this paper, we study JIT models as systems evolve. Through a longitudinal case study of 37,524 changes from the rapidly evolving Qt and OpenStack systems, we find that fluctuations in the properties of fix-inducing changes can impact the performance and interpretation of JIT models. More specifically: (a) the discriminatory power (AUC) and calibration (Brier) scores of JIT models drop considerably one year after being trained; (b) the role that code change properties (e.g., Size, Experience) play within JIT models fluctuates over time; and (c) those fluctuations yield over- and underestimates of the future impact of code change properties on the likelihood of inducing fixes. To avoid erroneous or misleading predictions, JIT models should be retrained using recently recorded data (within three months). Moreover, quality improvement plans should be informed by JIT models that are trained using six months (or more) of historical data, since they are more resilient to period-specific fluctuations in the importance of code change properties.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453123,Just In Time prediction;Defect prediction;Mining software repositories,Software engineering;Predictive models;History;Fluctuations;Training;Data models;Software,data mining;just-in-time;learning (artificial intelligence);public domain software;software fault tolerance;software management,fix-inducing changes;JIT models;just-in-time models;just-in-time defect prediction;Qt systems;OpenStack systems;fix-inducing code changes;code change properties,3
138,Studying Software Engineers 1,Understanding Developers' Needs on Deprecation as a Language Feature,A. A. Sawant; M. Aniche; A. van Deursen; A. Bacchelli,"Technische Universiteit Delft, Delft, Zuid-Holland, NL; Technische Universiteit Delft, Delft, Zuid-Holland, NL; Technische Universiteit Delft, Delft, Zuid-Holland, NL; Universitat Zurich, Zurich, ZH, CH",2018,"Deprecation is a language feature that allows API producers to mark a feature as obsolete. We aim to gain a deep understanding of the needs of API producers and consumers alike regarding deprecation. To that end, we investigate why API producers deprecate features, whether they remove deprecated features, how they expect consumers to react, and what prompts an API consumer to react to deprecation. To achieve this goal we conduct semi-structured interviews with 17 third-party Java API producers and survey 170 Java developers. We observe that the current deprecation mechanism in Java and the proposal to enhance it does not address all the needs of a developer. This leads us to propose and evaluate three further enhancements to the deprecation mechanism.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453124,API;deprecation;Java,Java;Interviews;Industries;Companies;Proposals;Guidelines;Software engineering,application program interfaces;Java;public domain software,Java developers;current deprecation mechanism;third-party Java API producers;API consumer;language feature,9
139,Studying Software Engineers 1,On the Dichotomy of Debugging Behavior Among Programmers,M. Beller; N. Spruit; D. Spinellis; A. Zaidman,"Technische Universiteit Delft, Delft, Zuid-Holland, NL; Technische Universiteit Delft, Delft, Zuid-Holland, NL; Athens University of Economics and Business, Athens, GR; Technische Universiteit Delft, Delft, Zuid-Holland, NL",2018,"Debugging is an inevitable activity in most software projects, often difficult and more time-consuming than expected, giving it the nickname the ""dirty little secret of computer science."" Surprisingly, we have little knowledge on how software engineers debug software problems in the real world, whether they use dedicated debugging tools, and how knowledgeable they are about debugging. This study aims to shed light on these aspects by following a mixed-methods research approach. We conduct an online survey capturing how 176 developers reflect on debugging. We augment this subjective survey data with objective observations on how 458 developers use the debugger included in their integrated development environments (IDEs) by instrumenting the popular Eclipse and IntelliJ IDEs with the purpose-built plugin WatchDog 2.0. To clarify the insights and discrepancies observed in the previous steps, we followed up by conducting interviews with debugging experts and regular debugging users. Our results indicate that IDE-provided debuggers are not used as often as expected, as ""printf debugging"" remains a feasible choice for many programmers. Furthermore, both knowledge and use of advanced debugging features are low. These results call to strengthen hands-on debugging experience in computer science curricula and have already refined the implementation of modern IDE debuggers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453125,Debugging;Testing;WatchDog;IntelliJ;Eclipse,Debugging;Software;Tools;Instruments;Interviews;Computer bugs;Software engineering,computer debugging;program debugging;software engineering;software maintenance,integrated development environments;debugging experts;regular debugging users;printf debugging;advanced debugging features;software projects;software engineers debug software problems;debugging tools;mixed-methods research approach,18
140,Studying Software Engineers 1,Measuring Program Comprehension: A Large-Scale Field Study with Professionals,X. Xia; L. Bao; D. Lo; Z. Xing; A. E. Hassan; S. Li,"Monash University, Clayton, VIC, AU; Zhejiang University, Hangzhou, Zhejiang, CN; Singapore Management University, Singapore, Singapore, SG; Australian National University, Canberra, ACT, AU; Queen's University Faculty of Health Sciences, Kingston, ON, CA; Zhejiang University, Hangzhou, Zhejiang, CN",2018,"This paper is published in IEEE Transaction on Software Engineering (DOI: 10.1109/TSE.2017.2734091). Comparing with previous programming comprehension studies that are usually in controlled settings or have a small number of participants, we perform a more realistic investigation of program comprehension activities. To do this, we extend our ActivitySpace framework to collect and analyze Human-Computer Interaction (HCI) data across many applications (not just the IDEs). We collect 3,148 working hour data from 78 professional developers in a field study. We follow Minelli et al.'s approach to assign developers' activities into four categories: navigation, editing, comprehension, and other. Then we measure comprehension time by calculating the time that developers spend on program comprehension. We find that on average developers spend ~58% of their time on program comprehension activities, and that they frequently use web browsers and document editors to perform program comprehension activities. We also investigate the impact of programming language, developers' experience, and project phase on the time that is spent on program comprehension.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453126,Program Comprehension;Field Study;Inference Model,Computer science;Software;Software engineering;Australia;Conferences;Software measurement;Information technology,online front-ends;reverse engineering;software engineering;software maintenance,large-scale field study;program comprehension activities;measure comprehension time;professional developers;program comprehension measurement,5
141,Studying Software Engineers 1,Data Scientists in Software Teams: State of the Art and Challenges,M. Kim; T. Zimmermann; R. DeLine; A. Begel,"University of California Los Angeles, Los Angeles, CA, US; Microsoft Research, Redmond, WA, US; Microsoft Research, Redmond, WA, US; Microsoft Research, Redmond, WA, US",2018,"The demand for analyzing large scale telemetry, machine, and quality data is rapidly increasing in software industry. Data scientists are becoming popular within software teams. For example, Facebook, LinkedIn and Microsoft are creating a new career path for data scientists. In this paper, we present a large-scale survey with 793 professional data scientists at Microsoft to understand their educational background, problem topics that they work on, tool usages, and activities. We cluster these data scientists based on the time spent for various activities and identify 9 distinct clusters of data scientists and their corresponding characteristics. We also discuss the challenges that they face and the best practices they share with other data scientists. Our study finds several trends about data scientists in the software engineering context at Microsoft, and should inform managers on how to leverage data science capability effectively within their teams.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453127,Software productivity;data science,Software;Tools;Data science;Software engineering;Face;Best practices;Instruments,DP industry;social networking (online),software teams;data scientists;software industry;Facebook;LinkedIn;Microsoft,1
142,Program Analysis 1,Dataflow Tunneling: Mining Inter-Request Data Dependencies for Request-Based Applications,X. Yu; G. Jin,"Department of Computer Science, North Carolina State University; Department of Computer Science, North Carolina State University",2018,"Request-based applications, e.g., most server-side applications, expose services to users in a request-based paradigm, in which requests are served by request-handler methods. An important task for request-based applications is inter-request analysis, which analyzes request-handler methods that are related by inter-request data dependencies together. However, in the request-based paradigm, data dependencies between related request-handler methods are implicitly established by the underlying frameworks that execute these methods. As a result, existing analysis tools are usually limited to the scope of each single method without the knowledge of dependencies between different methods. In this paper, we design an approach called dataflow tunneling to capture inter-request data dependencies from concrete application executions and produce data-dependency specifications. Our approach answers two key questions: (1) what request-handler methods have data dependencies and (2) what these data dependencies are. Our evaluation using applications developed with two representative and popular frameworks shows that our approach is general and accurate. We also present a characteristic study and a use case of cache tuning based on the mined specifications. We envision that our approach can provide key information to enable future inter-request analysis techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453128,web application;request based applications;web frameworks;inter request analysis;tracing,Data models;Java;Tunneling;Analytical models;Object recognition;Prototypes;Databases,cache storage;data mining,request-based applications;request-based paradigm;analyzes request-handler methods;single method;dataflow tunneling;concrete application executions;data-dependency specifications;future inter-request analysis techniques;inter-request data dependency mining,
143,Program Analysis 1,Launch-Mode-Aware Context-Sensitive Activity Transition Analysis,Y. Zhang; Y. Sui; J. Xue,"UNSW, Sydney, Australia; University of Technology, Sydney, Australia; UNSW, Sydney, Australia",2018,"Existing static analyses model activity transitions in Android apps context-insensitively, making it impossible to distinguish different activity launch modes, reducing the pointer analysis precision for an activity's callbacks, and potentially resulting in infeasible activity transition paths. In this paper, we introduce Chime, a launch-mode-aware context-sensitive activity transition analysis that models different instances of an activity class according to its launch mode and the transitions between activities context-sensitively, by working together with an object-sensitive pointer analysis. Our evaluation shows that our context-sensitive activity transition analysis is more precise than its context-insensitive counterpart in capturing activity transitions, facilitating GUI testing, and improving the pointer analysis precision.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453129,Android;Pointer Analysis;Activity Transition Analysis,Androids;Humanoid robots;Context modeling;Standards;Analytical models;Graphical user interfaces;Navigation,mobile computing;program diagnostics,infeasible activity transition paths;launch-mode-aware context-sensitive activity transition analysis;activity class;launch mode;activities context-sensitively;object-sensitive pointer analysis;pointer analysis precision;static analyses model activity transitions;Android apps context-insensitively,6
144,Program Analysis 1,UFO: Predictive Concurrency Use-After-Free Detection,J. Huang,"Texas A&M University College Station, College Station, TX, US",2018,"Use-After-Free (UAF) vulnerabilities are caused by the program operating on a dangling pointer and can be exploited to compromise critical software systems. While there have been many tools to mitigate UAF vulnerabilities, UAF remains one of the most common attack vectors. UAF is particularly di cult to detect in concurrent programs, in which a UAF may only occur with rare thread schedules. In this paper, we present a novel technique, UFO, that can precisely predict UAFs based on a single observed execution trace with a provably higher detection capability than existing techniques with no false positives. The key technical advancement of UFO is an extended maximal thread causality model that captures the largest possible set of feasible traces that can be inferred from a given multithreaded execution trace. By formulating UAF detection as a constraint solving problem atop this model, we can explore a much larger thread scheduling space than classical happens-before based techniques. We have evaluated UFO on several real-world large complex C/C++ programs including Chromium and FireFox. UFO scales to real-world systems with hundreds of millions of events in their execution and has detected a large number of real concurrency UAFs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453130,UAF;Concurrency;Vulnerabilities;UFO,Instruction sets;Concurrent computing;Schedules;Browsers;Tools;Chromium;Encoding,multi-threading;program debugging;program diagnostics;scheduling;security of data,program operating;critical software systems;UAF vulnerabilities;common attack vectors;concurrent programs;rare thread schedules;single observed execution trace;provably higher detection capability;extended maximal thread causality model;UAF detection;larger thread scheduling space;UFO scales;concurrency use-after-free detection;use-after-free vulnerabilities;multithreaded execution trace;concurrency UAF,12
145,Program Analysis 1,Collective Program Analysis,G. Upadhyaya; H. Rajan,"Dept. of Comput. Sci., Iowa State Univ. Ames, Ames, IA, USA; Dept. of Comput. Sci., Iowa State Univ. Ames, Ames, IA, USA",2018,"Popularity of data-driven software engineering has led to an increasing demand on the infrastructures to support efficient execution of tasks that require deeper source code analysis. While task optimization and parallelization are the adopted solutions, other research directions are less explored. We present collective program analysis (CPA), a technique for scaling large scale source code analyses, especially those that make use of control and data flow analysis, by leveraging analysis specific similarity. Analysis specific similarity is about, whether two or more programs can be considered similar for a given analysis. The key idea of collective program analysis is to cluster programs based on analysis specific similarity, such that running the analysis on one candidate in each cluster is sufficient to produce the result for others. For determining analysis specific similarity and clustering analysis-equivalent programs, we use a sparse representation and a canonical labeling scheme. Our evaluation shows that for a variety of source code analyses on a large dataset of programs, substantial reduction in the analysis time can be achieved; on average a 69% reduction when compared to a baseline and on average a 36% reduction when compared to a prior technique. We also found that a large amount of analysis-equivalent programs exists in large datasets.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453131,Source code analysis;Clustering;Boa,Transfer functions;Cloning;Software engineering;Task analysis;Syntactics;Analytical models;Labeling,data flow analysis;pattern clustering;software engineering,collective program analysis;cluster programs;data-driven software engineering;deeper source code analysis;scale source code analyses;data flow analysis,2
146,Human and Social Aspects of Computing 2,Statistical Learning of API Fully Qualified Names in Code Snippets of Online Forums,H. Phan; H. A. Nguyen; N. M. Tran; L. H. Truong; A. T. Nguyen; T. N. Nguyen,"Iowa State University, Ames, IA, US; Iowa State University, Ames, IA, US; University of Texas at Dallas, Richardson, TX, US; University of Texas at Dallas, Richardson, TX, US; NA; University of Texas at Dallas, Richardson, TX, US",2018,"Software developers often make use of the online forums such as StackOverflow to learn how to use software libraries and their APIs. However, the code snippets in such a forum often contain undeclared, ambiguous, or largely unqualified external references. Such declaration ambiguity and external reference ambiguity present challenges for developers in learning to correctly use the APIs. In this paper, we propose StatType, a statistical approach to resolve the fully qualified names (FQNs) for the API elements in such code snippets. Unlike existing approaches that are based on heuristics, StatType has two well-integrated factors. We first learn from a large training code corpus the FQNs that often co-occur. Then, to derive the FQN for an API name in a code snippet, we use that knowledge and leverage the context consisting of neighboring API names. To realize those factors, we treat the problem as statistical machine translation from source code with partially qualified names to source code with FQNs of the APIs. Our empirical evaluation on real-world code and StackOverflow posts shows that StatType achieves very high accuracy with 97.6% precision and 96.7% recall, which is 16.5% relatively higher than the state-of-the-art approach.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453132,Type Resolution;Type Inference;Type Annotations;Partial Program Analysis;Statistical Machine Translation;Naturalness;Big Code,Computational modeling;Software libraries;Training;Software;Tools,application program interfaces;Internet;language translation;learning (artificial intelligence);software libraries;statistical analysis,machine translation;StackOverflow posts;API fully qualified names;statistical learning;partially qualified names;source code;API name;training code corpus;API elements;FQNs;statistical approach;StatType;external reference;declaration ambiguity;code snippet;software libraries;online forums;software developers,18
147,Human and Social Aspects of Computing 2,When Not to Comment: Questions and Tradeoffs with API Documentation for C++ Projects,A. Head; C. Sadowski; E. Murphy-Hill; A. Knight,NA; NA; NA; NA,2018,"Without usable and accurate documentation of how to use an API, developers can find themselves deterred from reusing relevant code. In C++, one place developers can find documentation is in a header file. When information is missing, they may look at the corresponding implementation code. To understand what's missing from C++ API documentation and the factors influencing whether it will be fixed, we conducted a mixed-methods study involving two experience sampling surveys with hundreds of developers at the moment they visited implementation code, interviews with 18 of those developers, and interviews with 8 API maintainers. In many cases, updating documentation may provide only limited value for developers, while requiring effort maintainers don't want to invest. We identify a set of questions maintainers and tool developers should consider when improving API-level documentation.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453133,documentation;human factors;developer workflow;APIs;usability,Documentation;Interviews;Tools;Task analysis;C++ languages;Google;Software engineering,application program interfaces;C++ language;system documentation,API-level documentation improvement;tool developers;questions maintainers;updating documentation;experience sampling surveys;mixed-methods study;C++ API documentation;corresponding implementation code;header file;place developers;relevant code;accurate documentation;usable documentation,8
148,Human and Social Aspects of Computing 2,Deuce: A Lightweight User Interface for Structured Editing,B. Hempel; J. Lubin; G. Lu; R. Chugh,NA; NA; NA; NA,2018,"We present a structure-aware code editor, called Deuce, that is equipped with direct manipulation capabilities for invoking automated program transformations. Compared to traditional refactoring environments, Deuce employs a direct manipulation interface that is tightly integrated within a text-based editing workflow. In particular, Deuce draws (i) clickable widgets atop the source code that allow the user to structurally select the unstructured text for subexpressions and other relevant features, and (ii) a lightweight, interactive menu of potential transformations based on the current selections. We implement and evaluate our design with mostly standard transformations in the context of a small functional programming language. A controlled user study with 21 participants demonstrates that structural selection is preferred to a more traditional text-selection interface and may be faster overall once users gain experience with the tool. These results accord with Deuce's aim to provide human-friendly structural interactions on top of familiar text-based editing.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453134,Structured Editing;Refactoring;Sketch-n-Sketch,Tools;Syntactics;Human computer interaction;Task analysis;Software engineering;Visualization,functional programming;human computer interaction;interactive systems;source code (software);text analysis;text editing;user interfaces,direct manipulation interface;clickable widgets;source code;unstructured text;lightweight menu;interactive menu;functional programming language;structural selection;human-friendly structural interactions;lightweight user interface;structure-aware code editor;automated program transformations;Deuce;Structured Editing;refactoring environments;text-based editing workflow;text-selection interface,
149,Human and Social Aspects of Computing 2,From UI Design Image to GUI Skeleton: A Neural Machine Translator to Bootstrap Mobile GUI Implementation,C. Chen; T. Su; G. Meng; Z. Xing; Y. Liu,"Nanyang Technological University, Singapore, Singapore, SG; Nanyang Technological University, Singapore, Singapore, SG; Nanyang Technological University, Singapore, Singapore, SG; Australian National University, Canberra, ACT, AU; Nanyang Technological University, Singapore, Singapore, SG",2018,"A GUI skeleton is the starting point for implementing a UI design image. To obtain a GUI skeleton from a UI design image, developers have to visually understand UI elements and their spatial layout in the image, and then translate this understanding into proper GUI components and their compositions. Automating this visual understanding and translation would be beneficial for bootstraping mobile GUI implementation, but it is a challenging task due to the diversity of UI designs and the complexity of GUI skeletons to generate. Existing tools are rigid as they depend on heuristically-designed visual understanding and GUI generation rules. In this paper, we present a neural machine translator that combines recent advances in computer vision and machine translation for translating a UI design image into a GUI skeleton. Our translator learns to extract visual features in UI images, encode these features' spatial layouts, and generate GUI skeletons in a unified neural network framework, without requiring manual rule development. For training our translator, we develop an automated GUI exploration method to automatically collect large-scale UI data from real-world applications. We carry out extensive experiments to evaluate the accuracy, generality and usefulness of our approach.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453135,User interface;reverse engineering;deep learning,Graphical user interfaces;Skeleton;Layout;Visualization;Feature extraction;Tools;Task analysis,computer bootstrapping;computer vision;graphical user interfaces;language translation;mobile computing;neural nets,large-scale UI data;GUI exploration method;GUI components;computer vision;GUI generation rules;heuristically-designed visual understanding;bootstrap mobile GUI implementation;neural machine translator;GUI skeleton;UI design image,54
150,Testing 2,When Testing Meets Code Review: Why and How Developers Review Tests,D. Spadini; M. Aniche; M. -A. Storey; M. Bruntink; A. Bacchelli,"Technische Universiteit Delft, Delft, Zuid-Holland, NL; Technische Universiteit Delft, Delft, Zuid-Holland, NL; University of Victoria, Victoria, BC, CA; Software Improvement Group, Amsterdam, Netherlands; Universitat Zurich, Zurich, ZH, CH",2018,"Automated testing is considered an essential process for ensuring software quality. However, writing and maintaining high-quality test code is challenging and frequently considered of secondary importance. For production code, many open source and industrial software projects employ code review, a well-established software quality practice, but the question remains whether and how code review is also used for ensuring the quality of test code. The aim of this research is to answer this question and to increase our understanding of what developers think and do when it comes to reviewing test code. We conducted both quantitative and qualitative methods to analyze more than 300,000 code reviews, and interviewed 12 developers about how they review test files. This work resulted in an overview of current code reviewing practices, a set of identified obstacles limiting the review of test code, and a set of issues that developers would like to see improved in code review tools. The study reveals that reviewing test files is very different from reviewing production files, and that the navigation within the review itself is one of the main issues developers currently face. Based on our findings, we propose a series of recommendations and suggestions for the design of tools and future research.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453136,software testing;automated testing;code review;Gerrit,Production;Computer bugs;Testing;Tools;Software quality;Measurement,program testing;software quality,developer review tests;code review tools;current code reviewing practices;review test files;software quality practice;production code;high-quality test code;automated testing,13
151,Testing 2,Redefining Prioritization: Continuous Prioritization for Continuous Integration,J. Liang; S. Elbaum; G. Rothermel,"University of Nebraska-Lincoln, Lincoln, NE, US; University of Nebraska-Lincoln, Lincoln, NE, US; University of Nebraska-Lincoln, Lincoln, NE, US",2018,"Continuous integration (CI) development environments allow soft-ware engineers to frequently integrate and test their code. While CI environments provide advantages, they also utilize non-trivial amounts of time and resources. To address this issue, researchers have adapted techniques for test case prioritization (TCP) to CI environments. To date, however, the techniques considered have operated on test suites, and have not achieved substantial improvements. Moreover, they can be inappropriate to apply when system build costs are high. In this work we explore an alternative: prioritization of commits. We use a lightweight approach based on test suite failure and execution history that is highly efficient; our approach ""continuously"" prioritizes commits that are waiting for execution in response to the arrival of each new commit and the completion of each previously scheduled commit. We have evaluated our approach on three non-trivial CI data sets. Our results show that our approach can be more effective than prior techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453137,continuous integration;regression testing;large scale testing,Software engineering,program testing;software engineering,continuous prioritization;continuous integration development environments;CI environments;nontrivial amounts;test case prioritization;test suites;test suite failure;nontrivial CI data sets;software engineers,21
152,Testing 2,[Journal First] MAHAKIL: Diversity Based Oversampling Approach to Alleviate the Class Imbalance Issue in Software Defect Prediction,K. E. Bennin; J. Keung; P. Phannachitta; A. Monden; S. Mensah,"City University of Hong Kong, Kowloon, HK; City University of Hong Kong, Kowloon, HK; Chiang Mai University, Chiang Mai, TH; Okayama Daigaku, Okayama, Okayama, JP; City University of Hong Kong, Kowloon, HK",2018,"This study presents MAHAKIL, a novel and efficient synthetic oversampling approach for software defect datasets that is based on the chromosomal theory of inheritance. Exploiting this theory, MAHAKIL interprets two distinct sub-classes as parents and generates a new instance that inherits different traits from each parent and contributes to the diversity within the data distribution. We extensively compare MAHAKIL with five other sampling approaches using 20 releases of defect datasets from the PROMISE repository and five prediction models. Our experiments indicate that MAHAKIL improves the prediction performance for all the models and achieves better and more significant pf values than the other oversampling approaches, based on robust statistical tests.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453138,Software defect prediction;Class imbalance learning;Synthetic sample generation;Data sampling methods;Classification problems,Software;Software engineering;Software measurement;Urban areas;Biological system modeling;Predictive models;Robustness,learning (artificial intelligence);pattern classification;sampling methods;software fault tolerance;software metrics;software quality;statistical testing,sampling approaches;prediction models;prediction performance;oversampling approaches;class imbalance issue;software defect prediction;software defect datasets;chromosomal theory;synthetic oversampling approach;MAHAKIL approach;robust statistical tests,3
153,Testing 2,[Journal First] On the Use of Hidden Markov Model to Predict the Time to Fix Bugs,M. Habayeb; S. S. Murtaza; A. Miranskyy; A. B. Bener,"Ryerson University, Toronto, ON, CA; Ryerson University, Toronto, ON, CA; Ryerson University, Toronto, ON, CA; Ryerson University, Toronto, ON, CA",2018,"A significant amount of time is spent by software developers in investigating bug reports. It is useful to indicate when a bug report will be closed, since it would help software teams to prioritise their work. Several studies have been conducted to address this problem in the past decade. Most of these studies have used the frequency of occurrence of certain developer activities as input attributes in building their prediction models. However, these approaches tend to ignore the temporal nature of the occurrence of these activities. In this paper, a novel approach using Hidden Markov models (HMMs) and temporal sequences of developer activities is proposed. The approach is empirically demonstrated in a case study using eight years of bug reports collected from the Firefox project.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453139,Bug repositories;Temporal activities;Time to fix a bug;Hidden Markov model,Computer bugs;Hidden Markov models;Software;Software engineering;Data science;Predictive models;Industrial engineering,hidden Markov models;program debugging,software developers;bug report;software teams;prediction models;hidden Markov model;bug fixing;Firefox project;HMM,
154,Studying Software Engineers 2,[Journal First] What Makes a Great Manager of Software Engineers?,E. Kalliamvakou; C. Bird; T. Zimmermann; A. Begel; R. DeLine; D. M. German,"University of Victoria, Victoria, BC, CA; Microsoft Research, Redmond, WA, US; Microsoft Research, Redmond, WA, US; Microsoft Research, Redmond, WA, US; Microsoft Research, Redmond, WA, US; University of Victoria, Victoria, BC, CA",2018,"Having great managers is as critical to success as having a good team or organization. A great manager is seen as fuelling the team they manage, enabling it to use its full potential. Though software engineering research studies factors that may affect the performance and productivity of software engineers and teams (like tools and skill), it has overlooked the software engineering manager. On the one hand, experts are questioning how the abundant work in management applies to software engineering. On the other hand, practitioners are looking to researchers for evidence-based guidance on how to manage software teams. We conducted a mixed methods empirical study to investigate what manager attributes developers and engineering managers perceive important and why. We present a conceptual framework of manager attributes, and find that technical skills are not the sign of greatness for an engineering manager. Through statistical analysis we identify how engineers and managers relate in their views, and how software engineering differs from other knowledge work groups.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453140,Manager;Software Engineering;Knowledge Work;Empirical Study;Conceptual Framework;Manager Attributes;Technical Skills,Software;Software engineering;Birds;Productivity;Organizations;Tools;Knowledge engineering,DP management;project management;software development management;statistical analysis;team working,great manager;software engineers;good team;software engineering manager;software teams;manager attributes;software engineering research studies;statistical analysis;productivity,1
155,Studying Software Engineers 2,[Journal First] Older Adults and Hackathons: A Qualitative Study,W. Kopec; B. Balcerzak; R. Nielek; G. Kowalik; A. Wierzbicki; F. Casati,"Polish-Japanese Acad. of Inf. Technol., Warsaw, Japan; Polish-Japanese Acad. of Inf. Technol., Warsaw, Japan; Polish-Japanese Acad. of Inf. Technol., Warsaw, Japan; Polish-Japanese Acad. of Inf. Technol., Warsaw, Japan; Polish-Japanese Acad. of Inf. Technol., Warsaw, Japan; Universita degli Studi di Trento, Trento, Trentino-Alto Adige, IT",2018,"Globally observed trends in aging indicate that older adults constitute a growing share of the population and an increasing demographic in the modern technologies marketplace. Therefore, it has become important to address the issue of participation of older adults in the process of developing solutions suitable for their group. In this study, we approached this topic by organizing a hackathon involving teams of young programmers and older adult participants. In our paper we describe a case study of that hackathon, in which our objective was to motivate older adults to participate in software engineering processes. Based on our results from an array of qualitative methods, we propose a set of good practices that may lead to improved older adult participation in similar events and an improved process of developing apps that target older adults.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453141,older adults;elderly;participatory design;co-design;user-centered design;user experience;hackathons;qualitative methods;intergenerational interaction;intergenerational cooperation,Information technology;Software engineering;Software;Collaboration;Human computer interaction;Programming profession,age issues;geriatrics;human factors;software engineering,young programmers;software engineering processes;older adult participants;hackathon,
156,Studying Software Engineers 2,[Journal First] Does Syntax Highlighting Help Programming Novices?,C. Hannebauer; M. Hesenius; V. Gruhn,"NA; Universitat Duisburg-Essen, Duisburg, Nordrhein-Westfalen, DE; Universitat Duisburg-Essen, Duisburg, Nordrhein-Westfalen, DE",2018,"Background: Program comprehension is an important skill for programmers - extending and debugging existing source code is part of the daily routine. Syntax highlighting is one of the most common tools used to support developers in understanding algorithms. However, most research in this area originates from a time when programmers used a completely different tool chain. Objective: We examined the influence of syntax highlighting on novices' ability to comprehend source code. Additional analyses cover the influence of task type and programming experience on the code comprehension ability itself and its relation to syntax highlighting. Method: We conducted a controlled experiment with 390 undergraduate students in an introductory Java programming course. We measured the correctness with which they solved small coding tasks. Each test subject received some tasks with syntax highlighting and some without. Results: The data provided no evidence that syntax highlighting improves novices' ability to comprehend source code. Limitations: There are very few similar experiments and it is unclear as of yet which factors impact the effectiveness of syntax highlighting. One major limitation may be the types of tasks chosen for this experiment. Conclusion: The results suggest that syntax highlighting squanders a feedback channel from the IDE to the programmer that can be used more effectively.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453142,Syntax Highlighting;Source Code Typography;Code Colouring;IDE Interface;Program Comprehension,Syntactics;Task analysis;Programming;Software engineering;Visualization;Human computer interaction;Tools,computer science education;educational courses;feedback;Java;programming;programming environments,source code;syntax highlighting squanders;programming novices;code comprehension;undergraduate students;program comprehension;Java programming course;feedback channel;IDE,
157,Studying Software Engineers 2,Do Programmers Work at Night or During the Weekend?,M. Claes; M. V. MÃ¤ntylÃ¤; M. Kuutila; B. Adams,"Oulun Yliopisto, Oulu, FI; Oulun Yliopisto, Oulu, FI; Oulun Yliopisto, Oulu, FI; MCIS, Polytech. Montreal, Montreal, QC, Canada",2018,"Abnormal working hours can reduce work health, general well-being, and productivity, independent from a profession. To inform future approaches for automatic stress and overload detection, this paper establishes empirically collected measures of the work patterns of software engineers. To this aim, we perform the first largescale study of software engineers' working hours by investigating the time stamps of commit activities of 86 large open source software projects, both containing hired and volunteer developers. We find that two thirds of software engineers mainly follow typical office hours, empirically established to be from 10h to 18h, and do not usually work during nights and weekends. Large variations between projects and individuals exist. Surprisingly, we found no support that project maturation would decrease abnormal working hours. In the Firefox case study, we found that hired developers work more during office hours while seniority, either in terms of number of commits or job status, did not impact working hours. We conclude that the use of working hours or timestamps of work products for stress detection requires establishing baselines at the level of individuals.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453143,software repository mining;overtime;overwork;open source;apache;mozilla;weekend;night,Companies;Software;Computer bugs;Software engineering;Productivity;Rhythm;Stress,health care;occupational health;project management;public domain software;software development management,abnormal working hours;work health;work patterns;software engineers;work products;open source software projects;overload detection;time stamps;stress detection;Firefox,12
158,Program Analysis 2,Multi-granular Conflict and Dependency Analysis in Software Engineering Based on Graph Transformation,L. Lambers; D. StrÃ¼ber; G. Taentzer; K. Born; J. Huebert,"Hasso-Plattner-Inst. Potsdam, Potsdam, Germany; Universitat Koblenz-Landau, Koblenz, Rheinland-Pfalz, DE; Philipps-Universitat Marburg, Marburg, Hessen, DE; Philipps-Universitat Marburg, Marburg, Hessen, DE; Philipps-Universitat Marburg, Marburg, Hessen, DE",2018,"Conflict and dependency analysis (CDA) of graph transformation has been shown to be a versatile foundation for understanding interactions in many software engineering domains, including software analysis and design, model-driven engineering, and testing. In this paper, we propose a novel static CDA technique that is multi-granular in the sense that it can detect all conflicts and dependencies on multiple granularity levels. Specifically, we provide an efficient algorithm suite for computing binary, coarse-grained, and fine-grained conflicts and dependencies: Binary granularity indicates the presence or absence of conflicts and dependencies, coarse granularity focuses on root causes for conflicts and dependencies, and fine granularity shows each conflict and dependency in full detail. Doing so, we can address specific performance and usability requirements that we identified in a literature survey of CDA usage scenarios. In an experimental evaluation, our algorithm suite computes conflicts and dependencies rapidly. Finally, we present a user study, in which the participants found our coarse-grained results more understandable than the fine-grained ones reported in a state-of-the-art tool. Our overall contribution is twofold: (i) we significantly speed up the computation of fine-grained and binary CDA results and, (ii) complement them with coarse-grained ones, which offer usability benefits for numerous use cases.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453144,automated static analysis,Software engineering;Data mining;Usability;Task analysis;Model driven engineering,graph theory;program testing;software engineering,graph transformation;software engineering domains;model-driven engineering;multiple granularity levels;coarse granularity;CDA usage scenarios;fine granularity;multigranular conflict and dependency analysis;software testing;static CDA technique;binary granularity,3
159,Program Analysis 2,Self-Hiding Behavior in Android Apps: Detection and Characterization,Z. Shan; I. Neamtiu; R. Samuel,"Wichita State University, Wichita, KS, US; New Jersey Institute of Technology, Newark, NJ, US; New Jersey Institute of Technology, Newark, NJ, US",2018,"Applications (apps) that conceal their activities are fundamentally deceptive; app marketplaces and end-users should treat such apps as suspicious. However, due to its nature and intent, activity concealing is not disclosed up-front, which puts users at risk. In this paper, we focus on characterization and detection of such techniques, e.g., hiding the app or removing traces, which we call ""self hiding behavior"" (SHB). SHB has not been studied per se - rather it has been reported on only as a byproduct of malware investigations. We address this gap via a study and suite of static analyses targeted at SH in Android apps. Specifically, we present (1) a detailed characterization of SHB, (2) a suite of static analyses to detect such behavior, and (3) a set of detectors that employ SHB to distinguish between benign and malicious apps. We show that SHB ranges from hiding the app's presence or activity to covering an app's traces, e.g., by blocking phone calls/text messages or removing calls and messages from logs. Using our static analysis tools on a large dataset of 9,452 Android apps (benign as well as malicious) we expose the frequency of 12 such SH behaviors. Our approach is effective: it has revealed that malicious apps employ 1.5 SHBs per app on average. Surprisingly, SH behavior is also employed by legitimate (""benign"") apps, which can affect users negatively in multiple ways. When using our approach for separating malicious from benign apps, our approach has high precision and recall (combined F-measure = 87.19%). Our approach is also efficient, with analysis typically taking just 37 seconds per app. We believe that our findings and analysis tool are beneficial to both app marketplaces and end-users.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453145,Android;static analysis;malware;mobile security,Malware;Smart phones;Tools;Static analysis;Security;Google;Software engineering,Android (operating system);invasive software;mobile computing;program diagnostics,Android apps;trace removal;self hiding behavior detection;activity concealing;app marketplaces;benign apps;legitimate apps;malicious apps;static analyses,8
160,Program Analysis 2,[Journal First] The Scent of a Smell: An Extensive Comparison Between Textual and Structural Smells,F. Palomba; A. Panichella; A. Zaidman; R. Oliveto; A. De Lucia,"Universitat Zurich, Zurich, ZH, CH; Technische Universiteit Delft, Delft, Zuid-Holland, NL; Technische Universiteit Delft, Delft, Zuid-Holland, NL; Universita degli Studi del Molise, Campobasso, Molise, IT; Universita degli Studi di Salerno, Fisciano, Campania, IT",2018,"Code smells are symptoms of poor design or implementation choices that have a negative effect on several aspects of software maintenance and evolution, such as program comprehension or change-and fault-proneness. This is why researchers have spent a lot of effort on devising methods that help developers to automatically detect them in source code. Almost all the techniques presented in literature are based on the analysis of structural properties extracted from source code, although alternative sources of information (e.g., textual analysis) for code smell detection have also been recently investigated. Nevertheless, some studies have indicated that code smells detected by existing tools based on the analysis of structural properties are generally ignored (and thus not refactored) by the developers. In this paper, we aim at understanding whether code smells detected using textual analysis are perceived and refactored by developers in the same or different way than code smells detected through structural analysis. To this aim, we set up two different experiments. We have first carried out a software repository mining study to analyze how developers act on textually or structurally detected code smells. Subsequently, we have conducted a user study with industrial developers and quality experts in order to qualitatively analyze how they perceive code smells identified using the two different sources of information. Results indicate that textually detected code smells are easier to identify and for this reason they are considered easier to refactor with respect to code smells detected using structural properties. On the other hand, the latter are often perceived as more severe, but more difficult to exactly identify and remove.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453146,code smells;empirical study;mining software repositories,Tools;Software engineering;Maintenance engineering;Data mining;Detectors;Software systems;Software quality,data mining;software maintenance;source code (software),structural smells;source code;textual analysis;code smell detection;textually detected code smells;textual smells;software maintenance;software evolution;software repository mining study,
161,Program Analysis 2,ConflictJS: Finding and Understanding Conflicts Between JavaScript Libraries,J. Patra; P. N. Dixit; M. Pradel,NA; NA; NA,2018,"It is a common practice for client-side web applications to build on various third-party JavaScript libraries. Due to the lack of namespaces in JavaScript, these libraries all share the same global namespace. As a result, one library may inadvertently modify or even delete the APIs of another library, causing unexpected behavior of library clients. Given the quickly increasing number of libraries, manually keeping track of such conflicts is practically impossible both for library developers and users. This paper presents ConflictJS, an automated and scalable approach to analyze libraries for conflicts. The key idea is to tackle the huge search space of possible conflicts in two phases. At first, a dynamic analysis of individual libraries identifies pairs of potentially conflicting libraries. Then, targeted test synthesis validates potential conflicts by creating a client application that suffers from a conflict. The overall approach is free of false positives, in the sense that it reports a problem only when such a client exists. We use ConflictJS to analyze and study conflicts among 951 real-world libraries. The results show that one out of four libraries is potentially conflicting and that 166 libraries are involved in at least one certain conflict. The detected conflicts cause crashes and other kinds of unexpected behavior. Our work helps library developers to prevent conflicts, library users to avoid combining conflicting libraries, and provides evidence that designing a language without explicit namespaces has undesirable effects.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453147,JavaScript libraries;testing,Libraries;Loading;Computer crashes;Cryptography;Content distribution networks;Servers,application program interfaces;Internet;Java;libraries;system monitoring,ConflictJS;JavaScript libraries;client-side web applications;global namespace;search space;dynamic analysis,8
162,Software Comprehension,Debugging Data Flows in Reactive Programs,H. Banken; E. Meijer; G. Gousios,"Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands",2018,"Reactive Programming is a style of programming that provides developers with a set of abstractions that facilitate event handling and stream processing. Traditional debug tools lack support for Reactive Programming, leading developers to fallback to the most rudimentary debug tool available: logging to the console. In this paper, we present the design and implementation of RxFiddle, a visualization and debugging tool targeted to Rx, the most popular form of Reactive Programming. RxFiddle visualizes the dependencies and structure of the data flow, as well as the data inside the flow. We evaluate RxFiddle with an experiment involving 111 developers. The results show that RxFiddle can help developers finish debugging tasks faster than with traditional debugging tools.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453148,reactive programming;debugging;visualization;program comprehension,Debugging;Programming;Tools;Observers;Libraries;Companies;Interviews,program debugging;program visualisation,debugging tool;RxFiddle;data flow;debugging tasks;traditional debugging tools;stream processing;traditional debug tools;rudimentary debug tool;reactive programming,5
163,Software Comprehension,Do You Remember This Source Code?,J. KrÃ¼ger; J. Wiemann; W. Fenske; G. Saake; T. Leich,"Hochschule Harz - Hochschule fur angewandte Wissenschaften, Wernigerode, DE; Otto von Guericke Universitat Magdeburg, Magdeburg, Sachsen-Anhalt, DE; Otto von Guericke Universitat Magdeburg, Magdeburg, Sachsen-Anhalt, DE; Otto von Guericke Universitat Magdeburg, Magdeburg, Sachsen-Anhalt, DE; Hochschule Harz - Hochschule fur angewandte Wissenschaften, Wernigerode, DE",2018,"Being familiar with the source code of a program comprises knowledge about its purpose, structure, and details. Consequently, familiarity is an important factor in many contexts of software development, especially for maintenance and program comprehension. As a result, familiarity is considered to some extent in many different approaches, for example, to model costs or to identify experts. Still, all approaches we are aware of require a manual assessment of familiarity and empirical analyses of forgetting in software development are missing. In this paper, we address this issue with an empirical study that we conducted with 60 open-source developers. We used a survey to receive information on the developers' familiarity and analyze the responses based on data we extract from their used version control systems. The results show that forgetting is an important factor when considering familiarity and program comprehension of developers. We find that a forgetting curve is partly applicable for software development, investigate three factors - the number of edits, ratio of owned code, and tracking behavior - that can impact familiarity with code, and derive a general memory strength for our participants. Our findings can be used to scope approaches that have to consider familiarity and they provide insights into forgetting in the context of software development.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453149,familiarity;forgetting;empirical study;maintenance;program comprehension;expert identification;knowledge management,Software;Software engineering;Task analysis;Maintenance engineering;Psychology;Software reliability,configuration management;software development management;software maintenance,source code;software development;owned code;open-source developers,4
164,Software Comprehension,Inferring Hierarchical Motifs from Execution Traces,S. Alimadadi; A. Mesbah; K. Pattabiraman,"Northeastern University, Boston, MA, US; The University of British Columbia Faculty of Medicine, Vancouver, BC, CA; The University of British Columbia Faculty of Medicine, Vancouver, BC, CA",2018,"Program comprehension is a necessary step for performing many software engineering tasks. Dynamic analysis is effective in producing execution traces that assist comprehension. Traces are rich sources of information regarding the behaviour of a program. However, it is challenging to gain insight from traces due to their overwhelming amount of data and complexity. We propose a generic technique for facilitating comprehension by inferring recurring execution motifs. Inspired by bioinformatics, motifs are patterns in traces that are flexible to small changes in execution, and are captured in a hierarchical model. The hierarchical nature of the model provides an overview of the behaviour at a high-level, while preserving the execution details and intermediate levels in a structured manner. We design a visualization that allows developers to observe and interact with the model. We implement our approach in an open-source tool, called Sabalan, and evaluate it through a user experiment. The results show that using Sabalan improves developers' accuracy in performing comprehension tasks by 54%.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453150,Program comprehension;behavioural model;hierarchical motifs,Tools;Task analysis;Electronic mail;Biological system modeling;Software engineering;Bioinformatics;Data visualization,bioinformatics;object-oriented programming;program visualisation;software engineering;software maintenance,execution traces;program comprehension;software engineering tasks;dynamic analysis;assist comprehension;execution motifs;hierarchical model;hierarchical nature;execution details;comprehension tasks;inferring hierarchical motifs,9
165,Software Comprehension,[Journal First] A Comparison of Program Comprehension Strategies by Blind and Sighted Programmers,A. Armaly; P. Rodeghero; C. McMillan,"University of Notre Dame, Notre Dame, IN, US; University of Notre Dame, Notre Dame, IN, US; University of Notre Dame, Notre Dame, IN, US",2018,"Programmers who are blind use a screen reader to speak source code one word at a time, as though the code were text. This process of reading is in stark contrast to sighted programmers, who skim source code rapidly with their eyes. At present, it is not known whether the difference in these processes has effects on the program comprehension gained from reading code. These effects are important because they could reduce both the usefulness of accessibility tools and the generalizability of software engineering studies to persons with low vision. In this paper, we present an empirical study comparing the program comprehension of blind and sighted programmers. We found that both blind and sighted programmers prioritize reading method signatures over other areas of code. Both groups obtained an equal and high degree of comprehension, despite the different reading processes.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453151,Program comprehension;accessibility technology;blindness,Software engineering;Tools;Java;Measurement;Software maintenance;Visualization,handicapped aids;programming environments;public domain software;software engineering,program comprehension strategies;sighted programmers;source code;reading processes,1
166,Performance and Maintenance,Identifying Patch Correctness in Test-Based Program Repair,Y. Xiong; X. Liu; M. Zeng; L. Zhang; G. Huang,"Peking University, Beijing, Beijing, CN; Peking University, Beijing, Beijing, CN; Peking University, Beijing, Beijing, CN; Peking University, Beijing, Beijing, CN; Peking University, Beijing, Beijing, CN",2018,"Test-based automatic program repair has attracted a lot of attention in recent years However, the test suites in practice are often too weak to guarantee correctness and existing approaches often generate a large number of incorrect patches. To reduce the number of incorrect patches generated, we propose a novel approach that heuristically determines the correctness of the generated patches. The core idea is to exploit the behavior similarity of test case executions. The passing tests on original and patched programs are likely to behave similarly while the failing tests on original and patched programs are likely to behave differently. Also, if two tests exhibit similar runtime behavior, the two tests are likely to have the same test results. Based on these observations, we generate new test inputs to enhance the test suites and use their behavior similarity to determine patch correctness. Our approach is evaluated on a dataset consisting of 139 patches generated from existing program repair systems including jGenProg, Nopol, jKali, ACS, and HDRepair. Our approach successfully prevented 56.3% of the incorrect patches to be generated, without blocking any correct patches.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453152,Test based program repair;patch correctness;patch classification;test generation,Maintenance engineering;Search problems;Software;Computer crashes;Null value;Runtime,automatic programming;program debugging;program diagnostics;program testing;software maintenance,identifying patch correctness;test-based program repair;automatic program repair;incorrect patches;generated patches;behavior similarity;test case executions;passing tests;original programs;patched programs;similar runtime behavior;correct patches;program repair systems;jGenProg,42
167,Performance and Maintenance,How not to Structure Your Database-Backed Web Applications: A Study of Performance Bugs in the Wild,J. Yang; C. Yan; P. Subramaniam; S. Lu; A. Cheung,NA; NA; NA; NA; NA,2018,"Many web applications use databases for persistent data storage, and using Object Relational Mapping (ORM) frameworks is a common way to develop such database-backed web applications. Unfortunately, developing efficient ORM applications is challenging, as the ORM framework hides the underlying database query generation and execution. This problem is becoming more severe as these applications need to process an increasingly large amount of persistent data. Recent research has targeted specific aspects of performance problems in ORM applications. However, there has not been any systematic study to identify common performance anti-patterns in real-world such applications, how they affect resulting application performance, and remedies for them. In this paper, we try to answer these questions through a comprehensive study of 12 representative real-world ORM applications. We generalize 9 ORM performance anti-patterns from more than 200 performance issues that we obtain by studying their bug-tracking systems and profiling their latest versions. To prove our point, we manually fix 64 performance issues in their latest versions and obtain a median speedup of 2Ã— (and up to 39Ã— max) with fewer than 5 lines of code change in most cases. Many of the issues we found have been confirmed by developers, and we have implemented ways to identify other code fragments with similar issues as well.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453153,performance anti-patterns;Object-Relational Mapping frameworks;database-backed applications;bug study,Rails;Scalability;Servers;Indexes;Computer bugs;Semantics,Internet;program debugging;public domain software;query processing;relational databases,persistent data storage;efficient ORM applications;ORM framework hides;underlying database query generation;performance problems;common performance anti-patterns;resulting application performance;real-world ORM applications;200 performance issues;performance bugs;object relational mapping frameworks;database-backed Web applications;ORM performance anti-patterns,15
168,Performance and Maintenance,Speedoo: Prioritizing Performance Optimization Opportunities,Z. Chen; B. Chen; L. Xiao; X. Wang; L. Chen; Y. Liu; B. Xu,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; School of Computer Science, Systems Fudan University, China; School of Computer Science and Engineering Nanyang, Technological University, Singapore; School of Systems and Enterprises, Stevens Institute of Technology; School of Systems and Enterprises, Stevens Institute of Technology; State Key Laboratory for Novel Software Technology, Nanjing University, China",2018,"Performance problems widely exist in modern software systems. Existing performance optimization techniques, including profiling-based and pattern-based techniques, usually fail to consider the architectural impacts among methods that easily slow down the overall system performance. This paper contributes a new approach, named Speedoo, to identify groups of methods that should be treated together and deserve high priorities for performance optimization. The uniqueness of Speedoo is to measure and rank the performance optimization opportunities of a method based on 1) the architectural impact and 2) the optimization potential. For each highly ranked method, we locate a respective Optimization Space based on 5 performance patterns generalized from empirical observations. The top ranked optimization spaces are suggested to developers as potential optimization opportunities. Our evaluation on three real-life projects has demonstrated that 18.52% to 42.86% of methods in the top ranked optimization spaces indeed undertook performance optimization in the projects. This outperforms one of the state-of-the-art profiling tools YourKit by 2 to 3 times. An important implication of this study is that developers should treat methods in an optimization space together as a group rather than as individuals in performance optimization. The proposed approach can provide guidelines and reduce developers' manual effort.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453154,Performance;Metrics;Architecture,Optimization;Measurement;Software;System performance;Computer architecture;Complexity theory;Manuals,optimisation;program diagnostics;software performance evaluation,Speedoo;performance optimization techniques;pattern-based techniques;architectural impact;system performance;optimization potential;software systems;optimization space;profiling-based techniques,8
169,Performance and Maintenance,[Journal First] Empirical Study on the Discrepancy Between Performance Testing Results from Virtual and Physical Environments,M. M. Arif; W. Shang; E. Shihab,"Concordia University, Montreal, QC, CA; Concordia University, Montreal, QC, CA; Concordia University, Montreal, QC, CA",2018,"Large software systems often undergo performance tests to ensure their capability to handle expected loads. These performance tests often consume large amounts of computing resources and time since heavy loads need to be generated. Making it worse, the ever evolving eld requires frequent updates to the performance testing environment. In practice, virtual machines (VMs) are widely exploited to provide exible and less costly environments for performance tests. However, the use of VMs may introduce confounding overhead (e.g., a higher than expected memory utilization with unstable I/O tra c) to the testing environment and lead to unre-alistic performance testing results. Yet, little research has studied the impact on test results of using VMs in performance testing activities. To evaluate the discrepancy between the performance testing results from virtual and physical environments, we perform a case study on two open source systems - namely Dell DVD Store (DS2) and CloudStore. We conduct the same performance tests in both virtual and physical environments and compare the performance testing results based on the three aspects that are typically examined for performance testing results: 1) single performance metric (e.g. CPU Time from virtual environment vs. CPU Time from physical environment), 2) the relationship among performance metrics (e.g. correlation between CPU and I/O) and 3) performance models that are built to predict system performance. Our results show that 1) A single metric from virtual and physical environments do not follow the same distribution, hence practitioners cannot simply use a scaling factor to compare the performance between environments, 2) correlations among performance metrics in virtual environments are different from those in physical environments 3) statistical models built based on the performance metrics from virtual environments are different from the models built from physical environments suggesting that practitioners cannot use the performance testing results across virtual and physical environments. In order to assist the practitioners leverage performance testing results in both environments, we investigate ways to reduce the discrepancy. We find that such discrepancy can be reduced by normalizing performance metrics based on deviance. Overall, we suggest that practitioners should not use the performance testing results from virtual environment with the simple assumption of straightforward performance overhead. Instead, practitioners should consider leveraging normalization techniques to reduce the discrepancy before examining performance testing results from virtual and physical environments.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453155,Software performance engineering;Software performance analysis;Testing on virtual environments,Testing;Measurement;Software engineering;Virtual environments;Correlation;Software performance,program testing;public domain software;software metrics;statistical analysis;virtual machines,performance metrics;performance testing environment;unre-alistic performance testing results;performance testing activities;physical environments;software systems;open source systems;Dell DVD store;CloudStore;practitioners leverage performance testing;virtual environments;scaling factor;statistical models,
170,Requirements and Recommender Systems,The Evolution of Requirements Practices in Software Startups,C. Gralha; D. Damian; A. Wasserman; M. GoulÃ£o; J. AraÃºjo,NA; NA; NA; NA; NA,2018,"We use Grounded Theory to study the evolution of requirements practices of 16 software startups as they grow and introduce new products and services. These startups operate in a dynamic environment, with significant time and market pressure, and rarely have time for systematic requirements analysis. Our theory describes the evolution of practice along six dimensions that emerged as relevant to their requirements activities: requirements artefacts, knowledge management, requirements-related roles, planning, technical debt and product quality. Beyond the relationships among the dimensions, our theory also explains the turning points that drove the evolution along these dimensions. These changes are reactive, rather than planned, suggesting an overall pragmatic lightness, i.e., flexibility, in the startups' evolution towards engineering practices for requirements. Our theory organises knowledge about evolving requirements practice in maturing startups, and provides practical insights for startups' assessing their own evolution as they face challenges to their growth. Our research also suggests that a startup's evolution along the six dimensions is not fundamental to its success, but has significant effects on their product, their employees and the company.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453156,requirements engineering;startups;evolution;grounded theory,Companies;Interviews;Software;Biological system modeling;Turning;Software engineering,organisational aspects;software development management,requirements artefacts;engineering practices;maturing startups;practical insights;software startups;systematic requirement analysis,18
171,Requirements and Recommender Systems,Traceability in the Wild: Automatically Augmenting Incomplete Trace Links,M. Rath; J. Rendall; J. L. C. Guo; J. Cleland-Huang; P. MÃ¤der,"Technical University Ilmenau, Ilmenau, Germany; University of Notre Dame, South Bend, USA; McGill University, Montreal, Canada; University of Notre Dame, Notre Dame, IN, US; Technical University Ilmenau, Ilmenau, Germany",2018,"Software and systems traceability is widely accepted as an essential element for supporting many software development tasks. Today's version control systems provide inbuilt features that allow developers to tag each commit with one or more issue ID, thereby providing the building blocks from which project-wide traceability can be established between feature requests, bug fixes, commits, source code, and specific developers. However, our analysis of six open source projects showed that on average only 60% of the commits were linked to specific issues. Without these fundamental links the entire set of project-wide links will be incomplete, and therefore not trustworthy. In this paper we address the fundamental problem of missing links between commits and issues. Our approach leverages a combination of process and text-related features characterizing issues and code changes to train a classifier to identify missing issue tags in commit messages, thereby generating the missing links. We conducted a series of experiments to evaluate our approach against six open source projects and showed that it was able to effectively recommend links for tagging issues at an average of 96% recall and 33% precision. In a related task for augmenting a set of existing trace links, the classifier returned precision at levels greater than 89% in all projects and recall of 50%.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453157,Traceability;Link Recovery;Machine Learning;Open Source,Computer bugs;Software;Control systems;Task analysis;Software engineering;Jacobian matrices;Tagging,program debugging;public domain software;software maintenance,open source projects;systems traceability;software development tasks;project-wide traceability;source code;project-wide links;text-related features;code changes;missing issue tags identification,34
172,Requirements and Recommender Systems,A Temporal Permission Analysis and Enforcement Framework for Android,A. Sadeghi; R. Jabbarvand; N. Ghorbani; H. Bagheri; S. Malek,"University of California Irvine, Irvine, CA, US; University of California Irvine, Irvine, CA, US; University of California Irvine, Irvine, CA, US; University of Nebraska System, Lincoln, NE, US; University of California Irvine, Irvine, CA, US",2018,"Permission-induced attacks, i.e., security breaches enabled by permission misuse, are among the most critical and frequent issues threatening the security of Android devices. By ignoring the temporal aspects of an attack during the analysis and enforcement, the state-of-the-art approaches aimed at protecting the users against such attacks are prone to have low-coverage in detection and high-disruption in prevention of permission-induced attacks. To address the aforementioned shortcomings, we present TERMINATOR, a temporal permission analysis and enforcement framework for Android. Leveraging temporal logic model checking, TERMINATOR's analyzer identifies permission-induced threats with respect to dynamic permission states of the apps. At runtime, TERMINATOR's enforcer selectively leases (i.e., temporarily grants) permissions to apps when the system is in a safe state, and revokes the permissions when the system moves to an unsafe state realizing the identified threats. The results of our experiments, conducted over thousands of apps, indicate that TERMINATOR is able to provide an effective, yet non-disruptive defense against permission-induced attacks. We also show that our approach, which does not require modification to the Android framework or apps' implementation logic, is highly reliable and widely applicable.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453158,Android;Access Control (Permission);Temporal Logic,Androids;Humanoid robots;Security;Informatics;Malware;Analytical models;Tools,Android (operating system);authorisation;data privacy;temporal logic,permission misuse;temporal aspects;permission-induced attacks;temporal permission analysis;enforcement framework;temporal logic model checking;permission-induced threats;dynamic permission states;Android framework;TERMINATOR enforcer,12
173,Requirements and Recommender Systems,Global-Aware Recommendations for Repairing Violations in Exception Handling,E. A. Barbosa; A. Garcia,"Universidade Federal do Rio Grande do Norte, Natal, RN, BR; Pontificia Universidade Catolica do Rio de Janeiro, Rio de Janeiro, RJ, BR",2018,This paper presents an extended abstract incorporated as a journalrst paper into the ICSE'18 program.,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453159,Exception Handling;Recommender system;Software maintenance,Maintenance engineering;Software engineering;Software maintenance;Java;Tools;Recommender systems,exception handling;object-oriented programming;program diagnostics;recommender systems,global-aware recommendations;repairing violations;journalrst paper;exception handling,4
174,Testing 3,RFC-Directed Differential Testing of Certificate Validation in SSL/TLS Implementations,C. Chen; C. Tian; Z. Duan; L. Zhao,"ICTT & ISN Lab., Xidian Univ. Xi'an, Xi'an, China; ICTT & ISN Lab., Xidian Univ. Xi'an, Xi'an, China; ICTT & ISN Lab., Xidian Univ. Xi'an, Xi'an, China; ICTT & ISN Lab., Xidian Univ. Xi'an, Xi'an, China",2018,"Certificate validation in Secure Socket Layer or Transport Layer Security protocol (SSL/TLS) is critical to Internet security. Thus, it is significant to check whether certificate validation in SSL/TLS is correctly implemented. With this motivation, we propose a novel differential testing approach which is directed by the standard Request For Comments (RFC). First, rules of certificates are extracted automatically from RFCs. Second, low-level test cases are generated through dynamic symbolic execution. Third, high-level test cases, i.e. certificates, are assembled automatically. Finally, with the assembled certificates being test cases, certificate validations in SSL/TLS implementations are tested to reveal latent vulnerabilities or bugs. Our approach named RFCcert has the following advantages: (1) certificates of RFCcert are discrepancy-targeted since they are assembled according to standards instead of genetics; (2) with the obtained certificates, RFCcert not only reveals the invalidity of traditional differential testing but also is able to conduct testing that traditional differential testing cannot do; and (3) the supporting tool of RFCcert has been implemented and extensive experiments show that the approach is effective in finding bugs of SSL/TLS implementations.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453160,Differential testing;certificate validation;SSL/TLS;dynamic symbolic execution;Request For Comments,Testing;Computer bugs;Public key;Standards;Servers;Protocols,certification;computer network security;Internet;program testing;protocols;security of data,RFC-directed differential testing;certificate validation;Internet security;novel differential testing approach;low-level test cases;high-level test cases;assembled certificates;RFCcert;traditional differential testing;secure socket layer;transport layer security protocol;SSL-TLS implementations,7
175,Testing 3,Symbolic Verification of Regular Properties,H. Yu; Z. Chen; J. Wang; Z. Su; W. Dong,"State Key Laboratory of High Performance Computing, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; State Key Laboratory of High Performance Computing, National University of Defense Technology, Changsha, China; Department of Computer Science, University of California, Davis, USA; College of Computer, National University of Defense Technology, Changsha, China",2018,"Verifying the regular properties of programs has been a significant challenge. This paper tackles this challenge by presenting symbolic regular verification (SRV) that offers significant speedups over the state-of-the-art. SRV is based on dynamic symbolic execution (DSE) and enabled by novel techniques for mitigating path explosion: (1) a regular property-oriented path slicing algorithm, and (2) a synergistic combination of property-oriented path slicing and guiding. Slicing prunes redundant paths, while guiding boosts the search for counterexamples. We have implemented SRV for Java and evaluated it on 15 real-world open-source Java programs (totaling 259K lines of code). Our evaluation results demonstrate the effectiveness and efficiency of SRV. Compared with the state-of-the-art - pure DSE, pure guiding, and pure path slicing - SRV achieves average speedups of more than 8.4X, 8.6X, and 7X, respectively, making symbolic regular property verification significantly more practical.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453161,Regular property;Verification;Dynamic Symbolic Execution;Slicing;Guiding,Java;Software engineering;Software;Task analysis;History;Explosions;Heuristic algorithms,Java;program slicing;program verification;public domain software,symbolic regular verification;SRV;dynamic symbolic execution;regular property-oriented path slicing algorithm;real-world open-source Java programs;symbolic regular property verification;program verification;path explosion mitigation,6
176,Testing 3,Metamorphic Testing of RESTful Web APIs,S. Segura; J. A. Parejo; J. Troya; A. Ruiz-CortÃ©s,"Universidad de Sevilla, Sevilla, AndalucÃƒÂ­a, ES; Universidad de Sevilla, Sevilla, AndalucÃƒÂ­a, ES; Universidad de Sevilla, Sevilla, AndalucÃƒÂ­a, ES; Universidad de Sevilla, Sevilla, AndalucÃƒÂ­a, ES",2018,"Web Application Programming Interfaces (APIs) specify how to access services and data over the network, typically using Web services. Web APIs are rapidly proliferating as a key element to foster reusability, integration, and innovation, enabling new consumption models such as mobile or smart TV apps. Companies such as Facebook, Twitter, Google, eBay or Netflix receive billions of API calls every day from thousands of different third-party applications and devices, which constitutes more than half of their total traffic. As Web APIs are progressively becoming the cornerstone of software integration, their validation is getting more critical. In this context, the fast detection of bugs is of utmost importance to increase the quality of internal products and third-party applications. However, testing Web APIs is challenging mainly due to the difficulty to assess whether the output of an API call is correct, i.e., the oracle problem. For instance, consider the Web API of the popular music streaming service Spotify. Suppose a search for albums with the query ""redhouse"" returning 21 total matches: Is this output correct? Do all the albums in the result set contain the keyword? Are there any albums containing the keyword not included in the result set? Answering these questions is difficult, even with small result sets, and often infeasible when the results are counted by thousands or millions. Metamorphic testing alleviates the oracle problem by providing an alternative when the expected output of a test execution is complex or unknown. Rather than checking the output of an individual program execution, metamorphic testing checks whether multiple executions of the program under test fulfil certain necessary properties called metamorphic relations. For instance, consider the following metamorphic relation in Spotify: two searches for albums with the same query should return the same number of total results regardless of the size of pagination. Suppose that a new Spotify search is performed using the exact same query as before and increasing the maximum number of results per page from 20 (default value) to 50: This search returns 27 total albums (6 more matches than in the previous search), which reveals a bug. This is an example of a real and reproducible fault detected using the approach presented in this paper and reported to Spotify. According to Spotify developers, it was a regression fault caused by a fix with undesired side effects. In this paper [1], we present a metamorphic testing approach for the automated detection of faults in RESTful Web APIs (henceforth also referred to as simply Web APIs). We introduce the concept of metamorphic relation output patterns. A Metamorphic Relation Output Pattern (MROP) defines an abstract output relation typically identified in Web APIs, regardless of their application domain. Each MROP is defined in terms of set operations among test outputs such as equality, union, subset, or intersection. MROPs provide a helpful guide for the identification of metamorphic relations, broadening the scope of our work beyond a particular Web API. Based on the notion of MROP, a methodology is proposed for the application of the approach to any Web API following the REST architectural pattern. The approach was evaluated in several steps. First, we used the proposed methodology to identify 33 metamorphic relations in four Web APIs developed by undergraduate students. All the relations are instances of the proposed MROPs. Then, we assessed the effectiveness of the identified relations at revealing 317 automatically seeded faults (i.e., mutants) in the APIs under test. As a result, 302 seeded faults were detected, achieving a mutation score of 95.3%. Second, we evaluated the approach using real Web APIs and faults. In particular, we identified 20 metamorphic relations in the Web API of Spotify and 40 metamorphic relations in the Web API of YouTube. Each metamorphic relation was implemented and automatically executed using both random and manual test data. In total, 469K metamorphic tests were generated. As a result, 21 metamorphic relations were violated, and 11 issues revealed and reported (3 issues in Spotify and 8 issues in YouTube). To date, 10 of the reported issues have been either confirmed by the API developers or reproduced by other users supporting the effectiveness of our approach.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453162,Metamorphic testing;REST;RESTful Web services;Web API,Testing;Software engineering;Software;Computer bugs;YouTube;Computer languages;Technological innovation,application program interfaces;program testing;Web services,metamorphic relation output pattern;RESTful Web API;MROP;metamorphic testing;REST architectural pattern,4
177,Testing 3,Integrating Technical Debt Management and Software Quality Management Processes: A Framework and Field Tests,N. Ramasubbu; C. Kemerer,"University of Pittsburgh, Pittsburgh, PA, US; University of Pittsburgh, Pittsburgh, PA, US",2018,"Technical debt, defined as the maintenance obligations arising from shortcuts taken during the design, development, and deployment of software systems, has been shown to significantly impact the reliability and long-term evolution of software systems [1], [2]. Although academic research has moved beyond using technical debt only as a metaphor, and has begun compiling strong empirical evidence on the economic implications of technical debt, industry practitioners continue to find managing technical debt a challenging balancing act [3]. Despite the increasing awareness of the importance of managing technical debt in software product development, systematic processes for implementing technical debt management in software production have not been readily available. To address this gap, we developed and field tested a normative process framework that systematically incorporates steps for managing technical debt in commercial software production. The framework integrates processes required for technical debt management with existing software quality management processes prescribed by the project management body of knowledge (PMBOK) [4], and organizes the different processes for technical debt management under three steps: (1) make technical debt visible, (2) perform cost-benefit analysis, and (3) control technical debt. To implement the processes, we introduce a new artifact, called the technical debt register, which stores, for each software asset, the outstanding principal and the associated interest estimated for the technical debt embedded in the asset. The technical debt register also stores the desired control target for each software asset's technical debt, which is populated and used during the cost-benefit analysis and control target calculations. There are three main benefits from this integrated approach. First, it enables the uncovering of hidden technical debt embedded in systems. Established quality assurance and control practices can be utilized to effectively associate software defects with specific design and deployment decisions made by programmers. Such associations make technical debt visible to the team and thereby facilitate the quantification of debt-related principal and interest. Second, it helps to bridge the gaps that exist between the technical and economic assessments of technical debt, and aid in formulating actionable policies related to technical debt management. Finally, integrating technical debt management processes with established quality frameworks aids the wider adoption of emerging prescriptions for managing technical debt. We partnered with three commercial software product development organizations to implement the framework in real-world software production settings. All three organizations, irrespective of their varying software process maturity levels, were able to adopt the proposed framework and integrate the prescribed technical debt management processes with their existing software quality management processes. Our longitudinal data and case-study interviews indicate that the organizations were able to accrue economic benefits from the adoption and use of the integrated framework. And, based on our field study observations, we also identified a set of best practices that support the implementation and use of our framework: facilitating engagement between business and engineering stakeholders, adoption of policies based on a probabilistic analysis framework, and limiting process overheads.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453163,Technical debt;software quality;software maintenance;software engineering economics;cost of quality;software product development;software process;case study,Software quality;Economics;Software engineering;Software systems;Product development;Production,product development;project management;quality assurance;quality management;software development management;software maintenance;software quality,software systems;managing technical debt;existing software quality management processes;technical debt register;software asset;hidden technical debt;prescribed technical debt management processes;integrating technical debt management;control technical debt,5
178,Mining Software Repositories,Understanding the Factors for Fast Answers in Technical Q&A Websites: An Empirical Study of Four Stack Exchange Websites,S. Wang; T. -H. Chen; A. E. Hassan,"Queen's University Faculty of Health Sciences, Kingston, ON, CA; Concordia University, Montreal, QC, CA; Queen's University Faculty of Health Sciences, Kingston, ON, CA",2018,"Technical questions and answers (Q&A) websites accumulate a significant amount of knowledge from users. Developers are especially active on these Q&A websites, since developers are constantly facing new development challenges that require help from other experts. Over the years, Q&A website designers have derived several incentive systems (e.g., gamification) to encourage users to answer questions that are posted by others. However, the current incentive systems primarily focus on the quantity and quality of the answers instead of encouraging the rapid answering of questions. Improving the speed of getting an answer can significantly improve the user experience and increase user engagement on such Q&A websites. In this paper [1], we study the factors for fast answers on such Q&A websites. Our goal is to explore how one may improve the current incentive systems to motivate fast answering of questions. We use a logistic regression model to analyze 46 factors along four dimensions (i.e., question, asker, answer, and answerer dimension) in order to understand the relationship between the studied factors and the needed time to get an accepted answer. The question dimension calculates various textual and readability features of a question, as well as the popularity and difficulty of the question's tags. The asker dimension calculates the reputation of an asker and his/her historical tendency to get answers. The answer dimension computes textual features from the text of the accepted answer. The answerer dimension computes the historical activity level of the answerer who answered the question. We conduct our study on the four most popular (i.e., with the most questions) Q&A Stack Exchange websites: Stack Overflow, Mathematics, Ask Ubuntu, and Superuser. We find that (i) factors in the answerer dimension have the strongest effect on the needed time to get an accepted answer, after controlling for other factors; (ii) the current incentive system does not recognize non-frequent answerers who often answer questions which frequent answerers are not able to answer well. Such questions that are answered by non-frequent answerers are as important as those that are answered by frequent answerers; (iii) the current incentive system motivates frequent answerers well, but such frequent answerers tend to answer short questions. Our findings suggest that the designers of Q&A website should improve their incentive systems to motivate non-frequent answerers to be more active and to answer questions faster, in order to shorten the waiting time for an answer (especially for questions that require specific knowledge that frequent answerers might not possess). In addition, the question answering incentive system needs to factor in the value and difficulty of answering the questions (e.g., by providing more rewards to harder questions or questions that remain unanswered for a long period of time).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453164,Q&A website;Factor importance analysis;Response time,Software engineering;Software;Knowledge engineering;Computer science;Logistics;Analytical models;Mathematics,human factors;question answering (information retrieval);regression analysis;software engineering;Web sites,technical questions;incentive systems;answer questions;rapid answering;Q&A website;fast answering;answerer dimension;accepted answer;question dimension;answer dimension;nonfrequent answerers;current incentive system motivates frequent answerers;answer short questions;question answering incentive system;stack exchange Websites;answer Websites,5
179,Mining Software Repositories,Towards Reusing Hints from Past Fixes: An Exploratory Study on Thousands of Real Samples,H. Zhong; N. Meng,"Shanghai Jiao Tong University, Shanghai, CN; Virginia Polytechnic Institute and State University, Blacksburg, VA, US",2018,"Researchers have recently proposed various automatic program repair (APR) approaches that reuse past fixes to fix new bugs. However, some fundamental questions, such as how new fixes overlap with old fixes, have not been investigated. Intuitively, the overlap between old and new fixes decides how APR approaches can construct new fixes with old ones. Based on this intuition, we systematically designed six overlap metrics, and performed an empirical study on 5,735 bug fixes to investigate the usefulness of past fixes when composing new fixes. For each bug fix, we created delta dependency graphs (i.e., program dependency graphs for code changes), and identified how bug fixes overlapped with each other in terms of the content, code structure, and identifier names of fixes. Our results show that if an APR approach composes new fixes by fully or partially reusing the content of past fixes, only 2.1% and 3.2% new fixes can be created from single or multiple past fixes in the same project, compared with 0.9% and 1.2% fixes created from past fixes across projects. However, if an APR approach composes new fixes by fully or partially reusing the code structure of past fixes, up to 41.3% and 29.7% new fixes can be created.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453165,reusing past fix;empirical study;program repair,Computer bugs;Maintenance engineering;Syntactics;Software engineering;Computer science;Software;Measurement,program debugging;software maintenance,automatic program repair;APR approach;delta dependency graphs,1
180,Mining Software Repositories,Are Code Examples on an Online Q&A Forum Reliable?: A Study of API Misuse on Stack Overflow,T. Zhang; G. Upadhyaya; A. Reinhardt; H. Rajan; M. Kim,NA; NA; NA; NA; NA,2018,"Programmers often consult an online Q&A forum such as Stack Overflow to learn new APIs. This paper presents an empirical study on the prevalence and severity of API misuse on Stack Overflow. To reduce manual assessment effort, we design ExampleCheck, an API usage mining framework that extracts patterns from over 380K Java repositories on GitHub and subsequently reports potential API usage violations in Stack Overflow posts. We analyze 217,818 Stack Overflow posts using ExampleCheck and find that 31% may have potential API usage violations that could produce unexpected behavior such as program crashes and resource leaks. Such API misuse is caused by three main reasons-missing control constructs, missing or incorrect order of API calls, and incorrect guard conditions. Even the posts that are accepted as correct answers or upvoted by other programmers are not necessarily more reliable than other posts in terms of API misuse. This study result calls for a new approach to augment Stack Overflow with alternative API usage details that are not typically shown in curated examples.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453166,online Q&A forums;API usage pattern;code example assessment,Data mining;Java;Software;Software reliability;Syntactics;Libraries,application program interfaces;data mining;Java;learning (artificial intelligence);program diagnostics;public domain software;question answering (information retrieval);Web sites,API misuse;API usage mining framework;potential API usage violations;API calls;alternative API usage details;Stack Overflow posts,48
181,Mining Software Repositories,[Journal First] Inference of Development Activities from Interaction with Uninstrumented Applications,L. Bao; Z. Xing; X. Xia; D. Lo; A. E. Hassan,"Zhejiang University, Hangzhou, Zhejiang, CN; Australian National University, Canberra, ACT, AU; Monash University, Clayton, VIC, AU; Singapore Management University, Singapore, Singapore, SG; Queen's University Faculty of Health Sciences, Kingston, ON, CA",2018,"This paper is published in Journal of Empirical Software Engineering (DOI: 10.1007/s10664-017-9547-8). Studying developers' behavior is crucial for designing effective techniques and tools to support developers' daily work. However, there are two challenges in collecting and analyzing developers' behavior data. First, instrumenting many software tools commonly used in real work settings (e.g., IDEs, web browsers) is difficult and requires significant resources. Second, the collected behavior data consist of low-level and fine-grained event sequences, which must be abstracted into high-level development activities for further analysis. To address these two challenges, we first use our ActivitySpace framework to improve the generalizability of the data collection. Then, we propose a Condition Random Field (CRF) based approach to segment and label the developers' low-level actions into a set of basic, yet meaningful development activities. To evaluate our proposed approach, we deploy the ActivitySpace framework in an industry partner's company and collect the real working data from ten professional developers' one-week work. We conduct an experiment with the collected data and a small number of initial human-labeled training data using the CRF model and the other three baselines (i.e., a heuristic-rules based method, a SVM classifier, and a random weighted classifier). The proposed CRF model achieves better performance (i.e., 0.728 accuracy and 0.672 macro-averaged F1-score) than the other three baselines.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453167,Software development;Developers' interaction data;Condition Random Field,Software engineering;Australia;Tools;Software;Computer science;Information technology;Information systems,pattern classification;software engineering;software tools,empirical software engineering;Web browsers;condition random field based approach;low-level actions;data collection;high-level development activities;fine-grained event sequences;collected behavior data;work settings;software tools;uninstrumented applications;CRF model;one-week work;professional developers;working data;ActivitySpace framework;meaningful development activities,2
182,Models and Modeling 1,Propagating Configuration Decisions with Modal Implication Graphs,S. Krieter; T. ThÃ¼m; S. Schulze; R. SchrÃ¶ter; G. Saake,"Otto von Guericke Universitat Magdeburg, Magdeburg, Sachsen-Anhalt, DE; Technische Universitat Braunschweig, Braunschweig, Niedersachsen, DE; Otto von Guericke Universitat Magdeburg, Magdeburg, Sachsen-Anhalt, DE; Otto von Guericke Universitat Magdeburg, Magdeburg, Sachsen-Anhalt, DE; Otto von Guericke Universitat Magdeburg, Magdeburg, Sachsen-Anhalt, DE",2018,"Highly-configurable systems encompass thousands of interdependent configuration options, which require a non-trivial configuration process. Decision propagation enables a backtracking-free configuration process by computing values implied by user decisions. However, employing decision propagation for large-scale systems is a time-consuming task and, thus, can be a bottleneck in interactive configuration processes and analyses alike. We propose modal implication graphs to improve the performance of decision propagation by precomputing intermediate values used in the process. Our evaluation results show a significant improvement over state-of-the-art algorithms for 120 real-world systems.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453168,Software product line;Configuration;Decision Propagation,Servers;Frequency modulation;Hafnium;Operating systems;Monitoring;Task analysis,backtracking;configuration management;graph theory;large-scale systems;performance evaluation,modal implication graphs;interdependent configuration options;nontrivial configuration process;decision propagation;backtracking-free configuration process;large-scale systems;interactive configuration processes;configuration decisions,5
183,Models and Modeling 1,A Combinatorial Approach for Exposing Off-Nominal Behaviors,K. Madala; H. Do; D. Aceituna,"University of North Texas, Denton, Texas; University of North Texas, Denton, Texas; DISTek Integration, Inc.",2018,"Off-nominal behaviors (ONBs) have been a major concern in the areas of embedded systems and safety-critical systems. To address ONB problems, some researchers have proposed model-based approaches that can expose ONBs by analyzing natural language requirements documents. While these approaches produced promising results, they require a lot of human effort and time. In this paper, to reduce human effort and time, we propose a combinatorial-based approach, Combinatorial Causal Component Model (Combi-CCM), which uses structured requirements patterns and combinations generated using the IPOG algorithm. We conducted an empirical study using several requirements documents to evaluate our approach, and our results indicate that the proposed approach can reduce human effort and time while maintaining the same ONB exposure ability obtained by the control techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453169,Off-Nominal Behaviors;Requirements Verification;Combinatorial Approach;Model-based Approach,Ear;Electronic countermeasures;Robot sensing systems;Natural languages;Analytical models;Writing,combinatorial mathematics;embedded systems;formal specification;formal verification;natural language processing;safety-critical software,off-nominal behaviors;embedded systems;safety-critical systems;ONB problems;natural language requirements documents;combinatorial-based approach;requirements patterns;ONB exposure ability;combinatorial approach;combinatorial causal component model;IPOG algorithm,5
184,Models and Modeling 1,Identifying Design Problems in the Source Code: A Grounded Theory,L. Sousa; A. Oliveira; W. Oizumi; S. Barbosa; A. Garcia; J. Lee; M. Kalinowski; R. de Mello; B. Fonseca; R. Oliveira; C. Lucena; R. Paes,NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA; NA,2018,"The prevalence of design problems may cause re-engineering or even discontinuation of the system. Due to missing, informal or outdated design documentation, developers often have to rely on the source code to identify design problems. Therefore, developers have to analyze different symptoms that manifest in several code elements, which may quickly turn into a complex task. Although researchers have been investigating techniques to help developers in identifying design problems, there is little knowledge on how developers actually proceed to identify design problems. In order to tackle this problem, we conducted a multi-trial industrial experiment with professionals from 5 software companies to build a grounded theory. The resulting theory offers explanations on how developers identify design problems in practice. For instance, it reveals the characteristics of symptoms that developers consider helpful. Moreover, developers often combine different types of symptoms to identify a single design problem. This knowledge serves as a basis to further understand the phenomena and advance towards more effective identification techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453170,design problem;grounded theory;software design;symptoms,Software systems;Task analysis;Software design;Fats;Documentation;Companies,DP industry;software engineering;source code (software);system documentation,source code;grounded theory;design documentation;design problems identification;code elements;software companies,17
185,Models and Modeling 1,Predicting Future Developer Behavior in the IDE Using Topic Models,K. Damevski; H. Chen; D. C. Shepherd; N. A. Kraft; L. Pollock,"Virginia Commonwealth University, Richmond, VA, US; Brooklyn, New York, NY, USA; ABB Corp. Res., Raleigh, NC, USA; ABB Corp. Res., Raleigh, NC, USA; University of Delaware, Newark, DE, US",2018,"Interaction data, gathered from developers' daily clicks and key presses in the IDE, has found use in both empirical studies and in recommendation systems for software engineering. We observe that this data has several characteristics, common across IDEs: 1) exponentially distributed - some events or commands dominate the trace (e.g., cursor movement commands), while most other commands occur relatively infrequently; 2) noisy - the traces include spurious commands (or clicks), or unrelated events, that may not be important to the behavior of interest; 3) comprise of overlapping events and commands - specific commands can be invoked by separate mechanisms, and similar events can be triggered by different sources. These characteristics of this data are analogous to the characteristics of synonymy and polysemy in natural language corpora. Therefore, this paper (and presentation) presents a new modeling approach for this type of data, leveraging topic models typically applied to streams of natural language text.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453171,command recommendation systems;IDE interaction data,Software engineering,natural language processing;recommender systems;software engineering;text analysis,modeling approach;leveraging topic models;IDE;interaction data;key presses;recommendation systems;software engineering;spurious commands;unrelated events;future developer behavior prediction;developers daily clicks;cursor movement commands;overlapping events;overlapping commands;synonymy;polysemy;natural language corpora;natural language text,
186,"Code Search, Synthesis, Performance",Deep Code Search,X. Gu; H. Zhang; S. Kim,"Hong Kong University of Science and Technology, Kowloon, HK; The University of Newcastle, Callaghan, NSW, AU; Hong Kong University of Science and Technology, Kowloon, HK",2018,"To implement a program functionality, developers can reuse previously written code snippets by searching through a large-scale codebase. Over the years, many code search tools have been proposed to help developers. The existing approaches often treat source code as textual documents and utilize information retrieval models to retrieve relevant code snippets that match a given query. These approaches mainly rely on the textual similarity between source code and natural language query. They lack a deep understanding of the semantics of queries and source code. In this paper, we propose a novel deep neural network named CODEnn (Code-Description Embedding Neural Network). Instead of matching text similarity, CODEnn jointly embeds code snippets and natural language descriptions into a high-dimensional vector space, in such a way that code snippet and its corresponding description have similar vectors. Using the unified vector representation, code snippets related to a natural language query can be retrieved according to their vectors. Semantically related words can also be recognized and irrelevant/noisy keywords in queries can be handled. As a proof-of-concept application, we implement a code search tool named DeepCS using the proposed CODEnn model. We empirically evaluate DeepCS on a large scale codebase collected from GitHub. The experimental results show that our approach can effectively retrieve relevant code snippets and outperforms previous techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453172,code search;deep learning;joint embedding,Natural languages;XML;Tools;Semantics;Machine learning;Recurrent neural networks,natural language processing;neural nets;query processing;text analysis,code search tool;source code;information retrieval models;relevant code snippets;natural language query;deep neural network;DeepCS;deep code search;large-scale codebase;code-description embedding neural network;CODEnn model;DeepCS;textual documents;matching text similarity,172
187,"Code Search, Synthesis, Performance",[Journal First] Augmenting and Structuring User Queries to Support Efficient Free-Form Code Search,R. Sirres; T. F. BissyandÃ©; D. Kim; D. Lo; J. Klein; K. Kim; Y. Le Traon,"Nat. Libr. of Luxembourg, Luxembourg City, Luxembourg; Universite du Luxembourg, Luxembourg, LU; Universite du Luxembourg, Luxembourg, LU; Singapore Manage. Univ. - Singapore, Singapore, Singapore; Universite du Luxembourg, Luxembourg, LU; Universite du Luxembourg, Luxembourg, LU; Universite du Luxembourg, Luxembourg, LU",2018,"Source code terms such as method names and variable types are often different from conceptual words mentioned in a search query. This vocabulary mismatch problem can make code search inefficient. In this paper, we present Code voCaBulary (CoCaBu), an approach to resolving the vocabulary mismatch problem when dealing with free-form code search queries. Our approach leverages common developer questions and the associated expert answers to augment user queries with the relevant, but missing, structural code entities in order to improve the performance of matching relevant code examples within large code repositories. To instantiate this approach, we build GitSearch, a code search engine, on top of GitHub and Stack Overflow Q&A data. We evaluate GitSearch in several dimensions to demonstrate that (1) its code search results are correct with respect to user-accepted answers; (2) the results are qualitatively better than those of existing Internet-scale code search engines; (3) our engine is competitive against web search engines, such as Google, in helping users solve programming tasks; and (4) GitSearch provides code examples that are acceptable or interesting to the community as answers for Stack Overflow questions.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453173,Code search;GitHub;Free form search;Query augmentation;StackOverflow;Vocabulary mismatch,Search engines;Software engineering;Vocabulary;Indexes;Programming;Engines;Natural languages,Internet;query processing;question answering (information retrieval);search engines,structuring user queries;source code terms;search query;vocabulary mismatch problem;free-form code search queries;structural code entities;code repositories;GitSearch;user-accepted answers;web search engines;Internet-scale code search engines;expert answers;code vocabulary;CoCaBu;developer questions;stack overflow Q and A data;Google;programming tasks;user queries augmentation,3
188,"Code Search, Synthesis, Performance",FaCoY â€“ A Code-to-Code Search Engine,K. Kim; D. Kim; T. F. BissyandÃ©; E. Choi; L. Li; J. Klein; Y. Le Traon,"Universite du Luxembourg, Luxembourg, LU; Universite du Luxembourg, Luxembourg, LU; Universite du Luxembourg, Luxembourg, LU; Nara Sentan Kagaku Gijutsu Daigakuin Daigaku, Ikoma, Nara, JP; Monash University, Clayton, VIC, AU; Universite du Luxembourg, Luxembourg, LU; Universite du Luxembourg, Luxembourg, LU",2018,"Code search is an unavoidable activity in software development. Various approaches and techniques have been explored in the literature to support code search tasks. Most of these approaches focus on serving user queries provided as natural language free-form input. However, there exists a wide range of use-case scenarios where a code-to-code approach would be most beneficial. For example, research directions in code transplantation, code diversity, patch recommendation can leverage a code-to-code search engine to find essential ingredients for their techniques. In this paper, we propose FaCoY, a novel approach for statically finding code fragments which may be semantically similar to user input code. FaCoY implements a query alternation strategy: instead of directly matching code query tokens with code in the search space, FaCoY first attempts to identify other tokens which may also be relevant in implementing the functional behavior of the input code. With various experiments, we show that (1) FaCoY is more effective than online code-to-code search engines; (2) FaCoY can detect more semantic code clones (i.e., Type-4) in BigCloneBench than the state-of-the-art; (3) FaCoY, while static, can detect code fragments which are indeed similar with respect to runtime execution behavior; and (4) FaCoY can be useful in code/patch recommendation.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453174,code search;semantic clones;code to code search,Cloning;Search engines;Semantics;Software;Natural languages;Syntactics;Runtime,query processing;search engines;software engineering;software maintenance,code-to-code search engine;code search tasks;code-to-code approach;code-patch recommendation;FaCoY;semantic code clones;directly matching code query tokens;user input code;statically finding code fragments;code diversity;code transplantation,18
189,"Code Search, Synthesis, Performance",Generalized Data Structure Synthesis,C. Loncaric; M. D. Ernst; E. Torlak,"University of Washington, Seattle, WA, US; University of Washington, Seattle, WA, US; University of Washington, Seattle, WA, US",2018,"Data structure synthesis is the task of generating data structure implementations from high-level specifications. Recent work in this area has shown potential to save programmer time and reduce the risk of defects. Existing techniques focus on data structures for manipulating subsets of a single collection, but real-world programs often track multiple related collections and aggregate properties such as sums, counts, minimums, and maximums. This paper shows how to synthesize data structures that track subsets and aggregations of multiple related collections. Our technique decomposes the synthesis task into alternating steps of query synthesis and incrementalization. The query synthesis step implements pure operations over the data structure state by leveraging existing enumerative synthesis techniques, specialized to the data structures domain. The incrementalization step implements imperative state modifications by re-framing them as fresh queries that determine what to change, coupled with a small amount of code to apply the change. As an added benefit of this approach over previous work, the synthesized data structure is optimized for not only the queries in the specification but also the required update operations. We have evaluated our approach in four large case studies, demonstrating that these extensions are broadly applicable.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453175,Program synthesis;automatic programming;data structures,Data structures;Synthesizers;Task analysis;Software;Servers;Tools,data structures;query processing;reasoning about programs;set theory,data structure implementations;track subsets;incrementalization;data structure state;data structures domain;data structure synthesis;query synthesis;aggregations;re-framing;code,2
190,Software Tools and Environments,A Graph Solver for the Automated Generation of Consistent Domain-Specific Models,O. SemerÃ¡th; A. S. Nagy; D. VarrÃ³,"MTA-BME Lendulet Cyber-Phys. Syst. Res. Group, Budapest, Hungary; MTA-BME Lendulet Cyber-Phys. Syst. Res. Group, Budapest, Hungary; McGill University, Montreal, QC, CA",2018,"Many testing and benchmarking scenarios in software and systems engineering depend on the systematic generation of graph models. For instance, tool qualification necessitated by safety standards would require a large set of consistent (well-formed or malformed) instance models specific to a domain. However, automatically generating consistent graph models which comply with a metamodel and satisfy all well-formedness constraints of industrial domains is a significant challenge. Existing solutions which map graph models into first-order logic specification to use back-end logic solvers (like Alloy or Z3) have severe scalability issues. In the paper, we propose a graph solver framework for the automated generation of consistent domain-specific instance models which operates directly over graphs by combining advanced techniques such as refinement of partial models, shape analysis, incremental graph query evaluation, and rule-based design space exploration to provide a more efficient guidance. Our initial performance evaluation carried out in four domains demonstrates that our approach is able to generate models which are 1-2 orders of magnitude larger (with 500 to 6000 objects!) compared to mapping-based approaches natively using Alloy.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453176,Graph generation;Test generation;Domain Specific Modeling Languages;Logic Solver;Graph Solver,Analytical models;Object oriented modeling;Tools;Biological system modeling;IP networks;Testing,formal logic;formal specification;formal verification;graph theory;query processing;specification languages,consistent domain-specific models;automated model generation;software engineering;graph models;domain-specific instance models;mapping-based approach;incremental graph query evaluation;graph solver framework;back-end logic solvers;first-order logic specification;safety standards;tool qualification;systematic generation;systems engineering,9
191,Software Tools and Environments,Automatically Finding Bugs in a Commercial Cyber-Physical System Development Tool Chain With SLforge,S. A. Chowdhury; S. Mohian; S. Mehra; S. Gawsane; T. T. Johnson; C. Csallner,"Computer Science & Eng. Dept., University of Texas at Arlington, Arlington, TX, USA; Computer Science & Eng. Dept., University of Texas at Arlington, Arlington, TX, USA; Computer Science & Eng. Dept., University of Texas at Arlington, Arlington, TX, USA; Computer Science & Eng. Dept., University of Texas at Arlington, Arlington, TX, USA; EECS Department, Vanderbilt University, Nashville, TN, USA; Computer Science & Eng. Dept., University of Texas at Arlington, Arlington, TX, USA",2018,"Cyber-physical system (CPS) development tool chains are widely used in the design, simulation, and verification of CPS data-flow models. Commercial CPS tool chains such as MathWorks' Simulink generate artifacts such as code binaries that are widely deployed in embedded systems. Hardening such tool chains by testing is crucial since formally verifying them is currently infeasible. Existing differential testing frameworks such as CyFuzz can not generate models rich in language features, partly because these tool chains do not leverage the available informal Simulink specifications. Furthermore, no study of existing Simulink models is available, which could guide CyFuzz to generate realistic models. To address these shortcomings, we created the first large collection of public Simulink models and used the collected models' properties to guide random model generation. To further guide model generation we systematically collected semi-formal Simulink specifications. In our experiments on several hundred models, the resulting SLforge generator was more effective and efficient than the state-of-the-art tool CyFuzz. SLforge also found 8 new confirmed bugs in Simulink.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453177,cyber-physical system;Simulink;differential testing;tool chain bug,Software packages;Tools;Testing;Computer bugs;Object oriented modeling;Numerical models;Cyber-physical systems,embedded systems;formal verification;mathematics computing;program debugging;program testing,CPS data-flow models;commercial CPS tool chains;embedded systems;differential testing frameworks;available informal Simulink specifications;realistic models;public Simulink models;random model generation;semiformal Simulink specifications;finding bugs;commercial cyber-physical system development tool chain;cyber-physical system development tool chains;MathWork Simulink;CyFuzz;SLforge generator,13
192,Software Tools and Environments,Context-Aware Conversational Developer Assistants,N. Bradley; T. Fritz; R. Holmes,"The University of British Columbia Faculty of Medicine, Vancouver, BC, CA; Universitat Zurich, Zurich, ZH, CH; The University of British Columbia Faculty of Medicine, Vancouver, BC, CA",2018,"Building and maintaining modern software systems requires developers to perform a variety of tasks that span various tools and information sources. The crosscutting nature of these development tasks requires developers to maintain complex mental models and forces them (a) to manually split their high-level tasks into low-level commands that are supported by the various tools, and (b) to (re) establish their current context in each tool. In this paper we present Devy, a Conversational Developer Assistant (CDA) that enables developers to focus on their high-level development tasks. Devy reduces the number of manual, often complex, low-level commands that developers need to perform, freeing them to focus on their high-level tasks. Specifically, Devy infers high-level intent from developer's voice commands and combines this with an automatically-generated context model to determine appropriate workflows for invoking low-level tool actions; where needed, Devy can also prompt the developer for additional information. Through a mixed methods evaluation with 21 industrial developers, we found that Devy provided an intuitive interface that was able to support many development tasks while helping developers stay focused within their development environment. While industrial developers were largely supportive of the automation Devy enabled, they also provided insights into several other tasks and workflows CDAs could support to enable them to better focus on the important parts of their development tasks.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453178,Conversational Development Assistants;Natural User Interfaces,Task analysis;Tools;Context;Context modeling;Software;Switches,interactive systems;software engineering;ubiquitous computing,high-level tasks;low-level commands;Devy;high-level development tasks;high-level intent;automatically-generated context model;low-level tool actions;development environment;context-aware conversational developer assistants;modern software systems,22
193,Software Tools and Environments,"Open Source Barriers to Entry, Revisited: A Sociotechnical Perspective",C. Mendez; H. S. Padala; Z. Steine-Hanson; C. Hildebrand; A. Horvath; C. Hill; L. Simpson; N. Patil; A. Sarma; M. Burnett,"Oregon State University, Corvallis, OR, US; Oregon State University, Corvallis, OR, US; Oregon State University, Corvallis, OR, US; Oregon State University, Corvallis, OR, US; Oregon State University, Corvallis, OR, US; Oregon State University, Corvallis, OR, US; Oregon State University, Corvallis, OR, US; Oregon State University, Corvallis, OR, US; Oregon State University, Corvallis, OR, US; Oregon State University, Corvallis, OR, US",2018,"Research has revealed that significant barriers exist when entering Open-Source Software (OSS) communities and that women disproportionately experience such barriers. However, this research has focused mainly on social/cultural factors, ignoring the environment itself â€” the tools and infrastructure. To shed some light onto how tools and infrastructure might somehow factor into OSS barriers to entry, we conducted a field study with five teams of software professionals, who worked through five use-cases to analyze the tools and infrastructure used in their OSS projects. These software professionals found tool/infrastructure barriers in 7% to 71% of the use-case steps that they analyzed, most of which are tied to newcomer barriers that have been established in the literature. Further, over 80% of the barrier types they found include attributes that are biased against women.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453179,open source software;newcomer;gender,Tools;Databases;Problem-solving;Open source software;Cultural differences;Face,cultural aspects;public domain software,sociotechnical perspective;OSS barriers;software professionals;OSS projects;newcomer barriers;open source barriers;open-source software communities;social-cultural factors;tool-infrastructure barriers;use-case steps,36
194,Search-Based Software Engineering 1,Testing Vision-Based Control Systems Using Learnable Evolutionary Algorithms,R. Ben Abdessalem; S. Nejati; L. C. Briand; T. Stifter,"SnT Centre, University of Luxembourg; Universite du Luxembourg, Luxembourg, LU; SnT Centre, University of Luxembourg; IEE S.A., Luxembourg",2018,"Vision-based control systems are key enablers of many autonomous vehicular systems, including self-driving cars. Testing such systems is complicated by complex and multidimensional input spaces. We propose an automated testing algorithm that builds on learnable evolutionary algorithms. These algorithms rely on machine learning or a combination of machine learning and Darwinian genetic operators to guide the generation of new solutions (test scenarios in our context). Our approach combines multiobjective population-based search algorithms and decision tree classification models to achieve the following goals: First, classification models guide the search-based generation of tests faster towards critical test scenarios (i.e., test scenarios leading to failures). Second, search algorithms refine classification models so that the models can accurately characterize critical regions (i.e., the regions of a test input space that are likely to contain most critical test scenarios). Our evaluation performed on an industrial automotive automotive system shows that: (1) Our algorithm outperforms a baseline evolutionary search algorithm and generates 78% more distinct, critical test scenarios compared to the baseline algorithm. (2) Our algorithm accurately characterizes critical regions of the system under test, thus identifying the conditions that are likely to lead to system failures.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453180,Search-based Software Engineering;Evolutionary algorithms;Software Testing;Automotive Software Systems,Testing;Classification algorithms;Roads;Control systems;Evolutionary computation;Decision trees;Automotive engineering,decision trees;evolutionary computation;genetic algorithms;learning (artificial intelligence);mobile robots;robot vision;search problems,vision-based control systems;learnable evolutionary algorithms;autonomous vehicular systems;complex input spaces;multidimensional input spaces;automated testing algorithm;machine learning;multiobjective population-based search algorithms;decision tree classification models;search-based generation;search algorithms refine classification models;test input space;industrial automotive automotive system;baseline evolutionary search algorithm;distinct test scenarios,77
195,Search-Based Software Engineering 1,To Preserve or Not to Preserve Invalid Solutions in Search-Based Software Engineering: A Case Study in Software Product Lines,J. Guo; K. Shi,"Alibaba Group, China; East China University of Science and Technology, China",2018,"Multi-objective evolutionary algorithms (MOEAs) have been successfully applied for software product lines (SPLs) to search for optimal or near-optimal solutions that balance multiple objectives. However, MOEAs usually produce invalid solutions that violate the constraints predefined. As invalid solutions are unbuildable in practice, we debate the preservation of invalid solutions during the search. We conduct experiments on seven real-world SPLs, including five largest SPLs hitherto reported and two SPLs with realistic values and constraints of quality attributes. We identify three potential limitations of preserving invalid solutions. Furthermore, based on the state-of-the-art, we design five algorithm variants that adopt different evolutionary operators. By performance evaluation, we provide empirical guidance on how to preserve valid solutions. Our empirical study demonstrates that whether or not to preserve invalid solutions deserves more attention in the community, and in some cases, we have to preserve valid solutions all along the way.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453181,Search-based software engineering;software product lines;multi objective evolutionary algorithms;constraint solving;validity,Optimization;Sociology;Statistics;Software;Search problems;Software product lines,evolutionary computation;search problems;software engineering;software product lines,near-optimal solutions;search-based software engineering;software product lines;multiobjective evolutionary algorithms;MOEA;SPL,2
196,Search-Based Software Engineering 1,Nemo: Multi-criteria Test-Suite Minimization with Integer Nonlinear Programming,J. -W. Lin; R. Jabbarvand; J. Garcia; S. Malek,"University High School, Irvine, CA, US; University High School, Irvine, CA, US; University High School, Irvine, CA, US; University High School, Irvine, CA, US",2018,"Multi-criteria test-suite minimization aims to remove redundant test cases from a test suite based on some criteria such as code coverage, while trying to optimally maintain the capability of the reduced suite based on other criteria such as fault-detection effectiveness. Existing techniques addressing this problem with integer linear programming claim to produce optimal solutions. However, the multi-criteria test-suite minimization problem is inherently nonlinear, due to the fact that test cases are often dependent on each other in terms of test-case criteria. In this paper, we propose a framework that formulates the multi-criteria test-suite minimization problem as an integer nonlinear programming problem. To solve this problem optimally, we programmatically transform this nonlinear problem into a linear one and then solve the problem using modern linear solvers. We have implemented our framework as a tool, called Nemo, that supports a number of modern linear and nonlinear solvers. We have evaluated Nemo with a publicly available dataset and minimization problems involving multiple criteria including statement coverage, fault-revealing capability, and test execution time. The experimental results show that Nemo can be used to efficiently find an optimal solution for multi-criteria test-suite minimization problems with modern solvers, and the optimal solutions outperform the suboptimal ones by up to 164.29% in terms of the criteria considered in the problem.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453182,Test-suite minimization;integer programming,Minimization;Linear programming;Optimization;Mathematical model;Tools;Testing;Programming,integer programming;linear programming;minimisation;nonlinear programming;program testing,redundant test cases;optimal solution;multicriteria test-suite minimization problem;test-case criteria;integer nonlinear programming problem,3
197,Search-Based Software Engineering 1,"Is ""Better Data"" Better Than ""Better Data Miners""?",A. Agrawal; T. Menzies,"Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA",2018,"We report and fix an important systematic error in prior studies that ranked classifiers for software analytics. Those studies did not (a) assess classifiers on multiple criteria and they did not (b) study how variations in the data affect the results. Hence, this paper applies (a) multi-performance criteria while (b) fixing the weaker regions of the training data (using SMOTUNED, which is an auto-tuning version of SMOTE). This approach leads to dramatically large increases in software defect predictions when applied in a 5*5 cross-validation study for 3,681 JAVA classes (containing over a million lines of code) from open source systems, SMOTUNED increased AUC and recall by 60% and 20% respectively. These improvements are independent of the classifier used to predict for defects. Same kind of pattern (improvement) was observed when a comparative analysis of SMOTE and SMOTUNED was done against the most recent class imbalance technique. In conclusion, for software analytic tasks like defect prediction, (1) data pre-processing can be more important than classifier choice, (2) ranking studies are incomplete without such pre-processing, and (3) SMOTUNED is a promising candidate for pre-processing.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453183,Search based SE;defect prediction;classification;data analytics for software engineering;SMOTE;imbalanced data;preprocessing,Software;Measurement;Couplings;Tuning;Task analysis;Complexity theory;Software engineering,data mining;Java;learning (artificial intelligence);pattern classification,dramatically large increases;software defect predictions;open source systems;SMOTE;recent class imbalance technique;software analytic tasks;defect prediction;classifier choice;data miners;important systematic error;software analytics;multiperformance criteria;training data;JAVA classes;SMOTUNED,43
198,Testing 4,[Journal First] Analyzing the Effects of Test Driven Development in GitHub,N. Borle; M. Feghhi; E. Stroulia; R. Grenier; A. Hindle,"University of Alberta, Edmonton, AB, CA; University of Alberta, Edmonton, AB, CA; University of Alberta, Edmonton, AB, CA; University of Alberta, Edmonton, AB, CA; University of Alberta, Edmonton, AB, CA",2018,"Testing is an integral part of the software development lifecycle, approached with varying degrees of rigor by different process models. Agile process models recommend Test Driven Development (TDD) as a key practice for reducing costs and improving code quality. The objective of this work is to perform a cost-benefit analysis of this practice. Previous work by Fucci et al. engaged in laboratory studies of developers actively engaged in test-driven development practices. Fucci et al. found little difference between test-first behaviour of TDD and test-later behaviour. To that end, we opted to conduct a study about TDD behaviours in the ""wild"" rather than in the laboratory. Thus we have conducted a comparative analysis of GitHub repositories that adopts TDD to a lesser or greater extent, in order to determine how TDD affects software development productivity and software quality. We classified GitHub repositories archived in 2015 in terms of how rigorously they practiced TDD, thus creating a TDD spectrum. We then matched and compared various subsets of these repositories on this TDD spectrum with control sets of equal size. The control sets were samples from all GitHub repositories that matched certain characteristics, and that contained at least one test file. We compared how the TDD sets differed from the control sets on the following characteristics: number of test files, average commit velocity, number of bug-referencing commits, number of issues recorded, usage of continuous integration, number of pull requests, and distribution of commits per author. We found that Java TDD projects were relatively rare. In addition, there were very few significant differences in any of the metrics we used to compare TDD-like and non-TDD projects; therefore, our results do not reveal any observable benefits from using TDD.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453184,Test Driven Development;Human Factors in Software Development;GitHub Repositories;Continuous Integration,Software engineering;Software;Testing;Computational modeling;Java;Blogs;Analytical models,cost-benefit analysis;Java;program testing;software engineering;software maintenance;software quality;software reliability,GitHub repositories;TDD spectrum;control sets;test file;TDD sets;Java TDD projects;nonTDD projects;software development lifecycle;agile process models;improving code quality;cost-benefit analysis;test-driven development practices;test-first behaviour;test-later behaviour;software development productivity;software quality;test driven development,3
199,Testing 4,[Journal First] A Comparative Study to Benchmark Cross-Project Defect Prediction Approaches,S. Herbold; A. Trautsch; J. Grabowski,"University of Goettingen, Insititute of Computer Science, GÃ¶ttingen, Germany; University of Goettingen, Insititute of Computer Science, GÃ¶ttingen, Germany; University of Goettingen, Insititute of Computer Science, GÃ¶ttingen, Germany",2018,"Cross-Project Defect Prediction (CPDP) as a means to focus quality assurance of software projects was under heavy investigation in recent years. However, within the current state-of-the-art it is unclear which of the many proposals performs best due to a lack of replication of results and diverse experiment setups that utilize different performance metrics and are based on different underlying data. Within this article, we provide a benchmark for CPDP. We replicate 24 approaches proposed by researchers between 2008 and 2015 and evaluate their performance on software products from five different data sets. Based on our benchmark, we determined that an approach proposed by Camargo Cruz and Ochimizu (2009) based on data standardization performs best and is always ranked among the statistically significant best results for all metrics and data sets. Approaches proposed by Turhan et al. (2009), Menzies et al. (2011), and Watanabe et al. (2008) are also nearly always among the best results. Moreover, we determined that predictions only seldom achieve a high performance of 0.75 recall, precision, and accuracy. Thus, CPDP still has not reached a point where the performance of the results is sufficient for the application in practice.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453185,cross project defect prediction;benchmark;comparison;replication,Benchmark testing;Software;Measurement;Software engineering;Computer science;Proposals;Ranking (statistics),quality assurance;software metrics;software quality,experiment setups;performance metrics;benchmark cross-project defect prediction approaches;data standardization;software products;software projects;quality assurance;CPDP,2
200,Testing 4,[Journal First] MSeer â€“ An Advanced Technique for Locating Multiple Bugs in Parallel,R. Gao; W. E. Wong,"University of Texas at Dallas, Richardson, TX, US; University of Texas at Dallas, Richardson, TX, US",2018,"In practice, a program may contain multiple bugs. The simultaneous presence of these bugs may deteriorate the effectiveness of existing fault-localization techniques to locate program bugs. While it is acceptable to use all failed and successful tests to identify suspicious code for programs with exactly one bug, it is not appropriate to use the same approach for programs with multiple bugs because the due-to relationship between failed tests and underlying bugs cannot be easily identified. One solution is to generate fault-focused clusters by grouping failed tests caused by the same bug into the same clusters. We propose MSeer - an advanced fault localization technique for locating multiple bugs in parallel. Our major contributions include the use of (1) a revised Kendall tau distance to measure the distance between two failed tests, (2) an innovative approach to simultaneously estimate the number of clusters and assign initial medoids to these clusters, and (3) an improved K-medoids clustering algorithm to better identify the due-to relationship between failed tests and their corresponding bugs. Case studies on 840 multiple-bug versions of seven programs suggest that MSeer performs better in terms of effectiveness and efficiency than two other techniques for locating multiple bugs in parallel.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453186,Software fault localization;parallel debugging;multiple bugs;clustering;distance metrics,Computer bugs;Software;Fault diagnosis;Computer science;Clustering algorithms;Measurement;Software engineering,parallel processing;pattern clustering;program debugging;program testing;software fault tolerance,fault-focused clusters;failed tests;advanced fault localization technique;program bugs;bug locating;fault-localization techniques;K-medoids clustering;MSeer,1
201,Testing 4,[Journal First] Journal First Presentation of an Experience Report on Applying Software Testing Academic Results in Industry: We Need Usable Automated Test Generation,A. Arcuri,"Fac. of Technol., Westerdals Oslo ACT, Oslo, Norway",2018,"What is the impact of software engineering research on current practices in industry? In this paper, I report on my direct experience as a PhD/post-doc working in software engineering research projects, and then spending the following five years as an engineer in two different companies (the first one being the same I worked in collaboration with during my post-doc). Given a background in software engineering research, what cutting-edge techniques and tools from academia did I use in my daily work when developing and testing the systems of these companies? Regarding validation and verification (my main area of research), the answer is rather short: as far as I can tell, only FindBugs. In this paper, I report on why this was the case, and discuss all the challenging, complex open problems we face in industry and which somehow are ""neglected"" in the academic circles. In particular, I will first discuss what actual tools I could use in my daily work, such as JaCoCo and Selenium. Then, I will discuss the main open problems I faced, particularly related to environment simulators, unit and web testing. After that, popular topics in academia are presented, such as UML, regression and mutation testing. Their lack of impact on the type of projects I worked on in industry is then discussed. Finally, from this industrial experience, I provide my opinions about how this situation can be improved, in particular related to how academics are evaluated, and advocate for a greater involvement into open-source projects.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453187,Industry;Practice;Technology Transfer;Impact;Applied Research,Software engineering;Industries;Software testing;Test pattern generators;Companies;Tools,automatic test pattern generation;automatic test software;program testing;public domain software;software engineering,cutting-edge techniques;daily work;complex open problems;main open problems;web testing;regression;mutation testing;industrial experience;experience report;usable automated test generation;direct experience;PhD/post-doc;software engineering research projects;time 5.0 year,2
202,Software Evolution and Maintenance 2,CCAligner: A Token Based Large-Gap Clone Detector,P. Wang; J. Svajlenko; Y. Wu; Y. Xu; C. K. Roy,"University of Science and Technology of China, School of Computer Science; Department of Computer Science Canada, University of Saskatchewan; University of Science and Technology of China, School of Computer Science; University of Science and Technology of China, Hefei, Anhui, CN; Department of Computer Science Canada, University of Saskatchewan",2018,"Copying code and then pasting with large number of edits is a common activity in software development, and the pasted code is a kind of complicated Type-3 clone. Due to large number of edits, we consider the clone as a large-gap clone. Large-gap clone can reflect the extension of code, such as change and improvement. The existing state-of-the-art clone detectors suffer from several limitations in detecting large-gap clones. In this paper, we propose a tool, CCAligner, using code window that considers e edit distance for matching to detect large-gap clones. In our approach, a novel e-mismatch index is designed and the asymmetric similarity coefficient is used for similarity measure. We thoroughly evaluate CCAligner both for large-gap clone detection, and for general Type-1, Type-2 and Type-3 clone detection. The results show that CCAligner performs better than other competing tools in large-gap clone detection, and has the best execution time for 10MLOC input with good precision and recall in general Type-1 to Type-3 clone detection. Compared with existing state-of-the-art tools, CCAligner is the best performing large-gap clone detection tool, and remains competitive with the best clone detectors in general Type-1, Type-2 and Type-3 clone detection.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453188,Clone Detection;Large-gap Clone;Evaluation,Cloning;Tools;Detectors;Software;Computer science;Software engineering;Indexes,public domain software;software maintenance;software reusability,pasted code;CCAligner;Type-3 clone detection;performing large-gap clone detection tool;large-gap clone detector,37
203,Software Evolution and Maintenance 2,HireBuild: An Automatic Approach to History-Driven Repair of Build Scripts,F. Hassan; X. Wang,"The University of Texas at, San Antonio; The University of Texas at, San Antonio",2018,"Advancements in software build tools such as Maven reduce build management effort, but developers still need specialized knowledge and long time to maintain build scripts and resolve build failures. More recent build tools such as Gradle give developers greater extent of customization flexibility, but can be even more difficult to maintain. According to the TravisTorrent dataset of open-source software continuous integration, 22% of code commits include changes in build script files to maintain build scripts or to resolve build failures. Automated program repair techniques have great potential to reduce cost of resolving software failures, but the existing techniques mostly focus on repairing source code so that they cannot directly help resolving software build failures. To address this limitation, we propose HireBuild: History-Driven Repair of Build Scripts, the first approach to automatic patch generation for build scripts, using fix patterns automatically generated from existing build script fixes and recommending fix patterns based on build log similarity. From TravisTorrent dataset, we extracted 175 build failures and their corresponding fixes which revise Gradle build scripts. Among these 175 build failures, we used the 135 earlier build fixes for automatic fix-pattern generation and the more recent 40 build failures (fixes) for evaluation of our approach. Our experiment shows that our approach can fix 11 of 24 reproducible build failures, or 45% of the reproducible build failures, within comparable time of manual fixes.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453189,Patch Generation;Software Build Scripts;Build Logs,Maintenance engineering;Software;Tools;Task analysis;Computer bugs;Data mining;Software engineering,software fault tolerance;software maintenance;software tools;source code (software),software build failures;History-Driven Repair;build log similarity;Gradle build scripts;software build tools;build script files;fix patterns;reproducible build failures;HireBuild;Automated program repair techniques;source code;automatic patch generation;build script fixes,27
204,Software Evolution and Maintenance 2,The Road to Live Programming: Insights from the Practice,J. Kubelka; R. Robbes; A. Bergel,NA; NA; NA,2018,"Live Programming environments allow programmers to get feedback instantly while changing software. Liveness is gaining attention among industrial and open-source communities; several IDEs offer high degrees of liveness. While several studies looked at how programmers work during software evolution tasks, none of them consider live environments. We conduct such a study based on an analysis of 17 programming sessions of practitioners using Pharo, a mature Live Programming environment. The study is complemented by a survey and subsequent analysis of 16 programming sessions in additional languages, e.g., JavaScript. We document the approaches taken by developers during their work. We find that some liveness features are extensively used, and have an impact on the way developers navigate source code and objects in their work.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453190,Live Programming;Software Evolution;Exploratory Study,Programming;Tools;Task analysis;Software;Programming environments;Navigation;Visualization,Java;programming environments;public domain software;software engineering;software maintenance,open-source communities;software evolution tasks;live environments;mature Live Programming environment;subsequent analysis;liveness features;programming sessions;developer navigate source code,5
205,Software Evolution and Maintenance 2,Assessing the Threat of Untracked Changes in Software Evolution,A. Hora; D. Silva; M. T. Valente; R. Robbes,"FACOM, UFMS, Brazil; ASERG Group, UFMG, Brazil; ASERG Group, UFMG, Brazil; SwSE Group, Free University of Bozen-Bolzano, Italy",2018,"While refactoring is extensively performed by practitioners, many Mining Software Repositories (MSR) approaches do not detect nor keep track of refactorings when performing source code evolution analysis. In the best case, keeping track of refactorings could be unnecessary work; in the worst case, these untracked changes could significantly affect the performance of MSR approaches. Since the extent of the threat is unknown, the goal of this paper is to assess whether it is significant. Based on an extensive empirical study, we answer positively: we found that between 10 and 21% of changes at the method level in 15 large Java systems are untracked. This results in a large proportion (25%) of entities that may have their histories split by these changes, and a measurable effect on at least two MSR approaches. We conclude that handling untracked changes should be systematically considered by MSR studies.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453191,Mining Software Repositories;Software Evolution;Refactoring,Blogs;Software;Tracking;Bars;History;Tools;Computer bugs,data mining;Java;public domain software;software maintenance,software evolution;software repository approache mining;source code evolution analysis;MSR studies;extensive empirical study;MSR approaches;untracked changes,6
206,Models and Modeling 2,Programming Not Only by Example,H. Peleg; S. Shoham; E. Yahav,Technion; Tel Aviv University; Technion,2018,"Recent years have seen great progress in automated synthesis techniques that can automatically generate code based on some intent expressed by the programmer, but communicating this intent remains a major challenge. When the expressed intent is coarse-grained (for example, restriction on the expected type of an expression), the synthesizer often produces a long list of results for the programmer to choose from, shifting the heavy-lifting to the user. An alternative approach, successfully used in end-user synthesis, is programming by example (PBE), where the user leverages examples to interactively and iteratively refine the intent. However, using only examples is not expressive enough for programmers, who can observe the generated program and refine the intent by directly relating to parts of the generated program. We present a novel approach to interacting with a synthesizer using a granular interaction model. Our approach employs a rich interaction model where (i) the synthesizer decorates a candidate program with debug information that assists in understanding the program and identifying good or bad parts, and (ii) the user is allowed to provide feedback not only on the expected output of a program but also on the program itself. After identifying a program as (partially) correct or incorrect, the user can also explicitly indicate the good or bad parts, to allow the synthesizer to accept or discard parts of the program instead of discarding the program as a whole. We show the value of our approach in a controlled user study. Our study shows that participants have a strong preference for granular feedback instead of examples and can provide granular feedback much faster.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453192,program synthesis;programming by example,Synthesizers;Task analysis;Programming;Object oriented modeling;Tools;Agriculture;Vocabulary,automatic programming;program compilers;program debugging,granular feedback;automated synthesis techniques;end-user synthesis;granular interaction model;programming by example;debug information,3
207,Models and Modeling 2,Goal-Conflict Likelihood Assessment Based on Model Counting,R. Degiovanni; P. Castro; M. Arroyo; M. Ruiz; N. Aguirre; M. Frias,"Universidad Naeional de RÃ­o Cuarto, Argentina; Universidad Naeional de RÃ­o Cuarto and CONICET, Argentina; Universidad Naeional de RÃ­o Cuarto, Argentina; Universidad Naeional de RÃ­o Cuarto, Argentina; Universidad Naeional de RÃ­o Cuarto and CONICET, Argentina; Instituto TeenolÃ³gieo de Buenos Aires and CONICET, Argentina",2018,"In goal-oriented requirements engineering approaches, conflict analysis has been proposed as an abstraction for risk analysis. Intuitively, given a set of expected goals to be achieved by the system-to-be, a conflict represents a subtle situation that makes goals diverge, i.e., not be satisfiable as a whole. Conflict analysis is typically driven by the identify-assess-control cycle, aimed at identifying, assessing and resolving conflicts that may obstruct the satisfaction of the expected goals. In particular, the assessment step is concerned with evaluating how likely the identified conflicts are, and how likely and severe are their consequences. So far, existing assessment approaches restrict their analysis to obstacles (conflicts that prevent the satisfaction of a single goal), and assume that certain probabilistic information on the domain is provided, that needs to be previously elicited from experienced users, statistical data or simulations. In this paper, we present a novel automated approach to assess how likely a conflict is, that applies to general conflicts (not only obstacles) without requiring probabilistic information on the domain. Intuitively, given the LTL formulation of the domain and of a set of goals to be achieved, we compute goal conflicts, and exploit string model counting techniques to estimate the likelihood of the occurrence of the corresponding conflicting situations and the severity in which these affect the satisfaction of the goals. This information can then be used to prioritize conflicts to be resolved, and suggest which goals to drive attention to for refinements.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453193,Goal Conflicts;Risk Likelihood Assessment;Model Counting,Analytical models;Boundary conditions;Software;Probabilistic logic;Computational modeling;Software engineering;Requirements engineering,formal specification;formal verification;maximum likelihood estimation;risk analysis;systems analysis,goal-conflict likelihood assessment;conflict analysis;risk analysis;goal-oriented requirements engineering,3
208,Models and Modeling 2,"A Posteriori Typing for Model-Driven Engineering: Concepts, Analysis, and Applications",J. de Lara; E. Guerra,"Universidad Autonoma de Madrid, Madrid, Madrid, ES; Universidad Autonoma de Madrid, Madrid, Madrid, ES",2018,"Model-Driven Engineering (MDE) is a software engineering paradigm where models are actively used to specify, test, simulate, analyse and maintain the systems to be built, among other activities. Models can be defined using general-purpose modelling languages like the UML, but for particular domains, the use of domain-specific languages is pervasive. Either way, models must conform to a meta-model which defines their abstract syntax. In MDE, the definition of model management operations - often typed over project-specific meta-models - is recurrent. However, even if two operations are similar, they must be developed from scratch whenever they are applied to instances of different meta-models. This is so as operations defined (i.e., typed) over a meta-model cannot be directly reused for another. Part of this difficulty of reuse is because classes in meta-models are used in two ways: as templates to create objects and as static classifiers for them. These two aspects are inherently tied in most meta-modelling approaches, which results in unnecessarily rigid systems and hinders reusability of MDE artefacts. To enhance flexibility and reuse in MDE, we propose an approach to decouple object creation from typing [1]. The approach relies on standard mechanisms for object creation, and proposes the notion of a posteriori typing as a means to retype objects and enable multiple, partial, dynamic typings. A posteriori typing enhances flexibility because it allows models to be retyped with respect to other meta-models. Hence, we distinguish between creation meta-models used to construct models, and role meta-models into which models are retyped. This permits unanticipated reuse, as a model management operation defined for a role meta-model can be reused as-is with models built using a different creation meta-model, once such models are reclassified. Moreover, our approach permits expressing some types of bidirectional model transformations by reclassification. The transformations defined as reclassifications have better performance than the equivalent ones defined with traditional transformation languages, because reclassification does not require creating new objects. In [1], we propose two mechanisms to define a posteriori typings: type-level (mappings between meta-models) and instance-level (set of model queries). The paper presents the underlying theory and type correctness criteria of both mechanisms, defines some analysis methods, identifies practical restrictions for retyping specifications, and demonstrates the feasibility of the approach by an implementation atop our meta-modelling tool MetaDepth. We also explore application scenarios of a posteriori typing (to define transformations, for model transformation reuse, and to improve transformation expressiveness by dynamic type change), and present an experiment showing the potential performance gains when expressing transformations as retypings.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453194,Model-driven engineering;reuse;model typing;partial typing;dynamic typing;model transformations;bidirectionality;MetaDepth,Unified modeling language;Software engineering;Model driven engineering;Analytical models;Tools;Domain specific languages;Syntactics,formal specification;software engineering;software reusability;specification languages;Unified Modeling Language,posteriori typing;model transformation reuse;general-purpose modelling languages;model management operation;project-specific meta-models;meta-modelling approaches;creation meta-models;role meta-model;bidirectional model transformations;model queries;model-driven engineering;MetaDepth;meta-modelling tool,
209,Models and Modeling 2,A Static Verification Framework for Message Passing in Go Using Behavioural Types,J. Lange; N. Ng; B. Toninho; N. Yoshida,University of Kent; Imperial College London; Imperial College London; Imperial College London,2018,"The Go programming language has been heavily adopted in industry as a language that efficiently combines systems programming with concurrency. Go's concurrency primitives, inspired by process calculi such as CCS and CSP, feature channel-based communication and lightweight threads, providing a distinct means of structuring concurrent software. Despite its popularity, the Go programming ecosystem offers little to no support for guaranteeing the correctness of message-passing concurrent programs. This work proposes a practical verification framework for message passing concurrency in Go by developing a robust static analysis that infers an abstract model of a program's communication behaviour in the form of a behavioural type, a powerful process calculi typing discipline. We make use of our analysis to deploy a model and termination checking based verification of the inferred behavioural type that is suitable for a range of safety and liveness properties of Go programs, providing several improvements over existing approaches. We evaluate our framework and its implementation on publicly available real-world Go code.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453195,concurrency;static analysis;behavioural types;model checking;go programming language,Concurrent computing;System recovery;Programming;Safety;Message systems;Software;Computer languages,concurrency control;formal verification;message passing;program diagnostics;program verification,static verification framework;Go programming language;concurrency primitives;feature channel-based communication;lightweight threads;concurrent software;Go programming ecosystem;message-passing concurrent programs;practical verification framework;message passing concurrency;robust static analysis;powerful process calculi;termination checking based verification;inferred behavioural type,6
210,Inference and Invariants,Inferring and Asserting Distributed System Invariants,S. Grant; H. Cech; I. Beschastnikh,"University of British Columbia, Vancouver, BC, Canada; University of Bamberg, Bamberg, Germany; University of British Columbia, Vancouver, BC, Canada",2018,"Distributed systems are difficult to debug and understand. A key reason for this is distributed state, which is not easily accessible and must be pieced together from the states of the individual nodes in the system. We propose Dinv, an automatic approach to help developers of distributed systems uncover the runtime distributed state properties of their systems. Dinv uses static and dynamic program analyses to infer relations between variables at different nodes. For example, in a leader election algorithm, Dinv can relate the variable leader at different nodes to derive the invariant forall âˆ€ nodes i, j, leader_i = leader_j. This can increase the developer's confidence in the correctness of their system. The developer can also use Dinv to convert an inferred invariant into a distributed runtime assertion on distributed state. We applied Dinv to several popular distributed systems, such as etcd Raft, Hashicorp Serf, and Taipei-Torrent, which have between 1.7K and 144K LOC and are widely used. Dinv derived useful invariants for these systems, including invariants that capture the correctness of distributed routing strategies, leadership, and key hash distribution. We also used Dinv to assert correctness of the inferred etcd Raft invariants at runtime, using these asserts to detect injected silent bugs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453196,distributed systems;specification mining;runtime checking;program analysis,Instruments;Clocks;Runtime;Tools;Lattices;Protocols;Computer bugs,dynamic programming;Java;peer-to-peer computing;program debugging;program diagnostics;program verification,Dinv;useful invariants;distributed routing strategies;key hash distribution;individual nodes;runtime distributed state properties;leader election algorithm;variable leader;inferred invariant;distributed runtime assertion,2
211,Inference and Invariants,DroidStar: Callback Typestates for Android Classes,A. Radhakrishna; N. V. Lewchenko; S. Meier; S. Mover; K. C. Sripada; D. Zufferey; B. -Y. E. Chang; P. CernÃ½,University of Colorado Boulder; Microsoft; University of Colorado Boulder; University of Colorado Boulder; University of Colorado Boulder; NA; University of Colorado Boulder; University of Colorado Boulder,2018,"Event-driven programming frameworks, such as Android, are based on components with asynchronous interfaces. The protocols for interacting with these components can often be described by finite-state machines we dub *callback typestates. Callback typestates are akin to classical typestates, with the difference that their outputs (callbacks) are produced asynchronously. While useful, these specifications are not commonly available, because writing them is difficult and error-prone. Our goal is to make the task of producing callback typestates significantly easier. We present a callback typestate assistant tool, DroidStar, that requires only limited user interaction to produce a callback typestate. Our approach is based on an active learning algorithm, L*. We improved the scalability of equivalence queries (a key component of L*), thus making active learning tractable on the Android system. We use DroidStar to learn callback typestates for Android classes both for cases where one is already provided by the documentation, and for cases where the documentation is unclear. The results show that DroidStar learns callback typestates accurately and efficiently. Moreover, in several cases, the synthesized callback typestates uncovered surprising and undocumented behaviors.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453197,typestate;specification inference;Android;active learning,Androids;Humanoid robots;Protocols;Tools;Documentation;Learning automata;Task analysis,learning (artificial intelligence);program diagnostics,DroidStar;callback typestate assistant tool;synthesized callback typestates;dub *callback typestates,
212,Inference and Invariants,Debugging with Intelligence via Probabilistic Inference,Z. Xu; S. Ma; X. Zhang; S. Zhu; B. Xu,"Nanjing University, Nanjing, Jiangsu, CN; Purdue University System, West Lafayette, IN, US; Purdue University System, West Lafayette, IN, US; Nanjing University, Nanjing, Jiangsu, CN; Nanjing University, Nanjing, Jiangsu, CN",2018,"We aim to debug a single failing execution without the assistance from other passing/failing runs. In our context, debugging is a process with substantial uncertainty - lots of decisions have to be made such as what variables shall be inspected first. To deal with such uncertainty, we propose to equip machines with human-like intelligence. Specifically, we develop a highly automated debugging technique that aims to couple human-like reasoning (e.g., dealing with uncertainty and fusing knowledge) with program semantics based analysis, to achieve benefits from the two and mitigate their limitations. We model debugging as a probabilistic inference problem, in which the likelihood of each executed statement instance and variable being correct/faulty is modeled by a random variable. Human knowledge, human-like reasoning rules and program semantics are modeled as conditional probability distributions, also called probabilistic constraints. Solving these constraints identifies the most likely faulty statements. Our results show that the technique is highly effective. It can precisely identify root causes for a set of real-world bugs in a very small number of interactions with developers, much smaller than a recent proposal that does not encode human intelligence. Our user study also confirms that it substantially improves human productivity.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453198,Debugging;Probabilistic Inference;Python,Debugging;Probabilistic logic;Uncertainty;Cognition;Semantics;Python;Tools,inference mechanisms;program debugging,reasoning rules;program semantics;conditional probability distributions;faulty statements;human intelligence;human productivity;single failing execution;substantial uncertainty;highly automated debugging technique;couple human-like reasoning;fusing knowledge;model debugging;probabilistic inference problem;executed statement instance;human knowledge;passing-failing runs,6
213,Inference and Invariants,Reducer-Based Construction of Conditional Verifiers,D. Beyer; M. -C. Jakobs; T. Lemberger; H. Wehrheim,"ifo Institut fur Wirtschaftsforschung, Munchen, Bayern, DE; ifo Institut fur Wirtschaftsforschung, Munchen, Bayern, DE; ifo Institut fur Wirtschaftsforschung, Munchen, Bayern, DE; Universitat Paderborn, Paderborn, Nordrhein-Westfalen, DE",2018,"Despite recent advances, software verification remains challenging. To solve hard verification tasks, we need to leverage not just one but several different verifiers employing different technologies. To this end, we need to exchange information between verifiers. Conditional model checking was proposed as a solution to exactly this problem: The idea is to let the first verifier output a condition which describes the state space that it successfully verified and to instruct the second verifier to verify the yet unverified state space using this condition. However, most verifiers do not understand conditions as input. In this paper, we propose the usage of an off-the-shelf construction of a conditional verifier from a given traditional verifier and a reducer. The reducer takes as input the program to be verified and the condition, and outputs a residual program whose paths cover the unverified state space described by the condition. As a proof of concept, we designed and implemented one particular reducer and composed three conditional model checkers from the three best verifiers at SV-COMP 2017. We defined a set of claims and experimentally evaluated their validity. All experimental data and results are available for replication.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453199,Conditional Model Checking;Formal Verification;Testing;Program Analysis;Software Verification;Sequential Combination,Automata;Software;Model checking;Tools;Software engineering;Task analysis,formal verification;program diagnostics;program verification,conditional model checking;verifier output;unverified state space;conditional model checkers;hard verification tasks;software verification;conditional verifier;reducer-based construction,4
214,Surveys and Reviews,[Journal First] Challenges and Pitfalls on Surveying Evidence in the Software Engineering Technical Literature: An Exploratory Study with Novices,T. Vieira Ribeiro; J. Massollar; G. Horta Travassos,"Universidade Federal do Rio de Janeiro, Rio de Janeiro, RJ, BR; Universidade Federal do Rio de Janeiro, Rio de Janeiro, RJ, BR; Universidade Federal do Rio de Janeiro, Rio de Janeiro, RJ, BR",2018,"The evidence-based software engineering approach advocates the use of evidence from empirical studies to support the decisions on the adoption of software technologies by practitioners in the software industry. To this end, many guidelines have been proposed to contribute to the execution and repeatability of literature reviews, and to the confidence of their results, especially regarding systematic literature reviews (SLR). To investigate similarities and differences, and to characterize the challenges and pitfalls of the planning and generated results of SLR research protocols dealing with the same research question and performed by similar teams of novice researchers in the context of the software engineering field. We qualitatively compared (using Jaccard and Kappa coefficients) and evaluated (using DARE) same goal SLR research protocols and outcomes undertaken by similar research teams. Seven similar SLR protocols regarding quality attributes for use cases executed in 2010 and 2012 enabled us to observe unexpected differences in their planning and execution. Even when the participants reached some agreement in the planning, the outcomes were different. The research protocols and reports allowed us to observe six challenges contributing to the divergences in the results: researchers' inexperience in the topic, researchers' inexperience in the method, lack of clearness and completeness of the papers, lack of a common terminology regarding the problem domain, lack of research verification procedures, and lack of commitment to the SLR. According to our findings, it is not possible to rely on results of SLRs performed by novices. Also, similarities at a starting or intermediate step during different SLR executions may not directly translate to the next steps, since non-explicit information might entail differences in the outcomes, hampering the repeatability and confidence of the SLR process and results. Although we do have expectations that the presence and follow-up of a senior researcher can contribute to increasing SLRs' repeatability, this conclusion can only be drawn upon the existence of additional studies on this topic. Yet, systematic planning, transparency of decisions and verification procedures are key factors to guarantee the reliability of SLRs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453200,Novice researchers;Systematic literature review;Evidence based software engineering;Exploratory study,Software engineering;Software;Planning;Protocols;Knowledge engineering;Systematics;Terminology,DP industry;project management;software engineering,unexpected differences;research verification procedures;similar SLR protocols;software engineering field;SLR research protocols;systematic literature reviews;software industry;software technologies;evidence-based software engineering approach;software engineering technical literature;systematic planning;additional studies;senior researcher;SLR process,
215,Surveys and Reviews,Statistical Errors in Software Engineering Experiments: A Preliminary Literature Review,R. P. Reyes Ch.; O. Dieste; E. R. Fonseca C.; N. Juristo,"Universidad Politecnica de Madrid, Madrid, Comunidad de Madrid, ES; Universidad Politecnica de Madrid, Madrid, Comunidad de Madrid, ES; NA; Universidad Politecnica de Madrid, Madrid, Comunidad de Madrid, ES",2018,"Background: Statistical concepts and techniques are often applied incorrectly, even in mature disciplines such as medicine or psychology. Surprisingly, there are very few works that study statistical problems in software engineering (SE). Aim: Assess the existence of statistical errors in SE experiments. Method: Compile the most common statistical errors in experimental disciplines. Survey experiments published in ICSE to assess whether errors occur in high quality SE publications. Results: The same errors as identified in others disciplines were found in ICSE experiments, where 30 of the reviewed papers included several error types such as: a) missing statistical hypotheses, b) missing sample size calculation, c) failure to assess statistical test assumptions, and d) uncorrected multiple testing. This rather large error rate is greater for research papers where experiments are confined to the validation section. The origin of the errors can be traced back to: a) researchers not having sufficient statistical training, and b) a profusion of exploratory research. Conclusions: This paper provides preliminary evidence that SE research suffers from the same statistical problems as other experimental disciplines. However, the SE community appears to be unaware of any shortcomings in its experiments, whereas other disciplines work hard to avoid these threats. Further research is necessary to find the underlying causes and set up corrective measures, but there are some potentially effective actions and are a priori easy to implement: a) improve the statistical training of SE researchers, and b) enforce quality assessment and reporting guidelines in SE publications.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453201,Literature review;Survey;Prevalence;Statistical errors,Software engineering;Bibliographies;Training;Guidelines;Error analysis;Back,psychology;research and development;software engineering;statistical testing,software engineering experiments;SE experiments;high quality SE publications;ICSE experiments;statistical hypotheses;statistical test assumptions;SE community;SE researchers;quality assessment;statistical errors;statistical training;SE research,1
216,Surveys and Reviews,Synthesizing Qualitative Research in Software Engineering: A Critical Review,X. Huang; H. Zhang; X. Zhou; M. Ali Babar; S. Yang,"State Key Laboratory of Novel Software Technology, Nanjing University, China; State Key Laboratory of Novel Software Technology, Nanjing University, China; State Key Laboratory of Novel Software Technology, Nanjing University, China; School of Computer Science, University of Adelaide, Australia; State Key Laboratory of Novel Software Technology, Nanjing University, China",2018,"Synthesizing data extracted from primary studies is an integral component of the methodologies in support of Evidence Based Software Engineering (EBSE) such as System Literature Review (SLR). Since a large and increasing number of studies in Software Engineering (SE) incorporate qualitative data, it is important to systematically review and understand different aspects of the Qualitative Research Synthesis (QRS) being used in SE. We have reviewed the use of QRS methods in 328 SLRs published between 2005 and 2015. We also inquired the authors of 274 SLRs to confrm whether or not any QRS methods were used in their respective reviews. 116 of them provided the responses, which were included in our analysis. We found eight QRS methods applied in SE research, two of which, narrative synthesis and thematic synthesis, have been predominantly adopted by SE researchers for synthesizing qualitative data. Our study determines that a signifcant amount of missing knowledge and incomplete understanding of the defned QRS methods in the community. Our effort also identifes an initial set factors that may in?uence the selection and use of appropriate QRS methods in SE.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453202,research synthesis;qualitative (synthesis) methods;systematic (literature) review;evidence-based software engineering,Software engineering;Data mining;Bibliographies;Software;Systematics;Tools;Data models,reviews;software engineering,Software Engineering;critical Review;primary studies;integral component;System Literature Review;qualitative data;Qualitative Research Synthesis;SE research;SE researchers;defned QRS methods;appropriate QRS methods;SLR,8
217,Surveys and Reviews,Automatic Software Repair: A Survey,L. Gazzola; L. Mariani; D. Micucci,"Universita degli Studi di Milano-Bicocca, Milano, Lombardia, IT; Universita degli Studi di Milano-Bicocca, Milano, Lombardia, IT; Universita degli Studi di Milano-Bicocca, Milano, Lombardia, IT",2018,"Despite their growing complexity and increasing size, modern software applications must satisfy strict release requirements that impose short bug fixing and maintenance cycles, putting significant pressure on developers who are responsible for timely producing high-quality software. To reduce developers workload, repairing and healing techniques have been extensively investigated as solutions for efficiently repairing and maintaining software in the last few years. In particular, repairing solutions have been able to automatically produce useful fixes for several classes of bugs that might be present in software programs. A range of algorithms, techniques, and heuristics have been integrated, experimented, and studied, producing a heterogeneous and articulated research framework where automatic repair techniques are proliferating. This paper organizes the knowledge in the area by surveying a body of 108 papers about automatic software repair techniques, illustrating the algorithms and the approaches, comparing them on representative examples, and discussing the open challenges and the empirical evidence reported so far.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453203,Automatic Program Repair;Generate and Validate;Search Based;Semantics driven repair;Correct by Construction;Program Synthesis;Self Repairing,Maintenance engineering;Debugging;Software engineering;Fault diagnosis;Automation;Software systems,program debugging;software maintenance;software quality,articulated research framework;automatic repair techniques;automatic software repair techniques;modern software applications;strict release requirements;short bug;maintenance cycles;significant pressure;software programs;heterogeneous research framework;high-quality software,8
218,Search-Based Software Engineering 2,Search-Based Test Data Generation for SQL Queries,J. Castelein; M. Aniche; M. Soltani; A. Panichella; A. van Deursen,NA; NA; NA; NA; NA,2018,"Database-centric systems strongly rely on SQL queries to manage and manipulate their data. These SQL commands can range from very simple selections to queries that involve several tables, subqueries, and grouping operations. And, as with any important piece of code, developers should properly test SQL queries. In order to completely test a SQL query, developers need to create test data that exercise all possible coverage targets in a query, e.g., JOINs and WHERE predicates. And indeed, this task can be challenging and time-consuming for complex queries. Previous studies have modeled the problem of generating test data as a constraint satisfaction problem and, with the help of SAT solvers, generate the required data. However, such approaches have strong limitations, such as partial support for queries with JOINs, subqueries, and strings (which are commonly used in SQL queries). In this paper, we model test data generation for SQL queries as a search-based problem. Then, we devise and evaluate three different approaches based on random search, biased random search, and genetic algorithms (GAs). The GA, in particular, uses a fitness function based on information extracted from the physical query plan of a database engine as search guidance. We then evaluate each approach in 2,135 queries extracted from three open source software and one industrial software system. Our results show that GA is able to completely cover 98.6% of all queries in the dataset, requiring only a few seconds per query. Moreover, it does not suffer from the limitations affecting state-of-the art techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453204,search based software engineering;automated test data generation;SQL;databases,Databases;Genetic algorithms;Search problems;Structured Query Language;Toy manufacturing industry;Software systems;Engines,database management systems;genetic algorithms;program testing;query processing;search problems;SQL,SQL query;complex queries;model test data generation;physical query plan;Search-Based Test Data Generation;database-centric systems;SQL queries;SQL commands;constraint satisfaction problem;search-based problem;biased random search;genetic algorithms;database engine;search guidance;open source software;industrial software system,7
219,Search-Based Software Engineering 2,Multi-objective Integer Programming Approaches for Solving Optimal Feature Selection Problem: A New Perspective on Multi-objective Optimization Problems in SBSE,Y. Xue; Y. -F. Li,"University of Science and Technology of China, Hefei, China; Department of Industrial Engineering, Tsinghua University, Beijing, China",2018,"The optimal feature selection problem in software product line is typically addressed by the approaches based on Indicator-based Evolutionary Algorithm (IBEA). In this study, we first expose the mathematical nature of this problem - multi-objective binary integer linear programming. Then, we implement/propose three mathematical programming approaches to solve this problem at different scales. For small-scale problems (roughly, less than 100 features), we implement two established approaches to find all exact solutions. For medium-to-large problems (roughly, more than 100 features), we propose one efficient approach that can generate a representation of the entire Pareto front in linear time complexity. The empirical results show that our proposed method can find significantly more non-dominated solutions in similar or less execution time, in comparison with IBEA and its recent enhancement (i.e., IBED that combines IBEA and Differential Evolution).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453205,Optimal Feature Selection Problem;Multi Objective Optimization(MOO);Multi Objective Integer Programming (MOIP);Indicator Based Evolutionary Algorithm (IBEA);IBED,Feature extraction;Linear programming;IP networks;Optimization;Software;Encryption;Graphical user interfaces,computational complexity;evolutionary computation;integer programming;Pareto optimisation,optimal feature selection problem;multiobjective optimization problems;software product line;IBEA;multiobjective binary integer linear programming;mathematical programming approaches;small-scale problems;medium-to-large problems;indicator-based evolutionary algorithm;SBSE;Pareto front;linear time complexity,2
220,Search-Based Software Engineering 2,Automated Refactoring of OCL Constraints with Search,H. Lu; S. Wang; T. Yue; S. Ali; J. Nygard,"Simula Res. Lab., Lysaker, Norway; Simula Res. Lab., Lysaker, Norway; Simula Res. Lab., Lysaker, Norway; Simula Res. Lab., Lysaker, Norway; Kreftregisteret, Oslo, NO",2018,"Object Constraint Language (OCL) constraints are typically used for providing precise semantics to models developed with the Unified Modeling Language (UML). When OCL constraints evolve in a regular basis, it is essential that they are easy to understand and maintain. For instance, in cancer registries, to ensure the quality of cancer data, more than one thousand medical rules are defined and evolve regularly. Such rules can be specified with OCL. It is, therefore, important to ensure the understandability and maintainability of medical rules specified with OCL. To tackle such a challenge, we propose an automated search-based OCL constraint refactoring approach (SBORA) by defining and applying three OCL quality metrics (Complexity, Coupling, and Cohesion) and four semantics-preserving refactoring operators (i.e., Context Change, Swap, Split and Merge) which are encoded as potential solutions for search algorithms. A solution is therefore an optimal sequence of refactoring operators, which are sequentially applied to the original set of OCL constraints to automatically obtain a semantically equivalent set of OCL constraints with better understandability and maintainability in terms of Complexity, Coupling, and Cohesion. We evaluate SBORA along with six commonly used multi-objective search algorithms (e.g., Indicator-Based Evolutionary Algorithm (IBEA)) by employing four case studies from different domains: healthcare (i.e., cancer registry system from Cancer Registry of Norway (CRN)), Oil&Gas (i.e., subsea production systems), warehouse (i.e., handling systems), and an open source case study named SEPA. Results show: 1) IBEA achieves the best performance among all the search algorithms and 2) the refactoring approach along with IBEA can manage to reduce on average 29.25% Complexity and 39% Coupling and improve 47.75% Cohesion, as compared to the original OCL constraint set from CRN. To further test the performance of SBORA, we also applied it to refactor an OCL constraint set specified on the UML 2.3 metamodel and we obtained encouraging results. Furthermore, we conducted a controlled experiment with 96 subjects and results show that the understandability and maintainability of the original constraint set can be improved significantly from the perspectives of the 96 participants of the controlled experiment.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453206,Refactoring;Search;Object Constraint Language,Cancer;Unified modeling language;Complexity theory;Couplings;Software engineering;Semantics;Measurement,evolutionary computation;object-oriented programming;search problems;software maintenance;Unified Modeling Language,cancer registries;automated search-based OCL constraint refactoring approach;OCL quality metrics;semantics-preserving refactoring operators;search algorithms;Cancer Registry;original OCL constraint;original constraint set;object constraint language constraints,
221,Search-Based Software Engineering 2,Automatically Generating Search Heuristics for Concolic Testing,S. Cha; S. Hong; J. Lee; H. Oh,Korea University; Korea University; Korea University; Korea University,2018,"We present a technique to automatically generate search heuristics for concolic testing. A key challenge in concolic testing is how to effectively explore the program's execution paths to achieve high code coverage in a limited time budget. Concolic testing employs a search heuristic to address this challenge, which favors exploring particular types of paths that are most likely to maximize the final coverage. However, manually designing a good search heuristic is nontrivial and typically ends up with suboptimal and unstable outcomes. The goal of this paper is to overcome this shortcoming of concolic testing by automatically generating search heuristics. We define a class of search heuristics, namely a parameterized heuristic, and present an algorithm that efficiently finds an optimal heuristic for each subject program. Experimental results with open-source C programs show that our technique successfully generates search heuristics that significantly outperform existing manually-crafted heuristics in terms of branch coverage and bug-finding.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8453207,Software Testing;Concolic Testing;Search Heuristics,Heuristic algorithms;Manuals;Software engineering;Software algorithms;Software;Software testing,program debugging;program testing;search problems,concolic testing;good search heuristic;manually-crafted heuristics;program execution path,8
222,Not Mentioned,Detecting Atomicity Violations for Event-Driven Node.js Applications,X. Chang; W. Dou; Y. Gao; J. Wang; J. Wei; T. Huang,"State Key Lab of Computer Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, University of Chinese Academy of Sciences, Beijing, China",2019,"Node.js has been widely-used as an event-driven server-side architecture. To improve performance, a task in a Node.js application is usually divided into a group of events, which are non-deterministically scheduled by Node.js. Developers may assume that the group of events (named atomic event group) should be atomically processed, without interruption. However, the atomicity of an atomic event group is not guaranteed by Node.js, and thus other events may interrupt the execution of the atomic event group, break down the atomicity and cause unexpected results. Existing approaches mainly focus on event race among two events, and cannot detect high-level atomicity violations among a group of events. In this paper, we propose NodeAV, which can predictively detect atomicity violations in Node.js applications based on an execution trace. Based on happens-before relations among events in an execution trace, we automatically identify a pair of events that should be atomically processed, and use predefined atomicity violation patterns to detect atomicity violations. We have evaluated NodeAV on real-world Node.js applications. The experimental results show that NodeAV can effectively detect atomicity violations in these Node.js applications.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811949,Node.js;event-driven architecture;atomicity violation;happens-before,Task analysis;Programming;Instruction sets;Computer architecture;Message systems;Concurrent computing;Computer bugs,Java;program testing;scheduling,event-driven server-side architecture;Node.js application;atomic event group;event race;atomicity violation patterns;event-driven Node.js applications;NodeAV,9
223,Not Mentioned,Parallel Refinement for Multi-Threaded Program Verification,L. Yin; W. Dong; W. Liu; J. Wang,"Laboratory of Software Engineering for Complex Systems, National University of Defense Technology, Changsha, China; Laboratory of Software Engineering for Complex Systems, National University of Defense Technology, Changsha, China; Laboratory of Software Engineering for Complex Systems, National University of Defense Technology, Changsha, China; State Key Laboratory of High Performance Computing, National University of Defense Technology, Changsha, China",2019,"Program verification is one of the most important methods to ensuring the correctness of concurrent programs. However, due to the path explosion problem, concurrent program verification is usually time consuming, which hinders its scalability to industrial programs. Parallel processing is a mainstream technique to deal with those problems which require mass computing. Hence, designing parallel algorithms to improve the performance of concurrent program verification is highly desired. This paper focuses on parallelization of the abstraction refinement technique, one of the most efficient techniques for concurrent program verification. We present a parallel refinement framework which employs multiple engines to refine the abstraction in parallel. Different from existing work which parallelizes the search process, our method achieves the effect of parallelization by refinement constraint and learnt clause sharing, so that the number of required iterations can be significantly reduced. We have implemented this framework on the scheduling constraint based abstraction refinement method, one of the best methods for concurrent program verification. Experiments on SV-COMP 2018 show the encouraging results of our method. For those complex programs requiring a large number of iterations, our method can obtain a linear reduction of the iteration number and significantly improve the verification performance.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812136,Concurrent Program;Abstraction Refinement;Scheduling Constraint;Parallel Verification,Engines;Concurrent computing;Programming;Encoding;Instruction sets;Software engineering;Job shop scheduling,iterative methods;multi-threading;parallel algorithms;program diagnostics;program verification;scheduling,parallel processing;parallel algorithms;concurrent program verification;abstraction refinement technique;parallel refinement framework;scheduling constraint based abstraction refinement method;complex programs;multithreaded program;concurrent programs,4
224,Not Mentioned,Mining Software Defects: Should We Consider Affected Releases?,S. Yatish; J. Jiarpakdee; P. Thongtanunam; C. Tantithamthavorn,The University of Adelaide; Monash University; The University of Melbourne; Monash University,2019,"With the rise of the Mining Software Repositories (MSR) field, defect datasets extracted from software repositories play a foundational role in many empirical studies related to software quality. At the core of defect data preparation is the identification of post-release defects. Prior studies leverage many heuristics (e.g., keywords and issue IDs) to identify post-release defects. However, such the heuristic approach is based on several assumptions, which pose common threats to the validity of many studies. In this paper, we set out to investigate the nature of the difference of defect datasets generated by the heuristic approach and the realistic approach that leverages the earliest affected release that is realistically estimated by a software development team for a given defect. In addition, we investigate the impact of defect identification approaches on the predictive accuracy and the ranking of defective modules that are produced by defect models. Through a case study of defect datasets of 32 releases, we find that that the heuristic approach has a large impact on both defect count datasets and binary defect datasets. Surprisingly, we find that the heuristic approach has a minimal impact on defect count models, suggesting that future work should not be too concerned about defect count models that are constructed using heuristic defect datasets. On the other hand, using defect datasets generated by the realistic approach lead to an improvement in the predictive accuracy of defect classification models.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811982,Mining Software Repositories;Empirical Software Engineering;Software Quality;Defect Prediction Models,Predictive models;Object oriented modeling;Software quality;Feature extraction;Data mining;Control systems,data mining;pattern classification;software maintenance;software quality,defect count models;defect classification models;software quality;defect data preparation;post-release defects;software development team;software repositories;defect identification;software defect mining,51
225,Not Mentioned,Class Imbalance Evolution and Verification Latency in Just-in-Time Software Defect Prediction,G. G. Cabral; L. L. Minku; E. Shihab; S. Mujahid,"Federal Rural University of Pernambuco, Brazil; School of Computer Science, University of Birmingham, UK; Department of Computer Science and Software Engineering, Concordia University, Canada; Department of Computer Science and Software Engineering, Concordia University, Canada",2019,"Just-in-Time Software Defect Prediction (JIT-SDP) is an SDP approach that makes defect predictions at the software change level. Most existing JIT-SDP work assumes that the characteristics of the problem remain the same over time. However, JIT-SDP may suffer from class imbalance evolution. Specifically, the imbalance status of the problem (i.e., how much underrepresented the defect-inducing changes are) may be intensified or reduced over time. If occurring, this could render existing JIT-SDP approaches unsuitable, including those that re-build classifiers over time using only recent data. This work thus provides the first investigation of whether class imbalance evolution poses a threat to JIT-SDP. This investigation is performed in a realistic scenario by taking into account verification latency -- the often overlooked fact that labeled training examples arrive with a delay. Based on 10 GitHub projects, we show that JIT-SDP suffers from class imbalance evolution, significantly hindering the predictive performance of existing JIT-SDP approaches. Compared to state-of-the-art class imbalance evolution learning approaches, the predictive performance of JIT-SDP approaches was up to 97.2% lower in terms of g-mean. Hence, it is essential to tackle class imbalance evolution in JIT-SDP. We then propose a novel class imbalance evolution approach for the specific context of JIT-SDP. While maintaining top ranked g-means, this approach managed to produce up to 63.59% more balanced recalls on the defect-inducing and clean classes than state-of-the-art class imbalance evolution approaches. We thus recommend it to avoid overemphasizing one class over the other in JIT-SDP.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812072,Software defect prediction;class imbalance;verification latency;online learning;concept drift;ensembles,Software;Training;Machine learning algorithms;Machine learning;Prediction algorithms;Delays,learning (artificial intelligence);pattern classification;safety-critical software;sampling methods;software fault tolerance,SDP approach;predictive performance;class imbalance evolution approach;just-in-time software defect prediction;JIT-SDP approaches;top ranked g-means,36
226,Not Mentioned,FLOSS Participants' Perceptions About Gender and Inclusiveness: A Survey,A. Lee; J. C. Carver,"Department of Computer Science, The University of Alabama, Tuscaloosa, AL, USA; Department of Computer Science, The University of Alabama, Tuscaloosa, AL, USA",2019,"Background: While FLOSS projects espouse openness and acceptance for all, in practice, female contributors often face discriminatory barriers to contribution. Aims: In this paper, we examine the extent to which these problems still exist. We also study male and female contributors' perceptions of other contributors. Method: We surveyed participants from 15 FLOSS projects, asking a series of open-ended, closed-ended, and behavioral scale questions to gather information about the issue of gender in FLOSS projects. Results: Though many of those we surveyed expressed a positive sentiment towards females who participate in FLOSS projects, some were still strongly against their inclusion. Often, the respondents who were against inclusiveness also believed their own sentiments were the prevailing belief in the community, contrary to our findings. Others did not see the purpose of attempting to be inclusive, expressing the sentiment that a discussion of gender has no place in FLOSS. Conclusions: FLOSS projects have started to move forwards in terms of gender acceptance. However, there is still a need for more progress in the inclusion of gender-diverse contributors.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812068,FLOSS;gender;survey;Open Source,Computer science;Software;Face;Data mining;Software engineering;Cognition;IEEE Fellows,gender issues;public domain software;sentiment analysis;social aspects of automation,gender-diverse contributors;inclusiveness;female contributors;FLOSS projects;gender acceptance;FLOSS participant perceptions;positive sentiment;male contributor perceptions;female contributor perceptions,34
227,Not Mentioned,Going Farther Together: The Impact of Social Capital on Sustained Participation in Open Source,H. S. Qiu; A. Nolte; A. Brown; A. Serebrenik; B. Vasilescu,Carnegie Mellon Univ.; University of Tartu; Bryn Mawr College; Eindhoven Univ. of Tech.; Carnegie Mellon Univ.,2019,"Sustained participation by contributors in opensource software is critical to the survival of open-source projects and can provide career advancement benefits to individual contributors. However, not all contributors reap the benefits of open-source participation fully, with prior work showing that women are particularly underrepresented and at higher risk of disengagement. While many barriers to participation in open-source have been documented in the literature, relatively little is known about how the social networks that open-source contributors form impact their chances of long-term engagement. In this paper we report on a mixed-methods empirical study of the role of social capital (i.e., the resources people can gain from their social connections) for sustained participation by women and men in open-source GitHub projects. After combining survival analysis on a large, longitudinal data set with insights derived from a user survey, we confirm that while social capital is beneficial for prolonged engagement for both genders, women are at disadvantage in teams lacking diversity in expertise.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812044,social capital;open source software;gender,Social networking (online);Open source software;Cultural differences;Bonding;Task analysis;Collaboration;Merging,gender issues;project management;public domain software;social aspects of automation;software engineering,open-source software;open-source GitHub projects;sustained participation;social capital;social networks;open-source participation,51
228,Not Mentioned,Investigating the Effects of Gender Bias on GitHub,N. Imtiaz; J. Middleton; J. Chakraborty; N. Robson; G. Bai; E. Murphy-Hill,"Department of Computer Science, North Carolina State University; Department of Computer Science, North Carolina State University; Department of Computer Science, North Carolina State University; Department of Computer Science, North Carolina State University; Department of Computer Science, North Carolina State University; Google, LLC",2019,"Diversity, including gender diversity, is valued by many software development organizations, yet the field remains dominated by men. One reason for this lack of diversity is gender bias. In this paper, we study the effects of that bias by using an existing framework derived from the gender studies literature.We adapt the four main effects proposed in the framework by posing hypotheses about how they might manifest on GitHub,then evaluate those hypotheses quantitatively. While our results how that effects of gender bias are largely invisible on the GitHub platform itself, there are still signals of women concentrating their work in fewer places and being more restrained in communication than men.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812110,gender bias;software engineering,Software;Software engineering;Companies;Correlation;Encoding;Computer science,gender issues;software development management;software metrics,gender bias;gender diversity;software development organizations;gender studies;GitHub,51
229,Not Mentioned,Message from the Social Media Chairs of ICSE 2019,,,2019,Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812112,,,,,
230,Not Mentioned,SLF: Fuzzing without Valid Seed Inputs,W. You; X. Liu; S. Ma; D. Perry; X. Zhang; B. Liang,"Department of Computer Science, Purdue University, Indiana, USA; School of Computer Science and Technology, Zhejiang University, Zhejiang, China; Department of Computer Science, Purdue University, Indiana, USA; Department of Computer Science, Purdue University, Indiana, USA; Department of Computer Science, Purdue University, Indiana, USA; School of Information, Renmin University of China, Beijing, China",2019,"Fuzzing is an important technique to detect software bugs and vulnerabilities. It works by mutating a small set of seed inputs to generate a large number of new inputs. Fuzzers' performance often substantially degrades when valid seed inputs are not available. Although existing techniques such as symbolic execution can generate seed inputs from scratch, they have various limitations hindering their applications in real-world complex software. In this paper, we propose a novel fuzzing technique that features the capability of generating valid seed inputs. It piggy-backs on AFL to identify input validity checks and the input fields that have impact on such checks. It further classifies these checks according to their relations to the input. Such classes include arithmetic relation, object offset, data structure length and so on. A multi-goal search algorithm is developed to apply class-specific mutations in order to satisfy inter-dependent checks all together. We evaluate our technique on 20 popular benchmark programs collected from other fuzzing projects and the Google fuzzer test suite, and compare it with existing fuzzers AFL and AFLFast, symbolic execution engines KLEE and S2E, and a hybrid tool Driller that combines fuzzing with symbolic execution. The results show that our technique is highly effective and efficient, out-performing the other tools.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812105,fuzzing;seed inputs,Fuzzing;Software;Engines;Indexes;Tools;Libraries;Computer science,fuzzy set theory;program debugging;program testing,valid seed inputs;novel fuzzing technique;input validity checks;software bugs detection;software vulnerabilities;Google fuzzer test suite;multigoal search algorithm,20
231,Not Mentioned,Superion: Grammar-Aware Greybox Fuzzing,J. Wang; B. Chen; L. Wei; Y. Liu,"School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore",2019,"In recent years, coverage-based greybox fuzzing has proven itself to be one of the most effective techniques for finding security bugs in practice. Particularly, American Fuzzy Lop (AFL for short) is deemed to be a great success in fuzzing relatively simple test inputs. Unfortunately, when it meets structured test inputs such as XML and JavaScript, those grammar-blind trimming and mutation strategies in AFL hinder the effectiveness and efficiency. To this end, we propose a grammar-aware coverage-based greybox fuzzing approach to fuzz programs that process structured inputs. Given the grammar (which is often publicly available) of test inputs, we introduce a grammar-aware trimming strategy to trim test inputs at the tree level using the abstract syntax trees (ASTs) of parsed test inputs. Further, we introduce two grammar-aware mutation strategies (i.e., enhanced dictionary-based mutation and tree-based mutation). Specifically, tree-based mutation works via replacing subtrees using the ASTs of parsed test inputs. Equipped with grammar-awareness, our approach can carry the fuzzing exploration into width and depth. We implemented our approach as an extension to AFL, named Superion; and evaluated the effectiveness of Superion using large- scale programs (i.e., an XML engine libplist and three JavaScript engines WebKit, Jerryscript and ChakraCore). Our results have demonstrated that Superion can improve the code coverage (i.e., 16.7% and 8.8% in line and function coverage) and bug-finding capability (i.e., 34 new bugs, among which we discovered 22 new vulnerabilities with 19 CVEs assigned and 3.2K USD bug bounty rewards received) over AFL and jsfunfuzz.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811923,"Greybox Fuzzing, Structured Inputs, ASTs",Fuzzing;Grammar;Computer bugs;XML;Syntactics;Engines;Instruments,grammars;grey systems;program compilers;program debugging;program testing;security of data;trees (mathematics),abstract syntax trees;tree-based mutation;AFL;Superion;grammar-blind trimming mutation strategies;grammar-aware coverage-based greybox fuzzing approach;American fuzzy lop;security bugs;XML;JavaScript;ASTs;parsed test,83
232,Not Mentioned,Grey-Box Concolic Testing on Binary Code,J. Choi; J. Jang; C. Han; S. K. Cha,"KAIST, Daejeon, Republic of Korea; Samsung Research, Seoul, Republic of Korea; Naver Labs, Seongnam, Republic of Korea; KAIST, Daejeon, Republic of Korea",2019,"We present grey-box concolic testing, a novel path-based test case generation method that combines the best of both white-box and grey-box fuzzing. At a high level, our technique systematically explores execution paths of a program under test as in white-box fuzzing, a.k.a. concolic testing, while not giving up the simplicity of grey-box fuzzing: it only uses a lightweight instrumentation, and it does not rely on an SMT solver. We implemented our technique in a system called Eclipser, and compared it to the state-of-the-art grey-box fuzzers (including AFLFast, LAF-intel, Steelix, and VUzzer) as well as a symbolic executor (KLEE). In our experiments, we achieved higher code coverage and found more bugs than the other tools.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811950,software testing;concolic testing;fuzzing,Fuzzing;Computer bugs;Instruments;Binary codes;Tools;Security,binary codes;program debugging;program testing,grey-box concolic testing;grey-box fuzzing;white-box fuzzing;grey-box fuzzers;path-based test case generation method;binary code,33
233,Not Mentioned,RESTler: Stateful REST API Fuzzing,V. Atlidakis; P. Godefroid; M. Polishchuk,Columbia University; Columbia University; Columbia University,2019,"This paper introduces RESTler, the first stateful REST API fuzzer. RESTler analyzes the API specification of a cloud service and generates sequences of requests that automatically test the service through its API. RESTler generates test sequences by (1) inferring producer-consumer dependencies among request types declared in the specification (e.g., inferring that ""a request B should be executed after request A"" because B takes as an input a resource-id x produced by A) and by (2) analyzing dynamic feedback from responses observed during prior test executions in order to generate new tests (e.g., learning that ""a request C after a request sequence A;B is refused by the service"" and therefore avoiding this combination in the future). We present experimental results showing that these two techniques are necessary to thoroughly exercise a service under test while pruning the large search space of possible request sequences. We used RESTler to test GitLab, an open-source Git service, as well as several Microsoft Azure and Office365 cloud services. RESTler found 28 bugs in GitLab and several bugs in each of the Azure and Office365 cloud services tested so far. These bugs have been confirmed and fixed by the service owners.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811961,REST API;Fuzzing;cloud services;fuzzer;testing;bug finding,Computer bugs;Tools;Fuzzing;Dictionaries;Open source software;Test pattern generators,application program interfaces;cloud computing;fuzzy set theory;program debugging;program testing,prior test executions;request C;test GitLab;open-source Git service;Office365 cloud services;service owners;stateful REST API fuzzing;stateful REST API fuzzer;RESTler analyzes;API specification;cloud service;test sequences;request types;request B;request sequences;Microsoft Azure cloud services;bugs,70
234,Not Mentioned,Training Binary Classifiers as Data Structure Invariants,F. Molina; R. Degiovanni; P. Ponzio; G. Regis; N. Aguirre; M. Frias,"National Council for Scientific and Technical Research (CONICET), Argentina; SnT, University of Luxembourg, Luxembourg; National Council for Scientific and Technical Research (CONICET), Argentina; Department of Computer Science, University of Rio Cuarto, Argentina; National Council for Scientific and Technical Research (CONICET), Argentina; National Council for Scientific and Technical Research (CONICET), Argentina",2019,"We present a technique to distinguish valid from invalid data structure objects. The technique is based on building an artificial neural network, more precisely a binary classifier, and training it to identify valid and invalid instances of a data structure. The obtained classifier can then be used in place of the data structure's invariant, in order to attempt to identify (in)correct behaviors in programs manipulating the structure. In order to produce the valid objects to train the network, an assumed-correct set of object building routines is randomly executed. Invalid instances are produced by generating values for object fields that ""break"" the collected valid values, i.e., that assign values to object fields that have not been observed as feasible in the assumed-correct executions that led to the collected valid instances. We experimentally assess this approach, over a benchmark of data structures. We show that this learning technique produces classifiers that achieve significantly better accuracy in classifying valid/invalid objects compared to a technique for dynamic invariant detection, and leads to improved bug finding.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811951,Specification inference;Machine learning;Bug finding,Data structures;Tools;Java;Computer bugs;Software;Neural networks;Test pattern generators,data structures;learning (artificial intelligence);neural nets;pattern classification;program debugging;program testing,data structure invariants;invalid data structure objects;artificial neural network;invalid instances;object building routines;assign values;learning technique;dynamic invariant detection;valid instances;binary classifiers training;bug finding;valid data structure objects,7
235,Not Mentioned,Graph Embedding Based Familial Analysis of Android Malware using Unsupervised Learning,M. Fan; X. Luo; J. Liu; M. Wang; C. Nong; Q. Zheng; T. Liu,"Department of Computing, The Hong Kong Polytechnic University, China; Department of Computing, The Hong Kong Polytechnic University, China; School of Electronic and Information Engineering, Xi'an Jiaotong University, China; School of Computer Science and Engineering, Southeast University, China; School of Electronic and Information Engineering, Xi'an Jiaotong University, China; School of Electronic and Information Engineering, Xi'an Jiaotong University, China; School of Electronic and Information Engineering, Xi'an Jiaotong University, China",2019,"The rapid growth of Android malware has posed severe security threats to smartphone users. On the basis of the familial trait of Android malware observed by previous work, the familial analysis is a promising way to help analysts better focus on the commonalities of malware samples within the same families, thus reducing the analytical workload and accelerating malware analysis. The majority of existing approaches rely on supervised learning and face three main challenges, i.e., low accuracy, low efficiency, and the lack of labeled dataset. To address these challenges, we first construct a fine-grained behavior model by abstracting the program semantics into a set of subgraphs. Then, we propose SRA, a novel feature that depicts the similarity relationships between the Structural Roles of sensitive API call nodes in subgraphs. An SRA is obtained based on graph embedding techniques and represented as a vector, thus we can effectively reduce the high complexity of graph matching. After that, instead of training a classifier with labeled samples, we construct malware link network based on SRAs and apply community detection algorithms on it to group the unlabeled samples into groups. We implement these ideas in a system called GefDroid that performs Graph embedding based familial analysis of AnDroid malware using unsupervised learning. Moreover, we conduct extensive experiments to evaluate GefDroid on three datasets with ground truth. The results show that GefDroid can achieve high agreements (0.707-0.883 in term of NMI) between the clustering results and the ground truth. Furthermore, GefDroid requires only linear run-time overhead and takes around 8.6s to analyze a sample on average, which is considerably faster than the previous work.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812083,Android malware;graph embedding;familial analysis;unsupervised learning,Malware;Unsupervised learning;Semantics;Detection algorithms;Security;Feature extraction;Face,application program interfaces;graph theory;invasive software;learning (artificial intelligence);pattern classification;pattern clustering;smart phones;unsupervised learning,familial analysis;unsupervised learning;analytical workload;supervised learning;graph embedding techniques;malware link network;Android malware;SRA;similarity relationships;GefDroid,37
236,Not Mentioned,A Novel Neural Source Code Representation Based on Abstract Syntax Tree,J. Zhang; X. Wang; H. Zhang; H. Sun; K. Wang; X. Liu,"SKLSDE Lab. School of Computer Science and Engineering, Beihang University, Beijing, China; SKLSDE Lab. School of Computer Science and Engineering, Beihang University, Beijing, China; The University of Newcastle, Australia; SKLSDE Lab. School of Computer Science and Engineering, Beihang University, Beijing, China; SKLSDE Lab. School of Computer Science and Engineering, Beihang University, Beijing, China; SKLSDE Lab. School of Computer Science and Engineering, Beihang University, Beijing, China",2019,"Exploiting machine learning techniques for analyzing programs has attracted much attention. One key problem is how to represent code fragments well for follow-up analysis. Traditional information retrieval based methods often treat programs as natural language texts, which could miss important semantic information of source code. Recently, state-of-the-art studies demonstrate that abstract syntax tree (AST) based neural models can better represent source code. However, the sizes of ASTs are usually large and the existing models are prone to the long-term dependency problem. In this paper, we propose a novel AST-based Neural Network (ASTNN) for source code representation. Unlike existing models that work on entire ASTs, ASTNN splits each large AST into a sequence of small statement trees, and encodes the statement trees to vectors by capturing the lexical and syntactical knowledge of statements. Based on the sequence of statement vectors, a bidirectional RNN model is used to leverage the naturalness of statements and finally produce the vector representation of a code fragment. We have applied our neural network based source code representation method to two common program comprehension tasks: source code classification and code clone detection. Experimental results on the two tasks indicate that our model is superior to state-of-the-art approaches.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812062,"Abstract Syntax Tree, source code representation, neural network, code classification, code clone detection",Syntactics;Cloning;Semantics;Neural networks;Task analysis;Binary trees;Natural languages,information retrieval;learning (artificial intelligence);natural language processing;program diagnostics;recurrent neural nets;text analysis;tree data structures,abstract syntax tree;code fragment;natural language texts;ASTNN;statement trees;statement vectors;bidirectional RNN model;vector representation;source code representation method;source code classification;code clone detection;program analysis;program comprehension tasks;AST-based Neural Network;neural source code representation;information retrieval;machine learning,216
237,Not Mentioned,A Neural Model for Generating Natural Language Summaries of Program Subroutines,A. LeClair; S. Jiang; C. McMillan,"Dept. of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA; Dept. of Computer Science, Eastern Michigan University, Ypsilanti, MI, USA; Dept. of Computer Science and Engineering, University of Notre Dame, Notre Dame, IN, USA",2019,"Source code summarization -- creating natural language descriptions of source code behavior -- is a rapidly-growing research topic with applications to automatic documentation generation, program comprehension, and software maintenance. Traditional techniques relied on heuristics and templates built manually by human experts. Recently, data-driven approaches based on neural machine translation have largely overtaken template-based systems. But nearly all of these techniques rely almost entirely on programs having good internal documentation; without clear identifier names, the models fail to create good summaries. In this paper, we present a neural model that combines words from code with code structure from an AST. Unlike previous approaches, our model processes each data source as a separate input, which allows the model to learn code structure independent of the text in code. This process helps our approach provide coherent summaries in many cases even when zero internal documentation is provided. We evaluate our technique with a dataset we created from 2.1m Java methods. We find improvement over two baseline techniques from SE literature and one from NLP literature.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811932,automatic documentation generation;source code summarization;code comment generation,Algorithms;Documentation;Natural languages;Java;Software engineering;Standards;Task analysis,language translation;natural language processing;neural nets;software engineering;source code (software);system documentation,neural machine translation;neural model;program subroutines;source code summarization;natural language descriptions;natural language summaries;data-driven approaches;software maintenance;program comprehension;automatic documentation generation;source code behavior,131
239,Not Mentioned,The List is the Process: Reliable Pre-Integration Tracking of Commits on Mailing Lists,R. Ramsauer; D. Lohmann; W. Mauerer,"Technical University of Applied Sciences Regensburg; University of Hanover; Siemens AG, Corporate Technology, Munich",2019,"A considerable corpus of research on software evolution focuses on mining changes in software repositories, but omits their pre-integration history. We present a novel method for tracking this otherwise invisible evolution of software changes on mailing lists by connecting all early revisions of changes to their final version in repositories. Since artefact modifications on mailing lists are communicated by updates to fragments (i.e., patches) only, identifying semantically similar changes is a non-trivial task that our approach solves in a language-independent way. We evaluate our method on high-profile open source software (OSS) projects like the Linux kernel, and validate its high accuracy using an elaborately created ground truth. Our approach can be used to quantify properties of OSS development processes, which is an essential requirement for using OSS in reliable or safety-critical industrial products, where certifiability and conformance to processes are crucial. The high accuracy of our technique allows, to the best of our knowledge, for the first time to quantitatively determine if an open development process effectively aligns with given formal process requirements.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812060,software engineering;mining software repositories;mailing lists;patches;commits,Data mining;Software maintenance;Linux;Open source software;Electronic mail;Software reliability,data mining;electronic mail;Linux;public domain software;software maintenance,OSS development processes;open development process;mailing lists;software evolution;software repositories;open source software projects;Linux kernel;reliable safety-critical industrial products;formal process requirements,8
240,Not Mentioned,"Graph-Based Mining of In-the-Wild, Fine-Grained, Semantic Code Change Patterns",H. A. Nguyen; T. N. Nguyen; D. Dig; S. Nguyen; H. Tran; M. Hilton,"Computer Science Department, Iowa State University, USA; Computer Science Dept, The Univ. of Texas at Dallas, USA; Computer Science Department, Oregon State University, USA; Computer Science Dept, The Univ. of Texas at Dallas, USA; Computer Science Dept, The Univ. of Texas at Dallas, USA; School of Computer Science, Carnegie Mellon University, USA",2019,"Prior research exploited the repetitiveness of code changes to enable several tasks such as code completion, bug-fix recommendation, library adaption, etc. These and other novel applications require accurate detection of semantic changes, but the state-of-the-art methods are limited to algorithms that detect specific kinds of changes at the syntactic level. Existing algorithms relying on syntactic similarity have lower accuracy, and cannot effectively detect semantic change patterns. We introduce a novel graph-based mining approach, CPatMiner, to detect previously unknown repetitive changes in the wild, by mining fine-grained semantic code change patterns from a large number of repositories. To overcome unique challenges such as detecting meaningful change patterns and scaling to large repositories, we rely on fine-grained change graphs to capture program dependencies. We evaluate CPatMiner by mining change patterns in a diverse corpus of 5,000+ open-source projects from GitHub across a population of 170,000+ developers. We use three complementary methods. First, we sent the mined patterns to 108 open-source developers. We found that 70% of respondents recognized those patterns as their meaningful frequent changes. Moreover, 79% of respondents even named the patterns, and 44% wanted future IDEs to automate such repetitive changes. We found that the mined change patterns belong to various development activities: adaptive (9%), perfective (20%), corrective (35%) and preventive (36%, including refactorings). Second, we compared our tool with the state-of-the-art, AST-based technique, and reported that it detects 2.1x more meaningful patterns. Third, we use CPatMiner to search for patterns in a corpus of 88 GitHub projects with longer histories consisting of 164M SLOCs. It constructed 322K fine-grained change graphs containing 3M nodes, and detected 17K instances of change patterns from which we provide unique insights on the practice of change patterns among individuals and teams. We found that a large percentage (75%) of the change patterns from individual developers are commonly shared with others, and this holds true for teams. Moreover, we found that the patterns are not intermittent but spread widely over time. Thus, we call for a community-based change pattern database to provide important resources in novel applications.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812071,"Semantic Change Pattern Mining, Graph Mining",Semantics;Tools;Data mining;Syntactics;Computer science;Task analysis;Open source software,data mining;graph theory;program diagnostics;software engineering,fine-grained semantic code change patterns;fine-grained change;mined change patterns;community-based change pattern database;graph-based mining;code changes;semantic change patterns;GitHub projects;AST-based technique,27
241,Not Mentioned,Intention-Based Integration of Software Variants,M. Lillack; S. Stanciulescu; W. Hedman; T. Berger; A. WÄ…sowski,"Leipzig University, Germany; ABB Corporate Research, Switzerland; Chalmers | University of Gothenburg, Sweden; Chalmers | University of Gothenburg, Sweden; IT University of Copenhagen, Denmark",2019,"Cloning is a simple way to create new variants of a system. While cheap at first, it increases maintenance cost in the long term. Eventually, the cloned variants need to be integrated into a configurable platform. Such an integration is challenging: it involves merging the usual code improvements between the variants, and also integrating the variable code (features) into the platform. Thus, variant integration differs from traditional soft- ware merging, which does not produce or organize configurable code, but creates a single system that cannot be configured into variants. In practice, variant integration requires fine-grained code edits, performed in an exploratory manner, in multiple iterations. Unfortunately, little tool support exists for integrating cloned variants. In this work, we show that fine-grained code edits needed for integration can be alleviated by a small set of integration intentions-domain-specific actions declared over code snippets controlling the integration. Developers can interactively explore the integration space by declaring (or revoking) intentions on code elements. We contribute the intentions (e.g., 'keep functionality' or 'keep as a configurable feature') and the IDE tool INCLINE, which implements the intentions and five editable views that visualize the integration process and allow declaring intentions producing a configurable integrated platform. In a series of experiments, we evaluated the completeness of the pro- posed intentions, the correctness and performance of INCLINE, and the benefits of using intentions for variant integration. The experiments show that INCLINE can handle complex integration tasks, that views help to navigate the code, and that it consistently reduces mistakes made by developers during variant integration.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811913,software product line;variant integration;clone and own;re engineering variants;code merging;intention based integration,Tools;Merging;Task analysis;Software product lines;Open source software;Maintenance engineering,integrated software;programming environments;software maintenance,fine-grained code edits;integration intentions-domain-specific actions;code snippets;configurable integrated platform;intention-based integration;software variants;configurable platform;variable code;configurable code;IDE tool INCLINE,8
242,Not Mentioned,Supporting the Statistical Analysis of Variability Models,R. Heradio; D. Fernandez-Amoros; C. Mayr-Dorn; A. Egyed,"Universidad Nacional de Educacion a Distancia, Madrid, Spain; Universidad Nacional de Educacion a Distancia, Madrid, Spain; Johannes Kepler University, Linz, Austria; Johannes Kepler University, Linz, Austria",2019,"Variability models are broadly used to specify the configurable features of highly customizable software. In practice, they can be large, defining thousands of features with their dependencies and conflicts. In such cases, visualization techniques and automated analysis support are crucial for understanding the models. This paper contributes to this line of research by presenting a novel, probabilistic foundation for statistical reasoning about variability models. Our approach not only provides a new way to visualize, describe and interpret variability models, but it also supports the improvement of additional state-of-the-art methods for software product lines; for instance, providing exact computations where only approximations were available before, and increasing the sensitivity of existing analysis operations for variability models. We demonstrate the benefits of our approach using real case studies with up to 17,365 features, and written in two different languages (KConfig and feature models).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811977,Variability modeling;feature modeling;software product lines;software visualization;binary decision diagrams,Visualization;Computational modeling;Analytical models;Software;Cognition;Feature extraction;Complexity theory,data visualisation;probability;public domain software;software product lines;software reusability;statistical analysis,statistical analysis;variability models;automated analysis support;visualization techniques;software product lines;feature model language;KConfig language,4
243,Not Mentioned,Multifaceted Automated Analyses for Variability-Intensive Embedded Systems,S. Lazreg; M. Cordy; P. Collet; P. Heymans; S. Mosser,"CNRS, UniversitÃ© CÃ´te dâ€™Azur, I3S, France; SnT, University of Luxembourg, Luxembourg; CNRS, UniversitÃ© CÃ´te dâ€™Azur, I3S, France; University of Namur, Belgium; CNRS, UniversitÃ© CÃ´te dâ€™Azur, I3S, France",2019,"Embedded systems, like those found in the automotive domain, must comply with stringent functional and non-functional requirements. To fulfil these requirements, engineers are confronted with a plethora of design alternatives both at the software and hardware level, out of which they must select the optimal solution wrt. possibly-antagonistic quality attributes (e.g. cost of manufacturing vs. speed of execution). We propose a model-driven framework to assist engineers in this choice. It captures high-level specifications of the system in the form of variable dataflows and configurable hardware platforms. A mapping algorithm then derives the design space, i.e. the set of compatible pairs of application and platform variants, and a variability-aware executable model, which encodes the functional and non-functional behaviour of all viable system variants. Novel verification algorithms then pinpoint the optimal system variants efficiently. The benefits of our approach are evaluated through a real-world case study from the automotive industry.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812057,Embedded system design engineering;variability modeling;model checking;non functional property;multi objective optimization,Task analysis;Random access memory;Hardware;Graphics processing units;Rendering (computer graphics);Manufacturing,embedded systems;formal specification;program verification,possibly-antagonistic quality;model-driven framework;high-level specifications;variable dataflows;configurable hardware platforms;mapping algorithm;design space;variability-aware executable model;optimal system;automotive industry;multifaceted automated analyses;variability-intensive embedded systems;automotive domain;nonfunctional requirements;hardware level;verification algorithms,5
244,Not Mentioned,Exposing Library API Misuses Via Mutation Analysis,M. Wen; Y. Liu; R. Wu; X. Xie; S. -C. Cheung; Z. Su,"Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Shenzhen Key Laboratory of Computational Intelligence, Southern University of Science and Technology, Shenzhen, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Sun Yat-sen University, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; ETH Zurich, Switzerland",2019,"Misuses of library APIs are pervasive and often lead to software crashes and vulnerability issues. Various static analysis tools have been proposed to detect library API misuses. They often involve mining frequent patterns from a large number of correct API usage examples, which can be hard to obtain in practice. They also suffer from low precision due to an over-simplified assumption that a deviation from frequent usage patterns indicates a misuse. We make two observations on the discovery of API misuse patterns. First, API misuses can be represented as mutants of the corresponding correct usages. Second, whether a mutant will introduce a misuse can be validated via executing it against a test suite and analyzing the execution information. Based on these observations, we propose MutApi, the first approach to discovering API misuse patterns via mutation analysis. To effectively mimic API misuses based on correct usages, we first design eight effective mutation operators inspired by the common characteristics of API misuses. MutApi generates mutants by applying these mutation operators on a set of client projects and collects mutant-killing tests as well as the associated stack traces. Misuse patterns are discovered from the killed mutants that are prioritized according to their likelihood of causing API misuses based on the collected information. We applied MutApi on 16 client projects with respect to 73 popular Java APIs. The results show that MutApi is able to discover substantial API misuse patterns with a high precision of 0.78. It also achieves a recall of $0.49$ on the MuBench benchmark, which outperforms the state-of-the-art techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812114,Mutation Analysis;Library API Misuse,Libraries;Computer bugs;Java;Software;Tools;Detectors,application program interfaces;data mining;Java;program diagnostics;program testing;security of data,substantial API misuse patterns;library API misuses;mutation analysis;frequent usage patterns;Java API;static analysis tools;MutApi;mutant-killing tests;frequent pattern mining,22
245,Not Mentioned,PIVOT: Learning API-Device Correlations to Facilitate Android Compatibility Issue Detection,L. Wei; Y. Liu; S. -C. Cheung,"Dept. of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Shenzhen Key Laboratory of Computational Intelligence, Southern University of Science and Technology, Shenzhen, china; Dept. of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",2019,"The heavily fragmented Android ecosystem has induced various compatibility issues in Android apps. The search space for such fragmentation-induced compatibility issues (FIC issues) is huge, comprising three dimensions: device models, Android OS versions, and Android APIs. FIC issues, especially those arising from device models, evolve quickly with the frequent release of new device models to the market. As a result, an automated technique is desired to maintain timely knowledge of such FIC issues, which are mostly undocumented. In this paper, we propose such a technique, PIVOT, that automatically learns API-device correlations of FIC issues from existing Android apps. PIVOT extracts and prioritizes API-device correlations from a given corpus of Android apps. We evaluated PIVOT with popular Android apps on Google Play. Evaluation results show that PIVOT can effectively prioritize valid API-device correlations for app corpora collected at different time. Leveraging the knowledge in the learned API-device correlations, we further conducted a case study and successfully uncovered ten previously-undetected FIC issues in open-source Android apps.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811898,"Android fragmentation, compatibility, static analysis, learning",Correlation;Biological system modeling;Cameras;Ecosystems;Testing;Google;Open source software,Android (operating system);application program interfaces;learning (artificial intelligence);mobile computing;smart phones,PIVOT;heavily fragmented Android ecosystem;fragmentation-induced compatibility issues;device models;Android OS versions;Android APIs;popular Android apps;valid API-device correlations;learned API-device correlations;open-source Android apps;Android compatibility issue detection,23
246,Not Mentioned,SafeCheck: Safety Enhancement of Java Unsafe API,S. Huang; J. Guo; S. Li; X. Li; Y. Qi; K. Chow; J. Huang,"Department of Computer Science, Texas A&M University, USA; Alibaba Group, China; Alibaba Group, China; Alibaba Group, China; Alibaba Group, China; Alibaba Group, China; Department of Computer Science, Texas A&M University, USA",2019,"Java is a safe programming language by providing bytecode verification and enforcing memory protection. For instance, programmers cannot directly access the memory but have to use object references. Yet, the Java runtime provides an Unsafe API as a backdoor for the developers to access the low- level system code. Whereas the Unsafe API is designed to be used by the Java core library, a growing community of third-party libraries use it to achieve high performance. The Unsafe API is powerful, but dangerous, which leads to data corruption, resource leaks and difficult-to-diagnose JVM crash if used improperly. In this work, we study the Unsafe crash patterns and propose a memory checker to enforce memory safety, thus avoiding the JVM crash caused by the misuse of the Unsafe API at the bytecode level. We evaluate our technique on real crash cases from the openJDK bug system and real-world applications from AJDK. Our tool reduces the efforts from several days to a few minutes for the developers to diagnose the Unsafe related crashes. We also evaluate the runtime overhead of our tool on projects using intensive Unsafe operations, and the result shows that our tool causes a negligible perturbation to the execution of the applications.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811920,Java Unsafe API;Dynamic Analysis;Bytecode;Memoey Safety,Java;Safety;Tools;Computer bugs;Runtime;Libraries,application program interfaces;Java;program debugging;security of data;storage management,memory safety;bytecode verification;Java runtime;low- level system code;Java core library;memory checker;memory protection;safety enhancement;Java unsafe API;programming language;unsafe crash patterns;openJDK bug system;JVM crash,6
248,Not Mentioned,CTRAS: Crowdsourced Test Report Aggregation and Summarization,R. Hao; Y. Feng; J. A. Jones; Y. Li; Z. Chen,"State Key Laboratory for Novel Software Technology Nanjing University, Nanjing, China; Department of Informatics, University of California, Irvine, USA; Department of Informatics, University of California, Irvine, USA; State Key Laboratory for Novel Software Technology Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology Nanjing University, Nanjing, China",2019,"Crowdsourced testing has been widely adopted to improve the quality of various software products. Crowdsourced workers typically perform testing tasks and report their experiences through test reports. While the crowdsourced test reports provide feedbacks from real usage scenarios, inspecting such a large number of reports becomes a time-consuming yet inevitable task. To improve the efficiency of this task, existing widely used issue-tracking systems, such as JIRA, Bugzilla, and Mantis, have provided keyword-search-based methods to assist users in identifying duplicate test reports. However, on mobile devices (such as mobile phones), where the crowdsourced test reports often contain insufficient text descriptions but instead rich screenshots, these text-analysis-based methods become less effective because the data has fundamentally changed. In this paper, instead of focusing on only detecting duplicates based on textual descriptions, we present CTRAS: a novel approach to leveraging duplicates to enrich the content of bug descriptions and improve the efficiency of inspecting these reports. CTRAS is capable of automatically aggregating duplicates based on both textual information and screenshots, and further summarizes the duplicate test reports into a comprehensive and comprehensible report. To validate CTRAS, we conducted quantitative studies using more than 5000 test reports, collected from 12 industrial crowdsourced projects. The experimental results reveal that CTRAS can reach an accuracy of 0.87, on average, regarding automatically detecting duplicate reports, and it outperforms the classic Max-Coverage-based and MMR summarization methods under Jensen Shannon divergence metric. Moreover, we conducted a task-based user study with 30 participants, whose result indicates that CTRAS can save nearly 30% time cost on average without loss of correctness.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811987,crowdsourced testing;summarization;duplicate bug reports,Testing;Software;Computer bugs;Task analysis;Feature extraction;Mobile handsets;Debugging,information retrieval;natural language processing;program debugging;text analysis,Jensen Shannon divergence metric;industrial crowdsourced projects;comprehensible report;comprehensive report;text-analysis-based methods;duplicate test reports;keyword-search-based methods;crowdsourced test reports;crowdsourced workers;crowdsourced test report aggregation;task-based user study;duplicate reports;CTRAS,20
249,Not Mentioned,iSENSE: Completion-Aware Crowdtesting Management,J. Wang; Y. Yang; R. Krishna; T. Menzies; Q. Wang,"University of Chinese Academy of Sciences, Beijing, China; School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; Department of Computer Science, North Carolina State University, Raleigh, NC, USA; University of Chinese Academy of Sciences, Beijing, China",2019,"Crowdtesting has become an effective alternative to traditional testing, especially for mobile applications. However, crowdtesting is hard to manage in nature. Given the complexity of mobile applications and unpredictability of distributed crowdtesting processes, it is difficult to estimate (a) remaining number of bugs yet to be detected or (b) required cost to find those bugs. Experience-based decisions may result in ineffective crowdtesting processes, e.g., there is an average of 32% wasteful spending in current crowdtesting practices. This paper aims at exploring automated decision support to effectively manage crowdtesting processes. It proposes an approach named ISENSE which applies incremental sampling technique to process crowdtesting reports arriving in chronological order, organizes them into fixed-size groups as dynamic inputs, and predicts two test completion indicators in an incremental manner. The two indicators are: 1) total number of bugs predicted with Capture-ReCapture model, and 2) required test cost for achieving certain test objectives predicted with AutoRegressive Integrated Moving Average model. The evaluation of ISENSE is conducted on 46,434 reports of 218 crowdtesting tasks from one of the largest crowdtesting platforms in China. Its effectiveness is demonstrated through two application studies for automating crowdtesting management and semi-automation of task closing trade-off analysis. The results show that ISENSE can provide managers with greater awareness of testing progress to achieve cost-effectiveness gains of crowdtesting. Specifically, a median of 100% bugs can be detected with 30% saved cost based on the automated close prediction.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812109,Crowdtesting;automated close prediction;test completion;crowdtesting management,Computer bugs;Task analysis;Testing;Predictive models;Software;Mobile applications;Data models,autoregressive moving average processes;crowdsourcing;decision support systems;program debugging;program testing,distributed crowdtesting processes;bugs;experience-based decisions;automated decision support;incremental sampling technique;crowdtesting reports;test completion indicators;automating crowdtesting management;cost-effectiveness gains;automated close prediction;iSENSE;completion-aware crowdtesting management;mobile applications;autoregressive integrated moving average model;crowdtesting platforms;crowdtesting tasks,12
250,Not Mentioned,How Practitioners Perceive Coding Proficiency,X. Xia; Z. Wan; P. S. Kochhar; D. Lo,"Faculty of Information Technology, Monash University, Melbourne, Australia; College of Computer Science and Technology, Zhejiang University, Hangzhou, China; Microsoft, Vancouver, Canada; School of Information Systems, Singapore Management University, Singapore, Singapore",2019,"Coding proficiency is essential to software practitioners. Unfortunately, our understanding on coding proficiency often translates to vague stereotypes, e.g., ""able to write good code"". The lack of specificity hinders employers from measuring a software engineer's coding proficiency, and software engineers from improving their coding proficiency skills. This raises an important question: what skills matter to improve one's coding proficiency. To answer this question, we perform an empirical study by surveying 340 software practitioners from 33 countries across 5 continents. We first identify 38 coding proficiency skills grouped into nine categories by interviewing 15 developers from three companies. We then ask our survey respondents to rate the level of importance for these skills, and provide rationales of their ratings. Our study highlights a total of 21 important skills that receive an average rating of 4.0 and above (important and very important), along with rationales given by proponents and dissenters. We discuss implications of our findings to researchers, educators, and practitioners.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812050,practitioners;coding proficiency,Encoding;Software;Interviews;Companies;Computer bugs;Computer languages;Programming,project management;software development management,software practitioners;coding proficiency skills;coding proficiency;software engineers,11
251,Not Mentioned,Socio-Technical Work-Rate Increase Associates With Changes in Work Patterns in Online Projects,F. Sarker; B. Vasilescu; K. Blincoe; V. Filkov,"University of California, Davis; Carnegie Mellon University; University of Auckland; University of California, Davis",2019,"Software developers work on a variety of tasks ranging from the technical, e.g., writing code, to the social, e.g., participating in issue resolution discussions. The amount of work developers perform per week (their work-rate) also varies and depends on project needs and developer schedules. Prior work has shown that while moderate levels of increased technical work and multitasking lead to higher productivity, beyond a certain threshold, they can lead to lowered performance. Here, we study how increases in the short-term work-rate along both the technical and social dimensions are associated with changes in developers' work patterns, in particular communication sentiment, technical productivity, and social productivity. We surveyed active and prolific developers on GitHub to understand the causes and impacts of increased work-rates. Guided by the responses, we developed regression models to study how communication and committing patterns change with increased work-rates and fit those models to large-scale data gathered from traces left by thousands of GitHub developers. From our survey and models, we find that most developers do experience work-rate-increase-related changes in behavior. Most notably, our models show that there is a sizable effect when developers comment much more than their average: the negative sentiment in their comments increases, suggesting an increased level of stress. Our models also show that committing patterns do not change with increased commenting, and vice versa, suggesting that technical and social activities tend not to be multitasked.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811968,software engineering;work related stress;work patterns;multitasking;socio technical;work rate increase;survey;empirical analysis;regression modeling;GitHub;software developers;open source software;online projects;commits;comments;sentiment;social activities;technical activities;focus switching;recommendations;repositories;issues;pull requests;discussions;teams;collaboration;team members;social coding,Stress;Task analysis;Software;Productivity;Data models;Multitasking;Switches,Internet;project management;regression analysis;software engineering,GitHub developers;technical activities;social activities;work patterns;technical dimensions;social dimensions;technical productivity;social productivity;software developers;socio-technical work-rate increase;online projects;regression models,11
252,Not Mentioned,Why Do Episodic Volunteers Stay in FLOSS Communities?,A. Barcomb; K. -J. Stol; D. Riehle; B. Fitzgerald,"Open Source Research Group, Friedrich-Alexander-UniversitÃ¤t Erlangen-NÃ¼rnberg; Lero-the Irish Software Research Centre; Open Source Research Group, Friedrich-Alexander-UniversitÃ¤t Erlangen-NÃ¼rnberg; Dept. Computer Science and Information Systems, University of Limerick",2019,"Successful Free/Libre and Open Source Software (FLOSS) projects incorporate both habitual and infrequent, or episodic, contributors. Using the concept of episodic volunteering (EV) from the general volunteering literature, we derive a model consisting of five key constructs that we hypothesize affect episodic volunteers' retention in FLOSS communities. To evaluate the model we conducted a survey with over 100 FLOSS episodic volunteers. We observe that three of our model constructs (social norms, satisfaction and community commitment) are all positively associated with volunteers' intention to remain, while the two other constructs (psychological sense of community and contributor benefit motivations) are not. Furthermore, exploratory clustering on unobserved heterogeneity suggests that there are four distinct categories of volunteers: satisfied, classic, social and obligated. Based on our findings, we offer suggestions for projects to incorporate and manage episodic volunteers, so as to better leverage this type of contributors and potentially improve projects' sustainability.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811892,community management;episodic volunteering;open source software;volunteer management,Psychology;Analytical models;Sustainable development;Open source software;Computer science;Face,public domain software;software development management;software quality,FLOSS communities;habitual contributors;episodic contributors;episodic volunteering;general volunteering literature;community commitment;FLOSS episodic volunteers;free-libre and open source software,13
253,Not Mentioned,When Code Completion Fails: A Case Study on Real-World Completions,V. J. Hellendoorn; S. Proksch; H. C. Gall; A. Bacchelli,"Department of Computer Science, UC Davis Davis, USA; Department of Informatics, University of Zurich, ZÃ¼rich, Switzerland; Department of Informatics, University of Zurich, ZÃ¼rich, Switzerland; Department of Informatics, University of Zurich, ZÃ¼rich, Switzerland",2019,"Code completion is commonly used by software developers and is integrated into all major IDE's. Good completion tools can not only save time and effort but may also help avoid incorrect API usage. Many proposed completion tools have shown promising results on synthetic benchmarks, but these benchmarks make no claims about the realism of the completions they test. This lack of grounding in real-world data could hinder our scientific understanding of developer needs and of the efficacy of completion models. This paper presents a case study on 15,000 code completions that were applied by 66 real developers, which we study and contrast with artificial completions to inform future research and tools in this area. We find that synthetic benchmarks misrepresent many aspects of real-world completions; tested completion tools were far less accurate on real-world data. Worse, on the few completions that consumed most of the developers' time, prediction accuracy was less than 20% -- an effect that is invisible in synthetic benchmarks. Our findings have ramifications for future benchmarks, tool design and real-world efficacy: Benchmarks must account for completions that developers use most, such as intra-project APIs; models should be designed to be amenable to intra-project data; and real-world developer trials are essential to quantifying performance on the least predictable completions, which are both most time-consuming and far more typical than artificial data suggests. We publicly release our preprint [https://doi.org/10.5281/zenodo.2565673] and replication data and materials [https://doi.org/10.5281/zenodo.2562249].",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812116,Code completion;Benchmark;Language models,Tools;Benchmark testing;Data models;Context modeling;Syntactics;Vocabulary;Training,data handling;program testing;software engineering;software tools,code completion;real-world completions;software developers;synthetic benchmarks;real-world data;completion models;artificial completions;tested completion tools;tool design;real-world efficacy;real-world developer trials;predictable completions,25
254,Not Mentioned,Interactive Production Performance Feedback in the IDE,J. Cito; P. Leitner; M. Rinard; H. C. Gall,"MIT, Cambridge, MA, USA; Chalmers | University of Gothenburg, Gothenburg, Sweden; MIT, Cambridge, MA, USA; University of Zurich, Zurich, Switzerland",2019,"Because of differences between development and production environments, many software performance problems are detected only after software enters production. We present PerformanceHat, a new system that uses profiling information from production executions to develop a global performance model suitable for integration into interactive development environments. PerformanceHat's ability to incrementally update this global model as the software is changed in the development environment enables it to deliver near real-time predictions of performance consequences reflecting the impact on the production environment. We implement PerformanceHat as an Eclipse plugin and evaluate it in a controlled experiment with 20 professional software developers implementing several software maintenance tasks using our approach and a representative baseline (Kibana). Our results indicate that developers using PerformanceHat were significantly faster in (1) detecting the performance problem, and (2) finding the root-cause of the problem. These results provide encouraging evidence that our approach helps developers detect, prevent, and debug production performance problems during development before the problem manifests in production.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811928,"software performance engineering, IDE, user study",Monitoring;Software;Runtime;Tools;Data models;Task analysis,program debugging;programming environments;software maintenance;software performance evaluation,interactive production performance feedback;production environment;software performance problems;production executions;global performance model;interactive development environments;development environment;software maintenance tasks;performance problem;debug production performance problems;PerformanceHat;Eclipse plugin;software maintenance,8
256,Not Mentioned,Redundant Loads: A Software Inefficiency Indicator,P. Su; S. Wen; H. Yang; M. Chabbi; X. Liu,College of William & Mary; College of William & Mary; Beihang University; Scalable Machines Research; College of William & Mary,2019,"Modern software packages have become increasingly complex with millions of lines of code and references to many external libraries. Redundant operations are a common performance limiter in these code bases. Missed compiler optimization opportunities, inappropriate data structure and algorithm choices, and developers' inattention to performance are some common reasons for the existence of redundant operations. Developers mainly depend on compilers to eliminate redundant operations. However, compilers' static analysis often misses optimization opportunities due to ambiguities and limited analysis scope; automatic optimizations to algorithmic and data structural problems are out of scope. We develop LoadSpy, a whole-program profiler to pinpoint redundant memory load operations, which are often a symptom of many redundant operations. The strength of LoadSpy exists in identifying and quantifying redundant load operations in programs and associating the redundancies with program execution contexts and scopes to focus developers' attention on problematic code. LoadSpy works on fully optimized binaries, adopts various optimization techniques to reduce its overhead, and provides a rich graphic user interface, which make it a complete developer tool. Applying LoadSpy showed that a large fraction of redundant loads is common in modern software packages despite highest levels of automatic compiler optimizations. Guided by LoadSpy, we optimize several well-known benchmarks and real-world applications, yielding significant speedups.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811970,Whole-program profiling;Software optimization;Performance measurement;Tools,Redundancy;Optimization;Loading;Software;Tools;Monitoring;Registers,application program interfaces;data structures;graphical user interfaces;optimisation;optimising compilers;program debugging;program diagnostics;software libraries,redundant loads;software inefficiency indicator;modern software packages;common performance limiter;missed compiler optimization opportunities;algorithm choices;algorithmic data structural problems;LoadSpy;redundant memory load operations;automatic compiler optimizations;data structure;redundant load operations;graphic user interface,16
257,Not Mentioned,View-Centric Performance Optimization for Database-Backed Web Applications,J. Yang; C. Yan; C. Wan; S. Lu; A. Cheung,University of Chicago; University of Washington; University of Chicago; University of Chicago; University of Washington,2019,"Web developers face the stringent task of designing informative web pages while keeping the page-load time low. This task has become increasingly challenging as most web contents are now generated by processing ever-growing amount of user data stored in back-end databases. It is difficult for developers to understand the cost of generating every web-page element, not to mention explore and pick the web design with the best trade-off between performance and functionality. In this paper, we present Panorama, a view-centric and database-aware development environment for web developers. Using database-aware program analysis and novel IDE design, Panorama provides developers with intuitive information about the cost and the performance-enhancing opportunities behind every HTML element, as well as suggesting various global code refactorings that enable developers to easily explore a wide spectrum of performance and functionality trade-offs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811938,database backed web applications;ORM framework;view centric,Databases;Rails;Web pages;Data processing;Tools;Servers;Task analysis,database management systems;hypermedia markup languages;program diagnostics;Web design,view-centric performance optimization;page-load time;back-end databases;database-aware development environment;database-aware program analysis;performance-enhancing opportunities;database-backed Web applications;Web design;Web-page element;informative Web pages,10
258,Not Mentioned,AdJust: Runtime Mitigation of Resource Abusing Third-Party Online Ads,W. Wang; I. L. Kim; Y. Zheng,"University at Buffalo, Buffalo, New York, USA; Purdue University, West Lafayette, Indiana, USA; IBM T.J. Watson Research Center, New York, USA",2019,"Online advertising is the most critical revenue stream for many Internet companies. However, showing ads on websites comes with a price tag. Since website contents and third-party ads are blended together, third-party ads may compete with the publisher contents, delaying or even breaking the rendering of first-party contents. In addition, dynamically including scripts from ad networks all over the world may introduce buggy scripts that slow down page loads and even freeze the browser. The resulting poor usability problems lead to bad user experience and lower profits. The problems caused by such resource abusing ads are originated from two root causes: First, content publishers have no control over third-party ads. Second, publishers cannot differentiate resource consumed by ads from that consumed by their own contents. To address these challenges, we propose an effective technique, AdJust, that allows publishers to specify constraints on events associated with third-party ads (e.g., URL requests, HTML element creations, and timers), so that they can mitigate user experience degradations and enforce consistent ads experience to all users. We report on a series of experiments over the Alexa top 200 news websites. The results point to the efficacy of our proposed techniques: AdJust effectively mitigated degradations that freeze web browsers (on 36 websites), reduced the load time of publisher contents (on 61 websites), prioritized publisher contents (on 166 websites) and ensured consistent rendering orders among top ads (on 68 websites).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811929,online ads;defective ads;resource abusing;performance degradation,Servers;Runtime;Browsers;Delays;Web pages;Sports;Uniform resource locators,advertising data processing;rendering (computer graphics);Web sites,content publishers;third-party ads;prioritized publisher contents;resource abusing third-party online ads;website contents;first-party contents;AdJust technique;online advertising,2
259,Not Mentioned,Symbolic Repairs for GR(1) Specifications,S. Maoz; J. O. Ringert; R. Shalom,"Tel Aviv University, Tel Aviv, Israel; University of Leicester, Leicester, UK; Tel Aviv University, Tel Aviv, Israel",2019,"Unrealizability is a major challenge for GR(1), an expressive assume-guarantee fragment of LTL that enables efficient synthesis. Some works attempt to help engineers deal with unrealizability by generating counter-strategies or computing an unrealizable core. Other works propose to repair the unrealizable specification by suggesting repairs in the form of automatically generated assumptions. In this work we present two novel symbolic algorithms for repairing unrealizable GR(1) specifications. The first algorithm infers new assumptions based on the recently introduced JVTS. The second algorithm infers new assumptions directly from the specification. Both algorithms are sound. The first is incomplete but can be used to suggest many different repairs. The second is complete but suggests a single repair. Both are symbolic and therefore efficient. We implemented our work, validated its correctness, and evaluated it on benchmarks from the literature. The evaluation shows the strength of our algorithms, in their ability to suggest repairs and in their performance and scalability compared to previous solutions.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812056,reactive synthesis;repair;GR(1),Maintenance engineering;Safety;Glass;Cascading style sheets;Benchmark testing;Scalability;Standards,formal specification;maintenance engineering;program debugging,symbolic repairs;counter-strategies;symbolic algorithms;GR1 specification;JVTS,26
260,Not Mentioned,CRADLE: Cross-Backend Validation to Detect and Localize Bugs in Deep Learning Libraries,H. V. Pham; T. Lutellier; W. Qi; L. Tan,"University of Waterloo, Canada; University of Waterloo, Canada; USTC, China; Purdue University, USA",2019,"Deep learning (DL) systems are widely used in domains including aircraft collision avoidance systems, Alzheimer's disease diagnosis, and autonomous driving cars. Despite the requirement for high reliability, DL systems are difficult to test. Existing DL testing work focuses on testing the DL models, not the implementations (e.g., DL software libraries) of the models. One key challenge of testing DL libraries is the difficulty of knowing the expected output of DL libraries given an input instance. Fortunately, there are multiple implementations of the same DL algorithms in different DL libraries. Thus, we propose CRADLE, a new approach that focuses on finding and localizing bugs in DL software libraries. CRADLE (1) performs cross-implementation inconsistency checking to detect bugs in DL libraries, and (2) leverages anomaly propagation tracking and analysis to localize faulty functions in DL libraries that cause the bugs. We evaluate CRADLE on three libraries (TensorFlow, CNTK, and Theano), 11 datasets (including ImageNet, MNIST, and KGS Go game), and 30 pre-trained models. CRADLE detects 12 bugs and 104 unique inconsistencies, and highlights functions relevant to the causes of inconsistencies for all 104 unique inconsistencies.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812095,deep learning software testing;cross-implementation testing;bugs detection;software testing,Libraries;Computer bugs;Testing;Training;Atmospheric modeling;Task analysis;Deep learning,learning (artificial intelligence);neural nets;program debugging;software libraries,cross-backend validation;deep learning libraries;deep learning systems;DL software libraries;CRADLE;cross-implementation inconsistency checking;anomaly propagation tracking,62
261,Not Mentioned,Guiding Deep Learning System Testing Using Surprise Adequacy,J. Kim; R. Feldt; S. Yoo,"School of Computing KAIST, Daejeon, Republic of Korea; Dept. of Software Engineering, Blekinge Inst. of Technology, Karlskrona, Sweden; School of Computing KAIST, Daejeon, Republic of Korea",2019,"Deep Learning (DL) systems are rapidly being adopted in safety and security critical domains, urgently calling for ways to test their correctness and robustness. Testing of DL systems has traditionally relied on manual collection and labelling of data. Recently, a number of coverage criteria based on neuron activation values have been proposed. These criteria essentially count the number of neurons whose activation during the execution of a DL system satisfied certain properties, such as being above predefined thresholds. However, existing coverage criteria are not sufficiently fine grained to capture subtle behaviours exhibited by DL systems. Moreover, evaluations have focused on showing correlation between adversarial examples and proposed criteria rather than evaluating and guiding their use for actual testing of DL systems. We propose a novel test adequacy criterion for testing of DL systems, called Surprise Adequacy for Deep Learning Systems (SADL), which is based on the behaviour of DL systems with respect to their training data. We measure the surprise of an input as the difference in DL system's behaviour between the input and the training data (i.e., what was learnt during training), and subsequently develop this as an adequacy criterion: a good test input should be sufficiently but not overtly surprising compared to training data. Empirical evaluation using a range of DL systems from simple image classifiers to autonomous driving car platforms shows that systematic sampling of inputs based on their surprise can improve classification accuracy of DL systems against adversarial examples by up to 77.5% via retraining.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812069,Test Adequacy;Coverage Criteria;Deep Learning Systems,Neurons;Testing;Training;Training data;Deep learning;Autonomous vehicles;Density measurement,image classification;learning (artificial intelligence);program testing,DL system;training data;test adequacy criterion;deep learning system testing;Surprise Adequacy for Deep Learning Systems;autonomous driving car platforms;systematic sampling;coverage criteria,196
263,Not Mentioned,FOCUS: A Recommender System for Mining API Function Calls and Usage Patterns,P. T. Nguyen; J. Di Rocco; D. Di Ruscio; L. Ochoa; T. Degueule; M. Di Penta,"UniversitÃ degli Studi dell'Aquila, L'Aquila, Italy; UniversitÃ degli Studi dell'Aquila, L'Aquila, Italy; UniversitÃ degli Studi dell'Aquila, L'Aquila, Italy; Centrum Wiskunde & Informatica, Amsterdam, Netherlands; Centrum Wiskunde & Informatica, Amsterdam, Netherlands; UniversitÃ degli Studi del Sannio, Benevento, Italy",2019,"Software developers interact with APIs on a daily basis and, therefore, often face the need to learn how to use new APIs suitable for their purposes. Previous work has shown that recommending usage patterns to developers facilitates the learning process. Current approaches to usage pattern recommendation, however, still suffer from high redundancy and poor run-time performance. In this paper, we reformulate the problem of usage pattern recommendation in terms of a collaborative-filtering recommender system. We present a new tool, FOCUS, which mines open-source project repositories to recommend API method invocations and usage patterns by analyzing how APIs are used in projects similar to the current project. We evaluate FOCUS on a large number of Java projects extracted from GitHub and Maven Central and find that it outperforms the state-of-the-art approach PAM with regards to success rate, accuracy, and execution time. Results indicate the suitability of context-aware collaborative-filtering recommender systems to provide API usage patterns.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812051,recommender system;api mining;api usage pattern;api recommendation,Recommender systems;Collaboration;Java;Libraries;Documentation;Data mining;Tools,application program interfaces;collaborative filtering;data mining;Java;public domain software;recommender systems;software libraries;ubiquitous computing,API function calls;software developers interact;learning process;usage pattern recommendation;open-source project repositories;API method invocations;API usage patterns;collaborative-filtering recommender system;Maven Central;GitHub;Java projects;FOCUS,54
264,Not Mentioned,Test-Driven Code Review: An Empirical Study,D. Spadini; F. Palomba; T. Baum; S. Hanenberg; M. Bruntink; A. Bacchelli,"Delft University of Technology, The Netherlands; University of Zurich, Switzerland; Â§ Leibniz Universitat Hannover, Germany; Paluno, University of Duisburg-Essen, Germany; Software Improvement Group, The Netherlands; University of Zurich, Switzerland",2019,"Test-Driven Code Review (TDR) is a code review practice in which a reviewer inspects a patch by examining the changed test code before the changed production code. Although this practice has been mentioned positively by practitioners in informal literature and interviews, there is no systematic knowledge of its effects, prevalence, problems, and advantages. In this paper, we aim at empirically understanding whether this practice has an effect on code review effectiveness and how developers' perceive TDR. We conduct (i) a controlled experiment with 93 developers that perform more than 150 reviews, and (ii) 9 semi-structured interviews and a survey with 103 respondents to gather information on how TDR is perceived. Key results from the experiment show that developers adopting TDR find the same proportion of defects in production code, but more in test code, at the expenses of fewer maintainability issues in production code. Furthermore, we found that most developers prefer to review production code as they deem it more critical and tests should follow from it. Moreover, general poor test code quality and no tool support hinder the adoption of TDR. Public preprint: [https: //doi.org/10.5281/zenodo.2551217], data and materials: [https:// doi.org/10.5281/zenodo.2553139].",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811911,MSR;code review;TDR;software testing,Production;Tools;Software;Inspection;Interviews;Computer bugs;Graphical user interfaces,program testing;software development management;software quality,test-driven code review;TDR;code review effectiveness;critical tests;production code;semistructured interviews;test code quality,17
265,Not Mentioned,Why Does Code Review Work for Open Source Software Communities?,A. Alami; M. Leavitt Cohn; A. WÄ…sowski,"IT University of Copenhagen, Denmark; IT University of Copenhagen, Denmark; IT University of Copenhagen, Denmark",2019,"Open source software communities have demonstrated that they can produce high quality results. The overall success of peer code review, commonly used in open source projects, has likely contributed strongly to this success. Code review is an emotionally loaded practice, with public exposure of reputation and ample opportunities for conflict. We set off to ask why code review works for open source communities, despite this inherent challenge. We interviewed 21 open source contributors from four communities and participated in meetings of ROS community devoted to implementation of the code review process. It appears that the hacker ethic is a key reason behind the success of code review in FOSS communities. It is built around the ethic of passion and the ethic of caring. Furthermore, we observed that tasks of code review are performed with strong intrinsic motivation, supported by many non-material extrinsic motivation mechanisms, such as desire to learn, to grow reputation, or to improve one's positioning on the job market. In the paper, we describe the study design, analyze the collected data and formulate 20 proposals for how what we know about hacker ethics and human and social aspects of code review, could be exploited to improve the effectiveness of the practice in software projects.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812037,"Open Source, Code Review, Motivation",Interviews;Software;Standards;Ethics;Guidelines;Robot kinematics,human factors;public domain software;software engineering,open source projects;code review process;code review work;open source software communities;peer code review;open source contributors;ROS community;FOSS communities;nonmaterial extrinsic motivation mechanisms,26
266,Not Mentioned,Distance-Based Sampling of Software Configuration Spaces,C. Kaltenecker; A. Grebhahn; N. Siegmund; J. Guo; S. Apel,"University of Passau, Germany; University of Passau, Germany; University of Weimar, Germany; Alibaba Group, China; University of Passau, Germany",2019,"Configurable software systems provide a multitude of configuration options to adjust and optimize their functional and non-functional properties. For instance, to find the fastest configuration for a given setting, a brute-force strategy measures the performance of all configurations, which is typically intractable. Addressing this challenge, state-of-the-art strategies rely on machine learning, analyzing only a few configurations (i.e., a sample set) to predict the performance of other configurations. However, to obtain accurate performance predictions, a representative sample set of configurations is required. Addressing this task, different sampling strategies have been proposed, which come with different advantages (e.g., covering the configuration space systematically) and disadvantages (e.g., the need to enumerate all configurations). In our experiments, we found that most sampling strategies do not achieve a good coverage of the configuration space with respect to covering relevant performance values. That is, they miss important configurations with distinct performance behavior. Based on this observation, we devise a new sampling strategy, called distance-based sampling, that is based on a distance metric and a probability distribution to spread the configurations of the sample set according to a given probability distribution across the configuration space. This way, we cover different kinds of interactions among configuration options in the sample set. To demonstrate the merits of distance-based sampling, we compare it to state-of-the-art sampling strategies, such as t-wise sampling, on 10 real-world configurable software systems. Our results show that distance-based sampling leads to more accurate performance models for medium to large sample sets.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812049,Distance Based Sampling;Configuration Sampling;Configurable Systems;Performance Modeling,Software systems;Sociology;Statistics;Probability distribution;Task analysis;Performance evaluation,learning (artificial intelligence);sampling methods;software maintenance;software performance evaluation;statistical distributions,sampling strategy;configuration options;t-wise sampling;real-world configurable software systems;software configuration;brute-force strategy;representative sample set;distance-based sampling;sampling strategies;software configuration space;probability distribution,43
267,Not Mentioned,DeepPerf: Performance Prediction for Configurable Software with Deep Sparse Neural Network,H. Ha; H. Zhang,"The University of Newcastle Callaghan, Australia; The University of Newcastle Callaghan, Australia",2019,"Many software systems provide users with a set of configuration options and different configurations may lead to different runtime performance of the system. As the combination of configurations could be exponential, it is difficult to exhaustively deploy and measure system performance under all possible configurations. Recently, several learning methods have been proposed to build a performance prediction model based on performance data collected from a small sample of configurations, and then use the model to predict system performance under a new configuration. In this paper, we propose a novel approach to model highly configurable software system using a deep feedforward neural network (FNN) combined with a sparsity regularization technique, e.g. the L1 regularization. Besides, we also design a practical search strategy for automatically tuning the network hyperparameters efficiently. Our method, called DeepPerf, can predict performance values of highly configurable software systems with binary and/or numeric configuration options at much higher prediction accuracy with less training data than the state-of-the art approaches. Experimental results on eleven public real-world datasets confirm the effectiveness of our approach.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811988,"software performance prediction, deep sparse feedforward neural network, highly configurable systems, sparsity regularization",Software systems;Predictive models;Software performance;Tuning;System performance;Biological neural networks;Data models,feedforward neural nets;learning (artificial intelligence),deep feedforward neural network;network hyperparameters;performance values;binary configuration options;numeric configuration options;deep sparse neural network;system performance;performance prediction model;performance data;configurable software system;configurable software systems;runtime performance,42
268,Not Mentioned,GreenBundle: An Empirical Study on the Energy Impact of Bundled Processing,S. A. Chowdhury; A. Hindle; R. Kazman; T. Shuto; K. Matsui; Y. Kamei,"Department of Computing Science, University of Alberta, Edmonton, AB, Canada; Department of Computing Science, University of Alberta, Edmonton, AB, Canada; Information Technology Management, University of Hawaii, Honolulu, HI, USA; Information Sc. & Electrical Eng., Kyushu University, Fukuoka, Japan; Information Sc. & Electrical Eng., Kyushu University, Fukuoka, Japan; Information Sc. & Electrical Eng., Kyushu University, Fukuoka, Japan",2019,"Energy consumption is a concern in the data-center and at the edge, on mobile devices such as smartphones. Software that consumes too much energy threatens the utility of the end-user's mobile device. Energy consumption is fundamentally a systemic kind of performance and hence it should be addressed at design time via a software architecture that supports it, rather than after release, via some form of refactoring. Unfortunately developers often lack knowledge of what kinds of designs and architectures can help address software energy consumption. In this paper we show that some simple design choices can have significant effects on energy consumption. In particular we examine the Model-View-Controller architectural pattern and demonstrate how converting to Model-View-Presenter with bundling can improve the energy performance of both benchmark systems and real world applications. We show the relationship between energy consumption and bundled and delayed view updates: bundling events in the presenter can often reduce energy consumption by 30%.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811956,"software energy consumption, MVP, MVC, software architecture",Energy consumption;Benchmark testing;Computer architecture;Unified modeling language;Observers;Software;Mobile handsets,energy consumption;mobile computing;power aware computing;software architecture;software maintenance,energy impact;software energy consumption;energy performance;model-view-presenter;model-view-controller architectural pattern,9
269,Not Mentioned,Search-Based Energy Testing of Android,R. Jabbarvand; J. -W. Lin; S. Malek,"School of Information and Computer Sciences, University of California, Irvine, USA; School of Information and Computer Sciences, University of California, Irvine, USA; School of Information and Computer Sciences, University of California, Irvine, USA",2019,"The utility of a smartphone is limited by its battery capacity and the ability of its hardware and software to efficiently use the device's battery. To properly characterize the energy consumption of an app and identify energy defects, it is critical that apps are properly tested, i.e., analyzed dynamically to assess the app's energy properties. However, currently there is a lack of testing tools for evaluating the energy properties of apps. We present COBWEB, a search-based energy testing technique for Android. By leveraging a set of novel models, representing both the functional behavior of an app as well as the contextual conditions affecting the app's energy behavior, COBWEB generates a test suite that can effectively find energy defects. Our experimental results using real-world apps demonstrate not only its ability to effectively and efficiently test energy behavior of apps, but also its superiority over prior techniques by finding a wider and more diverse set of energy defects.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812097,Energy Testing;Android;Software Testing,Testing;Hardware;Global Positioning System;Graphical user interfaces;Smart phones;Receivers;Batteries,Android (operating system);mobile computing;program testing;search problems;smart phones,battery capacity;energy consumption;energy defects;energy properties;search-based energy testing technique;test suite;energy behavior;Android;COBWEB,17
271,Not Mentioned,Global Optimization of Numerical Programs Via Prioritized Stochastic Algebraic Transformations,X. Wang; H. Wang; Z. Su; E. Tang; X. Chen; W. Shen; Z. Chen; L. Wang; X. Zhang; X. Li,"State Key Laboratory of Novel Software Technology, Software Institute of Nanjing University, Nanjing, China; State Key Laboratory of Novel Software Technology, Software Institute of Nanjing University, Nanjing, China; Department of Computer Science, University of California, Davis, USA; State Key Laboratory of Novel Software Technology, Software Institute of Nanjing University, Nanjing, China; State Key Laboratory of Novel Software Technology, Software Institute of Nanjing University, Nanjing, China; State Key Laboratory of Novel Software Technology, Software Institute of Nanjing University, Nanjing, China; State Key Laboratory of Novel Software Technology, Software Institute of Nanjing University, Nanjing, China; State Key Laboratory of Novel Software Technology, Software Institute of Nanjing University, Nanjing, China; State Key Laboratory of Novel Software Technology, Software Institute of Nanjing University, Nanjing, China; State Key Laboratory of Novel Software Technology, Software Institute of Nanjing University, Nanjing, China",2019,"Numerical code is often applied in the safety-critical, but resource-limited areas. Hence, it is crucial for it to be correct and efficient, both of which are difficult to ensure. On one hand, accumulated rounding errors in numerical programs can cause system failures. On the other hand, arbitrary/infinite-precision arithmetic, although accurate, is infeasible in practice and especially in resource-limited scenarios because it performs thousands of times slower than floating-point arithmetic. Thus, it has been a significant challenge to obtain high-precision, easy-to-maintain, and efficient numerical code. This paper introduces a novel global optimization framework to tackle this challenge. Using our framework, a developer simply writes the infinite-precision numerical program directly following the problem's mathematical requirement specification. The resulting code is correct and easy-to-maintain, but inefficient. Our framework then optimizes the program in a global fashion (i.e., considering the whole program, rather than individual expressions or statements as in prior work), the key technical difficulty this work solves. To this end, it analyzes the program's numerical value flows across different statements through a symbolic trace extraction algorithm, and generates optimized traces via stochastic algebraic transformations guided by effective rule selection. We first evaluate our technique on numerical benchmarks from the literature; results show that our global optimization achieves significantly higher worst-case accuracy than the state-of-the-art numerical optimization tool. Second, we show that our framework is also effective on benchmarks having complicated program structures, which are challenging for numerical optimization. Finally, we apply our framework on real-world code to successfully detect numerical bugs that have been confirmed by developers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812093,program optimization;numerical analysis;program transformation,Optimization;Software;Benchmark testing;Tools;Measurement;Computer science;Computer bugs,algebra;optimisation,resource-limited scenarios;floating-point arithmetic;infinite-precision numerical program;resulting code;global fashion;numerical value;optimized traces;numerical benchmarks;program structures;real-world code;numerical bugs;numerical programs;prioritized stochastic algebraic transformations;safety-critical;resource-limited areas;accumulated rounding errors;numerical code;global optimization framework;numerical optimization tool,4
272,Not Mentioned,Type Migration in Ultra-Large-Scale Codebases,A. Ketkar; A. Mesbah; D. Mazinanian; D. Dig; E. Aftandilian,Oregon State University; University of British Columbia; University of British Columbia; Oregon State University; Google Inc.,2019,"Type migration is a refactoring activity in which an existing type is replaced with another one throughout the source code. Manually performing type migration is tedious as programmers need to find all instances of the type to be migrated, along with its dependencies that propagate over assignment operations, method hierarchies, and subtypes. Existing automated approaches for type migration are not adequate for ultra-large-codebases - they perform an intensive whole-program analysis that does not scale. If we could represent the type structure of the program as graphs, then we could employ a MAPREDUCE parallel and distributed process that scales to hundreds of millions of LOC. We implemented this approach as an IDE-independent tool called T2R, which integrates with most build systems. We evaluated T2R's accuracy, usefulness and scalability on seven open source projects and one proprietary codebase of 300M LOC. T2R generated 130 type migration patches, of which the original developers accepted 98%.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812061,Refactoring;Type Migration;MapReduce,Java;Google;Open source software;Tools;Scalability;Task analysis;Complexity theory,data mining;parallel processing;program diagnostics;public domain software;software maintenance,ultra-large-scale codebases;type migration patches;refactoring activity;whole-program analysis;MAPREDUCE parallel and distributed process;IDE-independent tool;T2R tool,15
273,Not Mentioned,Dynamic Slicing for Android,T. Azim; A. Alavi; I. Neamtiu; R. Gupta,"University of California, Riverside, USA; University of California, Riverside, USA; New Jersey Institute of Technology; University of California, Riverside, USA",2019,"Dynamic program slicing is useful for a variety of tasks, from testing to debugging to security. Prior slicing approaches have targeted traditional desktop/server platforms, rather than mobile platforms such as Android. Slicing mobile, event-based systems is challenging due to their asynchronous callback construction and the IPC (interprocess communication)- heavy, sensor-driven, timing-sensitive nature of the platform. To address these problems, we introduce AndroidSlicer1, the first slicing approach for Android. AndroidSlicer combines a novel asynchronous slicing approach for modeling data and control dependences in the presence of callbacks with lightweight and precise instrumentation; this allows slicing for apps running on actual phones, and without requiring the app's source code. Our slicer is capable of handling a wide array of inputs that Android supports without adding any noticeable overhead. Experiments on 60 apps from Google Play show that AndroidSlicer is effective (reducing the number of instructions to be examined to 0.3% of executed instructions) and efficient (app instrumentation and post-processing combined takes 31 seconds); all while imposing a runtime overhead of just 4%. We present three applications of AndroidSlicer that are particularly relevant in the mobile domain: (1) finding and tracking input parts responsible for an error/crash, (2) fault localization, i.e., finding the instructions responsible for an error/crash, and (3) reducing the regression test suite. Experiments with these applications on an additional set of 18 popular apps indicate that AndroidSlicer is effective for Android testing and debugging.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811953,"Mobile apps, Android, Dynamic analysis",Smart phones;Registers;Computer crashes;Testing;Runtime;Debugging;Pins,Android (operating system);mobile computing;program debugging;program slicing;program testing,timing-sensitive nature;AndroidSlicer1;modeling data;control dependences;lightweight instrumentation;precise instrumentation;executed instructions;mobile domain;tracking input parts;regression test suite;Android testing;debugging;dynamic program slicing;mobile platforms;mobile event-based systems;asynchronous callback construction;IPC;interprocess communication;asynchronous slicing approach,7
274,Not Mentioned,Recovering Variable Names for Minified Code with Usage Contexts,H. Tran; N. Tran; S. Nguyen; H. Nguyen; T. N. Nguyen,"Computer Science Department, The University of Texas at Dallas, USA; Computer Science Department, The University of Texas at Dallas, USA; Computer Science Department, The University of Texas at Dallas, USA; Computer Science Department, Iowa State University, USA; Computer Science Department, The University of Texas at Dallas, USA",2019,"To avoid the exposure of original source code in a Web application, the variable names in JS code deployed in the wild are often replaced by short, meaningless names, thus making the code extremely difficult to manually understand and analysis. This paper presents JSNeat, an information retrieval (IR)-based approach to recover the variable names in minified JS code. JSNeat follows a data-driven approach to recover names by searching for them in a large corpus of open-source JS code. We use three types of contexts to match a variable in given minified code against the corpus including the context of the properties and roles of the variable, the context of that variable and relations with other variables under recovery, and the context of the task of the function to which the variable contributes. We performed several empirical experiments to evaluate JSNeat on the dataset of more than 322K JS files with 1M functions, and 3.5M variables with 176K unique variable names. We found that JSNeat achieves a high accuracy of 69.1%, which is the relative improvements of 66.1% and 43% over two state-of-the-art approaches JSNice and JSNaughty, respectively. The time to recover for a file or a variable with JSNeat is twice as fast as with JSNice and 4x as fast as with JNaughty, respectively.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812034,"Minified JS Code, Variable Name Recovery, Naturalness of Code, Usage Contexts.",Task analysis;Reactive power;Information retrieval;Tools;Databases;Static VAr compensators;Computer science,authoring languages;information retrieval;Internet;Java;public domain software;source code (software),usage contexts;JSNeat;information retrieval-based approach;minified JS code;open-source JS code;variable relations;variable contributes;data-driven approach;Web application;variable names;minified code;source code,6
275,Not Mentioned,"Gigahorse: Thorough, Declarative Decompilation of Smart Contracts",N. Grech; L. Brent; B. Scholz; Y. Smaragdakis,"University of Athens and University of Malta, Greece, Malta; The University of Sydney, Australia; The University of Sydney, Australia; University of Athens, Greece",2019,"The rise of smart contracts - autonomous applications running on blockchains - has led to a growing number of threats, necessitating sophisticated program analysis. However, smart contracts, which transact valuable tokens and cryptocurrencies, are compiled to very low-level bytecode. This bytecode is the ultimate semantics and means of enforcement of the contract. We present the Gigahorse toolchain. At its core is a reverse compiler (i.e., a decompiler) that decompiles smart contracts from Ethereum Virtual Machine (EVM) bytecode into a highlevel 3-address code representation. The new intermediate representation of smart contracts makes implicit data- and control-flow dependencies of the EVM bytecode explicit. Decompilation obviates the need for a contract's source and allows the analysis of both new and deployed contracts. Gigahorse advances the state of the art on several fronts. It gives the highest analysis precision and completeness among decompilers for Ethereum smart contracts - e.g., Gigahorse can decompile over 99.98% of deployed contracts, compared to 88% for the recently-published Vandal decompiler and under 50% for the state-of-the-practice Porosity decompiler. Importantly, Gigahorse offers a full-featured toolchain for further analyses (and a â€œbatteries includedâ€ approach, with multiple clients already implemented), together with the highest performance and scalability. Key to these improvements is Gigahorse's use of a declarative, logic-based specification, which allows high-level insights to inform low-level decompilation.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811905,Ethereum;Blockchain;Decompilation;Program Analysis;Security,Smart contracts;Blockchain;Virtual machining;Task analysis;Java;Security,contracts;cryptocurrencies;distributed databases;formal specification;program compilers;program diagnostics;virtual machines,Gigahorse;low-level decompilation;program analysis;blockchain;logic-based specification;cryptocurrencies;Ethereum virtual machine bytecode;EVM bytecode;Porosity decompiler;Vandal decompiler;Ethereum smart contracts,37
276,Not Mentioned,Probabilistic Disassembly,K. Miller; Y. Kwon; Y. Sun; Z. Zhang; X. Zhang; Z. Lin,"Department of Computer Science, Purdue University, West Lafayette, USA; Department of Computer Science, University of Virginia, Charlottesville, USA; Department of Computer Science, Purdue University, West Lafayette, USA; Department of Computer Science, Purdue University, West Lafayette, USA; Department of Computer Science, Purdue University, West Lafayette, USA; Department of Computer Science and Engineering, Ohio State University, Columbus, USA",2019,"Disassembling stripped binaries is a prominent challenge for binary analysis, due to the interleaving of code segments and data, and the difficulties of resolving control transfer targets of indirect calls and jumps. As a result, most existing disassemblers have both false positives (FP) and false negatives (FN). We observe that uncertainty is inevitable in disassembly due to the information loss during compilation and code generation. Therefore, we propose to model such uncertainty using probabilities and propose a novel disassembly technique, which computes a probability for each address in the code space, indicating its likelihood of being a true positive instruction. The probability is computed from a set of features that are reachable to an address, including control flow and data flow features. Our experiments with more than two thousands binaries show that our technique does not have any FN and has only 3.7% FP. In comparison, a state-of-the-art superset disassembly technique has 85% FP. A rewriter built on our disassembly can generate binaries that are only half of the size of those by superset disassembly and run 3% faster. While many widely-used disassemblers such as IDA and BAP suffer from missing function entries, our experiment also shows that even without any function entry information, our disassembler can still achieve 0 FN and 6.8% FP.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812038,binary;disassembly;binary rewrite;probabilistic disassembly,Probabilistic logic;Uncertainty;Runtime;Registers;Computer science;Instruments;Aggregates,design for disassembly;probability;program assemblers;program compilers;program diagnostics,disassembling stripped binaries;binary analysis;code segments;control transfer targets;indirect calls;false negatives;code generation;code space;control flow;data flow features;disassembler;probabilistic disassembly;superset disassembly technique;compilation;true positive instruction;probability,16
277,Not Mentioned,Software Documentation Issues Unveiled,E. Aghajani; C. Nagy; O. L. Vega-MÃ¡rquez; M. Linares-VÃ¡squez; L. Moreno; G. Bavota; M. Lanza,"Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland; Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland; Systems and Computing Engineering Department, Universidad de los Andes, Colombia; Systems and Computing Engineering Department, Universidad de los Andes, Colombia; Department of Computer Science, Colorado State University, USA; Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland; Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland",2019,"(Good) Software documentation provides developers and users with a description of what a software system does, how it operates, and how it should be used. For example, technical documentation (e.g., an API reference guide) aids developers during evolution/maintenance activities, while a user manual explains how users are to interact with a system. Despite its intrinsic value, the creation and the maintenance of documentation is often neglected, negatively impacting its quality and usefulness, ultimately leading to a generally unfavourable take on documentation. Previous studies investigating documentation issues have been based on surveying developers, which naturally leads to a somewhat biased view of problems affecting documentation. We present a large scale empirical study, where we mined, analyzed, and categorized 878 documentation-related artifacts stemming from four different sources, namely mailing lists, Stack Overflow discussions, issue repositories, and pull requests. The result is a detailed taxonomy of documentation issues from which we infer a series of actionable proposals both for researchers and practitioners.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811931,"Documentation, Empirical Study",Documentation;Software;Tools;Maintenance engineering;Taxonomy;Interviews;Information services,application program interfaces;data mining;document handling;software maintenance;system documentation;Web sites,issue repositories;software system;API reference guide;user manual;software documentation issues;documentation-related artifacts;mailing lists;Stack Overflow discussions;pull requests,64
279,Not Mentioned,"9.6 Million Links in Source Code Comments: Purpose, Evolution, and Decay",H. Hata; C. Treude; R. G. Kula; T. Ishio,Nara Institute of Science and Technology; University of Adelaide; Nara Institute of Science and Technology; Nara Institute of Science and Technology,2019,"Links are an essential feature of the World Wide Web, and source code repositories are no exception. However, despite their many undisputed benefits, links can suffer from decay, insufficient versioning, and lack of bidirectional traceability. In this paper, we investigate the role of links contained in source code comments from these perspectives. We conducted a large-scale study of around 9.6 million links to establish their prevalence, and we used a mixed-methods approach to identify the links' targets, purposes, decay, and evolutionary aspects. We found that links are prevalent in source code repositories, that licenses, software homepages, and specifications are common types of link targets, and that links are often included to provide metadata or attribution. Links are rarely updated, but many link targets evolve. Almost 10% of the links included in source code comments are dead. We then submitted a batch of link-fixing pull requests to open source software repositories, resulting in most of our fixes being merged successfully. Our findings indicate that links in source code comments can indeed be fragile, and our work opens up avenues for future work to address these problems.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811933,code comment;link decay;knowledge sharing,C++ languages;Java;Licenses;Documentation;Python;Web sites,Internet;public domain software;software maintenance,source code repositories;link targets;source code comments;link-fixing pull requests;source software repositories;World Wide Web;mixed-methods approach,40
280,Not Mentioned,Leveraging Artifact Trees to Evolve and Reuse Safety Cases,A. Agrawal; S. Khoshmanesh; M. Vierhauser; M. Rahimi; J. Cleland-Huang; R. Lutz,"Dept of Computer Science and Eng., University of Notre Dame, South Bend, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA; Dept of Computer Science and Eng., University of Notre Dame, South Bend, USA; Dept. of Computer Science, Northern Illinois University, Chicago, IL, USA; Dept of Computer Science and Eng., University of Notre Dame, South Bend, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA",2019,"Safety Assurance Cases (SACs) are increasingly used to guide and evaluate the safety of software-intensive systems. They are used to construct a hierarchically organized set of claims, arguments, and evidence in order to provide a structured argument that a system is safe for use. However, as the system evolves and grows in size, a SAC can be difficult to maintain. In this paper we utilize design science to develop a novel solution for identifying areas of a SAC that are affected by changes to the system. Moreover, we generate actionable recommendations for updating the SAC, including its underlying artifacts and trace links, in order to evolve an existing safety case for use in a new version of the system. Our approach, Safety Artifact Forest Analysis (SAFA), leverages traceability to automatically compare software artifacts from a previously approved or certified version with a new version of the system. We identify, visualize, and explain changes in a Delta Tree. We evaluate our approach using the Dronology system for monitoring and coordinating the actions of cooperating, small Unmanned Aerial Vehicles. Results from a user study show that SAFA helped users to identify changes that potentially impacted system safety and provided information that could be used to help maintain and evolve a SAC.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812137,"Change Impact, Safety Assurance Cases, Evolution, Traceability",Vegetation;Hazards;Monitoring;Thermostats;Computer science;Forestry,safety-critical software;software reliability;trees (mathematics),artifact trees;software-intensive systems;structured argument;trace links;software artifacts;safety assurance cases;dronology system;safety artifact forest analysis,8
281,Not Mentioned,Detecting Incorrect Build Rules,N. Licker; A. Rice,"Department of Computer Science and Technology, University of Cambridge, Cambridge, UK; Department of Computer Science and Technology, University of Cambridge, Cambridge, UK",2019,"Automated build systems are routinely used by software engineers to minimize the number of objects that need to be recompiled after incremental changes to the source files of a project. In order to achieve efficient and correct builds, developers must provide the build tools with dependency information between the files and modules of a project, usually expressed in a macro language specific to each build tool. In order to guarantee correctness, the authors of these tools are responsible for enumerating all the files whose contents an output depends on. Unfortunately, this is a tedious process and not all dependencies are captured in practice, which leads to incorrect builds. We automatically uncover such missing dependencies through a novel method that we call build fuzzing. The correctness of build definitions is verified by modifying files in a project, triggering incremental builds and comparing the set of changed files to the set of expected changes. These sets are determined using a dependency graph inferred by tracing the system calls executed during a clean build. We evaluate our method by exhaustively testing build rules of open-source projects, uncovering issues leading to race conditions and faulty builds in 31 of them. We provide a discussion of the bugs we detect, identifying anti-patterns in the use of the macro languages. We fix some of the issues in projects where the features of build systems allow a clean solution.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812082,build tools;exhaustive testing;verification,Tools;Generators;Computer bugs;Linux;C++ languages;Open source software;Kernel,graph theory;program debugging;program testing;public domain software;software quality,build definitions;changed files;dependency graph;open-source projects;software engineers;dependency information,8
282,Not Mentioned,Adversarial Sample Detection for Deep Neural Network through Model Mutation Testing,J. Wang; G. Dong; J. Sun; X. Wang; P. Zhang,"Shenzhen University, Singapore U. of Tech. and Design; Zhejiang University; Singapore U. of Tech. and Design; Zhejiang University; Zhejiang University",2019,"Deep neural networks (DNN) have been shown to be useful in a wide range of applications. However, they are also known to be vulnerable to adversarial samples. By transforming a normal sample with some carefully crafted human imperceptible perturbations, even highly accurate DNN make wrong decisions. Multiple defense mechanisms have been proposed which aim to hinder the generation of such adversarial samples. However, a recent work show that most of them are ineffective. In this work, we propose an alternative approach to detect adversarial samples at runtime. Our main observation is that adversarial samples are much more sensitive than normal samples if we impose random mutations on the DNN. We thus first propose a measure of 'sensitivity' and show empirically that normal samples and adversarial samples have distinguishable sensitivity. We then integrate statistical hypothesis testing and model mutation testing to check whether an input sample is likely to be normal or adversarial at runtime by measuring its sensitivity. We evaluated our approach on the MNIST and CIFAR10 datasets. The results show that our approach detects adversarial samples generated by state-of-the-art attacking methods efficiently and accurately.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812047,adversarial sample;detection;deep neural network;mutation testing;sensitivity,Testing;Perturbation methods;Sensitivity;Runtime;Training;Biological neural networks,learning (artificial intelligence);neural nets;program testing;security of data;software fault tolerance;statistical analysis,adversarial sample detection;deep neural network;adversarial samples;normal sample;input sample;model mutation testing;DNN;MNIST dataset;CIFAR10 dataset;sensitivity measure,96
283,Not Mentioned,Deep Differential Testing of JVM Implementations,Y. Chen; T. Su; Z. Su,"Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Nanyang Technological University, Singapore; ETH Zurich, Switzerland",2019,"The Java Virtual Machine (JVM) is the cornerstone of the widely-used Java platform. Thus, it is critical to ensure the reliability and robustness of popular JVM implementations. However, little research exists on validating production JVMs. One notable effort is classfuzz, which mutates Java bytecode syntactically to stress-test different JVMs. It is shown that classfuzz mainly produces illegal bytecode files and uncovers defects in JVMs' startup processes. It remains a challenge to effectively test JVMs' bytecode verifiers and execution engines to expose deeper bugs. This paper tackles this challenge by introducing classming, a novel, effective approach to performing deep, differential JVM testing. The key of classming is a technique, live bytecode mutation, to generate, from a seed bytecode file f, likely valid, executable (live) bytecode files: (1) capture the seed f's live bytecode, the sequence of its executed bytecode instructions; (2) repeatedly manipulate the control- and data-flow in f's live bytecode to generate semantically different mutants; and (3) selectively accept the generated mutants to steer the mutation process toward live, diverse mutants. The generated mutants are then employed to differentially test JVMs. We have evaluated classming on mainstream JVM implementations, including OpenJDK's HotSpot and IBM's J9, by mutating the DaCapo benchmarks. Our results show that classming is very effective in uncovering deep JVM differences. More than 1,800 of the generated classes exposed JVM differences, and more than 30 triggered JVM crashes. We analyzed and reported the JVM runtime differences and crashes, of which 14 have already been confirmed/fixed, including a highly critical security vulnerability in J9 that allowed untrusted code to disable the security manager and elevate its privileges (CVE-2017-1376).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811957,Differential JVM testing;live bytecode mutation;semantically different mutants,Monitoring;Testing;Java;Engines;Security;Computer crashes;Runtime,Java;program testing;program verification;security of data;source code (software);virtual machines,Java Virtual Machine;Java platform;classfuzz;classming;differential JVM testing;live bytecode mutation;mutation process;deep JVM differences;JVM crashes;bytecode instructions;deep differential testing;OpenJDK HotSpot;IBM J9;DaCapo benchmarks;deeper bugs,28
287,Not Mentioned,Learning to Spot and Refactor Inconsistent Method Names,K. Liu; D. Kim; T. F. BissyandÃ©; T. Kim; K. Kim; A. Koyuncu; S. Kim; Y. Le Traon,"Interdisciplinary Centre for Security, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, University of Luxembourg, Luxembourg; Department of Software Engineering, Chonbuk National University, South Korea; Interdisciplinary Centre for Security, University of Luxembourg, Luxembourg; Interdisciplinary Centre for Security, University of Luxembourg, Luxembourg; Department of Software Engineering, Chonbuk National University, South Korea; Interdisciplinary Centre for Security, University of Luxembourg, Luxembourg",2019,"To ensure code readability and facilitate software maintenance, program methods must be named properly. In particular, method names must be consistent with the corresponding method implementations. Debugging method names remains an important topic in the literature, where various approaches analyze commonalities among method names in a large dataset to detect inconsistent method names and suggest better ones. We note that the state-of-the-art does not analyze the implemented code itself to assess consistency. We thus propose a novel automated approach to debugging method names based on the analysis of consistency between method names and method code. The approach leverages deep feature representation techniques adapted to the nature of each artifact. Experimental results on over 2.1 million Java methods show that we can achieve up to 15 percentage points improvement over the state-of-the-art, establishing a record performance of 67.9% F1- measure in identifying inconsistent method names. We further demonstrate that our approach yields up to 25% accuracy in suggesting full names, while the state-of-the-art lags far behind at 1.1% accuracy. Finally, we report on our success in fixing 66 inconsistent method names in a live study on projects in the wild.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812134,"Code refactoring, inconsistent method names, deep neural networks, code embedding",Feature extraction;Semantics;Neural networks;Training;Computational modeling;Debugging;Software,Java;program debugging;software maintenance,refactor inconsistent method names;program methods;debugging method names;method code;Java methods;code readability;software maintenance,58
288,Not Mentioned,Harnessing Evolution for Multi-Hunk Program Repair,S. Saha; R. k. Saha; M. r. Prasad,"University of California, Santa Barbara; Fujitsu Laboratories of America, Inc.; Fujitsu Laboratories of America, Inc.",2019,"Despite significant advances in automatic program repair (APR) techniques over the past decade, practical deployment remains an elusive goal. One of the important challenges in this regard is the general inability of current APR techniques to produce patches that require edits in multiple locations, i.e., multi-hunk patches. In this work, we present a novel APR technique that generalizes single-hunk repair techniques to include an important class of multi-hunk bugs, namely bugs that may require applying a substantially similar patch at a number of locations. We term such sets of repair locations as evolutionary siblings - similar looking code, instantiated in similar contexts, that are expected to undergo similar changes. At the heart of our proposed method is an analysis to accurately identify a set of evolutionary siblings, for a given bug. This analysis leverages three distinct sources of information, namely the test-suite spectrum, a novel code similarity analysis, and the revision history of the project. The discovered siblings are then simultaneously repaired in a similar fashion. We instantiate this technique in a tool called HERCULES and demonstrate that it is able to correctly fix 46 bugs in the Defects4J dataset, the highest of any individual APR technique to date. This includes 15 multi-hunk bugs and overall 11 bugs which have not been fixed by any other technique so far.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812131,"automatic program repair, multi-hunk patches, code similarity",Maintenance engineering;Computer bugs;Cloning;Tools;History;Search problems;Syntactics,program debugging;program testing,multihunk program repair;automatic program repair techniques;multihunk patches;single-hunk repair techniques;repair locations;evolutionary siblings;code similarity analysis;APR technique;HERCULES tool;Defects4J dataset,47
289,Not Mentioned,On Learning Meaningful Code Changes Via Neural Machine Translation,M. Tufano; J. Pantiuchina; C. Watson; G. Bavota; D. Poshyvanyk,"College of William and Mary, Williamsburg, Virginia, USA; Universita della Svizzera italiana (USI), Lugano, Switzerland; College of William and Mary, Williamsburg, Virginia, USA; Universita della Svizzera italiana (USI), Lugano, Switzerland; College of William and Mary, Williamsburg, Virginia, USA",2019,"Recent years have seen the rise of Deep Learning (DL) techniques applied to source code. Researchers have exploited DL to automate several development and maintenance tasks, such as writing commit messages, generating comments and detecting vulnerabilities among others. One of the long lasting dreams of applying DL to source code is the possibility to automate non-trivial coding activities. While some steps in this direction have been taken (e.g., learning how to fix bugs), there is still a glaring lack of empirical evidence on the types of code changes that can be learned and automatically applied by DL. Our goal is to make this first important step by quantitatively and qualitatively investigating the ability of a Neural Machine Translation (NMT) model to learn how to automatically apply code changes implemented by developers during pull requests. We train and experiment with the NMT model on a set of 236k pairs of code components before and after the implementation of the changes provided in the pull requests. We show that, when applied in a narrow enough context (i.e., small/medium-sized pairs of methods before/after the pull request changes), NMT can automatically replicate the changes implemented by developers during pull requests in up to 36% of the cases. Moreover, our qualitative analysis shows that the model is capable of learning and replicating a wide variety of meaningful code changes, especially refactorings and bug-fixing activities. Our results pave the way for novel research in the area of DL on code, such as the automatic learning and applications of refactoring.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811910,Neural-Machine Translation;Empirical Study,Vocabulary;Crawlers;Computer bugs;Java;Data mining;Software engineering;Task analysis,language translation;learning (artificial intelligence);program debugging;program testing;public domain software;software maintenance;statistical analysis,pull requests;pull request changes;bug-fixing activities;source code;maintenance tasks;nontrivial coding activities;NMT model;code components;code changes;vulnerabilities detection;neural machine translation model;qualitative analysis,93
290,Not Mentioned,Natural Software Revisited,M. Rahman; D. Palani; P. C. Rigby,"Department of Computer Science and Software Engineering, Concordia University, MontrÃ©al, QuÃ©bec, Canada; Department of Computer Science and Software Engineering, Concordia University, MontrÃ©al, QuÃ©bec, Canada; Department of Computer Science and Software Engineering, Concordia University, MontrÃ©al, QuÃ©bec, Canada",2019,"Recent works have concluded that software code is more repetitive and predictable, i.e. more natural, than English texts. On re-examination, we find that much of the apparent ""naturalness"" of source code is due to the presence of language specific syntax, especially separators, such as semi-colons and brackets. For example, separators account for 44% of all tokens in our Java corpus. When we follow the NLP practices of eliminating punctuation (e.g., separators) and stopwords (e.g., keywords), we find that code is still repetitive and predictable, but to a lesser degree than previously thought. We suggest that SyntaxTokens be filtered to reduce noise in code recommenders. Unlike the code written for a particular project, API code usage is similar across projects: a file is opened and closed in the same manner regardless of domain. When we restrict our n-grams to those contained in the Java API, we find that API usages are highly repetitive. Since API calls are common across programs, researchers have made reliable statistical models to recommend sophisticated API call sequences. Sequential n-gram models were developed for natural languages. Code is usually represented by an AST which contains control and data flow, making n-grams models a poor representation of code. Comparing n-grams to statistical graph representations of the same codebase, we find that graphs are more repetitive and contain higherlevel patterns than n-grams. We suggest that future work focus on statistical code graphs models that accurately capture complex coding patterns. Our replication package makes our scripts and data available to future researchers[1].",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811940,Basic Science;Entropy;Language Models;Statistical Code Graphs;StackOverflow,Java;Mathematical model;Syntactics;Particle separators;Programming;Python,application program interfaces;graph theory;Java;natural language processing;source code (software);statistical analysis;text analysis,natural software;software code;English texts;re-examination;apparent naturalness;source code;language specific syntax;separators;brackets;Java corpus;NLP practices;code recommenders;API code usage;Java API;API usages;sophisticated API call sequences;sequential n-gram models;natural languages;statistical graph representations;statistical code graphs models;complex coding patterns,23
291,Not Mentioned,Towards Automating Precision Studies of Clone Detectors,V. Saini; F. Farmahinifarahani; Y. Lu; D. Yang; P. Martins; H. Sajnani; P. Baldi; C. V. Lopes,"University of California Irvine, USA; University of California Irvine, USA; University of California Irvine, USA; University of California Irvine, USA; University of California Irvine, USA; Microsoft, USA; University of California Irvine, USA; University of California Irvine, USA",2019,"Current research in clone detection suffers from poor ecosystems for evaluating precision of clone detection tools. Corpora of labeled clones are scarce and incomplete, making evaluation labor intensive and idiosyncratic, and limiting intertool comparison. Precision-assessment tools are simply lacking. We present a semiautomated approach to facilitate precision studies of clone detection tools. The approach merges automatic mechanisms of clone classification with manual validation of clone pairs. We demonstrate that the proposed automatic approach has a very high precision and it significantly reduces the number of clone pairs that need human validation during precision experiments. Moreover, we aggregate the individual effort of multiple teams into a single evolving dataset of labeled clone pairs, creating an important asset for software clone research.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811972,"Precision Evaluation, Clone Detection, Machine learning, Open source labeled datasets",Cloning;Tools;Detectors;Manuals;Inspection;Software;Aggregates,program diagnostics;software maintenance;software management;software tools,clone detection tools;precision-assessment tools;semiautomated approach;clone classification;precision experiments;labeled clone pairs;software clone research;clone detectors;labor intensive evaluation;human validation,6
292,Not Mentioned,LEOPARD: Identifying Vulnerable Code for Vulnerability Assessment Through Program Metrics,X. Du; B. Chen; Y. Li; J. Guo; Y. Zhou; Y. Liu; Y. Jiang,"School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; KLISS, Tsinghua University, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; College of Information Science, Zhejiang Sci-Tech University, China",2019,"Identifying potentially vulnerable locations in a code base is critical as a pre-step for effective vulnerability assessment; i.e., it can greatly help security experts put their time and effort to where it is needed most. Metric-based and pattern-based methods have been presented for identifying vulnerable code. The former relies on machine learning and cannot work well due to the severe imbalance between non-vulnerable and vulnerable code or lack of features to characterize vulnerabilities. The latter needs the prior knowledge of known vulnerabilities and can only identify similar but not new types of vulnerabilities. In this paper, we propose and implement a generic, lightweight and extensible framework, LEOPARD, to identify potentially vulnerable functions through program metrics. LEOPARD requires no prior knowledge about known vulnerabilities. It has two steps by combining two sets of systematically derived metrics. First, it uses complexity metrics to group the functions in a target application into a set of bins. Then, it uses vulnerability metrics to rank the functions in each bin and identifies the top ones as potentially vulnerable. Our experimental results on 11 real-world projects have demonstrated that, LEOPARD can cover 74.0% of vulnerable functions by identifying 20% of functions as vulnerable and outperform machine learning-based and static analysis-based techniques. We further propose three applications of LEOPARD for manual code review and fuzzing, through which we discovered 22 new bugs in real applications like PHP, radare2 and FFmpeg, and eight of them are new vulnerabilities.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812029,"Program Metric, Vulnerability, Fuzzing",Measurement;Complexity theory;Security;Computer bugs;Fuzzing;Machine learning;Manuals,learning (artificial intelligence);program diagnostics;security of data;software metrics,pattern-based methods;LEOPARD;program metrics;systematically derived metrics;complexity metrics;vulnerability metrics;manual code review;vulnerability assessment;metric-based method;vulnerable code identification;machine learning-based technique,25
293,Not Mentioned,SMOKE: Scalable Path-Sensitive Memory Leak Detection for Millions of Lines of Code,G. Fan; R. Wu; Q. Shi; X. Xiao; J. Zhou; C. Zhang,Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; Hong Kong University of Science and Technology; Sourcebrella Inc.; Sourcebrella Inc.; Hong Kong University of Science and Technology,2019,"Detecting memory leak at industrial scale is still not well addressed, in spite of the tremendous effort from both industry and academia in the past decades. Existing work suffers from an unresolved paradox - a highly precise analysis limits its scalability and an imprecise one seriously hurts its precision or recall. In this work, we present SMOKE, a staged approach to resolve this paradox. In the ?rst stage, instead of using a uniform precise analysis for all paths, we use a scalable but imprecise analysis to compute a succinct set of candidate memory leak paths. In the second stage, we leverage a more precise analysis to verify the feasibility of those candidates. The ?rst stage is scalable, due to the design of a new sparse program representation, the use-?ow graph (UFG), that models the problem as a polynomial-time state analysis. The second stage analysis is both precise and ef?cient, due to the smaller number of candidates and the design of a dedicated constraint solver. Experimental results show that SMOKE can ?nish checking industrial-sized projects, up to 8MLoC, in forty minutes with an average false positive rate of 24.4%. Besides, SMOKE is signi?cantly faster than the state-of-the-art research techniques as well as the industrial tools, with the speedup ranging from 5.2X to 22.8X. In the twenty-nine mature and extensively checked benchmark projects, SMOKE has discovered thirty previously unknown memory leaks which were con?rmed by developers, and one even assigned a CVE ID.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812075,"memory leak, static bug finding, use-flow graph, value-flow graph",Scalability;Leak detection;Benchmark testing;Computer bugs;Complexity theory;Correlation;Tools,data flow analysis;program debugging,SMOKE;sparse program representation;polynomial-time state analysis;path-sensitive memory leak detection;static bug finding;value-flow graph;use-flow graph,30
295,Not Mentioned,Reasonably-Most-General Clients for JavaScript Library Analysis,E. K. Kristensen; A. MÃ¸ller,Aarhus University; Aarhus University,2019,"A well-known approach to statically analyze libraries without having access to their client code is to model all possible clients abstractly using a most-general client. In dynamic languages, however, a most-general client would be too general: it may interact with the library in ways that are not intended by the library developer and are not realistic in actual clients, resulting in useless analysis results. In this work, we explore the concept of a reasonably-most-general client, in the context of a new static analysis tool REAGENT that aims to detect errors in TypeScript declaration files for JavaScript libraries. By incorporating different variations of reasonably-most-general clients into an existing static analyzer for JavaScript, we use REAGENT to study how different assumptions of client behavior affect the analysis results. We also show how REAGENT is able to find type errors in real-world TypeScript declaration files, and, once the errors have been corrected, to guarantee that no remaining errors exist relative to the selected assumptions.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812107,program analysis;TypeScript;most general client,Libraries;Tools;Static analysis;Semantics;Contracts;Analytical models;Testing,Java;program diagnostics,client behavior;JavaScript library analysis;client code;library developer;static analysis tool;REAGENT;reasonably-most-general clients,5
296,Not Mentioned,Resource-Aware Program Analysis Via Online Abstraction Coarsening,K. Heo; H. Oh; H. Yang,University of Pennsylvania; Korea University; KAIST,2019,"We present a new technique for developing a resource-aware program analysis. Such an analysis is aware of constraints on available physical resources, such as memory size, tracks its resource use, and adjusts its behaviors during fixpoint computation in order to meet the constraint and achieve high precision. Our resource-aware analysis adjusts behaviors by coarsening program abstraction, which usually makes the analysis consume less memory and time until completion. It does so multiple times during the analysis, under the direction of what we call a controller. The controller constantly intervenes in the fixpoint computation of the analysis and decides how much the analysis should coarsen the abstraction. We present an algorithm for learning a good controller automatically from benchmark programs. We applied our technique to a static analysis for C programs, where we control the degree of flow-sensitivity to meet a constraint on peak memory consumption. The experimental results with 18 real-world programs show that our algorithm can learn a good controller and the analysis with this controller meets the constraint and utilizes available memory effectively.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812143,static analysis;resource constraint;learning,Memory management;Static analysis;Reinforcement learning;Task analysis;Software engineering;Benchmark testing;Indexes,C language;program diagnostics;program verification,fixpoint computation;benchmark programs;static analysis;real-world programs;resource-aware program analysis;online abstraction coarsening;program abstraction;C programs;physical resources,11
297,Not Mentioned,Automated Reporting of Anti-Patterns and Decay in Continuous Integration,C. Vassallo; S. Proksch; H. C. Gall; M. Di Penta,"Department of Informatics, University of Zurich, Zurich, Switzerland; Department of Informatics, University of Zurich, Zurich, Switzerland; Department of Informatics, University of Zurich, Zurich, Switzerland; Department of Engineering, University of Sannio, Benevento, Italy",2019,"Continuous Integration (CI) is a widely-used software engineering practice. The software is continuously built so that changes can be easily integrated and issues such as unmet quality goals or style inconsistencies get detected early. Unfortunately, it is not only hard to introduce CI into an existing project, but it is also challenging to live up to the CI principles when facing tough deadlines or business decisions. Previous work has identified common anti-patterns that reduce the promised benefits of CI. Typically, these anti-patterns slowly creep into a project over time before they are identified. We argue that automated detection can help with early identification and prevent such a process decay. In this work, we further analyze this assumption and survey 124 developers about CI anti-patterns. From the results, we build CI-Odor, a reporting tool for CI processes that detects the existence of four relevant anti-patterns by analyzing regular build logs and repository information. In a study on the 18,474 build logs of 36 popular JAVA projects, we reveal the presence of 3,823 high-severity warnings spread across projects. We validate our reports in a survey among 13 original developers of these projects and through general feedback from 42 developers that confirm the relevance of our reports.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811921,"Continuous Integration, Anti-Pattern, Detection, CI-Smell, CI-Decay",Tools;Pipelines;Detectors;Software;Best practices;Merging;Informatics,Java;program testing;software engineering,continuous integration;software engineering practice;CI principles;common anti-patterns;process decay;CI anti-patterns;Java projects;CI-Odor tool,28
298,Not Mentioned,A System Identification Based Oracle for Control-CPS Software Fault Localization,Z. He; Y. Chen; E. Huang; Q. Wang; Y. Pei; H. Yuan,"Department of Computing, The Hong Kong Polytechnic University, Hong Kong, SAR, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, SAR, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, SAR, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, SAR, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, SAR, China; Dept. of Mechanical and Automation Engineering, The Chinese Univ. of Hong Kong, Hong Kong, SAR, China",2019,"Control-CPS software fault localization (SFL, aka bug localization) is of critical importance as bugs may cause major failures, even injuries/deaths. To locate the bugs in control-CPSs, SFL tools often demand many labeled (""correct""/""incorrect"") source code execution traces as inputs. To label the correctness of these traces, we must judge the corresponding control-CPS physical trajectories' correctness. However, unlike discrete outputs, the boundaries between correct and incorrect physical trajectories are often vague. The mechanism (aka oracle) to judge the physical trajectories' correctness thus becomes a major challenge. So far, the ad hoc practice of ``human oracles'' is still widely used, whose qualities heavily depend on the human experts' expertise and availability. This paper proposes an oracle based on the well adopted autoregressive system identification (AR-SI). With proven success for controlling black-box physical systems, AR-SI is adapted by us to identify the buggy control-CPS as a black-box. We use this identification result as an oracle to judge the control-CPS's behaviors, and propose a methodology to prepare traces for control-CPS debugging. Comprehensive evaluations on classic control-CPSs with injected real-life and artificial bugs show that our proposed approach significantly outperforms the human oracle approach in SFL accuracy (recall) and latency, and in oracle false positive/negative rates. Our approach also helps discover a new real-life bug in a consumer-grade control-CPS.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811955,Oracle;Cyber-Physical System;Debug;Testing,Trajectory;Computer bugs;Tools;Software;Emulation;Debugging,autoregressive processes;cyber-physical systems;program debugging;software fault tolerance;software tools;source code (software),buggy control-CPS;control-CPS debugging;control-CPS software fault localization;aka bug localization;aka oracle;human oracles;autoregressive system identification;black-box physical systems;consumer-grade control;SFL tools;source code execution;AR-SI,12
299,Not Mentioned,ReCDroid: Automatically Reproducing Android Application Crashes from Bug Reports,Y. Zhao; T. Yu; T. Su; Y. Liu; W. Zheng; J. Zhang; W. G.J. Halfond,"University of Kentucky, USA; University of Kentucky, USA; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Northwestern Polytechnical University, China; Northwestern Polytechnical University, China; University of Southern California, USA",2019,"The large demand of mobile devices creates significant concerns about the quality of mobile applications (apps). Developers heavily rely on bug reports in issue tracking systems to reproduce failures (e.g., crashes). However, the process of crash reproduction is often manually done by developers, making the resolution of bugs inefficient, especially that bug reports are often written in natural language. To improve the productivity of developers in resolving bug reports, in this paper, we introduce a novel approach, called ReCDroid, that can automatically reproduce crashes from bug reports for Android apps. ReCDroid uses a combination of natural language processing (NLP) and dynamic GUI exploration to synthesize event sequences with the goal of reproducing the reported crash. We have evaluated ReCDroid on 51 original bug reports from 33 Android apps. The results show that ReCDroid successfully reproduced 33 crashes (63.5% success rate) directly from the textual description of bug reports. A user study involving 12 participants demonstrates that ReCDroid can improve the productivity of developers when resolving crash bug reports.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811942,Bug reproduction;Android;Natural language processing,Computer bugs;Graphical user interfaces;Data mining;Natural language processing;Google,graphical user interfaces;mobile computing;natural language processing;program debugging;smart phones;software maintenance,reported crash;ReCDroid;bug reports;crash bug reports;Android application crash;Android apps;natural language processing;dynamic GUI exploration,35
300,Not Mentioned,Mining Historical Test Logs to Predict Bugs and Localize Faults in the Test Logs,A. Amar; P. C. Rigby,"Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada",2019,"Software testing is an integral part of modern software development. However, test runs can produce thousands of lines of logged output that make it difficult to find the cause of a fault in the logs. This problem is exacerbated by environmental failures that distract from product faults. In this paper we present techniques with the goal of capturing the maximum number of product faults, while flagging the minimum number of log lines for inspection. We observe that the location of a fault in a log should be contained in the lines of a failing test log. In contrast, a passing test log should not contain the lines related to a failure. Lines that occur in both a passing and failing log introduce noise when attempting to find the fault in a failing log. We introduce an approach where we remove the lines that occur in the passing log from the failing log. After removing these lines, we use information retrieval techniques to flag the most probable lines for investigation. We modify TF-IDF to identify the most relevant log lines related to past product failures. We then vectorize the logs and develop an exclusive version of KNN to identify which logs are likely to lead to product faults and which lines are the most probable indication of the failure. Our best approach, LogFaultFlagger finds 89% of the total faults and flags less than 1% of the total failed log lines for inspection. LogFaultFlagger drastically outperforms the previous work CAM. We implemented LogFaultFlagger as a tool at Ericsson where it presents fault prediction summaries to base station testers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812113,Testing;Logs;Faults;Industry,Testing;Inspection;Computer bugs;Fault diagnosis;Base stations;Software;Software engineering,data mining;probability;program debugging;program testing;software fault tolerance,mining historical test logs;software testing;logged output;product faults;passing test log;failing log;passing log;relevant log lines;fault prediction summaries;LogFaultFlagger approach;TF-IDF,22
301,Not Mentioned,DLFinder: Characterizing and Detecting Duplicate Logging Code Smells,Z. Li; T. -H. Chen; J. Yang; W. Shang,"Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada",2019,"Developers rely on software logs for a wide variety of tasks, such as debugging, testing, program comprehension, verification, and performance analysis. Despite the importance of logs, prior studies show that there is no industrial standard on how to write logging statements. Recent research on logs often only considers the appropriateness of a log as an individual item (e.g., one single logging statement); while logs are typically analyzed in tandem. In this paper, we focus on studying duplicate logging statements, which are logging statements that have the same static text message. Such duplications in the text message are potential indications of logging code smells, which may affect developers' understanding of the dynamic view of the system. We manually studied over 3K duplicate logging statements and their surrounding code in four large-scale open source systems: Hadoop, CloudStack, ElasticSearch, and Cassandra. We uncovered five patterns of duplicate logging code smells. For each instance of the code smell, we further manually identify the problematic (i.e., require fixes) and justifiable (i.e., do not require fixes) cases. Then, we contact developers in order to verify our manual study result. We integrated our manual study result and developers' feedback into our automated static analysis tool, DLFinder, which automatically detects problematic duplicate logging code smells. We evaluated DLFinder on the four manually studied systems and two additional systems: Camel and Wicket. In total, combining the results of DLFinder and our manual analysis, we reported 82 problematic code smell instances to developers and all of them have been fixed.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811945,log;code smell;duplicate log;static analysis;empirical study,Manuals;Static analysis;Cloud computing;Debugging;Tools;Semantics;Java,cloud computing;data handling;parallel processing;program debugging;program diagnostics;public domain software,DLFinder;duplicate logging code smells;software logs;single logging statement;duplicate logging statements;problematic duplicate logging code;static text message;dynamic view;open source systems;Hadoop;CloudStack;ElasticSearch;Cassandra;developers feedback,32
303,Not Mentioned,The Seven Sins: Security Smells in Infrastructure as Code Scripts,A. Rahman; C. Parnin; L. Williams,"North Carolina State University, Raleigh, North Carolina; North Carolina State University, Raleigh, North Carolina; North Carolina State University, Raleigh, North Carolina",2019,"Practitioners use infrastructure as code (IaC) scripts to provision servers and development environments. While developing IaC scripts, practitioners may inadvertently introduce security smells. Security smells are recurring coding patterns that are indicative of security weakness and can potentially lead to security breaches. The goal of this paper is to help practitioners avoid insecure coding practices while developing infrastructure as code (IaC) scripts through an empirical study of security smells in IaC scripts. We apply qualitative analysis on 1,726 IaC scripts to identify seven security smells. Next, we implement and validate a static analysis tool called Security Linter for Infrastructure as Code scripts (SLIC) to identify the occurrence of each smell in 15,232 IaC scripts collected from 293 open source repositories. We identify 21,201 occurrences of security smells that include 1,326 occurrences of hard-coded passwords. We submitted bug reports for 1,000 randomly-selected security smell occurrences. We obtain 212 responses to these bug reports, of which 148 occurrences were accepted by the development teams to be fixed. We observe security smells can have a long lifetime, e.g., a hard-coded secret can persist for as long as 98 months, with a median lifetime of 20 months.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812041,"devops, devsecops, empirical study, infrastructure as code, puppet, security, smell, static analysis",Password;Encoding;Tools;Software;Servers;Static analysis,program debugging;program diagnostics;security of data,hard-coded passwords;security smells;hard-coded secret;security weakness;security breaches;IaC scripts;security linter tool;security linter for infrastructure as code scripts,87
304,Not Mentioned,DifFuzz: Differential Fuzzing for Side-Channel Analysis,S. Nilizadeh; Y. Noller; C. S. Pasareanu,"University of Texas at Arlington, Arlington, TX, USA; Humboldt-UniversitÃ¤t zu Berlin, Berlin, Germany; Carnegie Mellon University Silicon Valley, NASA Ames Research Center, Moffett Field, CA, USA",2019,"Side-channel attacks allow an adversary to uncover secret program data by observing the behavior of a program with respect to a resource, such as execution time, consumed memory or response size. Side-channel vulnerabilities are difficult to reason about as they involve analyzing the correlations between resource usage over multiple program paths. We present DifFuzz, a fuzzing-based approach for detecting side-channel vulnerabilities related to time and space. DifFuzz automatically detects these vulnerabilities by analyzing two versions of the program and using resource-guided heuristics to find inputs that maximize the difference in resource consumption between secret-dependent paths. The methodology of DifFuzz is general and can be applied to programs written in any language. For this paper, we present an implementation that targets analysis of Java programs, and uses and extends the Kelinci and AFL fuzzers. We evaluate DifFuzz on a large number of Java programs and demonstrate that it can reveal unknown side-channel vulnerabilities in popular applications. We also show that DifFuzz compares favorably against Blazer and Themis, two state-of-the-art analysis tools for finding side-channels in Java programs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812124,vulnerability detection;side-channel;dynamic analysis;fuzzing,Fuzzing;Java;Tools;Time factors;Instruments;Correlation;Performance analysis,cryptography;Java;program diagnostics;security of data,DifFuzz;differential fuzzing;side-channel analysis;side-channel attacks;secret program data;execution time;resource usage;fuzzing-based approach;resource consumption;secret-dependent paths;Java programs;unknown side-channel vulnerabilities;resource-guided heuristics,41
305,Not Mentioned,Automatically Generating Precise Oracles from Structured Natural Language Specifications,M. Motwani; Y. Brun,"College of Information and Computer Sciences, University of Massachusetts Amherst, Amherst, USA; College of Information and Computer Sciences, University of Massachusetts Amherst, Amherst, USA",2019,"Software specifications often use natural language to describe the desired behavior, but such specifications are difficult to verify automatically. We present Swami, an automated technique that extracts test oracles and generates executable tests from structured natural language specifications. Swami focuses on exceptional behavior and boundary conditions that often cause field failures but that developers often fail to manually write tests for. Evaluated on the official JavaScript specification (ECMA-262), 98.4% of the tests Swami generated were precise to the specification. Using Swami to augment developer-written test suites improved coverage and identified 1 previously unknown defect and 15 missing JavaScript features in Rhino, 1 previously unknown defect in Node.js, and 18 semantic ambiguities in the ECMA-262 specification.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812070,oracle;test oracle;test generation;natural language specification,Natural languages;Documentation;Lenses;Software;Boundary conditions;Semantics;Prototypes,formal specification;Java;natural language processing;program testing,structured natural language specifications;developer-written test suites;ECMA-262 specification;software specifications;test oracles;Swami technique;JavaScript features;JavaScript specification,22
306,Not Mentioned,The Product Backlog,T. Sedano; P. Ralph; C. PÃ©raire,"Pivotal Software, Silicon Valley Campus, Palo Alto, CA, USA; University of Auckland, Auckland, New Zealand; Carnegie Mellon University, Silicon Valley Campus, Moffett Field, CA, USA",2019,"Context: One of the most common artifacts in contemporary software projects is a product backlog comprising user stories, bugs, chores or other work items. However, little research has investigated how the backlog is generated or the precise role it plays in a project. Objective: The purpose of this paper is to determine what is a product backlog, what is its role, and how does it emerge? Method: Following Constructivist Grounded Theory, we conducted a two-year, five-month participant-observation study of eight software development projects at Pivotal, a large, international software company. We interviewed 56 software engineers, product designers, and product managers.We conducted a survey of 27 product designers. We alternated between analysis and theoretical sampling until achieving theoretical saturation. Results: We observed 13 practices and 6 obstacles related to product backlog generation. Limitations: Grounded Theory does not support statistical generalization. While the proposed theory of product backlogs appears widely applicable, organizations with different software development cultures may use different practices. Conclusion: The product backlog is simultaneously a model of work to be done and a boundary object that helps bridge the gap between the processes of generating user stories and realizing them in working code. It emerges from sensemaking (the team making sense of the project context) and coevolution (a cognitive process where the team simultaneously refines its understanding of the problematic context and fledgling solution concepts).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812076,Product-backlog;dual-track-agile;scrum;lean;extreme-programming;user-stories;design-thinking;user-centered-design;feature-engineering,Software;Interviews;Programming;Companies;Buildings;Stakeholders;Data collection,project management,software development projects;product backlog generation;puser stories;software development cultures;constructivist grounded theory,33
307,Not Mentioned,Easy Modelling and Verification of Unpredictable and Preemptive Interrupt-Driven Systems,M. Pan; S. Chen; Y. Pei; T. Zhang; X. Li,"Software Institute, Nanjing University, China; Department of Computer Science and Technology, Nanjing University, China; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; Department of Computer Science and Technology, Nanjing University, China; Department of Computer Science and Technology, Nanjing University, China",2019,"The widespread real-time and embedded systems are mostly interrupt-driven because their heavy interaction with the environment is often initiated by interrupts. With the interrupt arrival being unpredictable and the interrupt handling being preemptive, a large number of possible system behaviours are generated, which makes the correctness assurance of such systems difficult and costly. Model checking is considered to be one of the effective methods for exhausting behavioural state space for correctness. However, existing modelling approaches for interrupt-driven systems are based on either calculus or automata theory, and have a steep learning curve. To address this problem, we propose a new modelling language called interrupt sequence diagram (ISD). By extending the popular UML sequence diagram notations, the ISD supports the modelling of interrupts' essential features visually and concisely. We also propose an automata-based semantics for ISD, based on which ISD can be transformed to a subset of hybrid automata so as to leverage the abundant off-the-shelf checkers. Experiments on examples from both real-world and existing literature were conducted, and the results demonstrate our approach's usability and effectiveness.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812085,"Interrupt-driven systems, Sequence diagrams, System modelling, Model checking",Unified modeling language;Satellites;Task analysis;Semantics;Automata;Usability;Testing,automata theory;embedded systems;formal verification;interrupts;Unified Modeling Language,interrupt-driven systems;embedded systems;interrupt arrival;interrupt handling;model checking;behavioural state space;modelling language;interrupt sequence diagram;ISD;UML sequence diagram notations;hybrid automata,8
308,Not Mentioned,Towards Understanding and Reasoning About Android Interoperations,S. Bae; S. Lee; S. Ryu,"KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea",2019,"Hybrid applications (apps) have become one of the most attractive options for mobile app developers thanks to its support for portability and device-specific features. Android hybrid apps, for example, support portability via JavaScript, device-specific features via Android Java, and seamless interactions between them. However, their interoperation semantics is often under-documented and unintuitive, which makes hybrid apps vulnerable to errors. While recent research has addressed such vulnerabilities, none of them are based on any formal grounds. In this paper, we present the first formal specification of Android interoperability to establish a firm ground for understanding and reasoning about the interoperations. We identify its semantics via extensive testing and thorough inspection of Android source code. We extend an existing multi-language semantics to formally express the key features of hybrid mechanisms, dynamic and indistinguishable interoperability. Based on the extensions, we incrementally define a formal interoperation semantics and disclose its numerous unintuitive and inconsistent behaviors. Moreover, on top of the formal semantics, we devise a lightweight type system that can detect bugs due to the unintuitive inter-language communication. We show that it detects more bugs more efficiently than HybriDroid, the state-of-the-art analyzer of Android hybrid apps, in real-world Android hybrid apps.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811927,"Android hybrid applications, interoperability, multi-language systems, operational semantics, type system",Java;Semantics;Interoperability;Bridges;Computer bugs;Switches;Cognition,Android (operating system);formal specification;Java;mobile computing;open systems,Android interoperations;device-specific features;Android Java;formal specification;Android interoperability;Android source code;dynamic interoperability;formal interoperation semantics;real-world Android hybrid apps;multilanguage semantics,7
310,Not Mentioned,Zero-Overhead Path Prediction with Progressive Symbolic Execution,R. Rutledge; S. Park; H. Khan; A. Orso; M. Prvulovic; A. Zajic,"Georgia Institute of Technology, Atlanta, USA; Georgia Institute of Technology, Atlanta, USA; Georgia Institute of Technology, Atlanta, USA; Georgia Institute of Technology, Atlanta, USA; Georgia Institute of Technology, Atlanta, USA; Georgia Institute of Technology, Atlanta, USA",2019,"In previous work, we introduced zero-overhead profiling (ZOP), a technique that leverages the electromagnetic emissions generated by the computer hardware to profile a program without instrumenting it. Although effective, ZOP has several shortcomings: it requires test inputs that achieve extensive code coverage for its training phase; it predicts path profiles instead of complete execution traces; and its predictions can suffer unrecoverable accuracy losses. In this paper, we present zero-overhead path prediction (ZOP-2), an approach that extends ZOP and addresses its limitations. First, ZOP-2 achieves high coverage during training through progressive symbolic execution (PSE)-symbolic execution of increasingly small program fragments. Second, ZOP-2 predicts complete execution traces, rather than path profiles. Finally, ZOP-2 mitigates the problem of path mispredictions by using a stateless approach that can recover from prediction errors. We evaluated our approach on a set of benchmarks with promising results; for the cases considered, (1) ZOP-2 achieved over 90% path prediction accuracy, and (2) PSE covered feasible paths missed by traditional symbolic execution, thus boosting ZOP-2's accuracy.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812035,symbolic execution;path profiling;tracing,Training;Predictive models;Instruments;Electromagnetics;Benchmark testing;Task analysis;Microsoft Windows,program testing;symbol manipulation,symbolic execution;path prediction accuracy;electromagnetic emissions;computer hardware;extensive code coverage;path mispredictions;progressive symbolic execution;zero-overhead profiling;prediction errors;zero-overhead path prediction;complete execution traces;path profiles,9
311,Not Mentioned,Mimic: UI Compatibility Testing System for Android Apps,T. Ki; C. M. Park; K. Dantu; S. Y. Ko; L. Ziarek,"Department of Computer Science and Engineering, The State University of New York; Department of Computer Science and Engineering, The State University of New York; Department of Computer Science and Engineering, The State University of New York; Department of Computer Science and Engineering, The State University of New York; Department of Computer Science and Engineering, The State University of New York",2019,"This paper proposes Mimic, an automated UI compatibility testing system for Android apps. Mimic is designed specifically for comparing the UI behavior of an app across different devices, different Android versions, and different app versions. This design choice stems from a common problem that Android developers and researchers face-how to test whether or not an app behaves consistently across different environments or internal changes. Mimic allows Android app developers to easily perform backward and forward compatibility testing for their apps. It also enables a clear comparison between a stable version of app and a newer version of app. In doing so, Mimic allows multiple testing strategies to be used, such as randomized or sequential testing. Finally, Mimic programming model allows such tests to be scripted with much less developer effort than other comparable systems. Additionally, Mimic allows parallel testing with multiple testing devices and thereby speeds up testing time. To demonstrate these capabilities, we perform extensive tests for each of the scenarios described above. Our results show that Mimic is effective in detecting forward and backward compatibility issues, and verify runtime behavior of apps. Our evaluation also shows that Mimic significantly reduces the development burden for developers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811983,Mobile apps;UI compatibility testing;Parallel testing;Programming model,Testing;Programming;Performance evaluation;Runtime;Libraries;Instruments;Google,Android (operating system);mobile computing;program testing;user interfaces,Android apps;Mimic programming model;parallel testing;multiple testing devices;UI compatibility testing system,15
312,Not Mentioned,IconIntent: Automatic Identification of Sensitive UI Widgets Based on Icon Classification for Android Apps,X. Xiao; X. Wang; Z. Cao; H. Wang; P. Gao,Case Western Reserve University; The University of Texas at San Antonio; Case Western Reserve University; Case Western Reserve University; Princeton University,2019,"Many mobile applications (i.e., apps) include UI widgets to use or collect users' sensitive data. Thus, to identify suspicious sensitive data usage such as UI-permission mismatch, it is crucial to understand the intentions of UI widgets. However, many UI widgets leverage icons of specific shapes (object icons) and icons embedded with text (text icons) to express their intentions, posing challenges for existing detection techniques that analyze only textual data to identify sensitive UI widgets. In this work, we propose a novel app analysis framework, ICONINTENT, that synergistically combines program analysis and icon classification to identify sensitive UI widgets in Android apps. ICONINTENT automatically associates UI widgets and icons via static analysis on app's UI layout files and code, and then adapts computer vision techniques to classify the associated icons into eight categories of sensitive data. Our evaluations of ICONINTENT on 150 apps from Google Play show that ICONINTENT can detect 248 sensitive UI widgets in 97 apps, achieving a precision of 82.4%. When combined with SUPOR, the state-of-the-art sensitive UI widget identification technique based on text analysis, SUPOR +ICONINTENT can detect 487 sensitive UI widgets (101.2% improvement over SUPOR only), and reduces suspicious permissions to be inspected by 50.7% (129.4% improvement over SUPOR only).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812108,Mobile Security;Program Analysis;Computer Vision;Icon Recognition,Layout;Image color analysis;Optical character recognition software;Static analysis;Global Positioning System;Feature extraction;Shape,Android (operating system);computer vision;graphical user interfaces;image classification;mobile computing;program diagnostics;text analysis;user interfaces,icon classification;android apps;suspicious sensitive data usage;UI widget identification technique;sensitive UI widgets;ICONINTENT;SUPOR;suspicious permissions;Google Play;mobile applications;user sensitive data;textual data;computer vision techniques,29
313,Not Mentioned,Practical GUI Testing of Android Applications Via Model Abstraction and Refinement,T. Gu; C. Sun; X. Ma; C. Cao; C. Xu; Y. Yao; Q. Zhang; J. Lu; Z. Su,"University of California, Davis, USA; University of California, Davis, USA; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; Georgia Institute of Technology, USA; State Key Laboratory for Novel Software Technology, Nanjing University, China; ETH Zurich, Switzerland",2019,"This paper introduces a new, fully automated modelbased approach for effective testing of Android apps. Different from existing model-based approaches that guide testing with a static GUI model (i.e., the model does not evolve its abstraction during testing, and is thus often imprecise), our approach dynamically optimizes the model by leveraging the runtime information during testing. This capability of model evolution significantly improves model precision, and thus dramatically enhances the testing effectiveness compared to existing approaches, which our evaluation confirms.We have realized our technique in a practical tool, APE. On 15 large, widely-used apps from the Google Play Store, APE outperforms the state-of-the-art Android GUI testing tools in terms of both testing coverage and the number of detected unique crashes. To further demonstrate APE's effectiveness and usability, we conduct another evaluation of APE on 1,316 popular apps, where it found 537 unique crashes. Out of the 38 reported crashes, 13 have been fixed and 5 have been confirmed.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812132,GUI testing;mobile app testing;CEGAR,Graphical user interfaces;Testing;Tools;Computer crashes;Google;Indexes;Runtime,Android (operating system);graphical user interfaces;mobile computing;program testing,Android apps;static GUI model;testing effectiveness;APE;Android applications;GUI testing;Android GUI testing tools,50
314,Not Mentioned,AutoTap: Synthesizing and Repairing Trigger-Action Programs Using LTL Properties,L. Zhang; W. He; J. Martinez; N. Brackenbury; S. Lu; B. Ur,"Department of Computer Science, The University of Chicago; Department of Computer Science, The University of Chicago; Department of Computer Science, The University of Chicago; Department of Computer Science, The University of Chicago; Department of Computer Science, The University of Chicago; Department of Computer Science, The University of Chicago",2019,"End-user programming, particularly trigger-action programming (TAP), is a popular method of letting users express their intent for how smart devices and cloud services interact. Unfortunately, sometimes it can be challenging for users to correctly express their desires through TAP. This paper presents AutoTap, a system that lets novice users easily specify desired properties for devices and services. AutoTap translates these properties to linear temporal logic (LTL) and both automatically synthesizes property-satisfying TAP rules from scratch and repairs existing TAP rules. We designed AutoTap based on a user study about properties users wish to express. Through a second user study, we show that novice users made significantly fewer mistakes when expressing desired behaviors using AutoTap than using TAP rules. Our experiments show that AutoTap is a simple and effective option for expressive end-user programming.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811900,End-user programming;Trigger-action programming;Program synthesis;Program repair,Programming;Smart devices;Microsoft Windows;Rain;Maintenance engineering;Safety;Computer bugs,cloud computing;software maintenance;temporal logic,AutoTap;LTL properties;trigger-action programming;smart devices;property-satisfying TAP rules;cloud services;end-user programming;trigger-action programs;linear temporal logic;program repair;program synthesis,31
315,Not Mentioned,Active Inductive Logic Programming for Code Search,A. Sivaraman; T. Zhang; G. Van den Broeck; M. Kim,"University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles",2019,"Modern search techniques either cannot efficiently incorporate human feedback to refine search results or cannot express structural or semantic properties of desired code. The key insight of our interactive code search technique ALICE is that user feedback can be actively incorporated to allow users to easily express and refine search queries. We design a query language to model the structure and semantics of code as logic facts. Given a code example with user annotations, ALICE automatically extracts a logic query from code features that are tagged as important. Users can refine the search query by labeling one or more examples as desired (positive) or irrelevant (negative). ALICE then infers a new logic query that separates positive examples from negative examples via active inductive logic programming. Our comprehensive simulation experiment shows that ALICE removes a large number of false positives quickly by actively incorporating user feedback. Its search algorithm is also robust to user labeling mistakes. Our choice of leveraging both positive and negative examples and using nested program structure as an inductive bias is effective in refining search queries. Compared with an existing interactive code search technique, ALICE does not require a user to manually construct a search pattern and yet achieves comparable precision and recall with much fewer search iterations. A case study with real developers shows that ALICE is easy to use and helps express complex code patterns.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812091,"Code Search, Active Learning, Inductive Logic Programming",Labeling;Logic programming;Semantics;Tools;Database languages;Feature extraction;Computer bugs,inductive logic programming;query languages;query processing;search engines,active inductive logic programming;user feedback;user labeling mistakes;search query;search pattern;structural properties;semantic properties;interactive code search technique ALICE;query language;logic facts;user annotations;logic query;code features;search iterations;nested program structure;interactive code search technique,15
316,Not Mentioned,NL2Type: Inferring JavaScript Function Types from Natural Language Information,R. S. Malik; J. Patra; M. Pradel,TU Darmstadt; TU Darmstadt; TU Darmstadt,2019,"JavaScript is dynamically typed and hence lacks the type safety of statically typed languages, leading to suboptimal IDE support, difficult to understand APIs, and unexpected runtime behavior. Several gradual type systems have been proposed, e.g., Flow and TypeScript, but they rely on developers to annotate code with types. This paper presents NL2Type, a learning-based approach for predicting likely type signatures of JavaScript functions. The key idea is to exploit natural language information in source code, such as comments, function names, and parameter names, a rich source of knowledge that is typically ignored by type inference algorithms. We formulate the problem of predicting types as a classification problem and train a recurrent, LSTM-based neural model that, after learning from an annotated code base, predicts function types for unannotated code. We evaluate the approach with a corpus of 162,673 JavaScript files from real-world projects. NL2Type predicts types with a precision of 84.1% and a recall of 78.9% when considering only the top-most suggestion, and with a precision of 95.5% and a recall of 89.6% when considering the top-5 suggestions. The approach outperforms both JSNice, a state-of-the-art approach that analyzes implementations of functions instead of natural language information, and DeepTyper, a recent type prediction approach that is also based on deep learning. Beyond predicting types, NL2Type serves as a consistency checker for existing type annotations. We show that it discovers 39 inconsistencies that deserve developer attention (from a manual analysis of 50 warnings), most of which are due to incorrect type annotations.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811893,JavaScript;deep learning;type inference;comments;identifiers,Natural languages;Data mining;Predictive models;Semantics;Deep learning;Manuals,digital signatures;Java;learning (artificial intelligence);natural language processing;neural nets;pattern classification;program diagnostics;source code (software);type theory,NL2Type;natural language information;type signatures;type inference algorithms;JavaScript function types;statical typed languages;type prediction;source code;LSTM;DeepTyper;deep learning,56
318,Not Mentioned,Analyzing and Supporting Adaptation of Online Code Examples,T. Zhang; D. Yang; C. Lopes; M. Kim,"University of California, Los Angeles; University of California, Irvine; University of California, Irvine; University of California, Los Angeles",2019,"Developers often resort to online Q&A forums such as Stack Overflow (SO) for filling their programming needs. Although code examples on those forums are good starting points, they are often incomplete and inadequate for developers' local program contexts; adaptation of those examples is necessary to integrate them to production code. As a consequence, the process of adapting online code examples is done over and over again, by multiple developers independently. Our work extensively studies these adaptations and variations, serving as the basis for a tool that helps integrate these online code examples in a target context in an interactive manner. We perform a large-scale empirical study about the nature and extent of adaptations and variations of SO snippets. We construct a comprehensive dataset linking SO posts to GitHub counterparts based on clone detection, time stamp analysis, and explicit URL references. We then qualitatively inspect 400 SO examples and their GitHub counterparts and develop a taxonomy of 24 adaptation types. Using this taxonomy, we build an automated adaptation analysis technique on top of GumTree to classify the entire dataset into these types. We build a Chrome extension called ExampleStack that automatically lifts an adaptation-aware template from each SO example and its GitHub counterparts to identify hot spots where most changes happen. A user study with sixteen programmers shows that seeing the commonalities and variations in similar GitHub counterparts increases their confidence about the given SO example, and helps them grasp a more comprehensive view about how to reuse the example differently and avoid common pitfalls.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812028,online code examples;code adaptation,Cloning;Taxonomy;Java;Tools;Programming;Software engineering;Production,data mining;program debugging;program diagnostics;software maintenance,automated adaptation analysis technique;adaptation-aware template;production code;online Q&A forums;SO snippets;clone detection;time stamp analysis;explicit URL references;Chrome extension;ExampleStack,16
319,Not Mentioned,DockerizeMe: Automatic Inference of Environment Dependencies for Python Code Snippets,E. Horton; C. Parnin,"NC State University, Raleigh, NC, USA; NC State University, Raleigh, NC, USA",2019,"Platforms like Stack Overflow and GitHub's gist system promote the sharing of ideas and programming techniques via the distribution of code snippets designed to illustrate particular tasks. Python, a popular and fast-growing programming language, sees heavy use on both sites, with nearly one million questions asked on Stack Overflow and 400 thousand public gists on GitHub. Unfortunately, around 75% of the Python example code shared through these sites cannot be directly executed. When run in a clean environment, over 50% of public Python gists fail due to an import error for a missing library. We present DockerizeMe, a technique for inferring the dependencies needed to execute a Python code snippet without import error. DockerizeMe starts with offline knowledge acquisition of the resources and dependencies for popular Python packages from the Python Package Index (PyPI). It then builds Docker specifications using a graph-based inference procedure. Our inference procedure resolves import errors in 892 out of nearly 3,000 gists from the Gistable dataset for which Gistable's baseline approach could not find and install all dependencies.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811897,Docker;Configuration Management;Environment Inference;Dependencies;Python,Python;Libraries;Knowledge based systems;Wheels;Task analysis;Indexes;Inference algorithms,graph theory;inference mechanisms;knowledge acquisition;Python;software packages;source code (software),DockerizeMe;automatic inference;environment dependencies;Stack Overflow;programming techniques;programming language;missing library;Python code snippet;offline knowledge acquisition;graph-based inference procedure;GitHub gist system;Python package index,22
320,Not Mentioned,BugSwarm: Mining and Continuously Growing a Dataset of Reproducible Failures and Fixes,D. A. Tomassi; N. Dmeiri; Y. Wang; A. Bhowmick; Y. -C. Liu; P. T. Devanbu; B. Vasilescu; C. Rubio-GonzÃ¡lez,"University of California, Davis; University of California, Davis; University of California, Davis; University of California, Davis; University of California, Davis; University of California, Davis; Carnegie Mellon University; University of California, Davis",2019,"Fault-detection, localization, and repair methods are vital to software quality; but it is difficult to evaluate their generality, applicability, and current effectiveness. Large, diverse, realistic datasets of durably-reproducible faults and fixes are vital to good experimental evaluation of approaches to software quality, but they are difficult and expensive to assemble and keep current. Modern continuous-integration (CI) approaches, like TRAVIS-CI, which are widely used, fully configurable, and executed within custom-built containers, promise a path toward much larger defect datasets. If we can identify and archive failing and subsequent passing runs, the containers will provide a substantial assurance of durable future reproducibility of build and test. Several obstacles, however, must be overcome to make this a practical reality. We describe BUGSWARM, a toolset that navigates these obstacles to enable the creation of a scalable, diverse, realistic, continuously growing set of durably reproducible failing and passing versions of real-world, open-source systems. The BUGSWARM toolkit has already gathered 3,091 fail-pass pairs, in Java and Python, all packaged within fully reproducible containers. Furthermore, the toolkit can be run periodically to detect fail-pass activities, thus growing the dataset continually.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812141,Bug Database;Reproducibility;Software Testing;Program Analysis;Experiment Infrastructure,Tools;Software maintenance;Libraries;Open source software;Software quality;Java;Python,data mining;Java;program debugging;program testing;Python;software fault tolerance;software maintenance;software quality,reproducible failures;fault-detection;repair methods;software quality;realistic datasets;durably-reproducible faults;TRAVIS-CI;BUGSWARM toolkit;fail-pass pairs;fully reproducible containers;fail-pass activities;continuous-integration approach;mining;fault localization;Java;Python,33
321,Not Mentioned,ActionNet: Vision-Based Workflow Action Recognition From Programming Screencasts,D. Zhao; Z. Xing; C. Chen; X. Xia; G. Li,"Research School of Computer Science, Australian National University, Australia; Research School of Computer Science, Australian National University, Australia; Faculty of Information Technology, Monash University, Australia; Faculty of Information Technology, Monash University, Australia; School of Software, Shanghai Jiao Tong University, Shanghai, China",2019,"Programming screencasts have two important applications in software engineering context: study developer behaviors, information needs and disseminate software engineering knowledge. Although programming screencasts are easy to produce, they are not easy to analyze or index due to the image nature of the data. Existing techniques extract only content from screencasts, but ignore workflow actions by which developers accomplish programming tasks. This significantly limits the effective use of programming screencasts in downstream applications. In this paper, we are the first to present a novel technique for recognizing workflow actions in programming screencasts. Our technique exploits image differencing and Convolutional Neural Network (CNN) to analyze the correspondence and change of consecutive frames, based on which nine classes of frequent developer actions can be recognized from programming screencasts. Using programming screencasts from Youtube, we evaluate different configurations of our CNN model and the performance of our technique for developer action recognition across developers, working environments and programming languages. Using screencasts of developers' real work, we demonstrate the usefulness of our technique in a practical application for actionaware extraction of key-code frames in developers' work.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811922,Programming Screencast;Action Recognition;Deep learning,Programming;Microsoft Windows;Feature extraction;Software engineering;Tutorials;Mice,convolutional neural nets;feature extraction;image recognition;software engineering,programming screencasts;vision-based workflow action recognition;ActionNet;image differencing;convolutional neural network;CNN model;programming languages;software engineering;developer behaviors,29
322,Not Mentioned,How C++ Developers Use Immutability Declarations: An Empirical Study,J. Eyolfson; P. Lam,"University of California, Los Angeles Los Angeles, CA, USA; University of Waterloo, Waterloo, ON, Canada",2019,"Best practices for developers, as encoded in recent programming language designs, recommend the use of immutability whenever practical. However, there is a lack of empirical evidence about the uptake of this advice. Our goal is to understand the usage of immutability by C++ developers in practice. This work investigates how C++ developers use immutability by analyzing their use of the C++ immutability qualifier, const, and by analyzing the code itself. We answer the following broad questions about const usage: 1) do developers actually write non-trivial (more than 3 methods) immutable classes and immutable methods? 2) do developers label their immutable classes and methods? We analyzed 7 medium-to-large open source projects and collected two sources of empirical data: 1) const annotations by developers, indicating an intent to write immutable code; and 2) the results of a simple static analysis which identified easily const-able methods---those that clearly did not mutate state. We estimate that 5% of non-trivial classes (median) are immutable. We found the vast majority of classes do carry immutability labels on methods: surprisingly, developers const-annotate 46% of methods, and we estimate that at least 51% of methods could be const-annotated. Furthermore, developers missed immutability labels on at least 6% of unannotated methods. We provide an in-depth discussion on how developers use const and the results of our analyses.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812125,immutability;language design;empirical studies;static analysis,Software engineering,C++ language;program diagnostics;software engineering,immutable code;C++ developers;immutability declarations;immutability qualifier;programming language designs;static analysis,3
323,Not Mentioned,Latent Patterns in Activities: A Field Study of How Developers Manage Context,S. Chattopadhyay; N. Nelson; Y. Ramirez Gonzalez; A. Amelia Leon; R. Pandita; A. Sarma,"Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Phase Change Software, Golden, CO, USA; Oregon State University, Corvallis, OR, USA",2019,"In order to build efficient tools that support complex programming tasks, it is imperative that we understand how developers program. We know that developers create a context around their programming task by gathering relevant information. We also know that developers decompose their tasks recursively into smaller units. However, important gaps exist in our knowledge about: (1) the role that context plays in supporting smaller units of tasks, (2) the relationship that exists among these smaller units, and (3) how context flows across them. The goal of this research is to gain a better understanding of how developers structure their tasks and manage context through a field study of ten professional developers in an industrial setting. Our analysis reveals that developers decompose their tasks into smaller units with distinct goals, that specific patterns exist in how they sequence these smaller units, and that developers may maintain context between those smaller units with related goals.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811986,"context, task decomposition, field study",Task analysis;Software;Encoding;Tools;Java;Programming profession,formal specification;software development management;software quality,programming task;professional developers;complex programming tasks;latent patterns,7
324,Not Mentioned,Developer Reading Behavior While Summarizing Java Methods: Size and Context Matters,N. J. Abid; B. Sharif; N. Dragan; H. Alrasheed; J. I. Maletic,"Department of Computer Science, Taibah University, Madinah, Kingdom of Saudi Arabia; Department of Computer Science and Engineering, University of Nebraska-Lincoln, Lincoln, Nebraska, USA; Department of Computer Science, Kent State University, Kent, Ohio, USA; Department of Information Technology, King Saud University, Riyadh, Kingdom of Saudi Arabia; Department of Computer Science, Kent State University, Kent, Ohio, USA",2019,"An eye-tracking study of 18 developers reading and summarizing Java methods is presented. The developers provide a written summary for methods assigned to them. In total, 63 methods are used from five different systems. Previous studies on this topic use only short methods presented in isolation usually as images. In contrast, this work presents the study in the Eclipse IDE allowing access to all the source code in the system. The developer can navigate via scrolling and switching files while writing the summary. New eye-tracking infrastructure allows for this improvement in the study environment. Data collected includes eye gazes on source code, written summaries, and time to complete each summary. Unlike prior work that concluded developers focus on the signature the most, these results indicate that they tend to focus on the method body more than the signature. Moreover, both experts and novices tend to revisit control flow terms rather than reading them for a long period. They also spend a significant amount of gaze time and have higher gaze visits when they read call terms. Experts tend to revisit the body of the method significantly more frequently than its signature as the size of the method increases. Moreover, experts tend to write their summaries from source code lines that they read the most.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812039,source code summarization;eye tracking;program comprehension;empirical study,Java;Gaze tracking;Task analysis;Natural languages;Software engineering;Switches,gaze tracking;human factors;Java,Java methods;written summary;gaze time;source code lines;eye gazes;eye tracking infrastructure;developer reading behavior;Eclipse IDE;control flow,23
325,Not Mentioned,Distilling Neural Representations of Data Structure Manipulation using fMRI and fNIRS,Y. Huang; X. Liu; R. Krueger; T. Santander; X. Hu; K. Leach; W. Weimer,"University of Michigan; University of Michigan; University of Michigan; Univeristy of California, Santa Barbara; University of Michigan; University of Michigan; University of Michigan",2019,"Data structures permeate many aspects of software engineering, but their associated human cognitive processes are not thoroughly understood. We leverage medical imaging and insights from the psychological notion of spatial ability to decode the neural representations of several fundamental data structures and their manipulations. In a human study involving 76 participants, we examine list, array, tree, and mental rotation tasks using both functional near-infrared spectroscopy (fNIRS) and functional magnetic resonance imaging (fMRI). We find a nuanced relationship: data structure and spatial operations use the same focal regions of the brain but to different degrees. They are related but distinct neural tasks. In addition, more difficult computer science problems induce higher cognitive load than do problems of pure spatial reasoning. Finally, while fNIRS is less expensive and more permissive, there are some computing-relevant brain regions that only fMRI can reach.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812086,medical imaging;data structures;spatial ability,Functional magnetic resonance imaging;Data structures;Task analysis;Software engineering;Biomedical imaging;Neuroimaging;Brain,biomedical MRI;biomedical optical imaging;brain;cognition;data structures;infrared spectroscopy;medical image processing;neurophysiology;psychology;spatial reasoning,computing-relevant brain regions;pure spatial reasoning;higher cognitive load;difficult computer science problems;distinct neural tasks;spatial operations;functional magnetic resonance imaging;near-infrared spectroscopy;mental rotation tasks;tree;human study;manipulations;fundamental data structures;spatial ability;psychological notion;leverage medical imaging;associated human cognitive processes;software engineering;fNIRS;fMRI;data structure manipulation;neural representations,16
326,Not Mentioned,Message from the Journal-First Chair of ICSE 2019,,,2019,Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811919,,,,,
327,Not Mentioned,FastLane: Test Minimization for Rapidly Deployed Large-Scale Online Services,A. A. Philip; R. Bhagwan; R. Kumar; C. S. Maddila; N. Nagppan,Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research; Microsoft Research,2019,"Today, we depend on numerous large-scale services for basic operations such as email. These services, built on the basis of Continuous Integration/Continuous Deployment (CI/CD) processes, are extremely dynamic: developers continuously commit code and introduce new features, functionality and fixes. Hundreds of commits may enter the code-base in a single day. Therefore one of the most time-critical, yet resource-intensive tasks towards ensuring code-quality is effectively testing such large code-bases. This paper presents FastLane, a system that performs data-driven test minimization. FastLane uses light-weight machine-learning models built upon a rich history of test and commit logs to predict test outcomes. Tests for which we predict outcomes need not be explicitly run, thereby saving us precious test-time and resources. Our evaluation on a large-scale email and collaboration platform service shows that our techniques can save 18.04%, i.e., almost a fifth of test-time while obtaining a test outcome accuracy of 99.99%.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812033,test prioritization;commit risk;machine learning,Testing;Correlation;Minimization;Predictive models;Complexity theory;Prediction algorithms;Machine learning,electronic mail;groupware;Internet;learning (artificial intelligence);program testing;software quality,FastLane;rapidly deployed large-scale online services;code-base;resource-intensive tasks;code-quality;light-weight machine-learning models;large-scale email;collaboration platform service;time-critical tasks;data-driven test minimization;continuous integration-continuous deployment processes;CI-CD processes;commit logs,16
328,Not Mentioned,Scalable Approaches for Test Suite Reduction,E. Cruciani; B. Miranda; R. Verdecchia; A. Bertolino,"Gran Sasso Science Institute | Lâ€™ Aquila, Italy; Federal University of Pernambuco | Recife, Brazil; Vrije Universiteit Amsterdam | Amsterdam, The Netherlands; ISTI - Consiglio Nazionale delle Ricerche | Pisa, Italy",2019,"Test suite reduction approaches aim at decreasing software regression testing costs by selecting a representative subset from large-size test suites. Most existing techniques are too expensive for handling modern massive systems and moreover depend on artifacts, such as code coverage metrics or specification models, that are not commonly available at large scale. We present a family of novel very efficient approaches for similarity-based test suite reduction that apply algorithms borrowed from the big data domain together with smart heuristics for finding an evenly spread subset of test cases. The approaches are very general since they only use as input the test cases themselves (test source code or command line input). We evaluate four approaches in a version that selects a fixed budget B of test cases, and also in an adequate version that does the reduction guaranteeing some fixed coverage. The results show that the approaches yield a fault detection loss comparable to state-of-the-art techniques, while providing huge gains in terms of efficiency. When applied to a suite of more than 500K real world test cases, the most efficient of the four approaches could select B test cases (for varying B values) in less than 10 seconds.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812048,Clustering;Random projection;Similarity-based testing;Software testing;Test suite reduction,Testing;Big Data;Software;Measurement;Fault detection;Scalability;Clustering algorithms,Big Data;program testing;set theory,similarity-based test suite reduction;evenly spread subset;test source code;B test cases;scalable approaches;test suite reduction approaches;large-size test suites;modern massive systems;code coverage metrics;big data domain;smart heuristics;command line input,21
329,Not Mentioned,A Framework for Checking Regression Test Selection Tools,C. Zhu; O. Legunsen; A. Shi; M. Gligoric,The University of Texas at Austin; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; The University of Texas at Austin,2019,"Regression test selection (RTS) reduces regression testing costs by re-running only tests that can change behavior due to code changes. Researchers and large software organizations recently developed and adopted several RTS tools to deal with the rapidly growing costs of regression testing. As RTS tools gain adoption, it becomes critical to check that they are correct and efficient. Unfortunately, checking RTS tools currently relies solely on limited tests that RTS tool developers manually write. We present RTSCheck, the first framework for checking RTS tools. RTSCheck feeds evolving programs (i.e., sequences of program revisions) to an RTS tool and checks the output against rules inspired by existing RTS test suites. Violations of these rules are likely due to deviations from expected RTS tool behavior, and indicative of bugs in the tool. RTSCheck uses three components to obtain evolving programs: (1) AutoEP automatically generates evolving programs and corresponding tests, (2) DefectsEP uses buggy and fixed program revisions from bug databases, and (3) EvoEP uses sequences of program revisions from actual open-source projects' histories. We used RTSCheck to check three recently developed RTS tools for Java: Clover, Ekstazi, and STARTS. RTSCheck discovered 27 bugs in these three tools.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812073,regression test selection;program generation;evolution;checking software tools,Tools;Computer bugs;Testing;Safety;Java;Software;Feeds,Java;program debugging;program testing;software tools,program revisions;RTSCheck;checking regression test selection tools;regression testing costs;RTS tool developers;expected RTS tool behavior;RTS test suites;EvoEP;open-source project histories;Java;Clover;Ekstazi;STARTS,17
330,Not Mentioned,Supporting Analysts by Dynamic Extraction and Classification of Requirements-Related Knowledge,Z. Shakeri Hossein Abad; V. Gervasi; D. Zowghi; B. H. Far,"Department of Computer Science, University of Calgary, Calgary, Canada; Department of Computer Science, University of Pisa, Italy; School of Software, University of Technology Sydney, Australia; Schulich School of Engineering, University of Calgary, Calgary, Canada",2019,"In many software development projects, analysts are required to deal with systems' requirements from unfamiliar domains. Familiarity with the domain is necessary in order to get full leverage from interaction with stakeholders and for extracting relevant information from the existing project documents. Accurate and timely extraction and classification of requirements knowledge support analysts in this challenging scenario. Our approach is to mine real-time interaction records and project documents for the relevant phrasal units about the requirements related topics being discussed during elicitation. We propose to use both generative and discriminating methods. To extract the relevant terms, we leverage the flexibility and power of Weighted Finite State Transducers (WFSTs) in dynamic modelling of natural language processing tasks. We used an extended version of Support Vector Machines (SVMs) with variable-sized feature vectors to efficiently and dynamically extract and classify requirements-related knowledge from the existing documents. To evaluate the performance of our approach intuitively and quantitatively, we used edit distance and precision/recall metrics. We show in three case studies that the snippets extracted by our method are intuitively relevant and reasonably accurate. Furthermore, we found that statistical and linguistic parameters such as smoothing methods, and words contiguity and order features can impact the performance of both extraction and classification tasks.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812084,Requirements elicitation;Natural Language Processing;Requirements classification;Weighted Finite State Transducer;Dynamic language models;Information extraction,Feature extraction;Transducers;Stakeholders;Data mining;Real-time systems;Task analysis;Support vector machines,computational linguistics;feature extraction;information retrieval;natural language processing;pattern classification;project management;software development management;support vector machines;text analysis,natural language processing tasks;variable-sized feature vectors;requirements-related knowledge;software development projects;weighted finite state transducers;support vector machines;requirements knowledge;phrasal units;project documents;edit distance;precision/recall metrics;extraction tasks;classification tasks,4
331,Not Mentioned,Analysis and Detection of Information Types of Open Source Software Issue Discussions,D. Arya; W. Wang; J. L. C. Guo; J. Cheng,"School of Computer Science, McGill University, Montreal, Canada; School of Computer Science, McGill University, Montreal, Canada; School of Computer Science, McGill University, Montreal, Canada; Department of Computer and Software Engineering, Polytechnique Montreal, Montreal, Canada",2019,"Most modern Issue Tracking Systems (ITSs) for open source software (OSS) projects allow users to add comments to issues. Over time, these comments accumulate into discussion threads embedded with rich information about the software project, which can potentially satisfy the diverse needs of OSS stakeholders. However, discovering and retrieving relevant information from the discussion threads is a challenging task, especially when the discussions are lengthy and the number of issues in ITSs are vast. In this paper, we address this challenge by identifying the information types presented in OSS issue discussions. Through qualitative content analysis of 15 complex issue threads across three projects hosted on GitHub, we uncovered 16 information types and created a labeled corpus containing 4656 sentences. Our investigation of supervised, automated classification techniques indicated that, when prior knowledge about the issue is available, Random Forest can effectively detect most sentence types using conversational features such as the sentence length and its position. When classifying sentences from new issues, Logistic Regression can yield satisfactory performance using textual features for certain information types, while falling short on others. Our work represents a nontrivial first step towards tools and techniques for identifying and obtaining the rich information recorded in the ITSs to support various software engineering activities and to satisfy the diverse needs of OSS stakeholders.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811936,collaborative software engineering;issue tracking system;issue discussion analysis,Stakeholders;Message systems;Software engineering;Computer bugs;Task analysis;Open source software,learning (artificial intelligence);pattern classification;project management;public domain software;regression analysis;software development management,ITSs;open source software projects;software project;OSS stakeholders;OSS issue discussions;qualitative content analysis;sentence types;software engineering activities;information types;open source software issue discussions;random forest;logistic regression,37
332,Not Mentioned,Do Developers Discover New Tools On The Toilet?,E. Murphy-Hill; E. K. Smith; C. Sadowski; C. Jaspan; C. Winter; M. Jorde; A. Knight; A. Trenk; S. Gross,"Google, LLC; Bloomberg; Google, LLC; Google, LLC; Waymo; Google, LLC; Google, LLC; Google, LLC; Google, LLC",2019,"Maintaining awareness of useful tools is a substantial challenge for developers. Physical newsletters are a simple technique to inform developers about tools. In this paper, we evaluate such a technique, called Testing on the Toilet, by performing a mixed-methods case study. We first quantitatively evaluate how effective this technique is by applying statistical causal inference over six years of data about tools used by thousands of developers. We then qualitatively contextualize these results by interviewing and surveying 382 developers, from authors to editors to readers. We found that the technique was generally effective at increasing software development tool use, although the increase varied depending on factors such as the breadth of applicability of the tool, the extent to which the tool has reached saturation, and the memorability of the tool name.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812046,software engineering;diffusion of innovations,Tools;Software;Google;Testing;Software engineering;Advertising;Productivity,program testing;software development management;software tools;statistical analysis,physical newsletters;statistical causal inference;tool name;software development tool;testing on the toilet,5
333,Not Mentioned,Message from the Workshop Chairs of ICSE 2019,,,2019,Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812030,,,,,
334,Not Mentioned,Tool Choice Matters: JavaScript Quality Assurance Tools and Usage Outcomes in GitHub Projects,D. Kavaler; A. Trockman; B. Vasilescu; V. Filkov,"University of California, Davis; University of Evansville; Carnegie Mellon University; University of California, Davis",2019,"Quality assurance automation is essential in modern software development. In practice, this automation is supported by a multitude of tools that fit different needs and require developers to make decisions about which tool to choose in a given context. Data and analytics of the pros and cons can inform these decisions. Yet, in most cases, there is a dearth of empirical evidence on the effectiveness of existing practices and tool choices. We propose a general methodology to model the time-dependent effect of automation tool choice on four outcomes of interest: prevalence of issues, code churn, number of pull requests, and number of contributors, all with a multitude of controls. On a large data set of npm JavaScript projects, we extract the adoption events for popular tools in three task classes: linters, dependency managers, and coverage reporters. Using mixed methods approaches, we study the reasons for the adoptions and compare the adoption effects within each class, and sequential tool adoptions across classes. We find that some tools within each group are associated with more beneficial outcomes than others, providing an empirical perspective for the benefits of each. We also find that the order in which some tools are implemented is associated with varying outcomes.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812106,quality assurance tools;empirical study,Tools;Task analysis;Pipelines;Quality assurance;Automation;Software;Switches,decision making;Internet;Java;program debugging;program diagnostics;program testing;project management;public domain software;quality assurance;software quality,adoption effects;sequential tool adoptions;beneficial outcomes;varying outcomes;tool choice matters;JavaScript quality assurance tools;GitHub projects;quality assurance automation;empirical evidence;time-dependent effect;automation tool choice;data set;npm JavaScript projects;adoption events;task classes;dependency managers;software development,17
335,Not Mentioned,Hunting for Bugs in Code Coverage Tools via Randomized Differential Testing,Y. Yang; Y. Zhou; H. Sun; Z. Su; Z. Zuo; L. Xu; B. Xu,"State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China; State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China; Unaffiliated; Department of Computer Science, ETH Zurich, Switzerland; State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China; State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China; State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China",2019,"Reliable code coverage tools are critically important as it is heavily used to facilitate many quality assurance activities, such as software testing, fuzzing, and debugging. However, little attention has been devoted to assessing the reliability of code coverage tools. In this study, we propose a randomized differential testing approach to hunting for bugs in the most widely used C code coverage tools. Specifically, by generating random input programs, our approach seeks for inconsistencies in code coverage reports produced by different code coverage tools, and then identifies inconsistencies as potential code coverage bugs. To effectively report code coverage bugs, we addressed three specific challenges: (1) How to filter out duplicate test programs as many of them triggering the same bugs in code coverage tools; (2) how to automatically reduce large test programs to much smaller ones that have the same properties; and (3) how to determine which code coverage tools have bugs? The extensive evaluations validate the effectiveness of our approach, resulting in 42 and 28 confirmed/fixed bugs for gcov and llvm-cov, respectively. This case study indicates that code coverage tools are not as reliable as it might have been envisaged. It not only demonstrates the effectiveness of our approach, but also highlights the need to continue improving the reliability of code coverage tools. This work opens up a new direction in code coverage validation which calls for more attention in this area.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812045,Code Coverage;Differential Testing;Coverage Tools;Bug Detection.,Computer bugs;Tools;Software;Reliability;Fuzzing;Debugging,program debugging;program testing;software tools,random input programs;C code coverage tools;randomized differential testing;code coverage validation;potential code coverage bugs;code coverage reports;reliable code coverage tools,11
336,Not Mentioned,Rotten Green Tests,J. Delplanque; S. Ducasse; G. Polito; A. P. Black; A. Etien,"CNRS, Univ. Lille, France; RMOD - Inria Lille, France; CNRS, Univ. Lille, France; Dept of Computer Science, Portland State University, Oregon, USA; CNRS, Univ. Lille, France",2019,"Unit tests are a tenant of agile programming methodologies, and are widely used to improve code quality and prevent code regression. A green (passing) test is usually taken as a robust sign that the code under test is valid. However, some green tests contain assertions that are never executed. We call such tests Rotten Green Tests. Rotten Green Tests represent a case worse than a broken test: they report that the code under test is valid, but in fact do not test that validity. We describe an approach to identify rotten green tests by combining simple static and dynamic call-site analyses. Our approach takes into account test helper methods, inherited helpers, and trait compositions, and has been implemented in a tool called DrTest. DrTest reports no false negatives, yet it still reports some false positives due to conditional use or multiple test contexts. Using DrTest we conducted an empirical evaluation of 19,905 real test cases in mature projects of the Pharo ecosystem. The results of the evaluation show that the tool is effective; it detected 294 tests as rotten-green tests that contain assertions that are not executed. Some rotten tests have been â€œsleepingâ€ in Pharo for at least 5 years.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812040,,Testing;Software;Java;Tools;Writing;Computer bugs;Software engineering,object-oriented programming;program testing;software prototyping,rotten tests;unit tests;rotten green tests;agile programming methodologies;code quality;code regression;dynamic call-site analysis;static call-site analysis;DrTest;Pharo ecosystem,7
337,Not Mentioned,VFix: Value-Flow-Guided Precise Program Repair for Null Pointer Dereferences,X. Xu; Y. Sui; H. Yan; J. Xue,"School of Computer Science and Engineering, UNSW Sydney, Australia; Faculty of Engineering and Information Technology, University of Technology Sydney, Australia; School of Computer Science and Engineering, UNSW Sydney, Australia; School of Computer Science and Engineering, UNSW Sydney, Australia",2019,"Automated Program Repair (APR) faces a key challenge in efficiently generating correct patches from a potentially infinite solution space. Existing approaches, which attempt to reason about the entire solution space, can be ineffective (by often producing no plausible patches at all) and imprecise (by often producing plausible but incorrect patches). We present VFIX, a new value-flow-guided APR approach, to fix null pointer exception (NPE) bugs by considering a substantially reduced solution space in order to greatly increase the number of correct patches generated. By reasoning about the data and control dependences in the program, VFIX can identify bug-relevant repair statements more accurately and generate more correct repairs than before. VFIX outperforms a set of 8 state-of-the-art APR tools in fixing the NPE bugs in Defects4j in terms of both precision (by correctly fixing 3 times as many bugs as the most precise one and 50% more than all the bugs correctly fixed by these 8 tools altogether) and efficiency (by producing a correct patch in minutes instead of hours).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812101,"program repair, static analysis, null dereference",Maintenance engineering;Computer bugs;Tools;Aerospace electronics;Software;Australia,program debugging;program diagnostics;software maintenance,VFix;value-flow-guided precise program repair;null pointer dereferences;value-flow-guided APR approach;null pointer exception bugs;bug-relevant repair statements;NPE bugs;automated program repair,22
338,Not Mentioned,On Reliability of Patch Correctness Assessment,X. -B. D. Le; L. Bao; D. Lo; X. Xia; S. Li; C. Pasareanu,"Carnegie Mellon University, USA; Zhejiang University City College, China; Singapore Management University, Singapore; Monash University, Australia; Zhejiang University, China; NASA Ames Research Center, USA",2019,"Current state-of-the-art automatic software repair (ASR) techniques rely heavily on incomplete specifications, or test suites, to generate repairs. This, however, may cause ASR tools to generate repairs that are incorrect and hard to generalize. To assess patch correctness, researchers have been following two methods separately: (1) Automated annotation, wherein patches are automatically labeled by an independent test suite (ITS) - a patch passing the ITS is regarded as correct or generalizable, and incorrect otherwise, (2) Author annotation, wherein authors of ASR techniques manually annotate the correctness labels of patches generated by their and competing tools. While automated annotation cannot ascertain that a patch is actually correct, author annotation is prone to subjectivity. This concern has caused an on-going debate on the appropriate ways to assess the effectiveness of numerous ASR techniques proposed recently. In this work, we propose to assess reliability of author and automated annotations on patch correctness assessment. We do this by first constructing a gold set of correctness labels for 189 randomly selected patches generated by 8 state-of-the-art ASR techniques through a user study involving 35 professional developers as independent annotators. By measuring inter-rater agreement as a proxy for annotation quality - as commonly done in the literature - we demonstrate that our constructed gold set is on par with other high-quality gold sets. We then compare labels generated by author and automated annotations with this gold set to assess reliability of the patch assessment methodologies. We subsequently report several findings and highlight implications for future studies.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812054,Automated program repair;empirical study;test case generationn,Maintenance engineering;Tools;Gold;Reliability;Task analysis;Software;Best practices,program testing;software maintenance;software reliability,patch correctness assessment;ASR tools;independent test suite;correctness labels;automated annotations;annotation quality;patch assessment methodologies;automatic software repair techniques;randomly selected patches;ASR techniques;automated annotation;author annotation,43
339,Not Mentioned,How Reliable is the Crowdsourced Knowledge of Security Implementation?,M. Chen; F. Fischer; N. Meng; X. Wang; J. Grossklags,Virginia Tech; Technical University of Munich; Virginia Tech; University of Texas at San Antonio; Technical University of Munich,2019,"Stack Overflow (SO) is the most popular online Q&A site for developers to share their expertise in solving programming issues. Given multiple answers to a certain question, developers may take the accepted answer, the answer from a person with high reputation, or the one frequently suggested. However, researchers recently observed that SO contains exploitable security vulnerabilities in the suggested code of popular answers, which found their way into security-sensitive high-profile applications that millions of users install every day. This observation inspires us to explore the following questions: How much can we trust the security implementation suggestions on SO? If suggested answers are vulnerable, can developers rely on the community's dynamics to infer the vulnerability and identify a secure counterpart? To answer these highly important questions, we conducted a comprehensive study on security-related SO posts by contrasting secure and insecure advice with the community-given content evaluation. Thereby, we investigated whether SO's gamification approach on incentivizing users is effective in improving security properties of distributed code examples. Moreover, we traced the distribution of duplicated samples over given answers to test whether the community behavior facilitates or prevents propagation of secure and insecure code suggestions within SO. We compiled 953 different groups of similar security-related code examples and labeled their security, identifying 785 secure answer posts and 644 insecure answer posts. Compared with secure suggestions, insecure ones had higher view counts (36,508 vs. 18,713), received a higher score (14 vs. 5), and had significantly more duplicates (3.8 vs. 3.0) on average. 34% of the posts provided by highly reputable so-called trusted users were insecure. Our findings show that based on the distribution of secure and insecure code on SO, users being laymen in security rely on additional advice and guidance. However, the community-given feedback does not allow differentiating secure from insecure choices. The reputation mechanism fails in indicating trustworthy users with respect to security questions, ultimately leaving other users wandering around alone in a software security minefield.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812042,"Stack Overflow, crowdsourced knowledge, social dynamics, security implementation",Security;Cloning;Message systems;Reliability;Encoding;Crowdsourcing;Software,question answering (information retrieval);security of data;Web sites,programming issues;exploitable security vulnerabilities;suggested code;security-sensitive high-profile applications;security implementation suggestions;suggested answers;vulnerability;secure counterpart;secure advice;insecure advice;community-given content evaluation;security properties;distributed code examples;community behavior facilitates;secure code suggestions;insecure code suggestions;similar security-related code examples;community-given feedback;insecure choices;trustworthy users;security questions;software security minefield;crowdsourced knowledge;Stack Overflow;answer posts;online Q&A site;gamification approach,27
340,Not Mentioned,Pattern-Based Mining of Opinions in Q&A Websites,B. Lin; F. Zampetti; G. Bavota; M. Di Penta; M. Lanza,"Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland; University of Sannio, Italy; Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland; University of Sannio, Italy; Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland",2019,"Informal documentation contained in resources such as Q&A websites (e.g., Stack Overflow) is a precious resource for developers, who can find there examples on how to use certain APIs, as well as opinions about pros and cons of such APIs. Automatically identifying and classifying such opinions can alleviate developers' burden in performing manual searches, and can be used to recommend APIs that are good from some points of view (e.g., performance), or highlight those less ideal from other perspectives (e.g., compatibility). We propose POME (Pattern-based Opinion MinEr), an approach that leverages natural language parsing and pattern-matching to classify Stack Overflow sentences referring to APIs according to seven aspects (e.g., performance, usability), and to determine their polarity (positive vs negative). The patterns have been inferred by manually analyzing 4,346 sentences from Stack Overflow linked to a total of 30 APIs. We evaluated POME by (i) comparing the pattern-matching approach with machine learners leveraging the patterns themselves as well as n-grams extracted from Stack Overflow posts; (ii) assessing the ability of POME to detect the polarity of sentences, as compared to sentiment-analysis tools; (iii) comparing POME with the state-of-the-art Stack Overflow opinion mining approach, Opiner, through a study involving 24 human evaluators. Our study shows that POME exhibits a higher precision than a state-of-the-art technique (Opiner), in terms of both opinion aspect identification and polarity assessment.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811960,opinion mining;Stack Overflow;sentiment analysis;aspect detection,Sentiment analysis;Tools;Documentation;Data mining;Software engineering;Software;Databases,application program interfaces;data mining;natural language processing;pattern classification;Web sites,POME;opinion aspect identification;polarity assessment;informal documentation;Q&A websites;natural language parsing;Stack Overflow sentences;pattern-based opinion miner;stack overflow opinion mining approach;pattern-based mining;stack overflow posts;APIs;pattern-matching approach,44
341,Not Mentioned,Message from the Artifact Evaluation Chairs of ICSE 2019,,,2019,Presents the introductory welcome message from the conference proceedings. May include the conference officers' congratulations to all involved with the conference event and publication of the proceedings record.,https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811926,,,,,
342,Not Mentioned,Detection and Repair of Architectural Inconsistencies in Java,N. Ghorbani; J. Garcia; S. Malek,"School of Information and Computer Sciences, University of California, Irvine, USA; School of Information and Computer Sciences, University of California, Irvine, USA; School of Information and Computer Sciences, University of California, Irvine, USA",2019,"Java is one of the most widely used programming languages. However, the absence of explicit support for architectural constructs, such as software components, in the programming language itself has prevented software developers from achieving the many benefits that come with architecture-based development. To address this issue, Java 9 has introduced the Java Platform Module System (JPMS), resulting in the first instance of encapsulation of modules with rich software architectural interfaces added to a mainstream programming language. The primary goal of JPMS is to construct and maintain large applications efficiently-as well as improve the encapsulation, security, and maintainability of Java applications in general and the JDK itself. A challenge, however, is that module declarations do not necessarily reflect actual usage of modules in an application, allowing developers to mistakenly specify inconsistent dependencies among the modules. In this paper, we formally define 8 inconsistent modular dependencies that may arise in Java-9 applications. We also present DARCY, an approach that leverages these definitions and static program analyses to automatically (1) detect the specified inconsistent dependencies within Java applications and (2) repair those identified inconsistencies. The results of our experiments, conducted over 38 open-source Java-9 applications, indicate that architectural inconsistencies are widespread and demonstrate the benefits of DARCY in automated detection and repair of these inconsistencies.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812079,Java Platform Module System;Architectural Inconsistencies;Static Program Analysis;Software Architecture;Module;Detection;Repair;Security;Maintainability;Encapsulation;Software Bloat;Java,Java;Computer architecture;Software;Bars;Encapsulation;Security;Maintenance engineering,Java;object-oriented programming;program diagnostics;software architecture;software maintenance;software prototyping;systems analysis,encapsulation;mainstream programming language;JPMS;Java applications;module declarations;static program analyses;identified inconsistencies;architectural inconsistencies;automated detection;repair;architectural constructs;software components;software developers;architecture-based development;Java Platform Module System;programming languages;open-source Java-9 applications;inconsistent modular dependencies;software architectural interfaces;DARCY,5
343,Not Mentioned,Could I Have a Stack Trace to Examine the Dependency Conflict Issue?,Y. Wang; M. Wen; R. Wu; Z. Liu; S. H. Tan; Z. Zhu; H. Yu; S. -C. Cheung,"Northeastern University, Shenyang, China; The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China; Northeastern University, Shenyang, China; Southern University of Science and Technology, Shenzhen, China; Northeastern University, Shenyang, China; Northeastern University, Shenyang, China; The Hong Kong University of Science and Technology, Hong Kong, China",2019,"Intensive use of libraries in Java projects brings potential risk of dependency conflicts, which occur when a project directly or indirectly depends on multiple versions of the same library or class. When this happens, JVM loads one version and shadows the others. Runtime exceptions can occur when methods in the shadowed versions are referenced. Although project management tools such as Maven are able to give warnings of potential dependency conflicts when a project is built, developers often ask for crashing stack traces before examining these warnings. It motivates us to develop Riddle, an automated approach that generates tests and collects crashing stack traces for projects subject to risk of dependency conflicts. Riddle, built on top of Asm and Evosuite, combines condition mutation, search strategies and condition restoration. We applied Riddle on 19 real-world Java projects with duplicate libraries or classes. We reported 20 identified dependency conflicts including their induced crashing stack traces and the details of generated tests. Among them, 15 conflicts were confirmed by developers as real issues, and 10 were readily fixed. The evaluation results demonstrate the effectiveness and usefulness of Riddle.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812128,test generation;third party-library;mutation,Libraries;Runtime;Java;Computer crashes;Tools;Test pattern generators;Static analysis,Java;program debugging;program testing;project management;public domain software;system recovery,stack trace;Riddle;real-world Java;induced crashing stack traces;dependency conflict issue;Java projects;runtime exceptions;shadowed versions;project management tools;dependency conflicts;condition mutation;search strategies;condition restoration,13
344,Not Mentioned,Investigating the Impact of Multiple Dependency Structures on Software Defects,D. Cui; T. Liu; Y. Cai; Q. Zheng; Q. Feng; W. Jin; J. Guo; Y. Qu,"School of Electronic and Information Engineering, Xian Jiaotong University, Xian, China; School of Electronic and Information Engineering, Xian Jiaotong University, Xian, China; Department of Computer Science, Drexel University, Philadelphia, USA; School of Electronic and Information Engineering, Xian Jiaotong University, Xian, China; Department of Computer Science, Drexel University, Philadelphia, USA; School of Electronic and Information Engineering, Xian Jiaotong University, Xian, China; School of Electronic and Information Engineering, Xian Jiaotong University, Xian, China; School of Electronic and Information Engineering, Xian Jiaotong University, Xian, China",2019,"Over the past decades, numerous approaches were proposed to help practitioner to predict or locate defective files. These techniques often use syntactic dependency, history co-change relation, or semantic similarity. The problem is that, it remains unclear whether these different dependency relations will present similar accuracy in terms of defect prediction and localization. In this paper, we present our systematic investigation of this question from the perspective of software architecture. Considering files involved in each dependency type as an individual design space, we model such a design space using one DRSpace. We derived 3 DRSpaces for each of the 117 Apache open source projects, with 643,079 revision commits and 101,364 bug reports in total, and calculated their interactions with defective files. The experiment results are surprising: the three dependency types present significantly different architectural views, and their interactions with defective files are also drastically different. Intuitively, they play completely different roles when used for defect prediction/localization. The good news is that the combination of these structures has the potential to improve the accuracy of defect prediction/localization. In summary, our work provides a new perspective regarding to which type(s) of relations should be used for the task of defect prediction/localization. These quantitative and qualitative results also advance our knowledge of the relationship between software quality and architectural views formed using different dependency types.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812092,Software Structure;Software Maintenance;Software Quality,History;Syntactics;Semantics;Software;Computer bugs;Reverse engineering;Prediction algorithms,program debugging;public domain software;software architecture;software maintenance;software quality,dependency type;DRSpace;defective files;software quality;multiple dependency structures;software defects;syntactic dependency;history co-change relation;semantic similarity;software architecture;design space;Apache open source projects;dependency relations,14
345,Not Mentioned,StoryDroid: Automated Generation of Storyboard for Android Apps,S. Chen; L. Fan; C. Chen; T. Su; W. Li; Y. Liu; L. Xu,"East China Normal University, China; East China Normal University, China; Monash University, Australia; Nanyang Technological University, Singapore; New York University Shanghai, China; Nanyang Technological University, Singapore; New York University Shanghai, China",2019,"Mobile apps are now ubiquitous. Before developing a new app, the development team usually endeavors painstaking efforts to review many existing apps with similar purposes. The review process is crucial in the sense that it reduces market risks and provides inspiration for app development. However, manual exploration of hundreds of existing apps by different roles (e.g., product manager, UI/UX designer, developer) in a development team can be ineffective. For example, it is difficult to completely explore all the functionalities of the app in a short period of time. Inspired by the conception of storyboard in movie production, we propose a system, StoryDroid, to automatically generate the storyboard for Android apps, and assist different roles to review apps efficiently. Specifically, StoryDroid extracts the activity transition graph and leverages static analysis techniques to render UI pages to visualize the storyboard with the rendered pages. The mapping relations between UI pages and the corresponding implementation code (e.g., layout code, activity code, and method hierarchy) are also provided to users. Our comprehensive experiments unveil that StoryDroid is effective and indeed useful to assist app development. The outputs of StoryDroid enable several potential applications, such as the recommendation of UI design and layout code.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8812043,Android app;Storyboard;Competitive analysis;App review,Layout;Google;User interfaces;Semantics;Motion pictures;Production;Visualization,Android (operating system);graphical user interfaces;mobile computing;program diagnostics,storyboard;Android apps;mobile apps;StoryDroid;activity transition graph;UI pages;automated generation;static analysis techniques,50
346,Not Mentioned,Statistical Algorithmic Profiling for Randomized Approximate Programs,K. Joshi; V. Fernando; S. Misailovic,University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign; University of Illinois at Urbana-Champaign,2019,"Many modern applications require low-latency processing of large data sets, often by using approximate algorithms that trade accuracy of the results for faster execution or reduced memory consumption. Although the algorithms provide probabilistic accuracy and performance guarantees, a software developer who implements these algorithms has little support from existing tools. Standard profilers do not consider accuracy of the computation and do not check whether the outputs of these programs satisfy their accuracy specifications. We present AXPROF, an algorithmic profiling framework for analyzing randomized approximate programs. The developer provides the accuracy specification as a formula in a mathematical notation, using probability or expected value predicates. AXPROF automatically generates statistical reasoning code. It first constructs the empirical models of accuracy, time, and memory consumption. It then selects and runs appropriate statistical tests that can, with high confidence, determine if the implementation satisfies the specification. We used AXPROF to profile 15 approximate applications from three domains - data analytics, numerical linear algebra, and approximate computing. AXPROF was effective in finding bugs and identifying various performance optimizations. In particular, we discovered five previously unknown bugs in the implementations of the algorithms and created fixes, guided by AXPROF.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811971,profiler;randomized algorithms,Approximation algorithms;Computer bugs;Software algorithms;Hash functions;Probabilistic logic;Heuristic algorithms;Memory management,approximation theory;data analysis;linear algebra;optimisation;probability;program verification;randomised algorithms;statistical testing,AXPROF;statistical reasoning code;memory consumption;approximate computing;statistical algorithmic profiling;randomized approximate programs;low-latency processing;data sets;approximate algorithms;probabilistic accuracy;software developer;statistical tests;data analytics;bugs finding;performance optimizations;linear algebra;mathematical notation,11
347,Not Mentioned,Safe Automated Refactoring for Intelligent Parallelization of Java 8 Streams,R. Khatchadourian; Y. Tang; M. Bagherzadeh; S. Ahmed,CUNY Hunter College; CUNY Graduate Center; Oakland University; Oakland University,2019,"Streaming APIs are becoming more pervasive in mainstream Object-Oriented programming languages. For example, the Stream API introduced in Java 8 allows for functional-like, MapReduce-style operations in processing both finite and infinite data structures. However, using this API efficiently involves subtle considerations like determining when it is best for stream operations to run in parallel, when running operations in parallel can be less efficient, and when it is safe to run in parallel due to possible lambda expression side-effects. In this paper, we present an automated refactoring approach that assists developers in writing efficient stream code in a semantics-preserving fashion. The approach, based on a novel data ordering and typestate analysis, consists of preconditions for automatically determining when it is safe and possibly advantageous to convert sequential streams to parallel and unorder or de-parallelize already parallel streams. The approach was implemented as a plug-in to the Eclipse IDE, uses the WALA and SAFE analysis frameworks, and was evaluated on 11 Java projects consisting of ?642K lines of code. We found that 57 of 157 candidate streams (36.31%) were refactorable, and an average speedup of 3.49 on performance tests was observed. The results indicate that the approach is useful in optimizing stream code to their full potential.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8811925,refactoring;static analysis;automatic parallelization;typestate analysis;Java 8;streams,Java;Pipelines;Writing;Data structures;Semantics;Image color analysis;Instruction sets,application program interfaces;data structures;Java;software maintenance,semantics-preserving fashion;typestate analysis;sequential streams;parallel streams;optimizing stream code;safe automated refactoring;intelligent parallelization;Java 8;Stream API;MapReduce-style operations;finite data structures;infinite data structures;stream operations;running operations;automated refactoring approach;lambda expression side-effects;stream code;mainstream object-oriented programming languages,12
348,Continuous Integration,Learning-to-Rank vs Ranking-to-Learn: Strategies for Regression Testing in Continuous Integration,A. Bertolino; A. Guerriero; B. Miranda; R. Pietrantuono; S. Russo,"ISTI - CNR, Pisa, Italy; UniversitÃ di Napoli Federico II, Napoli, Italy; Federal University of Pernambuco, Recife, Brazil; UniversitÃ di Napoli Federico II, Napoli, Italy; UniversitÃ di Napoli Federico II, Napoli, Italy",2020,"In Continuous Integration (CI), regression testing is constrained by the time between commits. This demands for careful selection and/or prioritization of test cases within test suites too large to be run entirely. To this aim, some Machine Learning (ML) techniques have been proposed, as an alternative to deterministic approaches. Two broad strategies for ML-based prioritization are learning-to-rank and what we call ranking-to-learn (i.e., reinforcement learning). Various ML algorithms can be applied in each strategy. In this paper we introduce ten of such algorithms for adoption in CI practices, and perform a comprehensive study comparing them against each other using subjects from the Apache Commons project. We analyze the influence of several features of the code under test and of the test process. The results allow to draw criteria to support testers in selecting and tuning the technique that best fits their context.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283979,regression testing;test selection;test prioritization;continuous integration;machine learning,Machine learning algorithms;Software algorithms;Reinforcement learning;Time factors;Tuning;Testing;Software engineering,learning (artificial intelligence);program testing;regression analysis,learning-to-rank;ranking-to-learn;regression testing;continuous integration;test suites;broad strategies;ML-based prioritization;reinforcement learning;ML algorithms;CI practices;test process,
349,Continuous Integration,A Cost-efficient Approach to Building in Continuous Integration,X. Jin; F. Servant,"Department of Computer Science, Virginia Tech, Blacksburg, USA; Department of Computer Science, Virginia Tech, Blacksburg, USA",2020,"Continuous integration (CI) is a widely used practice in modern software engineering. Unfortunately, it is also an expensive practice - Google and Mozilla estimate their CI systems in millions of dollars. In this paper, we propose a novel approach for reducing the cost of CI. The cost of CI lies in the computing power to run builds and its value mostly lies on letting developers find bugs early - when their size is still small. Thus, we target reducing the number of builds that CI executes by still executing as many failing builds as early as possible. To achieve this goal, we propose SmartBuildSkip, a technique which predicts the first builds in a sequence of build failures and the remaining build failures separately. SmartBuildSkip is customizable, allowing developers to select different preferred trade-offs of saving many builds vs. observing build failures early. We evaluate the motivating hypothesis of SmartBuildSkip, its prediction power, and its cost savings in a realistic scenario. In its most conservative configuration, SmartBuildSkip saved a median 30% of builds by only incurring a median delay of 1 build in a median of 15% failing builds.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284054,continuous integration;build prediction;maintenance cost,Computer bugs;Buildings;Internet;Delays;Software engineering,embedded systems;program testing;software development management;software engineering,cost-efficient approach;continuous integration;widely used practice;modern software engineering;expensive practice - Google;CI systems;computing power;letting developers;CI executes;remaining build failures;SmartBuildSkip;different preferred trade-offs;observing build failures;prediction power;cost savings,
350,Continuous Integration,Practical Fault Detection in Puppet Programs,T. Sotiropoulos; D. Mitropoulos; D. Spinellis,"Athens University of Economics and Business; Athens University of Economics and Business, National Infrastructures for Research and Technology â€“ GRNET; Athens University of Economics and Business",2020,"Puppet is a popular computer system configuration management tool. By providing abstractions that model system resources it allows administrators to set up computer systems in a reliable, predictable, and documented fashion. Its use suffers from two potential pitfalls. First, if ordering constraints are not correctly specified whenever a Puppet resource depends on another, the nondeterministic application of resources can lead to race conditions and consequent failures. Second, if a service is not tied to its resources (through the notification construct), the system may operate in a stale state whenever a resource gets modified. Such faults can degrade a computing infrastructure's availability and functionality. We have developed an approach that identifies these issues through the analysis of a Puppet program and its system call trace. Specifically, a formal model for traces allows us to capture the interactions of Puppet resources with the file system. By analyzing these interactions we identify (1) resources that are related to each other (e.g., operate on the same file), and (2) resources that should act as notifiers so that changes are correctly propagated. We then check the relationships from the trace's analysis against the program's dependency graph: a representation containing all the ordering constraints and notifications declared in the program. If a mismatch is detected, our system reports a potential fault. We have evaluated our method on a large set of popular Puppet modules, and discovered 92 previously unknown issues in 33 modules. Performance benchmarking shows that our approach can analyze in seconds real-world configurations with a magnitude measured in thousands of lines and millions of system calls.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284083,Puppet;Ordering Relationships;Notifiers;Program Analysis;System Calls,Fault diagnosis;Computational modeling;Operating systems;Tools;Programming;Predictive models;Software engineering,configuration management;data visualisation;fault diagnosis;formal specification;graph theory;program compilers;program diagnostics;programming environments,"notification;computing infrastructure;Puppet program;formal model;Puppet resource;file system;trace;ordering constraints;potential fault;popular Puppet modules;system calls;practical fault detection;popular computer system configuration management tool;model system resources;computer systems;reliable documented fashion;predictable, documented fashion;potential pitfalls",
351,Continuous Integration,"Learning from, Understanding, and Supporting DevOps Artifacts for Docker",J. Henkel; C. Bird; S. K. Lahiri; T. Reps,"University of Wisconsin-Madison, USA; Microsoft Research, USA; Microsoft Research, USA; University of Wisconsin-Madison, USA",2020,"With the growing use of DevOps tools and frameworks, there is an increased need for tools and techniques that support more than code. The current state-of-the-art in static developer assistance for tools like Docker is limited to shallow syntactic validation. We identify three core challenges in the realm of learning from, understanding, and supporting developers writing DevOps artifacts: (i) nested languages in DevOps artifacts, (ii) rule mining, and (iii) the lack of semantic rule-based analysis. To address these challenges we introduce a toolset, binnacle, that enabled us to ingest 900,000 GitHub repositories. Focusing on Docker, we extracted approximately 178,000 unique Dockerfiles, and also identified a Gold Set of Dockerfiles written by Docker experts. We addressed challenge (i) by reducing the number of effectively uninterpretable nodes in our ASTs by over 80% via a technique we call phased parsing. To address challenge (ii), we introduced a novel rule-mining technique capable of recovering two-thirds of the rules in a benchmark we curated. Through this automated mining, we were able to recover 16 new rules that were not found during manual rule collection. To address challenge (iii), we manually collected a set of rules for Dockerfiles from commits to the files in the Gold Set. These rules encapsulate best practices, avoid docker build failures, and improve image size and build latency. We created an analyzer that used these rules, and found that, on average, Dockerfiles on GitHub violated the rules five times more frequently than the Dockerfiles in our Gold Set. We also found that industrial Dockerfiles fared no better than those sourced from GitHub. The learned rules and analyzer in binnacle can be used to aid developers in the IDE when creating Dockerfiles, and in a post-hoc fashion to identify issues in, and to improve, existing Dockerfiles.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284129,Docker;DevOps;Mining;Static Checking,Gold;Ecosystems;Tools;Maintenance engineering;Encoding;Data mining;Software development management,data mining;learning (artificial intelligence);software engineering,DevOps artifacts;DevOps tools;static developer assistance;shallow syntactic validation;core challenges;supporting developers;rule mining;semantic rule-based analysis;900 GitHub repositories;000 GitHub repositories;approximately 178 Dockerfiles;000 unique Dockerfiles;Gold Set;Docker experts;address challenge;novel rule-mining technique;automated mining;manual rule collection;docker build failures;industrial Dockerfiles;learned rules;creating Dockerfiles;existing Dockerfiles,13
352,Cyber-Physical Systems,Adapting Requirements Models to Varying Environments,D. Alrajeh; A. Cailliau; A. van Lamsweerde,"Department of Computing, Imperial College London, UK; ICTEAM, UCLouvain, Belgium; ICTEAM, UCLouvain, Belgium",2020,"The engineering of high-quality software requirements generally relies on properties and assumptions about the environment in which the software-to-be has to operate. Such properties and assumptions, referred to as environment conditions in this paper, are highly subject to change over time or from one software variant to another. As a consequence, the requirements engineered for a specific set of environment conditions may no longer be adequate, complete and consistent for another set. The paper addresses this problem through a tool-supported requirements adaptation technique. A goal-oriented requirements modelling framework is considered to make requirements' refinements and dependencies on environment conditions explicit. When environment conditions change, an adapted goal model is computed that is correct with respect to the new environment conditions. The space of possible adaptations is not fixed a priori; the required changes are expected to meet one or more environment-independent goal(s) to be satisfied in any version of the system. The adapted goal model is generated using a new counterexample-guided learning procedure that ensures the correctness of the updated goal model, and prefers more local adaptations and more similar goal models.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284116,Requirements adaptation;requirements evolution;context-dependent requirements;formal verification;logic-based learning,Adaptation models;Computational modeling;Software;Software engineering,formal specification;formal verification;learning (artificial intelligence);software engineering;systems analysis,adapting requirements models;varying environments;high-quality software requirements;software variant;tool-supported requirements adaptation technique;goal-oriented requirements;environment conditions explicit;adapted goal model;required changes;environment-independent goal;updated goal model;similar goal models,11
353,Cyber-Physical Systems,Comparing Formal Tools for System Design: a Judgment Study,A. Ferrari; F. Mazzanti; D. Basile; M. H. ter Beek; A. Fantechi,"ISTIâ€“CNR, Pisa, Italy; ISTIâ€“CNR, Pisa, Italy; ISTIâ€“CNR, Pisa, Italy; ISTIâ€“CNR, Pisa, Italy; University of Florence, Florence, Italy",2020,"Formal methods and tools have a long history of successful applications in the design of safety-critical railway products. However, most of the experiences focused on the application of a single method at once, and little work has been performed to compare the applicability of the different available frameworks to the railway context. As a result, companies willing to introduce formal methods in their development process have little guidance on the selection of tools that could fit their needs. To address this goal, this paper presents a comparison between 9 different formal tools, namely Atelier B, CADP, FDR4, NuSMV, ProB, Simulink, SPIN, UMC, and UPPAAL SMC. We performed a judgment study, involving 17 experts with experience in formal methods applied to railways. In the study, part of the experts were required to model a railway signaling problem (a moving-block train distancing system) with the different tools, and to provide feedback on their experience. The information produced was then synthesized, and the results were validated by the remaining experts. Based on the outcome of this process, we provide a synthesis that describes when to use a certain tool, and what are the problems that may be faced by modelers. Our experience shows that the different tools serve different purposes, and multiple formal methods are required to fully cover the needs of the railway system design process.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284073,formal methods;formal tools;empirical software engineering;judgment study;empirical formal methods;railway;moving-block system;formal methods diversity;human aspects of formal design,Systematics;Companies;Tools;Rail transportation;Usability;System analysis and design;Software engineering,aerospace computing;formal specification;formal verification;railway safety;railways;safety-critical software,judgment study;safety-critical railway products;different available frameworks;railway context;9 different formal tools;railway signaling problem;moving-block train distancing system;multiple formal methods;railway system design process,
354,Debugging 1,Debugging Inputs,L. Kirschner; E. Soremekun; A. Zeller,"CISPA - Helmholtz Center for Information Security, SaarbrÃ¼cken, Germany; CISPA - Helmholtz Center for Information Security, SaarbrÃ¼cken, Germany; CISPA - Helmholtz Center for Information Security, SaarbrÃ¼cken, Germany",2020,"When a program fails to process an input, it need not be the program code that is at fault. It can also be that the input data is faulty, for instance as result of data corruption. To get the data processed, one then has to debug the input data-that is, (1) identify which parts of the input data prevent processing, and (2) recover as much of the (valuable) input data as possible. In this paper, we present a general-purpose algorithm called ddmax that addresses these problems automatically. Through experiments, ddmax maximizes the subset of the input that can still be processed by the program, thus recovering and repairing as much data as possible; the difference between the original failing input and the â€œmaximizedâ€ passing input includes all input fragments that could not be processed. To the best of our knowledge, ddmax is the first approach that fixes faults in the input data without requiring program analysis. In our evaluation, ddmax repaired about 69% of input files and recovered about 78% of data within one minute per input.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284127,debugging;testing;inputs,Debugging;Software engineering,failure analysis;general purpose computers;program debugging;program diagnostics,program analysis;general-purpose algorithm;program code;inputs debugging;input files;input fragments;maximized passing input;data corruption,
355,Debugging 1,Causal Testing: Understanding Defects' Root Causes,B. Johnson; Y. Brun; A. Meliou,"University of Massachusetts Amherst, Amherst, MA, USA; University of Massachusetts Amherst, Amherst, MA, USA; University of Massachusetts Amherst, Amherst, MA, USA",2020,"Understanding the root cause of a defect is critical to isolating and repairing buggy behavior. We present Causal Testing, a new method of root-cause analysis that relies on the theory of counterfactual causality to identify a set of executions that likely hold key causal information necessary to understand and repair buggy behavior. Using the Defects4J benchmark, we find that Causal Testing could be applied to 71% of real-world defects, and for 77% of those, it can help developers identify the root cause of the defect. A controlled experiment with 37 developers shows that Causal Testing improves participants' ability to identify the cause of the defect from 80% of the time with standard testing tools to 86% of the time with Causal Testing. The participants report that Causal Testing provides useful information they cannot get using tools such as JUnit. Holmes, our prototype, open-source Eclipse plugin implementation of Causal Testing, is available at http://holmes.cs.umass.edu/.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284101,Causal Testing;causality;theory of counterfactual causality;software debugging;test fuzzing;automated test generation;Holmes,Prototypes;Tools;Maintenance engineering;Standards;Open source software;Testing;Software engineering,biology computing;causality;failure analysis;graph theory;molecular configurations;program debugging;program testing;public domain software;software maintenance;software quality,Causal Testing;Defects' root causes;buggy behavior;root-cause analysis;key causal information,
356,Ecosystems and Evolution,Impact Analysis of Cross-Project Bugs on Software Ecosystems,W. Ma; L. Chen; X. Zhang; Y. Feng; Z. Xu; Z. Chen; Y. Zhou; B. Xu,"State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China; State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China; Purdue University, West Lafayette, USA; Nanjing University, Nanjing, China; Nanjing University, Nanjing, China; Nanjing University, Nanjing, China; State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China; State Key Lab. for Novel Software Technology, Nanjing University, Nanjing, China",2020,"Software projects are increasingly forming social-technical ecosystems within which individual projects rely on the infrastructures or functional components provided by other projects, leading to complex inter-dependencies. Through inter-project dependencies, a bug in an upstream project may have profound impact on a large number of downstream projects, resulting in cross-project bugs. This emerging type of bugs has brought new challenges in bug fixing due to their unclear influence on downstream projects. In this paper, we present an approach to estimating the impact of a cross-project bug within its ecosystem by identifying the affected downstream modules (classes/methods). Note that a downstream project that uses a buggy upstream function may not be affected as the usage does not satisfy the failure inducing preconditions. For a reported bug with the known root cause function and failure inducing preconditions, we first collect the candidate downstream modules that call the upstream function through an ecosystem-wide dependence analysis. Then, the paths to the call sites of the buggy upstream function are encoded as symbolic constraints. Solving the constraints, together with the failure inducing preconditions, identifies the affected downstream modules. Our evaluation of 31 existing upstream bugs on the scientific Python ecosystem containing 121 versions of 22 popular projects (with a total of 16 millions LOC) shows that the approach is highly effective: from the 25490 candidate downstream modules that invoke the buggy upstream functions, it identifies 1132 modules where the upstream bugs can be triggered, pruning 95.6% of the candidates. The technique has no false negatives and an average false positive rate of 7.9%. Only 49 downstream modules (out of the 1132 we found) were reported before to be affected.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284011,Software Ecosystems;Cross-project Bugs;Bug Impact;Dependence Analysis;Symbolic Constraints,Computer bugs;Ecosystems;Software;Python;Software engineering,program debugging;public domain software;software engineering,cross-project bug;software ecosystems;software projects;inter-project dependencies;upstream project;downstream project;downstream modules;ecosystem-wide dependence analysis;upstream bugs;impact analysis,
357,Ecosystems and Evolution,Taming Behavioral Backward Incompatibilities via Cross-Project Testing and Analysis,L. Chen; F. Hassan; X. Wang; L. Zhang,The University of Texas at Dallas; The University of Texas at San Antonio; The University of Texas at San Antonio; The University of Texas at Dallas,2020,"In modern software development, software libraries play a crucial role in reducing software development effort and improving software quality. However, at the same time, the asynchronous upgrades of software libraries and client software projects often result in incompatibilities between different versions of libraries and client projects. When libraries evolve, it is often very challenging for library developers to maintain the so-called backward compatibility and keep all their external behavior untouched, and behavioral backward incompatibilities (BBIs) may occur. In practice, the regression test suites of library projects often fail to detect all BBIs. Therefore, in this paper, we propose DeBBI to detect BBIs via cross-project testing and analysis, i.e., using the test suites of various client projects to detect library BBIs. Since executing all the possible client projects can be extremely time consuming, DeBBI transforms the problem of cross-project BBI detection into a traditional information retrieval (IR) problem to execute the client projects with higher probability to detect BBIs earlier. Furthermore, DeBBI considers project diversity and test relevance information for even faster BBI detection. The experimental results show that DeBBI can reduce the end-to-end testing time for detecting the first and average unique BBIs by 99.1% and 70.8% for JDK compared to naive cross-project BBI detection. Also, DeBBI has been applied to other popular 3rd-party libraries. To date, DeBBI has detected 97 BBI bugs with 19 already confirmed as previously unknown bugs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283994,,Software libraries;Computer bugs;Transforms;Software quality;Information retrieval;Testing,information retrieval;Java;program testing;regression analysis;software engineering;software libraries;software maintenance;software quality,library projects;DeBBI;cross-project testing;library BBIs;possible client projects;project diversity;test relevance information;faster BBI detection;end-to-end testing time;first BBIs;average unique BBIs;naive cross-project BBI detection;3rd-party libraries;behavioral backward incompatibilities;modern software development;software libraries;software development effort;improving software quality;client software projects;library developers;backward compatibility;external behavior untouched;regression test suites,
358,Ecosystems and Evolution,Watchman: Monitoring Dependency Conflicts for Python Library Ecosystem,Y. Wang; M. Wen; Y. Liu; Y. Wang; Z. Li; C. Wang; H. Yu; S. -C. Cheung; C. Xu; Z. Zhu,"Software College, Northeastern University, China; School of Cyber Science and Engineering, HUST, China; Department of Computer Science and Engineering, SUSTech, China; Software College, Northeastern University, China; Software College, Northeastern University, China; Software College, Northeastern University, China; Software College, Northeastern University, China; Department of Computer Science and Engineering, HKUST, China; Department of Computer Science and Technology, State Key Lab for Novel Software Technology, Nanjing University, China; Software College, Northeastern University, China",2020,"The PyPI ecosystem has indexed millions of Python libraries to allow developers to automatically download and install dependencies of their projects based on the specified version constraints. Despite the convenience brought by automation, version constraints in Python projects can easily conflict, resulting in build failures. We refer to such conflicts as Dependency Conflict (DC) issues. Although DC issues are common in Python projects, developers lack tool support to gain a comprehensive knowledge for diagnosing the root causes of these issues. In this paper, we conducted an empirical study on 235 real-world DC issues. We studied the manifestation patterns and fixing strategies of these issues and found several key factors that can lead to DC issues and their regressions. Based on our findings, we designed and implemented Watchman, a technique to continuously monitor dependency conflicts for the PyPI ecosystem. In our evaluation, Watchman analyzed PyPI snapshots between 11 Jul 2019 and 16 Aug 2019, and found 117 potential DC issues. We reported these issues to the developers of the corresponding projects. So far, 63 issues have been confirmed, 38 of which have been quickly fixed by applying our suggested patches.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284138,Python;dependency conflicts;software ecosystem,Automation;Ecosystems;Tools;Libraries;Monitoring;Python;Software engineering,public domain software;software engineering;software tools,Watchman;monitoring Dependency conflicts;Python library ecosystem;PyPI ecosystem;Python libraries;specified version constraints;Python projects;build failures;Dependency Conflict issues;tool support;real-world DC issues;manifestation patterns;fixing strategies;PyPI snapshots;117 potential DC issues;corresponding projects,15
359,Empirical Studies for Security,One Size Does Not Fit All: A Grounded Theory and Online Survey Study of Developer Preferences for Security Warning Types,A. Danilova; A. Naiakshina; M. Smith,"University of Bonn; University of Bonn; University of Bonn, Fraunhofer FKIE",2020,"A wide range of tools exist to assist developers in creating secure software. Many of these tools, such as static analysis engines or security checkers included in compilers, use warnings to communicate security issues to developers. The effectiveness of these tools relies on developers heeding these warnings, and there are many ways in which these warnings could be displayed. Johnson et al. [46] conducted qualitative research and found that warning presentation and integration are main issues. We built on Johnson et al.'s work and examined what developers want from security warnings, including what form they should take and how they should integrate into their workflow and work context. To this end, we conducted a Grounded Theory study with 14 professional software developers and 12 computer science students as well as a focus group with 7 academic researchers to gather qualitative insights. To back up the theory developed from the qualitative research, we ran a quantitative survey with 50 professional software developers. Our results show that there is significant heterogeneity amongst developers and that no one warning type is preferred over all others. The context in which the warnings are shown is also highly relevant, indicating that it is likely to be beneficial if IDEs and other development tools become more flexible in their warning interactions with developers. Based on our findings, we provide concrete recommendations for both future research as well as how IDEs and other security tools can improve their interaction with developers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284069,developer security warnings;software development;code security,Static analysis;Organizations;Machine learning;Tools;Software;Security;Software engineering,program diagnostics;security of data;software engineering,security checkers;security issues;qualitative research;security warnings;Grounded Theory study;14 professional software developers;50 professional software developers;warning type;development tools;warning interactions;security tools;developer preferences;security warning types;secure software;static analysis engines,
360,Empirical Studies for Security,SchrÃ¶dinger's Security: Opening the Box on App Developers' Security Rationale,D. van der Linden; P. Anthonysamy; B. Nuseibeh; T. T. Tun; M. Petre; M. Levine; J. Towse; A. Rashid,"University of Bristol; Google; The Open University Lero, University of Limerick; The Open University; The Open University; Lancaster University; Lancaster University; University of Bristol",2020,"Research has established the wide variety of security failures in mobile apps, their consequences, and how app developers introduce or exacerbate them. What is not well known is why developers do so-what is the rationale underpinning the decisions they make which eventually strengthen or weaken app security? This is all the more complicated in modern app development's increasingly diverse demographic: growing numbers of independent, solo, or small team developers who do not have the organizational structures and support that larger software development houses enjoy. Through two studies, we open the box on developer rationale, by performing a holistic analysis of the rationale underpinning various activities in which app developers engage when developing an app. The first study does so through a task-based study with app developers ( N=44) incorporating six distinct tasks for which this developer demographic must take responsibility: setting up a development environment, reviewing code, seeking help, seeking testers, selecting an advertisement SDK, and software licensing. We found that, while on first glance in several activities participants seemed to prioritize security, only in the code task such prioritization was underpinned by a security rationale-indicating that development behavior perceived to be secure may only be an illusion until the box is opened on their rationale. The second study confirms these findings through a wider survey of app developers ( N=274) investigating to what extent they find the activities of the task-based study to affect their app's security. In line with the task-based study, we found that developers perceived actively writing code and actively using external SDKs as the only security-relevant, while similarly disregarding other activities having an impact on app security. Our results suggest the need for a stronger focus on the tasks and activities surrounding the coding task - all of which need to be underpinned by a security rationale. Without such a holistic focus, developers may write â€œsecure codeâ€ but not produce â€œsecure appsâ€.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283987,,Writing;Licenses;Software;Mobile applications;Security;Task analysis;Software engineering,mobile computing;organisational aspects;public domain software;security of data;software engineering,secure apps;schrÃ¶dinger;security failures;mobile apps;app developers introduce;app security;modern app development;team developers;larger software development houses;developer rationale;task-based study;developer demographic;development environment;security rationale-indicating;development behavior;security-relevant,
361,Human Practice,How Software Practitioners Use Informal Local Meetups to Share Software Engineering Knowledge,C. Ingram; A. Drachen,"Department of Computer Science, University of York, York, UK; Department of Computer Science, University of York, York, UK",2020,"Informal technology `meetups' have become an important aspect of the software development community, engaging many thousands of practitioners on a regular basis. However, although local technology meetups are well-attended by developers, little is known about their motivations for participating, the type or usefulness of information that they acquire, and how local meetups might differ from and complement other available communication channels for software engineering information. We interviewed the leaders of technology-oriented Meetup groups, and collected quantitative information via a survey distributed to participants in technology-oriented groups. Our findings suggest that participants in these groups are primarily experienced software practitioners, who use Meetup for staying abreast of new developments, building local networks and achieving transfer of rich tacit knowledge with peers to improve their practice. We also suggest that face to face meetings are useful forums for exchanging tacit knowledge and contextual information needed for software engineering practice.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283934,Informal networks;Meetup;developer communities;knowledge sharing;tacit knowledge,Knowledge engineering;Meetings;Communication channels;Software;Faces;Software engineering,knowledge management;social networking (online),informal local meetups;software engineering knowledge;informal technology meetups;software development community;local technology meetups;communication channels;software engineering information;technology-oriented Meetup groups;quantitative information;technology-oriented groups;local networks;contextual information;software engineering,
362,Human Practice,Predicting Developers' Negative Feelings about Code Review,C. D. Egelman; E. Murphy-Hill; E. Kammer; M. M. Hodges; C. Green; C. Jaspan; J. Lin,Google; Google; Google; Google; Google; Google; Google,2020,"During code review, developers critically examine each others' code to improve its quality, share knowledge, and ensure conformance to coding standards. In the process, developers may have negative interpersonal interactions with their peers, which can lead to frustration and stress; these negative interactions may ultimately result in developers abandoning projects. In this mixed-methods study at one company, we surveyed 1,317 developers to characterize the negative experiences and cross-referenced the results with objective data from code review logs to predict these experiences. Our results suggest that such negative experiences, which we call â€œpushbackâ€, are relatively rare in practice, but have negative repercussions when they occur. Our metrics can predict feelings of pushback with high recall but low precision, making them potentially appropriate for highlighting interactions that may benefit from a self-intervention.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283927,code review;interpersonal conflict,Measurement;Standards organizations;Companies;Encoding;Stress;Monitoring;Software engineering,organisational aspects;project management;software engineering,negative experiences;code review logs;negative repercussions;share knowledge;coding standards;negative interpersonal interactions;frustration;stress,
363,Web Testing,Near-Duplicate Detection in Web App Model Inference,R. Yandrapally; A. Stocco; A. Mesbah,"University of British Columbia, Vancouver, BC, Canada; UniversitÃ della Svizzera italiana, Lugano, Switzerland; University of British Columbia, Vancouver, BC, Canada",2020,"Automated web testing techniques infer models from a given web app, which are used for test generation. From a testing viewpoint, such an inferred model should contain the minimal set of states that are distinct, yet, adequately cover the app's main functionalities. In practice, models inferred automatically are affected by near-duplicates, i.e., replicas of the same functional webpage differing only by small insignificant changes. We present the first study of near-duplicate detection algorithms used in within app model inference. We first characterize functional near-duplicates by classifying a random sample of state-pairs, from 493k pairs of webpages obtained from over 6,000 websites, into three categories, namely clone, near-duplicate, and distinct. We systematically compute thresholds that define the boundaries of these categories for each detection technique. We then use these thresholds to evaluate 10 near-duplicate detection techniques from three different domains, namely, information retrieval, web testing, and computer vision on nine open-source web apps. Our study highlights the challenges posed in automatically inferring a model for any given web app. Our findings show that even with the best thresholds, no algorithm is able to accurately detect all functional near-duplicates within apps, without sacrificing coverage.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284089,near-duplicate detection;reverse engineering;model-based testing,Computational modeling;Software algorithms;Information retrieval;Test pattern generators;Open source software;Testing;Software engineering,Internet;program testing;Web sites,functional near-duplicates;test generation;testing viewpoint;inferred model;near-duplicate detection algorithms;state-pairs;open-source Web apps;Web app model inference;automated Web testing techniques;functional Webpage;information retrieval;computer vision,
364,Web Testing,Extracting Taint Specifications for JavaScript Libraries,C. -A. Staicu; M. T. Torp; M. SchÃ¤fer; A. MÃ¸ller; M. Pradel,TU Darmstadt; Aarhus University; GitHub; Aarhus University; University of Stuttgart,2020,"Modern JavaScript applications extensively depend on third-party libraries. Especially for the Node.js platform, vulnerabilities can have severe consequences to the security of applications, resulting in, e.g., cross-site scripting and command injection attacks. Existing static analysis tools that have been developed to automatically detect such issues are either too coarse-grained, looking only at package dependency structure while ignoring dataflow, or rely on manually written taint specifications for the most popular libraries to ensure analysis scalability. In this work, we propose a technique for automatically extracting taint specifications for JavaScript libraries, based on a dynamic analysis that leverages the existing test suites of the libraries and their available clients in the npm repository. Due to the dynamic nature of JavaScript, mapping observations from dynamic analysis to taint specifications that fit into a static analysis is non-trivial. Our main insight is that this challenge can be addressed by a combination of an access path mechanism that identifies entry and exit points, and the use of membranes around the libraries of interest. We show that our approach is effective at inferring useful taint specifications at scale. Our prototype tool automatically extracts 146 additional taint sinks and 7 840 propagation summaries spanning 1 393 npm modules. By integrating the extracted specifications into a commercial, state-of-the-art static analysis, 136 new alerts are produced, many of which correspond to likely security vulnerabilities. Moreover, many important specifications that were originally manually written are among the ones that our tool can now extract automatically.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284099,taint analysis;static analysis;dynamic analysis,Static analysis;Tools;Writing;Libraries;Security;Testing;Software engineering,authoring languages;program diagnostics;program testing;security of data,test suites;dynamic analysis;security vulnerabilities;JavaScript libraries;third-party libraries;Node.js platform;cross-site scripting;command injection attacks;static analysis tools;package dependency structure;taint specifications;taint sinks;JavaScript applications;npm repository;exit points;entry points;access path mechanism,
365,Web Testing,SLACC: Simion-based Language Agnostic Code Clones,G. Mathew; C. Parnin; K. T. Stolee,North Carolina State University; North Carolina State University; North Carolina State University,2020,"Successful cross-language clone detection could enable researchers and developers to create robust language migration tools, facilitate learning additional programming languages once one is mastered, and promote reuse of code snippets over a broader codebase. However, identifying cross-language clones presents special challenges to the clone detection problem. A lack of common underlying representation between arbitrary languages means detecting clones requires one of the following solutions: 1) a static analysis framework replicated across each targeted language with annotations matching language features across all languages, or 2) a dynamic analysis framework that detects clones based on runtime behavior. In this work, we demonstrate the feasibility of the latter solution, a dynamic analysis approach called SLACC for cross-language clone detection. Like prior clone detection techniques, we use input/output behavior to match clones, though we overcome limitations of prior work by amplifying the number of inputs and covering more data types; and as a result, achieve better clusters than prior attempts. Since clusters are generated based on input/output behavior, SLACC supports cross-language clone detection. As an added challenge, we target a static typed language, Java, and a dynamic typed language, Python. Compared to HitoshiIO, a recent clone detection tool for Java, SLACC retrieves 6 times as many clusters and has higher precision (86.7% vs. 30.7%). This is the first work to perform clone detection for dynamic typed languages (precision = 87.3%) and the first to perform clone detection across languages that lack a common underlying representation (precision = 94.1%). It provides a first step towards the larger goal of scalable language migration tools.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283951,semantic code clone detection;cross-language analysis,Java;Runtime;Semantics;Cloning;Static analysis;Tools;Python,Java;learning (artificial intelligence);program diagnostics;programming languages;public domain software;reverse engineering;software libraries;software maintenance,SLACC;simion-based language agnostic code clones;successful cross-language clone detection;robust language migration tools;additional programming languages;cross-language clones;clone detection problem;common underlying representation;arbitrary languages;targeted language;language features;dynamic analysis framework;prior clone detection techniques;static typed language;dynamic typed language;recent clone detection tool;scalable language migration tools,
366,Web Testing,Finding Client-side Business Flow Tampering Vulnerabilities,I. L. Kim; Y. Zheng; H. Park; W. Wang; W. You; Y. Aafer; X. Zhang,"Department of Computer Science, Purdue University, West Lafayette, Indiana, USA; IBM T. J. Watson Research Center, Yorktown Heights, USA; University at Buffalo, SUNY, Buffalo, USA; Renmin University of China, Beijing, China; Department of Computer Science, Purdue University, West Lafayette, USA; University at Buffalo, SUNY, Buffalo, USA; University at Buffalo, SUNY, Buffalo, USA",2020,"The sheer complexity of web applications leaves open a large attack surface of business logic. Particularly, in some scenarios, developers have to expose a portion of the logic to the client-side in order to coordinate multiple parties (e.g. merchants, client users, and third-party payment services) involved in a business process. However, such client-side code can be tampered with on the fly, leading to business logic perturbations and financial loss. Although developers become familiar with concepts that the client should never be trusted, given the size and the complexity of the client-side code that may be even incorporated from third parties, it is extremely challenging to understand and pinpoint the vulnerability. To this end, we investigate client-side business flow tampering vulnerabilities and develop a dynamic analysis based approach to automatically identifying such vulnerabilities. We evaluate our technique on 200 popular real-world websites. With negligible overhead, we have successfully identified 27 unique vulnerabilities on 23 websites, such as New York Times, HBO, and YouTube, where an adversary can interrupt business logic to bypass paywalls, disable adblocker detection, earn reward points illicitly, etc.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284014,JavaScript;vulnerability detection;business flow tampering;dynamic analysis,Measurement;Perturbation methods;Complexity theory;Surface treatment;Business;Videos;Software engineering,Internet;security of data;Web sites,client-side business flow tampering vulnerabilities;sheer complexity;web applications;open a large attack surface;multiple parties;client users;payment services;business process;client-side code;business logic perturbations;vulnerability;27 unique vulnerabilities,
367,Analysis for Security,Securing UnSafe Rust Programs with XRust,P. Liu; G. Zhao; J. Huang,"Texas A&M University, College Station, Texas, U.S.A; Texas A&M University, College Station, Texas, U.S.A; Texas A&M University, College Station, Texas, U.S.A",2020,"Rust is a promising systems programming language that embraces both high-level memory safety and low-level resource manipulation. However, the dark side of Rust, unsafe Rust, leaves a large security hole as it bypasses the Rust type system in order to support low-level operations. Recently, several real-world memory corruption vulnerabilities have been discovered in Rust's standard libraries. We present XRust, a new technique that mitigates the security threat of unsafe Rust by ensuring the integrity of data flow from unsafe Rust code to safe Rust code. The cornerstone of XRust is a novel heap allocator that isolates the memory of unsafe Rust from that accessed only in safe Rust, and prevents any cross-region memory corruption. Our design of XRust supports both single- and multi-threaded Rust programs. Our extensive experiments on real-world Rust applications and standard libraries show that XRust is both highly efficient and effective in practice.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283982,,Computer languages;Runtime;Libraries;Safety;Security;Standards;Stress,C++ language;multi-threading;program diagnostics;program verification;security of data;storage management,UnSafe Rust programs;XRust;high-level memory safety;low-level resource manipulation;security hole;Rust type system;low-level operations;real-world memory corruption vulnerabilities;Rust's standard libraries;unsafe Rust code;safe Rust code;cross-region memory corruption;multithreaded Rust programs;real-world Rust applications,
368,Analysis for Security,Is Rust Used Safely by Software Developers?,A. N. Evans; B. Campbell; M. L. Soffa,University of Virginia; University of Virginia; University of Virginia,2020,"Rust, an emerging programming language with explosive growth, provides a robust type system that enables programmers to write memory-safe and data-race free code. To allow access to a machine's hardware and to support low-level performance optimizations, a second language, Unsafe Rust, is embedded in Rust. It contains support for operations that are difficult to statically check, such as C-style pointers for access to arbitrary memory locations and mutable global variables. When a program uses these features, the compiler is unable to statically guarantee the safety properties Rust promotes. In this work, we perform a large-scale empirical study to explore how software developers are using Unsafe Rust in real-world Rust libraries and applications. Our results indicate that software engineers use the keyword unsafe in less than 30% of Rust libraries, but more than half cannot be entirely statically checked by the Rust compiler because of Unsafe Rust hidden somewhere in a library's call chain. We conclude that although the use of the keyword unsafe is limited, the propagation of unsafeness offers a challenge to the claim of Rust as a memory-safe language. Furthermore, we recommend changes to the Rust compiler and to the central Rust repository's interface to help Rust software developers be aware of when their Rust code is unsafe.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283950,,Program processors;Software;Libraries;Hardware;Safety;Optimization;Software engineering,constraint handling;program compilers;program diagnostics;programming language semantics;software engineering;type theory,emerging programming language;robust type system;data-race free code;support low-level performance optimizations;Unsafe Rust;arbitrary memory locations;safety properties Rust;real-world Rust libraries;keyword unsafe;Rust compiler;memory-safe language;central Rust repository;Rust software developers;Rust code,
369,Analysis for Security,Burn After Reading: A Shadow Stack with Microsecond-level Runtime Rerandomization for Protecting Return Addresses,C. Zou; J. Xue,"School of Computer Science and Engineering, UNSW Sydney, Australia; School of Computer Science and Engineering, UNSW Sydney, Australia",2020,"Return-oriented programming (ROP) is an effective code-reuse attack in which short code sequences (i.e., gadgets) ending in a ret instruction are found within existing binaries and then executed by taking control of the call stack. The shadow stack, control flow integrity (CFI) and code (re)randomization are three popular techniques for protecting programs against return address overwrites. However, existing runtime rerandomization techniques operate on concrete return addresses, requiring expensive pointer tracking. By adding one level of indirection, we introduce BarRA, the first shadow stack mechanism that applies continuous runtime rerandomization to abstract return addresses for protecting their corresponding concrete return addresses (protected also by CFI), thus avoiding expensive pointer tracking. As a nice side-effect, BarRA naturally combines the shadow stack, CFI and runtime rerandomization in the same framework. The key novelty of BarRA, however, is that once some abstract return addresses are leaked, BarRA will enforce the burn-after-reading property by rerandomizing the mapping from the abstract to the concrete return address space in the order of microseconds instead of seconds required for rerandomizing a concrete return address space. As a result, BarRA can be used as a superior replacement for the shadow stack, as demonstrated by comparing both using the 19 C/C++ benchmarks in SPEC CPU2006 (totalling 2,047,447 LOC) and analyzing a proof-of-concept attack, provided that we can tolerate some slight binary code size increases (by an average of 29.44%) and are willing to use 8MB of dedicated memory for holding up to 220 return addresses (on a 64-bit platform). Under an information leakage attack (for some return addresses), the shadow stack is always vulnerable but BarRA is significantly more resilient (by reducing an attacker's success rate to $\frac{1}{2^{20}}$ on average). In terms of the average performance overhead introduced, both are comparable: 6.09% (BarRA) vs. 5.38% (the shadow stack).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283966,Shadow Stack;Runtime Rerandomization;Control Flow Integrity;Return-Oriented Programming,Runtime;Binary codes;Programming;Benchmark testing,binary codes;instruction sets;program compilers;program diagnostics;security of data;storage management,microsecond-level runtime rerandomization;return-oriented programming;code-reuse attack;short code sequences;CFI;expensive pointer tracking;BarRA;shadow stack mechanism;continuous runtime rerandomization;abstract return addresses;concrete return address space;runtime rerandomization techniques;ROP;ret instruction;control flow integrity;C++ benchmarks;C benchmarks;SPEC CPU2006;proof-of-concept attack;binary code;information leakage attack,
370,Analysis for Security,"SAVER: Scalable, Precise, and Safe Memory-Error Repair",S. Hong; J. Lee; J. Lee; H. Oh,Korea University Republic of Korea; Korea University Republic of Korea; Korea University Republic of Korea; Korea University Republic of Korea,2020,"We present SAVER, a new memory-error repair technique for C programs. Memory errors such as memory leak, double-free, and use-after-free are highly prevalent and fixing them requires significant effort. Automated program repair techniques hold the promise of reducing this burden but the state-of-the-art is still unsatisfactory. In particular, no existing techniques are able to fix those errors in a scalable, precise, and safe way, all of which are required for a truly practical tool. SAVER aims to address these shortcomings. To this end, we propose a method based on a novel representation of the program called object flow graph, which summarizes the program's heap-related behavior using static analysis. We show that fixing memory errors can be formulated as a graph labeling problem over object flow graph and present an efficient algorithm. We evaluated SAVER in combination with Infer, an industrial-strength static bug-finder, and show that 74% of the reported errors can be fixed automatically for a range of open-source C programs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284096,Program Repair;Program Analysis;Memory Errors;Debugging,Memory management;Static analysis;Maintenance engineering;Tools;Flow graphs;Open source software;Software engineering,graph theory;program debugging;program diagnostics;program verification;software maintenance,"open-source C programs;reported errors;object flow graph;truly practical tool;automated program repair techniques;memory leak;memory errors;memory-error repair technique;safe memory-error repair;precise, memory-error repair;scalable memory-error repair;SAVER",3
371,Android and Web Application Testing,Revealing Injection Vulnerabilities by Leveraging Existing Tests,K. Hough; G. Welearegai; C. Hammer; J. Bell,"George Mason University, Fairfax, VA, USA; University of Potsdam, Potsdam, Germany; University of Potsdam, Potsdam, Germany; George Mason University, Fairfax, VA, USA",2020,"Code injection attacks, like the one used in the high-profile 2017 Equifax breach, have become increasingly common, now ranking #1 on OWASP's list of critical web application vulnerabilities. Static analyses for detecting these vulnerabilities can overwhelm developers with false positive reports. Meanwhile, most dynamic analyses rely on detecting vulnerabilities as they occur in the field, which can introduce a high performance overhead in production code. This paper describes a new approach for detecting injection vulnerabilities in applications by harnessing the combined power of human developers' test suites and automated dynamic analysis. Our new approach, Rivulet, monitors the execution of developer-written functional tests in order to detect information flows that may be vulnerable to attack. Then, Rivulet uses a white-box test generation technique to repurpose those functional tests to check if any vulnerable flow could be exploited. When applied to the version of Apache Struts exploited in the 2017 Equifax attack, Rivulet quickly identifies the vulnerability, leveraging only the tests that existed in Struts at that time. We compared Rivulet to the state-of-the-art static vulnerability detector Julia on benchmarks, finding that Rivulet outperformed Julia in both false positives and false negatives. We also used Rivulet to detect new vulnerabilities.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284056,injection attacks;vulnerability testing;taint tracking,Pathology;Static analysis;Production;Benchmark testing;Test pattern generators;Open source software;Software engineering,Internet;program testing;security of data;system monitoring,injection vulnerabilities;code injection attacks;high-profile 2017 Equifax breach;static analyses;false positive reports;dynamic analyses;production code;human developers;automated dynamic analysis;Rivulet;developer-written functional tests;white-box test generation technique;vulnerable flow;Julia static vulnerability detector;Web application vulnerabilities,
372,Android and Web Application Testing,RoScript: A Visual Script Driven Truly Non-Intrusive Robotic Testing System for Touch Screen Applications,J. Qian; Z. Shang; S. Yan; Y. Wang; L. Chen,"MIIT Key Laboratory of Safety-Critical Software, Nanjing University of Aeronautics and Astronautics, Nanjing, China; Nanjing University of Aeronautics and Astronautics, Nanjing, China; Nanjing University of Aeronautics and Astronautics, Nanjing, China; Nanjing University of Aeronautics and Astronautics, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",2020,"Existing intrusive test automation techniques for touch screen applications (e.g., Appium and Sikuli) are difficult to work on many closed or uncommon systems, such as a GoPro. Being non-intrusive can largely extend the application scope of the test automation techniques. To this end, this paper presents RoScript, a truly non- intrusive test-script-driven robotic testing system for test automation of touch screen applications. RoScript leverages visual test scripts to express GUI actions on a touch screen application and uses a physical robot to drive automated test execution. To reduce the test script creation cost, a non-intrusive computer vision based technique is also introduced in RoScript to automatically record touch screen actions into test scripts from videos of human actions on the device under test. RoScript is applicable to touch screen applications running on almost arbitrary platforms, whatever the underlying operating systems or GUI frameworks are. We conducted experiments applying it to automate the testing of 21 touch screen applications on 6 different devices. The results show that RoScript is highly usable. In the experiments, it successfully automated 104 test scenarios containing over 650 different GUI actions on the subject applications. RoScript accurately performed GUI actions on over 90% of the test script executions and accurately recorded about 85% of human screen click actions into test code.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283959,test automation;non-intrusive;robot;GUI testing;computer vision,Visualization;Automation;Touch sensitive screens;Graphical user interfaces;Testing;Videos;Software engineering,authoring languages;computer vision;control engineering computing;graphical user interfaces;operating systems (computers);program testing;robot programming;touch sensitive screens,intrusive test automation techniques;visual script driven truly nonintrusive robotic testing system;test script executions;104 test scenarios;21 touch screen applications;automatically record touch screen actions;nonintrusive computer vision based technique;test script creation cost;automated test execution;RoScript leverages visual test scripts;non intrusive test-script-driven robotic testing system;touch screen application,
373,Android and Web Application Testing,Translating Video Recordings of Mobile App Usages into Replayable Scenarios,C. Bernal-CÃ¡rdenas; N. Cooper; K. Moran; O. Chaparro; A. Marcus; D. Poshyvanyk,"William & Mary, Williamsburg, Virginia, USA; William & Mary, Williamsburg, Virginia, USA; William & Mary, Williamsburg, Virginia, USA; William & Mary, Williamsburg, Virginia, USA; The University of Texas at Dallas, Dallas, Texas, USA; William & Mary, Williamsburg, Virginia, USA",2020,"Screen recordings of mobile applications are easy to obtain and capture a wealth of information pertinent to software developers (e.g., bugs or feature requests), making them a popular mechanism for crowdsourced app feedback. Thus, these videos are becoming a common artifact that developers must manage. In light of unique mobile development constraints, including swift release cycles and rapidly evolving platforms, automated techniques for analyzing all types of rich software artifacts provide benefit to mobile developers. Unfortunately, automatically analyzing screen recordings presents serious challenges, due to their graphical nature, compared to other types of (textual) artifacts. To address these challenges, this paper introduces V2S, a lightweight, automated approach for translating video recordings of Android app usages into replayable scenarios. V2S is based primarily on computer vision techniques and adapts recent solutions for object detection and image classification to detect and classify user actions captured in a video, and convert these into a replayable test scenario. We performed an extensive evaluation of V2S involving 175 videos depicting 3,534 GUI-based actions collected from users exercising features and reproducing bugs from over 80 popular Android apps. Our results illustrate that V2S can accurately replay scenarios from screen recordings, and is capable of reproducing â‰ˆ 89% of our collected videos with minimal overhead. A case study with three industrial partners illustrates the potential usefulness of V2S from the viewpoint of developers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283997,Bug Reporting;Screen Recordings;Object Detection,Computer bugs;Object detection;Software;Mobile applications;Videos;Software engineering;Image classification,Android (operating system);computer vision;graphical user interfaces;image classification;mobile computing;object detection;program debugging;program testing;video recording,rich software artifacts;mobile developers;screen recordings;V2S;lightweight approach;automated approach;video recordings;Android app usages;replayable scenarios;computer vision techniques;adapts recent solutions;object detection;image classification;replayable test scenario;reproducing bugs;80 popular Android apps;collected videos;mobile app usages;mobile applications;software developers;feature requests;popular mechanism;crowdsourced app feedback;common artifact;unique mobile development constraints;swift release cycles;automated techniques,
374,Android and Web Application Testing,Unblind Your Apps: Predicting Natural-Language Labels for Mobile GUI Components by Deep Learning,J. Chen; C. Chen; Z. Xing; X. Xu; L. Zhut; G. Li; J. Wang,"Australian National University, Australia; Monash University, Australia; Australian National University, Australia; Data61, CSIRO, Australia; Australian National University, Australia; Shanghai Jiao Tong University, China; Fujian University of Technology, China",2020,"According to the World Health Organization(WHO), it is estimated that approximately 1.3 billion people live with some forms of vision impairment globally, of whom 36 million are blind. Due to their disability, engaging these minority into the society is a challenging problem. The recent rise of smart mobile phones provides a new solution by enabling blind users' convenient access to the information and service for understanding the world. Users with vision impairment can adopt the screen reader embedded in the mobile operating systems to read the content of each screen within the app, and use gestures to interact with the phone. However, the prerequisite of using screen readers is that developers have to add natural-language labels to the image-based components when they are developing the app. Unfortunately, more than 77% apps have issues of missing labels, according to our analysis of 10,408 Android apps. Most of these issues are caused by developers' lack of awareness and knowledge in considering the minority. And even if developers want to add the labels to UI components, they may not come up with concise and clear description as most of them are of no visual issues. To overcome these challenges, we develop a deep-learning based model, called Labeldroid, to automatically predict the labels of image-based buttons by learning from large-scale commercial apps in Google Play. The experimental results show thatour model can make accurate predictions and the generated labels are of higher quality than that from real Android developers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284063,Accessibility;neural networks;user interface;image-based buttons;content description,Visualization;Operating systems;Predictive models;Internet;Smart phones;Software engineering;Graphical user interfaces,Android (operating system);deep learning (artificial intelligence);graphical user interfaces;handicapped aids;image processing;mobile computing;mobile handsets;natural language processing;smart phones,deep learning;world health organization;vision impairment;smart mobile phones;blind users;screen reader;mobile operating systems;image-based components;UI components;visual issues;deep-learning based model;image-based buttons;large-scale commercial apps;mobile GUI components;natural-language labels;Android apps,
375,Autonomous Driven System,SLEMI: Equivalence Modulo Input (EMI) Based Mutation of CPS Models for Finding Compiler Bugs in Simulink,S. A. Chowdhury; S. L. Shrestha; T. T. Johnson; C. Csallner,"Computer Science and Engineering Department, University of Texas at Arlington, Arlington, Texas, USA; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, Texas, USA; EECS Department, Vanderbilt University, Nashville, Tennessee, USA; Computer Science and Engineering Department, University of Texas at Arlington, Arlington, Texas, USA",2020,"Finding bugs in commercial cyber-physical system development tools (or â€œmodel-based designâ€ tools) such as MathWorks's Simulink is important in practice, as these tools are widely used to generate embedded code that gets deployed in safety-critical applications such as cars and planes. Equivalence Modulo Input (EMI) based mutation is a new twist on differential testing that promises lower use of computational resources and has already been successful at finding bugs in compilers for procedural languages. To provide EMI-based mutation for differential testing of cyber-physical system (CPS) development tools, this paper develops several novel mutation techniques. These techniques deal with CPS language features that are not found in procedural languages, such as an explicit notion of execution time and zombie code, which combines properties of live and dead procedural code. In our experiments the most closely related work (SLforge) found two bugs in the Simulink tool. In comparison, SLEMI found a super-set of issues, including 9 confirmed as bugs by MathWorks Support.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283988,Cyber-physical systems;differential testing;equivalence modulo input;model mutation;Simulink,Software packages;Computer bugs;Electromagnetic interference;Tools;Cyber-physical systems;Automobiles;Testing,electromagnetic interference;embedded systems;optimising compilers;program compilers;program debugging;program testing,EMI-based mutation;differential testing;mutation techniques;CPS language features;procedural languages;Simulink tool;equivalence Modulo Input based mutation;CPS models;finding compiler bugs;commercial cyber-physical system development tools;MathWorks's Simulink;Equivalence Modulo Input based mutation,
376,Autonomous Driven System,DeepBillboard: Systematic Physical-World Testing of Autonomous Driving Systems,H. Zhou; W. Li; Z. Kong; J. Guo; Y. Zhang; B. Yu; L. Zhang; C. Liu,"The University of Texas at Dallas, Dallas, USA; Southern University of Science and Technology, Shenzhen, China; The University of Texas at Dallas, Dallas, USA; The University of Texas at Dallas, Dallas, USA; Southern University of Science and Technology, Shenzhen, China; The Chinese University of Hong Kong, Hong Kong, China; The University of Texas at Dallas, Dallas, USA; The University of Texas at Dallas, Dallas, USA",2020,"Deep Neural Networks (DNNs) have been widely applied in autonomous systems such as self-driving vehicles. Recently, DNN testing has been intensively studied to automatically generate adversarial examples, which inject small-magnitude perturbations into inputs to test DNNs under extreme situations. While existing testing techniques prove to be effective, particularly for autonomous driving, they mostly focus on generating digital adversarial perturbations, e.g., changing image pixels, which may never happen in the physical world. Thus, there is a critical missing piece in the literature on autonomous driving testing: understanding and exploiting both digital and physical adversarial perturbation generation for impacting steering decisions. In this paper, we propose a systematic physical-world testing approach, namely DeepBillboard, targeting at a quite common and practical driving scenario: drive-by billboards. DeepBillboard is capable of generating a robust and resilient printable adversarial billboard test, which works under dynamic changing driving conditions including viewing angle, distance, and lighting. The objective is to maximize the possibility, degree, and duration of the steering-angle errors of an autonomous vehicle driving by our generated adversarial billboard. We have extensively evaluated the efficacy and robustness of DeepBillboard by conducting both experiments with digital perturbations and physical-world case studies. The digital experimental results show that DeepBillboard is effective for various steering models and scenes. Furthermore, the physical case studies demonstrate that DeepBillboard is sufficiently robust and resilient for generating physical-world adversarial billboard tests for real-world driving under various weather conditions, being able to mislead the average steering angle error up to 26.44 degrees. To the best of our knowledge, this is the first study demonstrating the possibility of generating realistic and continuous physical-world tests for practical autonomous driving systems; moreover, DeepBillboard can be directly generalized to a variety of other physical entities/surfaces along the curbside, e.g., a graffiti painted on a wall.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283977,Autonomous Driving;Adversarial Machine Learning;Steering Model Testing,Systematics;Perturbation methods;Robustness;Vehicle dynamics;Autonomous vehicles;Testing;Software engineering,neural nets;optimisation;road safety;road vehicles;steering systems;traffic engineering computing,digital perturbations;generated adversarial billboard;autonomous vehicle;steering-angle errors;dynamic changing driving conditions;resilient printable adversarial billboard test;robust adversarial billboard test;physical-world testing approach;physical adversarial perturbation generation;digital adversarial perturbation generation;autonomous driving testing;physical world;testing techniques;small-magnitude perturbations;DNN testing;autonomous systems;deep neural networks;autonomous driving systems;physical-world tests;average steering angle error;physical-world adversarial billboard tests;DeepBillboard;digital experimental results;physical-world case studies,
377,Autonomous Driven System,Misbehaviour Prediction for Autonomous Driving Systems,A. Stocco; M. Weiss; M. Calzana; P. Tonella,"UniversitÃ¡ della Svizzera italiana, Lugano, Switzerland; UniversitÃ¡ della Svizzera italiana, Lugano, Switzerland; UniversitÃ¡ della Svizzera italiana, Lugano, Switzerland; UniversitÃ¡ della Svizzera italiana, Lugano, Switzerland",2020,"Deep Neural Networks (DNNs) are the core component of modern autonomous driving systems. To date, it is still unrealistic that a DNN will generalize correctly to all driving conditions. Current testing techniques consist of offline solutions that identify adversarial or corner cases for improving the training phase. In this paper, we address the problem of estimating the confidence of DNNs in response to unexpected execution contexts with the purpose of predicting potential safety-critical misbehaviours and enabling online healing of DNN-based vehicles. Our approach SelfOracle is based on a novel concept of self-assessment oracle, which monitors the DNN confidence at runtime, to predict unsupported driving scenarios in advance. SelfOracle uses autoencoder- and time series-based anomaly detection to reconstruct the driving scenarios seen by the car, and to determine the confidence boundary between normal and unsupported conditions. In our empirical assessment, we evaluated the effectiveness of different variants of SelfOracle at predicting injected anomalous driving contexts, using DNN models and simulation environment from Udacity. Results show that, overall, SelfOracle can predict 77% misbehaviours, up to six seconds in advance, outperforming the online input validation approach of DeepRoad.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284027,misbehaviour prediction;testing;deep learning;anomaly detection,Training;Runtime;Neural networks;Predictive models;Autonomous vehicles;Context modeling;Testing,learning (artificial intelligence);mobile robots;neural nets;road vehicles;safety-critical software;time series,offline solutions;unexpected execution contexts;safety-critical misbehaviours;online healing;DNN-based vehicles;self-assessment oracle;time series-based anomaly detection;normal conditions;empirical assessment;anomalous driving contexts;DNN models;misbehaviour prediction;deep neural networks;autonomous driving systems,
378,Autonomous Driven System,Approximation-Refinement Testing of Compute-Intensive Cyber-Physical Models: An Approach Based on System Identification,C. Menghi; S. Nejati; L. Briand; Y. I. Parache,"University of Luxembourg, Luxembourg, Luxembourg; University of Luxembourg, Luxembourg, Luxembourg; University of Luxembourg, Luxembourg, Luxembourg; Luxspace SÃ rl, Luxembourg, Luxembourg",2020,"Black-box testing has been extensively applied to test models of Cyber-Physical systems (CPS) since these models are not often amenable to static and symbolic testing and verification. Black-box testing, however, requires to execute the model under test for a large number of candidate test inputs. This poses a challenge for a large and practically-important category of CPS models, known as compute-intensive CPS (CI-CPS) models, where a single simulation may take hours to complete. We propose a novel approach, namely ARIsTEO, to enable effective and efficient testing of CI-CPS models. Our approach embeds black-box testing into an iterative approximation-refinement loop. At the start, some sampled inputs and outputs of the CI-CPS model under test are used to generate a surrogate model that is faster to execute and can be subjected to black-box testing. Any failure-revealing test identified for the surrogate model is checked on the original model. If spurious, the test results are used to refine the surrogate model to be tested again. Otherwise, the test reveals a valid failure. We evaluated ARIsTEO by comparing it with S-Taliro, an open-source and industry-strength tool for testing CPS models. Our results, obtained based on five publicly-available CPS models, show that, on average, ARIsTEO is able to find 24% more requirements violations than S-Taliro and is 31% faster than S-Taliro in finding those violations. We further assessed the effectiveness and efficiency of ARIsTEO on a large industrial case study from the satellite domain. In contrast to S-Taliro, ARIsTEO successfully tested two different versions of this model and could identify three requirements violations, requiring four hours, on average, for each violation.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283957,Cyber-Physical Systems;Model Testing;Search-Based Testing;Robustness;Falsification,Satellites;Software packages;Computational modeling;Tools;System identification;Testing;Software engineering,approximation theory;formal verification;iterative methods;object-oriented languages;program testing;software fault tolerance,compute-intensive Cyber-Physical models;test models;Cyber-Physical systems;static testing;symbolic testing;candidate test inputs;compute-intensive CPS models;ARIsTEO;effective testing;efficient testing;CI-CPS model;approach embeds black-box testing;surrogate model;failure-revealing test;testing CPS models;publicly-available CPS models;approximation-refinement testing,
379,Autonomous Driven System,A Comprehensive Study of Autonomous Vehicle Bugs,J. Garcia; Y. Feng; J. Shen; S. Almanee; Y. Xia; Q. A. Chen,"University of California, Irvine, California, USA; Nanjing University, Nanjing, China; University of California, Irvine, California, USA; University of California, Irvine, California, USA; University of California, Irvine, California, USA; University of California, Irvine, California, USA",2020,"Self-driving cars, or Autonomous Vehicles (AVs), are increasingly becoming an integral part of our daily life. About 50 corporations are actively working on AVs, including large companies such as Google, Ford, and Intel. Some AVs are already operating on public roads, with at least one unfortunate fatality recently on record. As a result, understanding bugs in AVs is critical for ensuring their security, safety, robustness, and correctness. While previous studies have focused on a variety of domains (e.g., numerical software; machine learning; and error-handling, concurrency, and performance bugs) to investigate bug characteristics, AVs have not been studied in a similar manner. Recently, two software systems for AVs, Baidu Apollo and Autoware, have emerged as frontrunners in the open-source community and have been used by large companies and governments (e.g., Lincoln, Volvo, Ford, Intel, Hitachi, LG, and the US Department of Transportation). From these two leading AV software systems, this paper describes our investigation of 16,851 commits and 499 AV bugs and introduces our classification of those bugs into 13 root causes, 20 bug symptoms, and 18 categories of software components those bugs often affect. We identify 16 major findings from our study and draw broader lessons from them to guide the research community towards future directions in software bug detection, localization, and repair.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284001,bugs;defects;autonomous vehicles;empirical software engineering,Computer bugs;Companies;Maintenance engineering;Software systems;Open source software;Autonomous vehicles;Software engineering,mobile robots;pattern classification;program debugging;public domain software;security of data,open-source community;AV bugs;bug symptoms;software components;AV software systems;self-driving cars;autonomous vehicle bugs;bug characteristics;performance bugs;software bug detection,
380,Debugging 2,Studying the Use of Java Logging Utilities in the Wild,B. Chen; Z. M. Jiang,"York University, Toronto, Canada; York University, Toronto, Canada",2020,"Software logging is widely used in practice. Logs have been used for a variety of purposes like debugging, monitoring, security compliance, and business analytics. Instead of directly invoking the standard output functions, developers usually prefer to use logging utilities (LUs) (e.g., SLF4J), which provide additional functionalities like thread-safety and verbosity level support, to instrument their source code. Many of the previous research works on software logging are focused on the log printing code. There are very few works studying the use of LUs, although new LUs are constantly being introduced by companies and researchers. In this paper, we conducted a large-scale empirical study on the use of Java LUs in the wild. We analyzed the use of 3,856 LUs from 11, 194 projects in GitHub and found that many projects have complex usage patterns for LUs. For example, 75.8% of the large-sized projects have implemented their own LUs in their projects. More than 50% of these projects use at least three LUs. We conducted further qualitative studies to better understand and characterize the complex use of LUs. Our findings show that different LUs are used for a variety of reasons (e.g., internationalization of the log messages). Some projects develop their own LUs to satisfy project-specific logging needs (e.g., defining the logging format). Multiple uses of LUs in one project are pretty common for large and very large-sized projects mainly for context like enabling and configuring the logging behavior for the imported packages. Interviewing with 13 industrial developers showed that our findings are also generally true for industrial projects and are considered as very helpful for them to better configure and manage the logging behavior for their projects. The findings and the implications presented in this paper will be useful for developers and researchers who are interested in developing and maintaining LUs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284132,empirical software engineering;logging code;logging practices,Printing;Java;Software;Security;Standards;Software development management;Software engineering,business data processing;Java;program debugging;program diagnostics;program testing;public domain software;security of data;system monitoring,Java logging utilities;software logging;log printing code;Java LUs;3 LUs;856 LUs;large-sized projects;project-specific logging needs;logging behavior,6
381,Human Aspects of Software Engineering 1,"A Study on the Prevalence of Human Values in Software Engineering Publications, 2015 - 2018",H. Perera; W. Hussain; J. Whittle; A. Nurwidyantoro; D. Mougouei; R. A. Shams; G. Oliver,"Monash University, Clayton, Australia; Monash University, Clayton, Australia; Monash University, Clayton, Australia; Monash University, Clayton, Australia; Monash University, Clayton, Australia; Monash University, Clayton, Australia; Monash University, Clayton, Australia",2020,"Failure to account for human values in software (e.g., equality and fairness) can result in user dissatisfaction and negative socioeconomic impact. Engineering these values in software, however, requires technical and methodological support throughout the development life cycle. This paper investigates to what extent top Software Engineering (SE) conferences and journals have included research on human values in SE. We investigate the prevalence of human values in recent (2015 - 2018) publications in these top venues. We classify these publications, based on their relevance to different values, against a widely used value structure adopted from the social sciences. Our results show that: (a) only a small proportion of the publications directly consider values, classified as directly relevant publications; (b) for the majority of the values, very few or no directly relevant publications were found; and (c) the prevalence of directly relevant publications was higher in SE conferences compared to SE journals. This paper shares these and other insights that may motivate future research on human values in software engineering.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284125,Human Values;Software Engineering;Paper Classification,Social sciences;Software;Software engineering,social aspects of automation;social sciences;software engineering,human values;value structure;software engineering publications;user dissatisfaction;negative socioeconomic impact;SE journals,16
382,Human Aspects of Software Engineering 1,Explaining Pair Programming Session Dynamics from Knowledge Gaps,F. Zieris; L. Prechelt,"Freie UniverstitÃ¤t Berlin, Berlin, Germany; Freie UniverstitÃ¤t Berlin, Berlin, Germany",2020,"Background: Despite a lot of research on the effectiveness of Pair Programming (PP), the question when it is useful or less useful remains unsettled. Method: We analyze recordings of many industrial PP sessions with Grounded Theory Methodology and build on prior work that identified various phenomena related to within-session knowledge build-up and transfer. We validate our findings with practitioners. Result: We identify two fundamentally different types of required knowledge and explain how different constellations of knowledge gaps in these two respects lead to different session dynamics. Gaps in project-specific systems knowledge are more hampering than gaps in general programming knowledge and are dealt with first and foremost in a PP session. Conclusion: Partner constellations with complementary knowledge make PP a particularly effective practice. In PP sessions, differences in system understanding are more important than differences in general software development knowledge.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284068,,Knowledge engineering;Programming;Software;Dynamic programming;Software engineering,computer science education;groupware;knowledge management;programming;software engineering;software prototyping,pair programming session dynamics;knowledge gaps;industrial PP sessions;Grounded Theory Methodology;within-session knowledge;fundamentally different types;required knowledge;different constellations;different session dynamics;project-specific systems knowledge;general programming knowledge;PP session;complementary knowledge;particularly effective practice;general software development knowledge,
383,Human Aspects of Software Engineering 1,Engineering Gender-Inclusivity into Software: Ten Teams' Tales from the Trenches,C. Hilderbrand; C. Perdriau; L. Letaw; J. Emard; Z. Steine-Hanson; M. Burnett; A. Sarma,"Pacific Northwest National Laboratory, Richland, WA, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA",2020,"Although the need for gender-inclusivity in software is gaining attention among SE researchers and SE practitioners, and at least one method (GenderMag) has been published to help, little has been reported on how to make such methods work in real-world settings. Real-world teams are ever-mindful of the practicalities of adding new methods on top of their existing processes. For example, how can they keep the time costs viable? How can they maximize impacts of using it? What about controversies that can arise in talking about gender? To find out how software teams â€œin the trenchesâ€ handle these and similar questions, we collected the GenderMag-based processes of 10 real-world software teams-more than 50 people-for periods ranging from 5 months to 3.5 years. We present these teams' insights and experiences in the form of 9 practices, 2 potential pitfalls, and 2 open issues, so as to provide their insights to other real-world software teams trying to engineer gender-inclusivity into their software products.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283992,Inclusive software;software engineering practices;GenderMag,Systematics;Instruments;Inspection;Software;Distance measurement;Software engineering,gender issues;software engineering,engineering gender-inclusivity;SE researchers;SE practitioners;GenderMag-based processes;real-world software teams;software products;ten team tales,
384,Version Control and Programming Practice,How Has Forking Changed in the Last 20 Years? A Study of Hard Forks on GitHub,S. Zhou; B. Vasilescu; C. KÃ¤stner,"Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA",2020,"The notion of forking has changed with the rise of distributed version control systems and social coding environments, like GitHub. Traditionally forking refers to splitting off an independent development branch (which we call hard forks); research on hard forks, conducted mostly in pre-GitHub days showed that hard forks were often seen critical as they may fragment a community. Today, in social coding environments, open-source developers are encouraged to fork a project in order to contribute to the community (which we call social forks), which may have also influenced perceptions and practices around hard forks. To revisit hard forks, we identify, study, and classify 15,306 hard forks on GitHub and interview 18 owners of hard forks or forked repositories. We find that, among others, hard forks often evolve out of social forks rather than being planned deliberately and that perception about hard forks have indeed changed dramatically, seeing them often as a positive non-competitive alternative to the original project.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284038,Fork based development;Open source;Github;Hard fork,Control systems;Encoding;Interviews;Open source software;Software development management;Software engineering,,,
385,Android Application Testing,Multiple-Entry Testing of Android Applications by Constructing Activity Launching Contexts,J. Yan; H. Liu; L. Pan; J. Yan; J. Zhang; B. Liang,"Tech. Center of Softw. Eng. Institute of Software, CAS, China Univ. of Chinese Academy of Sciences, Beijing, China; Dept. of Informatics, Beijing University of Tech., China, Beijing, China; State Key Lab. of Computer Science Institute of Software, CAS, China Univ. of Chinese Academy of Sciences, Beijing, China; State Key Lab. of Computer Science Institute of Software, CAS, China Univ. of Chinese Academy of Sciences, Beijing, China; State Key Lab. of Computer Science Institute of Software, CAS, China Univ. of Chinese Academy of Sciences, Beijing, China; School of Information, Renmin University of China, Beijing, China",2020,"Existing GUI testing approaches of Android apps usually test apps from a single entry. In this way, the marginal activities far away from the default entry are difficult to be covered. The marginal activities may fail to be launched due to requiring a great number of activity transitions or involving complex user operations, leading to uneven coverage on activity components. Besides, since the test space of GUI programs is infinite, it is difficult to test activities under complete launching contexts using single-entry testing approaches. In this paper, we address these issues by constructing activity launching contexts and proposing a multiple-entry testing framework. We perform an inter-procedural, flow-, context- and path-sensitive analysis to build activity launching models and generate complete launching contexts. By activity exposing and static analysis, we could launch activities directly under various contexts without performing long event sequence on GUI. Besides, to achieve an in-depth exploration, we design an adaptive exploration framework which supports the multiple-entry exploration and dynamically assigns weights to entries in each turn. Our approach is implemented in a tool called Fax, with an activity launching strategy Faxla and an exploration strategy Faxex. The experiments on 20 real-world apps show that Faxla can cover 96.4% and successfully launch 60.6% activities, based on which Faxex further achieves a relatively 19.7% improvement on method coverage compared with the most popular tool Monkey. Our tool also behaves well in revealing hidden bugs. Fax can trigger over seven hundred unique crashes, including 180 Errors and 539 Warnings, which is significantly higher than those of other tools. Among the 46 bugs reported to developers on Github, 33 have been fixed up to now.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284007,Android app;Static Analysis;ICC;Multiple-Entry Testing,Analytical models;Adaptation models;Computer bugs;Tools;Testing;Graphical user interfaces;Context modeling,graphical user interfaces;mobile computing;program debugging;program diagnostics;program testing,Android applications;constructing activity launching contexts;GUI testing approaches;Android apps;single entry;marginal activities;default entry;activity transitions;involving complex user operations;activity components;test space;GUI programs;complete launching contexts;single-entry testing approaches;multiple-entry testing framework;path-sensitive analysis;activity launching models;static analysis;adaptive exploration framework;multiple-entry exploration;activity launching strategy Fax;exploration strategy Fax;real-world apps,
386,Android Application Testing,ComboDroid: Generating High-Quality Test Inputs for Android Apps via Use Case Combinations,J. Wang; Y. Jiang; C. Xu; C. Cao; X. Ma; J. Lu,"Department of Computer Science and Technology, State Key Lab for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Lab for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Lab for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Lab for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Lab for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Lab for Novel Software Technology, Nanjing University, Nanjing, China",2020,"Android apps demand high-quality test inputs, whose generation remains an open challenge. Existing techniques fall short on exploring complex app functionalities reachable only by a long, meaningful, and effective test input. Observing that such test inputs can usually be decomposed into relatively independent short use cases, this paper presents ComboDroid, a fundamentally different Android app testing framework. ComboDroid obtains use cases for manifesting a specific app functionality (either manually provided or automatically extracted), and systematically enumerates the combinations of use cases, yielding high-quality test inputs. The evaluation results of ComboDroid on real-world apps are encouraging. Our fully automatic variant outperformed the best existing technique APE by covering 4.6% more code (APE only outperformed Monkey by 2.1%), and revealed four previously unknown bugs in extensively tested subjects. Our semi-automatic variant boosts the manual use cases obtained with little manual labor, achieving a comparable coverage (only 3.2% less) with a white-box human testing expert.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283926,Software testing;mobile apps,Computer bugs;Manuals;Software;Testing;Software engineering,mobile computing;program debugging;program testing,use case combinations;app functionality;APE;white-box human testing expert;high-quality test inputs;ComboDroid;Android app testing framework,
387,Android Application Testing,Time-travel Testing of Android Apps,Z. Dong; M. BÃ¶hme; L. Cojocaru; A. Roychoudhury,"National University of Singapore; Monash University, Australia; Politehnica University of Bucharest; National University of Singapore",2020,"Android testing tools generate sequences of input events to exercise the state space of the app-under-test. Existing search-based techniques systematically evolve a population of event sequences so as to achieve certain objectives such as maximal code coverage. The hope is that the mutation of fit event sequences leads to the generation of even fitter sequences. However, the evolution of event sequences may be ineffective. Our key insight is that pertinent app states which contributed to the original sequence's fitness may not be reached by a mutated event sequence. The original path through the state space is truncated at the point of mutation. In this paper, we propose instead to evolve a population of states which can be captured upon discovery and resumed when needed. The hope is that generating events on a fit program state leads to the transition to even fitter states. For instance, we can quickly deprioritize testing the main screen state which is visited by most event sequences, and instead focus our limited resources on testing more interesting states that are otherwise difficult to reach. We call our approach time-travel testing because of this ability to travel back to any state that has been observed in the past. We implemented time-travel testing into TimeMachine, a time-travel enabled version of the successful, automated Android testing tool Monkey. In our experiments on a large number of open- and closed source Android apps, TimeMachine outperforms the state-of-the-art search-based/model-based Android testing tools Sapienz and Stoat, both in terms of coverage achieved and crashes found. We call our approach time-travel testing because of this ability to travel back to any state that has been observed in the past. We implemented time-travel testing into TimeMachine, a time-travel enabled version of the successful, automated Android testing tool Monkey. In our experiments on a large number of open- and closed source Android apps, TimeMachine outperforms the state-of-the-art search-based/model-based Android testing tools Sapienz and Stoat, both in terms of coverage achieved and crashes found.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284058,Android testing;State based;Time travel testing;Directed fuzzing,Sociology;Tools;Space exploration;Statistics;Virtualization;Testing;Accidents,Android (operating system);mobile computing;program diagnostics;program testing;search problems,fit event sequences;fitter sequences;pertinent app states;mutated event sequence;state space;fit program state;fitter states;main screen state;time-travel testing;automated Android testing tool Monkey;source Android;Android testing tools;input events;app-under-test;search-based techniques;maximal code coverage;event sequences;TimeMachine;time-travel enabled version;closed-source Android apps;open-source Android apps;model-based Android testing tools Sapienz;Stoat,
388,Clones and Changes,HeteroRefactor: Refactoring for Heterogeneous Computing with FPGA,J. Lau; A. Sivaraman; Q. Zhang; M. A. Gulzar; J. Cong; M. Kim,"University of California, Los Angeles, Los Angeles, CA, USA; University of California, Los Angeles, Los Angeles, CA, USA; University of California, Los Angeles, Los Angeles, CA, USA; University of California, Los Angeles; University of California, Los Angeles; University of California, Los Angeles",2020,"Heterogeneous computing with field-programmable gate-arrays (FPGAs) has demonstrated orders of magnitude improvement in computing efficiency for many applications. However, the use of such platforms so far is limited to a small subset of programmers with specialized hardware knowledge. High-level synthesis (HLS) tools made significant progress in raising the level of programming abstraction from hardware programming languages to C/C++, but they usually cannot compile and generate accelerators for kernel programs with pointers, memory management, and recursion, and require manual refactoring to make them HLS-compatible. Besides, experts also need to provide heavily handcrafted optimizations to improve resource efficiency, which afÎ“ects the maximum operating frequency, parallelization, and power efficiency. We propose a new dynamic invariant analysis and automated refactoring technique, called HeteroRefactor. First, HeteroRefactor monitors FPGA-specific dynamic invariants-the required bitwidth of integer and floating-point variables, and the size of recursive data structures and stacks. Second, using this knowledge of dynamic invariants, it refactors the kernel to make traditionally HLS-incompatible programs synthesizable and to optimize the accelerator's resource usage and frequency further. Third, to guarantee correctness, it selectively omoads the computation from CPU to FPGA, only if an input falls within the dynamic invariant. On average, for a recursive program of size 175 LOC, an expert FPGA programmer would need to write 185 more LOC to implement an HLS compatible version, while HETEROREFAcTOR automates such transformation. Our results on Xilinx FPGA show that HETEROREFAcTOR minimizes BRAM by 83% and increases frequency by 42% for recursive programs; reduces BRAM by 41% through integer bitwidth reduction; and reduces DSP by 50% through floating-point precision tuning.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283938,heterogeneous computing;automated refactoring;FPGA;high-level synthesis;dynamic analysis,Dynamic scheduling;Heterogeneous networks;Hardware;Software;Kernel;Field programmable gate arrays;Optimization,data structures;field programmable gate arrays;high level synthesis,recursive data structures;HLS-incompatible programs;accelerator;recursive program;expert FPGA programmer;HLS compatible version;Xilinx FPGA;integer bitwidth reduction;heterogeneous computing;field-programmable gate-arrays;magnitude improvement;specialized hardware knowledge;high-level synthesis;programming abstraction;hardware programming languages;kernel programs;memory management;recursion;manual refactoring;heavily handcrafted optimizations;resource efficiency;maximum operating frequency;power efficiency;dynamic invariant analysis;floating-point variables;FPGA-specific dynamic invariants;HeteroRefactor,
389,Clones and Changes,HARP: Holistic Analysis for Refactoring Python-Based Analytics Programs,W. Zhou; Y. Zhao; G. Zhang; X. Shen,"North Carolina State University, Raleigh, NC; Facebook, Menlo Park, CA; North Carolina State University, Raleigh, NC; North Carolina State University, Raleigh, NC",2020,"Modern machine learning programs are often written in Python, with the main computations specified through calls to some highly optimized libraries (e.g., TensorFlow, PyTorch). How to maximize the computing efficiency of such programs is essential for many application domains, which has drawn lots of recent attention. This work points out a common limitation in existing efforts: they focus their views only on the static computation graphs specified by library APIs, but leave the influence from the hosting Python code largely unconsidered. The limitation often causes them to miss the big picture and hence many important optimization opportunities. This work proposes a new approach named HARP to address the problem. HARP enables holistic analysis that spans across computation graphs and their hosting Python code. HARP achieves it through a set of novel techniques: analytics-conscious speculative analysis to circumvent Python complexities, a unified representation augmented computation graphs to capture all dimensions of knowledge related with the holistic analysis, and conditioned feedback mechanism to allow risk-controlled aggressive analysis. Refactoring based on HARP gives 1.3-3X and 2.07X average speedups on a set of TensorFlow and PyTorch programs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284016,machine learning program;computation graph;dynamic language;program analysis,Runtime;Semantics;Machine learning;Programming;Libraries;Optimization;Python,application program interfaces;data structures;graph theory;learning (artificial intelligence);multiprocessing systems,holistic analysis;risk-controlled aggressive analysis;HARP;TensorFlow;PyTorch programs;Python-based analytics programs;modern machine learning programs;main computations;highly optimized libraries;computing efficiency;application domains;common limitation;static computation graphs;library APIs;hosting Python code;important optimization opportunities;analytics-conscious speculative analysis;Python complexities,
390,Clones and Changes,CC2Vec: Distributed Representations of Code Changes,T. Hoang; H. J. Kang; D. Lo; J. Lawall,"Singapore Management University, Singapore; Singapore Management University, Singapore; Singapore Management University, Singapore; Sorbonne University/Inria/LIP6, France",2020,"Existing work on software patches often use features specific to a single task. These works often rely on manually identified features, and human effort is required to identify these features for each task. In this work, we propose CC2Vec, a neural network model that learns a representation of code changes guided by their accompanying log messages, which represent the semantic intent of the code changes. CC2Vec models the hierarchical structure of a code change with the help of the attention mechanism and uses multiple comparison functions to identify the differences between the removed and added code. To evaluate if CC2Vec can produce a distributed representation of code changes that is general and useful for multiple tasks on software patches, we use the vectors produced by CC2Vec for three tasks: log message generation, bug fixing patch identification, and just-in-time defect prediction. In all tasks, the models using CC2Vec outperform the state-of-the-art techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284081,code embedding;deep learning;code changes,Semantics;Neural networks;Tools;Predictive models;Software;Task analysis;Software engineering,learning (artificial intelligence);neural nets;program debugging;program testing;public domain software;software maintenance,code change;CC2Vec models;removed added code;distributed representation;software patches;manually identified features,
391,Contracts,"Empirical Review of Automated Analysis Tools on 47,587 Ethereum Smart Contracts",T. Durieux; J. F. Ferreira; R. Abreu; P. Cruz,"INESC-ID and IST, University of Lisbon, Portugal; INESC-ID and IST, University of Lisbon, Portugal; INESC-ID and IST, University of Lisbon, Portugal; INESC-ID and IST, University of Lisbon, Portugal",2020,"Over the last few years, there has been substantial research on automated analysis, testing, and debugging of Ethereum smart contracts. However, it is not trivial to compare and reproduce that research. To address this, we present an empirical evaluation of 9 state-of-the-art automated analysis tools using two new datasets: i) a dataset of 69 annotated vulnerable smart contracts that can be used to evaluate the precision of analysis tools; and ii) a dataset with all the smart contracts in the Ethereum Blockchain that have Solidity source code available on Etherscan (a total of 47,518 contracts). The datasets are part of SmartBugs, a new extendable execution framework that we created to facilitate the integration and comparison between multiple analysis tools and the analysis of Ethereum smart contracts. We used SmartBugs to execute the 9 automated analysis tools on the two datasets. In total, we ran 428,337 analyses that took approximately 564 days and 3 hours, being the largest experimental setup to date both in the number of tools and in execution time. We found that only 42% of the vulnerabilities from our annotated dataset are detected by all the tools, with the tool Mythril having the higher accuracy (27%). When considering the largest dataset, we observed that 97% of contracts are tagged as vulnerable, thus suggesting a considerable number of false positives. Indeed, only a small number of vulnerabilities (and of only two categories) were detected simultaneously by four or more tools.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284023,Smart contracts;Solidity;Ethereum;Blockchain;Tools;Debugging;Testing;Reproducible Bugs,Smart contracts;Computer bugs;Debugging;Blockchain;Tools;Testing;Software engineering,blockchains;contracts;program debugging,automated analysis tools;vulnerable smart contracts;extendable execution framework;SmartBugs;Solidity source code;Ethereum smart contracts;largest dataset;annotated dataset;multiple analysis tools;Ethereum blockchain,
392,Contracts,Gap between Theory and Practice: An Empirical Study of Security Patches in Solidity,S. Hwang; S. Ryu,"KAIST, Daejeon, South Korea; KAIST, Daejeon, South Korea",2020,"Ethereum, one of the most popular blockchain platforms, provides financial transactions like payments and auctions through smart contracts. Due to the immense interest in smart contracts in academia, the research community of smart contract security has made a significant improvement recently. Researchers have reported various security vulnerabilities in smart contracts, and developed static analysis tools and verification frameworks to detect them. However, it is unclear whether such great efforts from academia has indeed enhanced the security of smart contracts in reality. To understand the security level of smart contracts in the wild, we empirically studied 55,046 real-world Ethereum smart contracts written in Solidity, the most popular programming language used by Ethereum smart contract developers. We first examined how many well-known vulnerabilities the Solidity compiler has patched, and how frequently the Solidity team publishes compiler releases. Unfortunately, we observed that many known vulnerabilities are not yet patched, and some patches are not even sufficient to avoid their target vulnerabilities. Subsequently, we investigated whether smart contract developers use the most recent compiler with vulnerabilities patched. We reported that developers of more than 98% of real-world Solidity contracts still use older compilers without vulnerability patches, and more than 25% of the contracts are potentially vulnerable due to the missing security patches. To understand actual impacts of the missing patches, we manually investigated potentially vulnerable contracts that are detected by our static analyzer and identified common mistakes by Solidity developers, which may cause serious security issues such as financial loss. We detected hundreds of vulnerable contracts and about one fourth of the vulnerable contracts are used by thousands of people. We recommend the Solidity team to make patches that resolve known vulnerabilities correctly, and developers to use the latest Solidity compiler to avoid missing security patches.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283944,Empirical Study;Smart Contracts;Solidity;Security Patches,Smart contracts;Static analysis;Manuals;Tools;Encoding;Security;Software engineering,C++ language;contracts;electronic commerce;program compilers;program diagnostics;security of data;telecommunication security,smart contract security;security vulnerabilities;real-world Ethereum smart contracts;Ethereum smart contract developers;Solidity team;real-world Solidity contracts;vulnerability patches;missing security patches;potentially vulnerable contracts,
393,Defect Prediction,An Investigation of Cross-Project Learning in Online Just-In-Time Software Defect Prediction,S. Tabassum; L. L. Minku; D. Feng; G. G. Cabral; L. Song,"University of Birmingham, UK; University of Birmingham, UK; Xiliu Tech, China; Federal Rural University of Pernambuco, Brazil; University of Birmingham, UK",2020,"Just-In-Time Software Defect Prediction (JIT-SDP) is concerned with predicting whether software changes are defect-inducing or clean based on machine learning classifiers. Building such classifiers requires a sufficient amount of training data that is not available at the beginning of a software project. Cross-Project (CP) JIT-SDP can overcome this issue by using data from other projects to build the classifier, achieving similar (not better) predictive performance to classifiers trained on Within-Project (WP) data. However, such approaches have never been investigated in realistic online learning scenarios, where WP software changes arrive continuously over time and can be used to update the classifiers. It is unknown to what extent CP data can be helpful in such situation. In particular, it is unknown whether CP data are only useful during the very initial phase of the project when there is little WP data, or whether they could be helpful for extended periods of time. This work thus provides the first investigation of when and to what extent CP data are useful for JIT-SDP in a realistic online learning scenario. For that, we develop three different CP JIT-SDP approaches that can operate in online mode and be updated with both incoming CP and WP training examples over time. We also collect 2048 commits from three software repositories being developed by a software company over the course of 9 to 10 months, and use 19,8468 commits from 10 active open source GitHub projects being developed over the course of 6 to 14 years. The study shows that training classifiers with incoming CP+WP data can lead to improvements in G-mean of up to 53.90% compared to classifiers using only WP data at the initial stage of the projects. For the open source projects, which have been running for longer periods of time, using CP data to supplement WP data also helped the classifiers to reduce or prevent large drops in predictive performance that may occur over time, leading to up to around 40% better G-Mean during such periods. Such use of CP data was shown to be beneficial even after a large number of WP data were received, leading to overall G-means up to 18.5% better than those of WP classifiers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283981,Software defect prediction;cross-project learning;transfer learning;online learning;verification latency;concept drift;class imbalance,Training;Filtering;Training data;Software;Data models;Software engineering;Software development management,data mining;just-in-time;learning (artificial intelligence);pattern classification;public domain software;software metrics;software quality,cross-project JIT-SDP;cross-project learning;online just-in-time software defect prediction;WP classifiers;10 active open source GitHub projects;software repositories;WP software changes;realistic online learning scenario;within-project data;software project;machine learning classifiers,
394,Defect Prediction,Understanding the Automated Parameter Optimization on Transfer Learning for Cross-Project Defect Prediction: An Empirical Study,K. Li; Z. Xiang; T. Chen; S. Wang; K. C. Tan,"Department of Computer Science, University of Exeter, Exeter, UK; College of Computer Science and Engineering, UESTC, Chengdu, China; Department of Computer Science, Loughborough University, Loughborough, UK; School of Computer Science, University of Birmingham, Birmingham, UK; Department of Computer Science, City University of Hong Kong, Hong Kong SAR",2020,"Data-driven defect prediction has become increasingly important in software engineering process. Since it is not uncommon that data from a software project is insufficient for training a reliable defect prediction model, transfer learning that borrows data/konwledge from other projects to facilitate the model building at the current project, namely cross-project defect prediction (CPDP), is naturally plausible. Most CPDP techniques involve two major steps, i.e., transfer learning and classification, each of which has at least one parameter to be tuned to achieve their optimal performance. This practice fits well with the purpose of automated parameter optimization. However, there is a lack of thorough understanding about what are the impacts of automated parameter optimization on various CPDP techniques. In this paper, we present the first empirical study that looks into such impacts on 62 CPDP techniques, 13 of which are chosen from the existing CPDP literature while the other 49 ones have not been explored before. We build defect prediction models over 20 real-world software projects that are of different scales and characteristics. Our findings demonstrate that: (1) Automated parameter optimization substantially improves the defect prediction performance of 77% CPDP techniques with a manageable computational cost. Thus more efforts on this aspect are required in future CPDP studies. (2) Transfer learning is of ultimate importance in CPDP. Given a tight computational budget, it is more cost-effective to focus on optimizing the parameter configuration of transfer learning algorithms (3) The research on CPDP is far from mature where it is â€˜not difficultâ€™ to find a better alternative by making a combination of existing transfer learning and classification techniques. This finding provides important insights about the future design of CPDP techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284059,Cross-project defect prediction;transfer learning;classification techniques;automated parameter optimization,Training;Predictive models;Software;Data models;Optimization;Tuning;Software engineering,knowledge management;learning (artificial intelligence);optimisation;software engineering;software metrics;software performance evaluation;software quality,software engineering process;software project;reliable defect prediction model;current project;cross-project defect prediction;optimal performance;62 CPDP techniques;existing CPDP literature;defect prediction models;real-world software projects;defect prediction performance;77% CPDP techniques;future CPDP studies;transfer learning algorithms;data-driven defect prediction,9
395,Defect Prediction,Software Visualization and Deep Transfer Learning for Effective Software Defect Prediction,J. Chen; K. Hu; Y. Yu; Z. Chen; Q. Xuan; Y. Liu; V. Filkov,"College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; College of Computer, National University of Defense Technology, Hefei, China; College of Information Engineering, Zhejiang University of Technology, Hangzhou, China; Institute of Cyberspace Security, Zhejiang University of Technology, Hangzhou, China; Institute of Process Equipment and Control Engineering, Zhejiang University of Technology, Hangzhou, China; Department of Computer Science, University of California, Davis, CA, USA",2020,"Software defect prediction aims to automatically locate defective code modules to better focus testing resources and human effort. Typically, software defect prediction pipelines are comprised of two parts: the first extracts program features, like abstract syntax trees, by using external tools, and the second applies machine learning-based classification models to those features in order to predict defective modules. Since such approaches depend on specific feature extraction tools, machine learning classifiers have to be custom-tailored to effectively build most accurate models. To bridge the gap between deep learning and defect prediction, we propose an end-to-end framework which can directly get prediction results for programs without utilizing feature-extraction tools. To that end, we first visualize programs as images, apply the self-attention mechanism to extract image features, use transfer learning to reduce the difference in sample distributions between projects, and finally feed the image files into a pre-trained, deep learning model for defect prediction. Experiments with 10 open source projects from the PROMISE dataset show that our method can improve cross-project and within-project defect prediction. Our code and data pointers are available at https://zenodo.org/record/3373409#.XV0Oy5Mza35.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284026,Cross-project defect prediction;within-project defect prediction;deep transfer learning;self-attention;software visualization,Deep learning;Data visualization;Tools;Predictive models;Feature extraction;Software;Testing,,,
396,Human Aspects of Software Engineering 2,Software Documentation: The Practitioners' Perspective,E. Aghajani; C. Nagy; M. Linares-VÃ¡squez; L. Moreno; G. Bavota; M. Lanza; D. C. Shepherd,"REVEAL @ Software Institute, USI UniversitÃ della Svizzera italiana, Lugano, Switzerland; REVEAL @ Software Institute, USI UniversitÃ della Svizzera italiana, Lugano, Switzerland; Systems and Computing Engineering Department, Universidad de los Andes, Colombia; Department of Computer Science, Colorado State University, USA; REVEAL @ Software Institute, USI UniversitÃ della Svizzera italiana, Lugano, Switzerland; REVEAL @ Software Institute, USI UniversitÃ della Svizzera italiana, Lugano, Switzerland; Virginia Commonwealth University, USA",2020,"In theory, (good) documentation is an invaluable asset to any software project, as it helps stakeholders to use, understand, maintain, and evolve a system. In practice, however, documentation is generally affected by numerous shortcomings and issues, such as insufficient and inadequate content and obsolete, ambiguous information. To counter this, researchers are investigating the development of advanced recommender systems that automatically suggest high-quality documentation, useful for a given task. A crucial first step is to understand what quality means for practitioners and what information is actually needed for specific tasks. We present two surveys performed with 146 practitioners to investigate (i) the documentation issues they perceive as more relevant together with solutions they apply when these issues arise; and (ii) the types of documentation considered as important in different tasks. Our findings can help researchers in designing the next generation of documentation recommender systems.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284065,Documentation;Empirical Study,Taxonomy;Documentation;Software;Stakeholders;Task analysis;Recommender systems;Software engineering,document handling;information retrieval;recommender systems;system documentation;team working,documentation recommender systems;high-quality documentation;software project;software documentation,
397,Program Repair,DLFix: Context-based Code Transformation Learning for Automated Program Repair,Y. Li; S. Wang; T. N. Nguyen,"New Jersey Inst. of Technology, USA; New Jersey Inst. of Technology, USA; University of Texas at Dallas, USA",2020,"Automated Program Repair (APR) is very useful in helping developers in the process of software development and maintenance. Despite recent advances in deep learning (DL), the DL-based APR approaches still have limitations in learning bug-fixing code changes and the context of the surrounding source code of the bug-fixing code changes. These limitations lead to incorrect fixing locations or fixes. In this paper, we introduce DLFix, a two-tier DL model that treats APR as code transformation learning from the prior bug fixes and the surrounding code contexts of the fixes. The first layer is a tree-based RNN model that learns the contexts of bug fixes and its result is used as an additional weighting input for the second layer designed to learn the bug-fixing code transformations. We conducted several experiments to evaluate DLFix in two benchmarks: Defect4J and Bugs.jar, and a newly built bug datasets with a total of +20K real-world bugs in eight projects. We compared DLFix against a total of 13 state-of-the-art pattern-based APR tools. Our results show that DLFix can auto-fix more bugs than 11 of them, and is comparable and complementary to the top two pattern-based APR tools in which there are 7 and 11 unique bugs that they cannot detect, respectively, but we can. Importantly, DLFix is fully automated and data-driven, and does not require hard-coding of bug-fixing patterns as in those tools. We compared DLFix against 4 state-of-the-art deep learning based APR models. DLFix is able to fix 2.5 times more bugs than the best performing baseline.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284100,Deep Learning;Automated Program Repair;Context-based Code Transformation Learning,Deep learning;Computer bugs;Tools;Maintenance engineering;Benchmark testing;Context modeling;Software engineering,deep learning (artificial intelligence);program debugging;public domain software;recurrent neural nets;software maintenance;source coding,APR models;DLFix;context-based code transformation;automated program repair;software development;maintenance;bug-fixing code changes;surrounding source code;bug fixes;code contexts;tree-based RNN model;bug-fixing code transformations;bug datasets;real-world bugs;pattern-based APR tools;hard-coding;bug-fixing patterns;deep learning;fixing locations,
398,Program Repair,On the Efficiency of Test Suite based Program Repair A Systematic Assessment of 16 Automated Repair Systems for Java Programs,K. Liu; S. Wang; A. K. Kisub Kim; T. F. BissyandÃ©; D. Kim; P. Wu; J. Klein; X. Mao; Y. L. Traon,"Nanjing University of Aeronautics and Astronautics, China; National University of Defense Technology, China; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; Furiosa.ai, Republic of Korea; National University of Defense Technology, China; University of Luxembourg, Luxembourg; National University of Defense Technology, China; University of Luxembourg, Luxembourg",2020,"Test-based automated program repair has been a prolific field of research in software engineering in the last decade. Many approaches have indeed been proposed, which leverage test suites as a weak, but affordable, approximation to program specifications. Although the literature regularly sets new records on the number of benchmark bugs that can be fixed, several studies increasingly raise concerns about the limitations and biases of state-of-the-art approaches. For example, the correctness of generated patches has been questioned in a number of studies, while other researchers pointed out that evaluation schemes may be misleading with respect to the processing of fault localization results. Nevertheless, there is little work addressing the efficiency of patch generation, with regard to the practicality of program repair. In this paper, we fill this gap in the literature, by providing an extensive review on the efficiency of test suite based program repair. Our objective is to assess the number of generated patch candidates, since this information is correlated to (1) the strategy to traverse the search space efficiently in order to select sensical repair attempts, (2) the strategy to minimize the test effort for identifying a plausible patch, (3) as well as the strategy to prioritize the generation of a correct patch. To that end, we performa large-scale empirical study on the efficiency, in terms of quantity of generated patch candidates of the 16 open-source repair tools for Java programs. The experiments are carefully conducted under the same fault localization configurations to limit biases. Eventually, among other findings, we note that: (1) many irrelevant patch candidates are generated by changing wrong code locations; (2) however, if the search space is carefully triaged, fault localization noise has little impact on patch generation efficiency; (3) yet, current template-based repair systems, which are known to be most effective in fixing a large number of bugs, are actually least efficient as they tend to generate majoritarily irrelevant patch candidates.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283931,Patch generation;Program repair;Efficiency;Empirical assessment,Java;Systematics;Computer bugs;Maintenance engineering;Tools;Search problems;Software engineering,Java;program debugging;program testing;software engineering,generated patch candidates;search space;plausible patch;large-scale empirical study;open-source repair tools;Java programs;fault localization configurations;template-based repair systems;majoritarily irrelevant patch candidates;systematic assessment;automated repair systems;automated program repair;patch generation;fault localization results;test suite based program repair;benchmark bugs,
399,Requirement Discovery,Caspar: Extracting and Synthesizing User Stories of Problems from App Reviews,H. Guo; M. P. Singh,"Secure Computing Institute, North Carolina State University, Raleigh, North Carolina; Secure Computing Institute, North Carolina State University, Raleigh, North Carolina",2020,"A user's review of an app often describes the user's interactions with the app. These interactions, which we interpret as mini stories, are prominent in reviews with negative ratings. In general, a story in an app review would contain at least two types of events: user actions and associated app behaviors. Being able to identify such stories would enable an app's developer in better maintaining and improving the app's functionality and enhancing user experience. We present Caspar, a method for extracting and synthesizing user-reported mini stories regarding app problems from reviews. By extending and applying natural language processing techniques, Caspar extracts ordered events from app reviews, classifies them as user actions or app problems, and synthesizes action-problem pairs. Our evaluation shows that Caspar is effective in finding action-problem pairs from reviews. First, Caspar classifies the events with an accuracy of 82.0% on manually labeled data. Second, relative to human evaluators, Caspar extracts event pairs with 92.9% precision and 34.2% recall. In addition, we train an inference model on the extracted action-problem pairs that automatically predicts possible app problems for different use cases. Preliminary evaluation shows that our method yields promising results. Caspar illustrates the potential for a deeper understanding of app reviews and possibly other natural language artifacts arising in software engineering.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283933,natural language processing;app review analysis;event extraction;event inference;requirements,Predictive models;Natural language processing;Software engineering,learning (artificial intelligence);mobile computing;natural language processing;software engineering,extracted action-problem pairs;automatically predicts possible app problems;Caspar;app review;extracting user stories;synthesizing user stories;reviews;user actions;associated app behaviors;extracting synthesizing user-reported mini stories;synthesizes action-problem pairs,
400,Requirement Discovery,Detection of Hidden Feature Requests from Massive Chat Messages via Deep Siamese Network,L. Shi; M. Xing; M. Li; Y. Wang; S. Li; Q. Wang,"Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software Chinese Academy of Sciences, Beijing, China",2020,"Online chatting is gaining popularity and plays an increasingly significant role in software development. When discussing functionalities, developers might reveal their desired features to other developers. Automated mining techniques towards retrieving feature requests from massive chat messages can benefit the requirements gathering process. But it is quite challenging to perform such techniques because detecting feature requests from dialogues requires a thorough understanding of the contextual information, and it is also extremely expensive on annotating feature-request dialogues for learning. To bridge that gap, we recast the traditional text classification task of mapping single dialog to its class into the task of determining whether two dialogues are similar or not by incorporating few-shot learning. We propose a novel approach, named FRMiner, which can detect feature-request dialogues from chat messages via deep Siamese network. We design a BiLSTM-based dialog model that can learn the contextual information of a dialog in both forward and reverse directions. Evaluation on the realworld projects shows that our approach achieves average precision, recall and F1-score of 88.52%, 88.50% and 88.51%, which confirms that our approach could effectively detect hidden feature requests from chat messages, thus can facilitate gathering comprehensive requirements from the crowd in an automated way.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283914,Feature Requests;Requirements Engineering;Deep Learning;Siamese Network,Text categorization;Feature extraction;Software;Data mining;Task analysis;Software engineering;Context modeling,data mining;information retrieval;interactive systems;learning (artificial intelligence);pattern classification;text analysis,gathering comprehensive requirements;hidden feature requests;deep Siamese network;online chatting;increasingly significant role;software development;desired features;automated mining techniques;massive chat messages;contextual information;feature-request dialogues;traditional text classification task,
401,Cognition,A Tale from the Trenches: Cognitive Biases and Software Development,S. Chattopadhyay; N. Nelson; A. Au; N. Morales; C. Sanchez; R. Pandita; A. Sarma,"Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Phase Change Software, Golden, CO, USA; Oregon State University, Corvallis, OR, USA",2020,"Cognitive biases are hard-wired behaviors that influence developer actions and can set them on an incorrect course of action, necessitating backtracking. While researchers have found that cognitive biases occur in development tasks in controlled lab studies, we still don't know how these biases affect developers' everyday behavior. Without such an understanding, development tools and practices remain inadequate. To close this gap, we conducted a 2-part field study to examine the extent to which cognitive biases occur, the consequences of these biases on developer behavior, and the practices and tools that developers use to deal with these biases. About 70% of observed actions that were reversed were associated with at least one cognitive bias. Further, even though developers recognized that biases frequently occur, they routinely are forced to deal with such issues with ad hoc processes and sub-optimal tool support. As one participant (IP12) lamented: There is no salvation!",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283965,cognitive bias;software development;field study;interviews,Employment;Tools;Software;Problem-solving;Task analysis;Interviews;Software engineering,behavioural sciences computing;cognition;software development management,software development;cognitive bias;influence developer actions;development tools;developer behavior;hard-wired behaviors;ad hoc processes;sub-optimal tool support,
402,Cognition,Recognizing Developers' Emotions while Programming,D. Girardi; N. Novielli; D. Fucci; F. Lanubile,"University of Bari, Italy; University of Bari, Italy; University of Hamburg, Germany; University of Bari, Italy",2020,"Developers experience a wide range of emotions during programming tasks, which may have an impact on job performance. In this paper, we present an empirical study aimed at (i) investigating the link between emotion and progress, (ii) understanding the triggers for developers' emotions and the strategies to deal with negative ones, (iii) identifying the minimal set of non-invasive biometric sensors for emotion recognition during programming tasks. Results confirm previous findings about the relation between emotions and perceived productivity. Furthermore, we show that developers' emotions can be reliably recognized using only a wristband capturing the electrodermal activity and heart-related metrics.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284015,Emotion awareness;emotion detection;biometric sensors;empirical software engineering;human factors in software engineering,Measurement;Emotion recognition;Programming;Software;Software reliability;Task analysis;Software engineering,biometrics (access control);emotion recognition;feature extraction;software engineering,programming tasks;job performance;noninvasive biometric sensors;emotion recognition;wristband;electrodermal activity;heart-related metrics,
403,Cognition,Neurological Divide: An fMRI Study of Prose and Code Writing,R. Krueger; Y. Huang; X. Liu; T. Santander; W. Weimer; K. Leach,University of Michigan; University of Michigan; Georgia Institute of Technology; UC Santa Barbara; University of Michigan; University of Michigan,2020,"Software engineering involves writing new code or editing existing code. Recent efforts have investigated the neural processes associated with reading and comprehending code - however, we lack a thorough understanding of the human cognitive processes underlying code writing. While prose reading and writing have been studied thoroughly, that same scrutiny has not been applied to code writing. In this paper, we leverage functional brain imaging to investigate neural representations of code writing in comparison to prose writing. We present the first human study in which participants wrote code and prose while undergoing a functional magnetic resonance imaging (fMRI) brain scan, making use of a full-sized fMRI-safe QWERTY keyboard. We find that code writing and prose writing are significantly dissimilar neural tasks. While prose writing entails significant left hemisphere activity associated with language, code writing involves more activations of the right hemisphere, including regions associated with attention control, working memory, planning and spatial cognition. These findings are unlike existing work in which code and prose comprehension were studied. By contrast, we present the first evidence suggesting that code and prose writing are quite dissimilar at the neural level.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284060,medical imaging;spatial;memory;attention;synthesis;keyboard,Visualization;Writing;Functional magnetic resonance imaging;Planning;Task analysis;Software engineering;Biomedical imaging,biomedical MRI;brain;cognition;medical image processing;neurophysiology;software engineering,code writing;prose writing;human cognitive processes;functional magnetic resonance imaging brain scan;fMRI-safe QWERTY keyboard,
404,Cognition,Here We Go Again: Why Is It Difficult for Developers to Learn Another Programming Language?,N. Shrestha; C. Botta; T. Barik; C. Parnin,"NC State University, Raleigh, North Carolina; NC State University, Raleigh, North Carolina; Microsoft, Redmond, Washington; NC State University, Raleigh, North Carolina",2020,"Once a programmer knows one language, they can leverage concepts and knowledge already learned, and easily pick up another programming language. But is that always the case? To understand if programmers have difficulty learning additional programming languages, we conducted an empirical study of Stack Overflow questions across 18 different programming languages. We hypothesized that previous knowledge could potentially interfere with learning a new programming language. From our inspection of 450 Stack Overflow questions, we found 276 instances of interference that occurred due to faulty assumptions originating from knowledge about a different language. To understand why these difficulties occurred, we conducted semi-structured interviews with 16 professional programmers. The interviews revealed that programmers make failed attempts to relate a new programming language with what they already know. Our findings inform design implications for technical authors, toolsmiths, and language designers, such as designing documentation and automated tools that reduce interference, anticipating uncommon language transitions during language design, and welcoming programmers not just into a language, but its entire ecosystem.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284077,interference theory;learning;program comprehension;programming environments;programming languages,Computer languages;Ecosystems;Interference;Documentation;Tools;Syntactics;Interviews,computer aided instruction;software tools;Web sites,programming language;stack overflow questions;language designers;uncommon language transitions;language design;semistructured interviews;professional programmers,
405,Deep Learning Testing and Debugging 1,Importance-Driven Deep Learning System Testing,S. Gerasimou; H. F. Eniser; A. Sen; A. Cakan,"University of York, York, UK; MPI-SWS, Kaiserslautern, Germany; Bogazici University, Istanbul, Turkey; Bogazici University, Istanbul, Turkey",2020,"Deep Learning (DL) systems are key enablers for engineering intelligent applications due to their ability to solve complex tasks such as image recognition and machine translation. Nevertheless, using DL systems in safety- and security-critical applications requires to provide testing evidence for their dependable operation. Recent research in this direction focuses on adapting testing criteria from traditional software engineering as a means of increasing confidence for their correct behaviour. However, they are inadequate in capturing the intrinsic properties exhibited by these systems. We bridge this gap by introducing DeepImportance, a systematic testing methodology accompanied by an Importance-Driven (IDC) test adequacy criterion for DL systems. Applying IDC enables to establish a layer-wise functional understanding of the importance of DL system components and use this information to assess the semantic diversity of a test set. Our empirical evaluation on several DL systems, across multiple DL datasets and with state-of-the-art adversarial generation techniques demonstrates the usefulness and effectiveness of DeepImportance and its ability to support the engineering of more robust DL systems.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283940,Deep Learning Systems;Test Adequacy;Safety-Critical Systems,Deep learning;System testing;Systematics;Neurons;Semantics;Task analysis;Software engineering,,,
406,Deep Learning Testing and Debugging 1,ReluDiff: Differential Verification of Deep Neural Networks,B. Paulsen; J. Wang; C. Wang,"University of Southern California, Los Angeles, California, USA; University of Southern California, Los Angeles, California, USA; University of Southern California, Los Angeles, California, USA",2020,"As deep neural networks are increasingly being deployed in practice, their efficiency has become an important issue. While there are compression techniques for reducing the network's size, energy consumption and computational requirement, they only demonstrate empirically that there is no loss of accuracy, but lack formal guarantees of the compressed network, e.g., in the presence of adversarial examples. Existing verification techniques such as Reluplex, ReluVal, and DeepPoly provide formal guarantees, but they are designed for analyzing a single network instead of the relationship between two networks. To fill the gap, we develop a new method for differential verification of two closely related networks. Our method consists of a fast but approximate forward interval analysis pass followed by a backward pass that iteratively refines the approximation until the desired property is verified. We have two main innovations. During the forward pass, we exploit structural and behavioral similarities of the two networks to more accurately bound the difference between the output neurons of the two networks. Then in the backward pass, we leverage the gradient differences to more accurately compute the most beneficial refinement. Our experiments show that, compared to state-of-the-art verification tools, our method can achieve orders-of-magnitude speedup and prove many more properties than existing tools.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283939,Verification;Differential Verification;Deep Neural Networks;AI Safety,Technological innovation;Energy consumption;Neurons;Tools;Biological neural networks;Formal verification,formal verification;iterative methods;neural nets;program verification,differential verification;closely related networks;fast but approximate forward interval analysis pass;backward pass;forward pass;state-of-the-art verification tools;deep neural networks;compression techniques;lack formal guarantees;verification techniques,
407,Deep Learning Testing and Debugging 1,DISSECTOR: Input Validation for Deep Learning Applications by Crossing-layer Dissection,H. Wang; J. Xu; C. Xu; X. Ma; J. Lu,"Department of Computer Science and Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, Nanjing University, Nanjing, China",2020,"Deep learning (DL) applications are becoming increasingly popular. Their reliabilities largely depend on the performance of DL models integrated in these applications as a central classifying module. Traditional techniques need to retrain the models or rebuild and redeploy the applications for coping with unexpected conditions beyond the models' handling capabilities. In this paper, we take a fault tolerance approach, Dissector, to distinguishing those inputs that represent unexpected conditions (beyond-inputs) from normal inputs that are still within the models' handling capabilities (within-inputs), thus keeping the applications still function with expected reliabilities. The key insight of Dissector is that a DL model should interpret a within-input with increasing confidence, while a beyond-input would probably cause confused guesses in the prediction process. Dissector works in an application-specific way, adaptive to DL models used in applications, and extremely efficiently, scalable to large-size datasets from complex scenarios. The experimental evaluation shows that Dissector outperformed state-of-the-art techniques in the effectiveness (AUC: avg. 0.8935 and up to 0.9894) and efficiency (runtime overhead: only 3.3-5.8 milliseconds). Besides, it also exhibited encouraging usefulness indefensing against adversarial inputs (AUC: avg. 0.9983) and improving a DL model's actual accuracy in use (up to 16% for CIFAR-100 and 20% for ImageNet).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283929,deep learning;fault tolerance;input validation,Deep learning;Adaptation models;Fault tolerance;Runtime;Fault tolerant systems;Predictive models,fault tolerance;image classification;learning (artificial intelligence),DISSECTOR;input validation;deep learning applications;crossing-layer dissection;DL model;central classifying module;unexpected conditions;fault tolerance approach;dissector;normal inputs;expected reliabilities;increasing confidence;application-specific way;large-size datasets;adversarial inputs;time 3.3 ms to 5.8 ms,
408,Deep Learning Testing and Debugging 1,Towards Characterizing Adversarial Defects of Deep Learning Software from the Lens of Uncertainty,X. Zhang; X. Xie; L. Ma; X. Du; Q. Hu; Y. Liu; J. Zhao; M. Sun,"Peking University, China; Nanyang Technological University, Singapore; Kyushu University, Japan; Nanyang Technological University, Singapore; Kyushu University, Japan; Nanyang Technological University, Singapore; Kyushu University, Japan; Peking University, China",2020,"Over the past decade, deep learning (DL) has been successfully applied to many industrial domain-specific tasks. However, the current state-of-the-art DL software still suffers from quality issues, which raises great concern especially in the context of safety- and security-critical scenarios. Adversarial examples (AEs) represent a typical and important type of defects needed to be urgently addressed, on which a DL software makes incorrect decisions. Such defects occur through either intentional attack or physical-world noise perceived by input sensors, potentially hindering further industry deployment. The intrinsic uncertainty nature of deep learning decisions can be a fundamental reason for its incorrect behavior. Although some testing, adversarial attack and defense techniques have been recently proposed, it still lacks a systematic study to uncover the relationship between AEs and DL uncertainty. In this paper, we conduct a large-scale study towards bridging this gap. We first investigate the capability of multiple uncertainty metrics in differentiating benign examples (BEs) and AEs, which enables to characterize the uncertainty patterns of input data. Then, we identify and categorize the uncertainty patterns of BEs and AEs, and find that while BEs and AEs generated by existing methods do follow common uncertainty patterns, some other uncertainty patterns are largely missed. Based on this, we propose an automated testing technique to generate multiple types of uncommon AEs and BEs that are largely missed by existing techniques. Our further evaluation reveals that the uncommon data generated by ourmethod is hard to be defended by the existing defense techniques with the average defense success rate reduced by 35%. Our results call for attention and necessity to generate more diverse data for evaluating quality assurance solutions of DL software.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284139,Deep learning;uncertainty;adversarial attack;software testing,Deep learning;Uncertainty;Systematics;Software;Task analysis;Testing;Software engineering,learning (artificial intelligence);program testing;security of data,adversarial defects;deep learning software;industrial domain-specific tasks;security-critical scenarios;adversarial examples;typical type;important type;DL software;incorrect decisions;intentional attack;input sensors;industry deployment;intrinsic uncertainty nature;deep learning decisions;incorrect behavior;adversarial attack;large-scale study;multiple uncertainty metrics;benign examples;BEs;common uncertainty patterns;automated testing technique;uncommon AEs;existing defense techniques;average defense success rate,26
409,Fuzzing 1,Gang of Eight: A Defect Taxonomy for Infrastructure as Code Scripts,A. Rahman; E. Farhana; C. Parnin; L. Williams,"Tennessee Tech University, Tennessee, USA; NC State University, North Carolina, USA; NC State University, North Carolina, USA; NC State University, North Carolina, USA",2020,"Defects in infrastructure as code (IaC) scripts can have serious consequences, for example, creating large-scale system outages. A taxonomy of IaC defects can be useful for understanding the nature of defects, and identifying activities needed to fix and prevent defects in IaC scripts. The goal of this paper is to help practitioners improve the quality of infrastructure as code (IaC) scripts by developing a defect taxonomy for IaC scripts through qualitative analysis. We develop a taxonomy of IaC defects by applying qualitative analysis on 1,448 defect-related commits collected from open source software (OSS) repositories of the Openstack organization. We conduct a survey with 66 practitioners to assess if they agree with the identified defect categories included in our taxonomy. We quantify the frequency of identified defect categories by analyzing 80,425 commits collected from 291 OSS repositories spanning across 2005 to 2019. Our defect taxonomy for IaC consists of eight categories, including a category specific to IaC called idempotency (i.e., defects that lead to incorrect system provisioning when the same IaC script is executed multiple times). We observe the surveyed 66 practitioners to agree most with idempotency. The most frequent defect category is configuration data i.e., providing erroneous configuration data in IaC scripts. Our taxonomy and the quantified frequency of the defect categories may help in advancing the science of IaC script quality.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284113,bug;category;configuration as code;configuration scripts;defect;devops;infrastructure as code;puppet;software quality;taxonomy,Time-frequency analysis;Taxonomy;Organizations;Tools;Large-scale systems;Open source software;Software engineering,organisational aspects;public domain software;software engineering,defect taxonomy;code scripts;IaC defects;qualitative analysis;1 defect-related commits;448 defect-related commits;identified defect categories;frequent defect category;IaC script quality,16
410,Fuzzing 1,MEMLOCK: Memory Usage Guided Fuzzing,C. Wen; H. Wang; Y. Li; S. Qin; Y. Liu; Z. Xu; H. Chen; X. Xie; G. Pu; T. Liu,"CSSE, Shenzhen University, Shenzhen, China; CSSE, Shenzhen University, Shenzhen, China; Nanyang Technological University, Singapore; CSSE, Shenzhen University, Shenzhen, China; Nanyang Technological University, Singapore; CSSE, Shenzhen University, Shenzhen, China; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; East China Normal University, Shanghai, China; Xi'an Jiaotong University, Xi'an, China",2020,"Uncontrolled memory consumption is a kind of critical software security weaknesses. It can also become a security-critical vulnerability when attackers can take control of the input to consume a large amount of memory and launch a Denial-of-Service attack. However, detecting such vulnerability is challenging, as the state-of-the-art fuzzing techniques focus on the code coverage but not memory consumption. To this end, we propose a memory usage guided fuzzing technique, named MemLock, to generate the excessive memory consumption inputs and trigger uncontrolled memory consumption bugs. The fuzzing process is guided with memory consumption information so that our approach is general and does not require any domain knowledge. We perform a thorough evaluation for MemLock on 14 widely-used real-world programs. Our experiment results show that MemLock substantially outperforms the state-of-the-art fuzzing techniques, including AFL, AFLfast, PerfFuzz, FairFuzz, Angora and QSYM, in discovering memory consumption bugs. During the experiments, we discovered many previously unknown memory consumption bugs and received 15 new CVEs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284141,Fuzz Testing;Software Vulnerability;Memory Consumption,Memory management;Computer bugs;Fuzzing;Writing;Software;Security;Software engineering,program testing;safety-critical software;security of data;telecommunication security,memory usage;critical software security weaknesses;security-critical vulnerability;Denial-of-Service attack;state-of-the-art fuzzing techniques;fuzzing technique;named MemLock;excessive memory consumption inputs;trigger uncontrolled memory consumption bugs;fuzzing process;memory consumption information;unknown memory consumption bugs,22
411,Fuzzing 1,sFuzz: An Efficient Adaptive Fuzzer for Solidity Smart Contracts,T. D. Nguyen; L. H. Pham; J. Sun; Y. Lin; Q. T. Minh,"Singapore Management University, Singapore; Singapore Management University, Singapore; Singapore Management University, Singapore; National University of Singapore, Singapore; Ho Chi Minh City University of Technology, Vietnam",2020,"Smart contracts are Turing-complete programs that execute on the infrastructure of the blockchain, which often manage valuable digital assets. Solidity is one of the most popular programming languages for writing smart contracts on the Ethereum platform. Like traditional programs, smart contracts may contain vulnerabilities. Unlike traditional programs, smart contracts cannot be easily patched once they are deployed. It is thus important that smart contracts are tested thoroughly before deployment. In this work, we present an adaptive fuzzer for smart contracts on the Ethereum platform called sFuzz. Compared to existing Solidity fuzzers, sFuzz combines the strategy in the AFL fuzzer and an efficient lightweight multi-objective adaptive strategy targeting those hard-to-cover branches. sFuzz has been applied to more than 4 thousand smart contracts and the experimental results show that (1) sFuzz is efficient, e.g., two orders of magnitude faster than state-of-the-art tools; (2) sFuzz is effective in achieving high code coverage and discovering vulnerabilities; and (3) the different fuzzing strategies in sFuzz complement each other.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284013,Fuzzing;Smart Contract;Vulnerability;Ethereum;Blockchain,Computer languages;Smart contracts;Blockchain;Writing;Tools;Fuzzing;Software engineering,program diagnostics;program testing;programming languages;security of data;Turing machines,sFuzz;Solidity smart contracts;traditional programs;4 thousand smart contracts,
412,Fuzzing 1,Targeted Greybox Fuzzing with Static Lookahead Analysis,V. WÃ¼stholz; M. Christakis,"ConsenSys Diligence/MythX, Germany; MPI-SWS, Germany",2020,"Automatic test generation typically aims to generate inputs that explore new paths in the program under test in order to find bugs. Existing work has, therefore, focused on guiding the exploration toward program parts that are more likely to contain bugs by using an offline static analysis. In this paper, we introduce a novel technique for targeted greybox fuzzing using an online static analysis that guides the fuzzer toward a set of target locations, for instance, located in recently modified parts of the program. This is achieved by first semantically analyzing each program path that is explored by an input in the fuzzer's test suite. The results of this analysis are then used to control the fuzzer's specialized power schedule, which determines how often to fuzz inputs from the test suite. We implemented our technique by extending a state-of-the-art, industrial fuzzer for Ethereum smart contracts and evaluate its effectiveness on 27 real-world benchmarks. Using an online analysis is particularly suitable for the domain of smart contracts since it does not require any code instrumentation-adding instrumentation to contracts changes their semantics. Our experiments show that targeted fuzzing significantly outperforms standard greybox fuzzing for reaching 83% of the challenging target locations (up to 14x of median speed-up).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284040,,Instruments;Smart contracts;Computer bugs;Static analysis;Fuzzing;Benchmark testing;Standards,benchmark testing;program debugging;program diagnostics;program testing;program verification,targeted greybox fuzzing;static lookahead analysis;automatic test generation;explore new paths;bugs;program parts;offline static analysis;online static analysis;recently modified parts;program path;fuzz inputs;test suite;industrial fuzzer;Ethereum smart contracts;online analysis;targeted fuzzing;standard greybox fuzzing;challenging target locations,
413,Fuzzing 1,Planning for Untangling: Predicting the Difficulty of Merge Conflicts,C. Brindescu; I. Ahmed; R. Leano; A. Sarma,"Oregon State University, Corvallis, OR, USA; University of California, Irvine, Irvine, CA, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA",2020,"Merge conflicts are inevitable in collaborative software development and are disruptive. When they occur, developers have to stop their current work, understand the conflict and the surrounding code, and plan an appropriate resolution. However, not all conflicts are equally problematic-some can be easily fixed, while others might be complicated enough to need multiple people. Currently, there is not much support to help developers plan their conflict resolution. In this work, we aim to predict the difficulty of a merge conflict so as to help developers plan their conflict resolution. The ability to predict the difficulty of a merge conflict and to identify the underlying factors for its difficulty can help tool builders improve their conflict detection tools to prioritize and warn developers of difficult conflicts. In this work, we investigate the characteristics of difficult merge conflicts, and automatically classify them. We analyzed 6,380 conflicts across 128 java projects and found that merge conflict difficulty can be accurately predicted (AUC of 0.76) through machine learning algorithms, such as bagging.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284005,Merge conflict difficulty prediction;Merge conflict resolution;Empirical analysis,Java;Machine learning algorithms;Collaborative software;Tools;Planning;Bagging;Software engineering,Java;learning (artificial intelligence);software engineering,conflict resolution;conflict detection tools;merge conflicts;conflict difficulty;collaborative software development;untangling planning;Java projects;machine learning,
414,Static Analysis 1,Conquering the Extensional Scalability Problem for Value-Flow Analysis Frameworks,Q. Shi; R. Wu; G. Fan; C. Zhang,"The Hong Kong University of Science and Technology, Hong Kong, China; Xiamen University, Xiamen, China; The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China",2020,"Modern static analyzers often need to simultaneously check a few dozen or even hundreds of value-flow properties, causing serious scalability issues when high precision is required. A major factor to this deficiency, as we observe, is that the core static analysis engine is oblivious of the mutual synergy among the properties being checked, thus inevitably losing many optimization opportunities. Our work is to leverage the inter-property awareness and to capture redundancies and inconsistencies when many properties are considered at the same time. We have evaluated our approach by checking twenty value-flow properties in standard benchmark programs and ten real-world software systems. The results demonstrate that our approach is more than 8Ã— faster than existing ones but consumes only 1/7 of the memory. Such substantial improvement in analysis efficiency is not achieved by sacrificing the effectiveness: at the time of writing, thirty-nine bugs found by our approach have been fixed by developers and four of them have been assigned CVE IDs due to their security impact.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283935,Static bug finding;demand-driven analysis;compositional program analysis;value-flow analysis,Scalability;Computer bugs;Static analysis;Software systems;Security;Standards;Software engineering,data flow analysis;program diagnostics;program testing;security of data,extensional scalability problem;analysis efficiency;inter-property awareness;core static analysis engine;serious scalability issues;value-flow properties;modern static analyzers;value-flow analysis frameworks,
415,Static Analysis 1,Tailoring Programs for Static Analysis via Program Transformation,R. van Tonder; C. Le Goues,"School of Computer Science, Carnegie Mellon University, USA; School of Computer Science, Carnegie Mellon University, USA",2020,"Static analysis is a proven technique for catching bugs during software development. However, analysis tooling must approximate, both theoretically and in the interest of practicality. False positives are a pervading manifestation of such approximations-tool configuration and customization is therefore crucial for usability and directing analysis behavior. To suppress false positives, developers readily disable bug checks or insert comments that suppress spurious bug reports. Existing work shows that these mechanisms fall short of developer needs and present a significant pain point for using or adopting analyses. We draw on the insight that an analysis user always has one notable ability to influence analysis behavior regardless of analyzer options and implementation: modifying their program. We present a new technique for automated, generic, and temporary code changes that tailor to suppress spurious analysis errors. We adopt a rule-based approach where simple, declarative templates describe general syntactic changes for code patterns that are known to be problematic for the analyzer. Our technique promotes program transformation as a general primitive for improving the fidelity of analysis reports (we treat any given analyzer as a black box). We evaluate using five different static analyzers supporting three different languages (C, Java, and PHP) on large, real world programs (up to 800KLOC). We show that our approach is effective in sidestepping long-standing and complex issues in analysis implementations.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284082,program transformation;program analysis;syntax;static analysis;false positives,Pain;Computer bugs;Static analysis;Syntactics;Software;Usability;Software engineering,knowledge based systems;program debugging;program diagnostics;real-time systems;software engineering,static analysis;bugs;software development;analysis tooling;approximations-tool configuration;analysis behavior;spurious bug reports;user analysis;temporary code changes;analysis reports;analysis implementations;rule-based approach;code patterns;program transformation,
416,Static Analysis 1,Pipelining Bottom-up Data Flow Analysis,Q. Shi; C. Zhang,"The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China",2020,"Bottom-up program analysis has been traditionally easy to parallelize because functions without caller-callee relations can be analyzed independently. However, such function-level parallelism is significantly limited by the calling dependence - functions with caller-callee relations have to be analyzed sequentially because the analysis of a function depends on the analysis results, a.k.a., function summaries, of its callees. We observe that the calling dependence can be relaxed in many cases and, as a result, the parallelism can be improved. In this paper, we present Coyote, a framework of bottom-up data flow analysis, in which the analysis task of each function is elaborately partitioned into multiple sub-tasks to generate pipelineable function summaries. These sub-tasks are pipelined and run in parallel, even though the calling dependence exists. We formalize our idea under the IFDS/IDE framework and have implemented an application to checking null-dereference bugs and taint issues in C/C++ programs. We evaluate Coyote on a series of standard benchmark programs and open-source software systems, which demonstrates significant speedup over a conventional parallel design.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283921,Compositional program analysis;modular program analysis;bottom-up analysis;data flow analysis;IFDS/IDE,Computer bugs;Software systems;Task analysis;Standards;Open source software;Pipeline processing;Software engineering,data flow analysis;Java;object-oriented languages;program debugging;program diagnostics;program verification;public domain software;software maintenance,pipelineable function summaries;parallel design;program analysis;caller-callee relations;function-level parallelism;analysis task;calling dependence;pipelined bottom-up data flow analysis;Coyote;IFDS/IDE framework,
417,Traceability,A Novel Approach to Tracing Safety Requirements and State-Based Design Models,M. Alenazi; N. Niu; J. Savolainen,"University of Cincinnati, Cincinnati, Ohio; University of Cincinnati, Cincinnati, Ohio; Danfoss Drives A/S, GrÃ¥sten, Denmark",2020,"Traceability plays an essential role in assuring that software and systems are safe to use. Automated requirements traceability faces the low precision challenge due to a large number of false positives being returned and mingled with the true links. To overcome this challenge, we present a mutation-driven method built on the novel idea of proactively creating many seemingly correct tracing targets (i.e., mutants of a state machine diagram), and then exploiting model checking within process mining to automatically verify whether the safety requirement's properties hold in the mutants. A mutant is killed if its model checking fails; otherwise, it is survived. We leverage the underlying killed-survived distinction, and develop a correlation analysis procedure to identify the traceability links. Experimental evaluation results on two automotive systems with 27 safety requirements show considerable precision improvements compared with the state-of-the-art.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284105,mutation analysis;process mining;requirements engineering;Systems Modeling Language (SysML),Analytical models;Visualization;Correlation;Model checking;Safety;Usability;Software engineering,data mining;finite state machines;formal specification;formal verification;program verification;software maintenance;Unified Modeling Language,state-based design models;automated requirements traceability;low precision challenge;false positives;mutation-driven method;seemingly correct tracing targets;mutant;state machine diagram;model checking;process mining;safety requirement;underlying killed-survived distinction;traceability links;automotive systems;27 safety requirements;considerable precision improvements,
418,Traceability,Establishing Multilevel Test-to-Code Traceability Links,R. White; J. Krinke; R. Tan,"University College London, London, UK; University College London, London, UK; University College London, London, UK",2020,"Test-to-code traceability links model the relationships between test artefacts and code artefacts. When utilised during the development process, these links help developers to keep test code in sync with tested code, reducing the rate of test failures and missed faults. Test-to-code traceability links can also help developers to maintain an accurate mental model of the system, reducing the risk of architectural degradation when making changes. However, establishing and maintaining these links manually places an extra burden on developers and is error-prone. This paper presents TCtracer, an approach and implementation for the automatic establishment of test-to-code traceability links. Unlike existing work, TCtracer operates at both the method level and the class level, allowing us to establish links between tests and functions, as well as between test classes and tested classes. We improve over existing techniques by combining an ensemble of new and existing techniques and exploiting a synergistic flow of information between the method and class levels. An evaluation of TCtracer using four large, well-studied open source systems demonstrates that, on average, we can establish test-to-function links with a mean average precision (MAP) of 78% and test-class-to-class links with an MAP of 93%.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283989,Software Testing;Software Traceability;Code to Test Traceability,Degradation;Cognitive science;Synchronization;Software engineering,program testing,TCtracer;test-class-to-class links;test-to-function links;test classes;test failures;test code;code artefacts;test artefacts;test-to-code traceability links model;multilevel test-to-code traceability links,
419,Traceability,Improving the Effectiveness of Traceability Link Recovery using Hierarchical Bayesian Networks,K. Moran; D. N. Palacio; C. Bernal-CÃ¡rdenas; D. McCrystal; D. Poshyvanyk; C. Shenefiel; J. Johnson,"William & Mary, Williamsburg, VA, USA; William & Mary, Williamsburg, VA, USA; William & Mary, Williamsburg, VA, USA; William & Mary, Williamsburg, VA, USA; William & Mary, Williamsburg, VA, USA; Cisco Advanced Security Research Group, Morrisville, NC, USA; Cisco Security and Trust Engineering, Morrisville, NC, USA",2020,"Traceability is a fundamental component of the modern software development process that helps to ensure properly functioning, secure programs. Due to the high cost of manually establishing trace links, researchers have developed automated approaches that draw relationships between pairs of textual software artifacts using similarity measures. However, the effectiveness of such techniques are often limited as they only utilize a single measure of artifact similarity and cannot simultaneously model (implicit and explicit) relationships across groups of diverse development artifacts. In this paper, we illustrate how these limitations can be overcome through the use of a tailored probabilistic model. To this end, we design and implement a HierarchiCal PrObabilistic Model for SoftwarE Traceability (Comet) that is able to infer candidate trace links. Comet is capable of modeling relationships between artifacts by combining the complementary observational prowess of multiple measures of textual similarity. Additionally, our model can holistically incorporate information from a diverse set of sources, including developer feedback and transitive (often implicit) relationships among groups of software artifacts, to improve inference accuracy. We conduct a comprehensive empirical evaluation of Comet that illustrates an improvement over a set of optimally configured baselines of â‰ˆ14% in the best case and â‰ˆ5% across all subjects in terms of average precision. The comparative effectiveness of Comet in practice, where optimal configuration is typically not possible, is likely to be higher. Finally, we illustrate Comet's potential for practical applicability in a survey with developers from Cisco Systems who used a prototype Comet Jenkins plugin.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284034,Software Traceability;Probabilistic Modeling;Information Retrieval,Prototypes;Probabilistic logic;Software;Bayes methods;Software measurement;Software engineering,Bayes methods;belief networks;inference mechanisms;information retrieval;probability;program diagnostics;public domain software;software engineering;software maintenance;software tools,traceability link recovery;hierarchical bayesian networks;fundamental component;modern software development process;secure programs;textual software artifacts;similarity measures;single measure;artifact similarity;diverse development artifacts;tailored probabilistic model;HierarchiCal PrObabilistic Model;SoftwarE Traceability;candidate trace links;complementary observational prowess;multiple measures;textual similarity;developer feedback;transitive relationships;often implicit;inference accuracy;comprehensive empirical evaluation;comparative effectiveness;prototype Comet Jenkins plugin,
420,API,How Android Developers Handle Evolution-induced API Compatibility Issues: A Large-scale Study,H. Xia; Y. Zhang; Y. Zhou; X. Chen; Y. Wang; X. Zhang; S. Cui; G. Hong; X. Zhang; M. Yang; Z. Yang,"Fudan University; Fudan University; Fudan University; Fudan University; Fudan University; Purdue University; Fudan University; Fudan University; Fudan University; Ministry of Education, Shanghai Institute of Intelligent Electronics & Systems, Shanghai Institute for Advanced Communication and Data Science, and Engineering Research Center of CyberSecurity Auditing and Monitoring, China; Fudan University",2020,"As Android platform evolves in a fast pace, API-related compatibility issues become a significant challenge for developers. To handle an incompatible API invocation, developers mainly have two choices: merely performing sufficient checks to avoid invoking incompatible APIs on platforms that do not support them, or gracefully providing replacement implementations on those incompatible platforms. As providing more consistent app behaviors, the latter one is more recommended and more challenging to adopt. However, it is still unknown how these issues are handled in the real world, do developers meet difficulties and what can we do to help them. In light of this, this paper performs the first large-scale study on the current practice of handling evolution-induced API compatibility issues in about 300,000 Android market apps, and more importantly, their solutions (if exist). Actually, it is in general very challenging to determine if developers have put in countermeasure for a compatibility issue, as different APIs have diverse behaviors, rendering various repair. To facilitate a large-scale study, this paper proposes RAPID, an automated tool to determine whether a compatibility issue has been addressed or not, by incorporating both static analysis and machine learning techniques. Results show that our trained classifier is quite effective by achieving a F1-score of 95.21% and 91.96% in the training stage and the validation stage respectively. With the help of RAPID, our study yields many interesting findings, e.g. developers are not willing to provide alternative implementations when handling incompatible API invocations (only 38.4%); for those incompatible APIs that Google gives replacement recommendations, the ratio of providing alternative implementations is significantly higher than those without recommendations; developers find more ways to repair compatibility issues than Google's recommendations and the knowledge acquired from these experienced developers would be extremely useful to novice developers and may significantly improve the current status of compatibility issue handling.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284041,Compatibility Issues;API Evolution;Android App Analysis,Training;Static analysis;Maintenance engineering;Tools;Rendering (computer graphics);Internet;Software engineering,Android (operating system);application program interfaces;learning (artificial intelligence);mobile computing;power aware computing,alternative implementations;repair compatibility issues;novice developers;large-scale study;API-related compatibility issues;incompatible API invocation;consistent app behaviors;Android developers handle evolution;API compatibility issues;Android market apps;static analysis;machine learning techniques,
421,API,An Empirical Study on API Parameter Rules,H. Zhong; N. Meng; Z. Li; L. Jia,"Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Virginia Polytechnic Institute and State University, USA; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China; Department of Computer Science and Engineering, Shanghai Jiao Tong University, China",2020,"Developers build programs based on software libraries to reduce coding effort. If a program inappropriately sets an API parameter, the program may exhibit unexpected runtime behaviors. To help developers correctly use library APIs, researchers built tools to mine API parameter rules. However, it is still unknown (1) what types of parameter rules there are, and (2) how these rules distribute inside documents and source files. In this paper, we conducted an empirical study to investigate the above-mentioned questions. To analyze as many parameter rules as possible, we took a hybrid approach that combines automatic localization of constrained parameters with manual inspection. Our automatic approach-PaRu-locates parameters that have constraints either documented in Javadoc (i.e., document rules) or implied by source code (i.e., code rules). Our manual inspection (1) identifies and categorizes rules for the located parameters, and (2) establishes mapping between document and code rules. By applying PaRu to 9 widely used libraries, we located 5,334 parameters with either document or code rules. Interestingly, there are only 187 parameters that have both types of rules, and 79 pairs of these parameter rules are unmatched. Additionally, PaRu extracted 1,688 rule sentences from Javadoc and code. We manually classified these sentences into six categories, two of which are overlooked by prior approaches. We found that 86.2% of parameters have only code rules; 10.3% of parameters have only document rules; and only 3.5% of parameters have both document and code rules. Our research reveals the challenges for automating parameter rule extraction. Based on our findings, we discuss the potentials of prior approaches and present our insights for future tool design.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283984,,Software libraries;Runtime;Manuals;Tools;Inspection;Encoding,application program interfaces;Java;object-oriented programming;software libraries,library APIs;mine API parameter rules;constrained parameters;automatic approach-PaRu-locates parameters;document rules;code rules;located parameters;1 rule sentences;688 rule sentences;automating parameter rule extraction,
422,API,When APIs are Intentionally Bypassed: An Exploratory Study of API Workarounds,M. Lamothe; W. Shang,"Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada",2020,"Application programming interfaces (APIs) have become ubiquitous in software development. However, external APIs are not guaranteed to contain every desirable feature, nor are they immune to software defects. Therefore, API users will sometimes be faced with situations where a current API does not satisfy all of their requirements, but migrating to another API is costly. In these cases, due to the lack of communication channels between API developers and users, API users may intentionally bypass an existing API after inquiring into workarounds for their API problems with online communities. This mechanism takes the API developer out of the conversation, potentially leaving API defects unreported and desirable API features undiscovered. In this paper we explore API workaround inquiries from API users on Stack Overflow. We uncover general reasons why API users inquire about API workarounds, and general solutions to API workaround requests. Furthermore, using workaround implementations in Stack Overflow answers, we develop three API workaround implementation patterns. We identify instances of these patterns in real-life open source projects and determine their value for API developers from their responses to feature requests based on the identified API workarounds.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283953,software maintenance;API workarounds;Mining software repositories;Empirical software engineering,Knowledge engineering;Communication channels;Feature extraction;Software;Software engineering,application program interfaces;software maintenance,external APIs;API users;API developer;API problems;API workaround inquiries;API workaround requests;API workaround implementation patterns;application programming interfaces;software development;stack overflow answer,
423,API,"Demystify Official API Usage Directives with Crowdsourced API Misuse Scenarios, Erroneous Code Examples and Patches",X. Ren; J. Sun; Z. Xing; X. Xia; J. Sun,"PengCheng Laboratory; Australian National University, Canberra, ACT, Australia; Data61, CSIRO; Monash University, Melbourne, VIC, Australia; Zhejiang University, Hangzhou, Zhejiang, China",2020,"API usage directives in official API documentation describe the contracts, constraints and guidelines for using APIs in natural language. Through the investigation of API misuse scenarios on Stack Overflow, we identify three barriers that hinder the understanding of the API usage directives, i.e., lack of specific usage context, indirect relationships to cooperative APIs, and confusing APIs with subtle differences. To overcome these barriers, we develop a text mining approach to discover the crowdsourced API misuse scenarios on Stack Overflow and extract from these scenarios erroneous code examples and patches, as well as related API and confusing APIs to construct demystification reports to help developers understand the official API usage directives described in natural language. We apply our approach to API usage directives in official Android API documentation and android-tagged discussion threads on Stack Overflow. We extract 159,116 API misuse scenarios for 23,969 API usage directives of 3138 classes and 7471 methods, from which we generate the demystification reports. Our manual examination confirms that the extracted information in the generated demystification reports are of high accuracy. By a user study of 14 developers on 8 API-misuse related error scenarios, we show that our demystification reports help developer understand and debug API-misuse related program errors faster and more accurately, compared with reading only plain API usage-directive sentences.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283947,API usage directive;API misuse;Stack Overflow;Open information extraction,Text mining;Natural languages;Documentation;Manuals;Information retrieval;Software engineering;Message systems,application program interfaces;data mining;program debugging;program diagnostics;text analysis,official Android API documentation;Stack Overflow;159 API misuse scenarios;116 API misuse scenarios;23 API usage directives;969 API usage directives;8 API-misuse related error scenarios;API-misuse related program errors;plain API usage-directive sentences;demystify official API usage directives;crowdsourced API misuse scenarios;official API documentation;APIs;specific usage context;scenarios erroneous code examples;related API,
424,Bug Analysis,Simulee: Detecting CUDA Synchronization Bugs via Memory-Access Modeling,M. Wu; Y. Ouyang; H. Zhou; L. Zhang; C. Liu; Y. Zhang,"Southern University of Science and Technology, Shenzhen, China; Southern University of Science and Technology, Shenzhen, China; University of Texas at Dallas, Dallas, USA; University of Texas at Dallas, Dallas, USA; University of Texas at Dallas, Dallas, USA; Southern University of Science and Technology, Shenzhen, China",2020,"While CUDA has become a mainstream parallel computing platform and programming model for general-purpose GPU computing, how to effectively and efficiently detect CUDA synchronization bugs remains a challenging open problem. In this paper, we propose the first lightweight CUDA synchronization bug detection framework, namely Simulee, to model CUDA program execution by interpreting the corresponding LLVM bytecode and collecting the memory-access information for automatically detecting general CUDA synchronization bugs. To evaluate the effectiveness and efficiency of Simulee, we construct a benchmark with 7 popular CUDA-related projects from GitHub, upon which we conduct an extensive set of experiments. The experimental results suggest that Simulee can detect 21 out of the 24 manually identified bugs in our preliminary study and also 24 previously unknown bugs among all projects, 10 of which have already been confirmed by the developers. Furthermore, Simulee significantly outperforms state-of-the-art approaches for CUDA synchronization bug detection.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284094,,Computational modeling;Computer bugs;Graphics processing units;Benchmark testing;Synchronization;Software engineering;Software development management,graphics processing units;multiprocessing systems;parallel architectures;parallel programming;program compilers;program debugging,Simulee;CUDA program execution;memory-access information;general CUDA synchronization bugs;7 popular CUDA-related projects;24 manually identified bugs;24 previously unknown bugs;memory-access modeling;mainstream parallel computing platform;programming model;general-purpose GPU computing;challenging open problem;lightweight CUDA synchronization bug detection framework,5
425,Deep Learning Testing and Debugging 2,White-box Fairness Testing through Adversarial Sampling,P. Zhang; J. Wang; J. Sun; G. Dong; X. Wang; X. Wang; J. S. Dong; T. Dai,Zhejiang University; National University of Singapore; Singapore Management University; Zhejiang University; Zhejiang University; Zhejiang University; National University of Singapore; Huawei International Pte. Ltd.,2020,"Although deep neural networks (DNNs) have demonstrated astonishing performance in many applications, there are still concerns on their dependability. One desirable property of DNN for applications with societal impact is fairness (i.e., non-discrimination). In this work, we propose a scalable approach for searching individual discriminatory instances of DNN. Compared with state-of-the-art methods, our approach only employs lightweight procedures like gradient computation and clustering, which makes it significantly more scalable than existing methods. Experimental results show that our approach explores the search space more effectively (9 times) and generates much more individual discriminatory instances (25 times) using much less time (half to 1/7).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283952,,Neural networks;Benchmark testing;Space exploration;Software engineering,neural nets,white-box fairness;adversarial sampling;deep neural networks;DNNs;gradient computation;clustering;search space,
426,Deep Learning Testing and Debugging 2,Structure-Invariant Testing for Machine Translation,P. He; C. Meister; Z. Su,"Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland",2020,"In recent years, machine translation software has increasingly been integrated into our daily lives. People routinely use machine translation for various applications, such as describing symptoms to a foreign doctor and reading political news in a foreign language. However, the complexity and intractability of neural machine translation (NMT) models that power modern machine translation make the robustness of these systems difficult to even assess, much less guarantee. Machine translation systems can return inferior results that lead to misunderstanding, medical misdiagnoses, threats to personal safety, or political conflicts. Despite its apparent importance, validating the robustness of machine translation systems is very difficult and has, therefore, been much under-explored. To tackle this challenge, we introduce structure-invariant testing (SIT), a novel metamorphic testing approach for validating machine translation software. Our key insight is that the translation results of â€œsimilarâ€ source sentences should typically exhibit similar sentence structures. Specifically, SIT (1) generates similar source sentences by substituting one word in a given sentence with semantically similar, syntactically equivalent words; (2) represents sentence structure by syntax parse trees (obtained via constituency or dependency parsing); (3) reports sentence pairs whose structures differ quantitatively by more than some threshold. To evaluate SIT, we use it to test Google Translate and Bing Microsoft Translator with 200 source sentences as input, which led to 64 and 70 buggy issues with 69.5% and 70% top-1 accuracy, respectively. The translation errors are diverse, including under-translation, over-translation, incorrect modification, word/phrase mistranslation, and unclear logic.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284002,Metamorphic testing;Machine translation;Structural invariance,Systematics;Tools;Syntactics;Software;Robustness;Internet;Testing,grammars;language translation;natural language processing,machine translation software;foreign doctor;reading political news;neural machine translation models;power modern machine translation;machine translation systems;structure-invariant testing;SIT;novel metamorphic testing approach;translation results;similar source sentences;similar sentence structures;sentence structure;sentence pairs whose structures;Google Translate;Bing Microsoft Translator;200 source sentences;translation errors;under-translation;over-translation,
427,Deep Learning Testing and Debugging 2,Automatic Testing and Improvement of Machine Translation,Z. Sun; J. M. Zhang; M. Harman; M. Papadakis; L. Zhang,"Key Laboratory of HCST, Peking University, MoE; University College London; Facebook London; University of Luxembourg; Key Laboratory of HCST, Peking University, MoE",2020,"This paper presents TransRepair, a fully automatic approach for testing and repairing the consistency of machine translation systems. TransRepair combines mutation with metamorphic testing to detect inconsistency bugs (without access to human oracles). It then adopts probability-reference or cross-reference to post-process the translations, in a grey-box or black-box manner, to repair the inconsistencies. Our evaluation on two state-of-the-art translators, Google Translate and Transformer, indicates that TransRepair has a high precision (99%) on generating input pairs with consistent translations. With these tests, using automatic consistency metrics and manual assessment, we find that Google Translate and Transformer have approximately 36% and 40% inconsistency bugs. Black-box repair fixes 28% and 19% bugs on average for Google Translate and Transformer. Grey-box repair fixes 30% bugs on average for Transformer. Manual inspection indicates that the translations repaired by our approach improve consistency in 87% of cases (degrading it in 2%), and that our repairs have better translation acceptability in 27% of the cases (worse in 8%).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284046,machine translation;testing and repair;translation consistency,Measurement;Computer bugs;Manuals;Maintenance engineering;Inspection;Internet;Software engineering,language translation;probability;program diagnostics;program testing,automatic testing;TransRepair;fully automatic approach;machine translation systems;metamorphic testing;probability-reference;cross-reference;black-box manner;state-of-the-art translators;Google Translate;Transformer;consistent translations;automatic consistency metrics;40% inconsistency bugs;black-box repair;grey-box repair;translation acceptability,
428,Deep Learning Testing and Debugging 2,TRADER: Trace Divergence Analysis and Embedding Regulation for Debugging Recurrent Neural Networks,G. Tao; S. Ma; Y. Liu; Q. Xu; X. Zhang,Purdue University; Rutgers University; Purdue University; Purdue University; Purdue University,2020,"Recurrent Neural Networks (RNN) can deal with (textual) input with various length and hence have a lot of applications in software systems and software engineering applications. RNNs depend on word embeddings that are usually pre-trained by third parties to encode textual inputs to numerical values. It is well known that problematic word embeddings can lead to low model accuracy. In this paper, we propose a new technique to automatically diagnose how problematic embeddings impact model performance, by comparing model execution traces from correctly and incorrectly executed samples. We then leverage the diagnosis results as guidance to harden/repair the embeddings. Our experiments show that TRADER can consistently and effectively improve accuracy for real world models and datasets by 5.37% on average, which represents substantial improvement in the literature of RNN models.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284017,,Analytical models;Recurrent neural networks;Software algorithms;Software systems;Regulation;Numerical models;Software engineering,learning (artificial intelligence);neural nets;recurrent neural nets,RNN models;world models;incorrectly executed samples;correctly executed samples;model execution traces;problematic embeddings impact model performance;low model accuracy;problematic word embeddings;textual inputs;software engineering applications;software systems;Recurrent Neural Networks;recurrent Neural Networks;embedding regulation;trace divergence analysis;TRADER,
429,Fuzzing 2,Typestate-Guided Fuzzer for Discovering Use-after-Free Vulnerabilities,H. Wang; X. Xie; Y. Li; C. Wen; Y. Li; Y. Liu; S. Qin; H. Chen; Y. Sui,"CSSE, Shenzhen University, China; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Shenzhen University, Shenzhen, China; Nanyang Technological University, Singapore; Zhejiang Sci-Tech University, China; CSSE, Shenzhen University, China; Nanyang Technological University, Singapore; University of Technology Sydney, Australia",2020,"Existing coverage-based fuzzers usually use the individual control flow graph (CFG) edge coverage to guide the fuzzing process, which has shown great potential in finding vulnerabilities. However, CFG edge coverage is not effective in discovering vulnerabilities such as use-after-free (UaF). This is because, to trigger UaF vulnerabilities, one needs not only to cover individual edges, but also to traverse some (long) sequence of edges in a particular order, which is challenging for existing fuzzers. To this end, we propose to model UaF vulnerabilities as typestate properties, and develop a typestate-guided fuzzer, named UAFL, for discovering vulnerabilities violating typestate properties. Given a typestate property, we first perform a static typestate analysis to find operation sequences potentially violating the property. Our fuzzing process is then guided by the operation sequences in order to progressively generate test cases triggering property violations. In addition, we also employ an information flow analysis to improve the efficiency of the fuzzing process. We have performed a thorough evaluation of UAFL on 14 widely-used real-world programs. The experiment results show that UAFL substantially outperforms the state-of-the-art fuzzers, including AFL, AFLFast, FairFuzz, MOpt, Angora and QSYM, in terms of the time taken to discover vulnerabilities. We have discovered 10 previously unknown vulnerabilities, and received 5 new CVEs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283945,Fuzzing;Typestate-guided fuzzing;Use-after-Free vulnerabilities,Process control;Fuzzing;Flow graphs;Software engineering,data flow analysis;program diagnostics;program testing;program verification;security of data,typestate-guided fuzzer;discovering vulnerabilities;typestate property;static typestate analysis;operation sequences;fuzzing process;property violations;information flow analysis;state-of-the-art fuzzers;10 previously unknown vulnerabilities;existing coverage-based fuzzers;individual control flow graph edge coverage;CFG edge coverage;UaF vulnerabilities;individual edges;existing fuzzers,
430,Fuzzing 2,JVM Fuzzing for JIT-Induced Side-Channel Detection,T. Brennan; S. Saha; T. Bultan,"University of California Santa Barbara, Santa Barbara, CA, USA; University of California Santa Barbara, Santa Barbara, CA, USA; University of California Santa Barbara, Santa Barbara, CA, USA",2020,"Timing side channels arise in software when a program's execution time can be correlated with security-sensitive program input. Recent results on software side-channel detection focus on analysis of program's source code. However, runtime behavior, in particular optimizations introduced during just-in-time (JIT) compilation, can impact or even introduce timing side channels in programs. In this paper, we present a technique for automatically detecting such JIT-induced timing side channels in Java programs. We first introduce patterns to detect partitions of secret input potentially separable by side channels. Then we present an automated approach for exploring behaviors of the Java Virtual Machine (JVM) to identify states where timing channels separating these partitions arise. We evaluate our technique on three datasets used in recent work on side-channel detection. We find that many code variants labeled â€œsafeâ€ with respect to side-channel vulnerabilities are in fact vulnerable to JIT-induced timing side channels. Our results directly contradict the conclusions of four separate state-of-the-art program analysis tools for side-channel detection and demonstrate that JIT-induced side channels are prevalent and can be detected automatically.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284028,Side channel analysis;computer security;fuzzing,Java;Runtime;Government;Tools;Software;Virtual machining;Timing,Java;just-in-time;program diagnostics;virtual machines,JIT-induced side channels;separate state-of-the-art program analysis tools;side-channel vulnerabilities;timing channels;Java programs;JIT-induced timing side channels;just-in-time compilation;side-channel detection focus;security-sensitive program input;JIT-induced side-channel detection,
431,Fuzzing 2,Ankou: Guiding Grey-box Fuzzing towards Combinatorial Difference,V. J. M. ManÃ¨s; S. Kim; S. K. Cha,"CSRC, KAIST, Daejeon, Korea; KAIST, Daejeon, Korea; KAIST, Daejeon, Korea",2020,"Grey-box fuzzing is an evolutionary process, which maintains and evolves a population of test cases with the help of a fitness function. Fitness functions used by current grey-box fuzzers are not informative in that they cannot distinguish different program executions as long as those executions achieve the same coverage. The problem is that current fitness functions only consider a union of data, but not their combination. As such, fuzzers often get stuck in a local optimum during their search. In this paper, we introduce Ankou, the first grey-box fuzzer that recognizes different combinations of execution information, and present several scalability challenges encountered while designing and implementing Ankou. Our experimental results show that Ankou is 1.94Ã— and 8.0Ã— more effective in finding bugs than AFL and Angora, respectively.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283954,fuzz testing;guided fuzzing;grey-box fuzzing;software testing;principal component analysis,Scalability;Computer bugs;Sociology;Transforms;Fuzzing;Software engineering;Principal component analysis,evolutionary computation;genetic algorithms;grey systems;program debugging;program testing,fitness function;grey-box fuzzer;different program executions;current fitness functions;fuzzers;execution information;designing implementing Ankou;guiding grey-box fuzzing;combinatorial difference;evolutionary process,
432,Static Analysis 2,BCFA: Bespoke Control Flow Analysis for CFA at Scale,R. Ramu; G. B. Upadhyaya; H. A. Nguyen; H. Rajan,"Microsoft, Redmond, Washington; Harmony, Mountain View, California; Amazon, Seattle, Washinton; Dept. of Computer Science, Iowa State University, Ames, Iowa",2020,"Many data-driven software engineering tasks such as discovering programming patterns, mining API specifications, etc., perform source code analysis over control flow graphs (CFGs) at scale. Analyzing millions of CFGs can be expensive and performance of the analysis heavily depends on the underlying CFG traversal strategy. State-of-the-art analysis frameworks use a fixed traversal strategy. We argue that a single traversal strategy does not fit all kinds of analyses and CFGs and propose bespoke control flow analysis (BCFA). Given a control flow analysis (CFA) and a large number of CFGs, BCFA selects the most efficient traversal strategy for each CFG. BCFA extracts a set of properties of the CFA by analyzing the code of the CFA and combines it with properties of the CFG, such as branching factor and cyclicity, for selecting the optimal traversal strategy. We have implemented BCFA in Boa, and evaluated BCFA using a set of representative static analyses that mainly involve traversing CFGs and two large datasets containing 287 thousand and 162 million CFGs. Our results show that BCFA can speedup the large scale analyses by 1%-28%. Further, BCFA has low overheads; less than 0.2%, and low misprediction rate; less than 0.01%.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284024,Source code analysis;Boa,Static analysis;Programming;Flow graphs;Data mining;Task analysis;Software engineering,application program interfaces;data mining;flow graphs;graph theory;program diagnostics;software engineering,data-driven software engineering tasks;source code analysis;control flow graphs;underlying CFG traversal strategy;state-of-the-art analysis frameworks;fixed traversal strategy;single traversal strategy;efficient traversal strategy;optimal traversal strategy;287 thousand CFGs;162 million CFGs;CFA;control flow analysis;BCFA,
433,Static Analysis 2,On the Recall of Static Call Graph Construction in Practice,L. Sui; J. Dietrich; A. Tahir; G. Fourtounis,"Massey University, Palmerston North, New Zealand; Victoria University of Wellington, Wellington, New Zealand; Massey University, Palmerston North, New Zealand; University of Athens, Athens, Greece",2020,"Static analyses have problems modelling dynamic language features soundly while retaining acceptable precision. The problem is well-understood in theory, but there is little evidence on how this impacts the analysis of real-world programs. We have studied this issue for call graph construction on a set of 31 real-world Java programs using an oracle of actual program behaviour recorded from executions of built-in and synthesised test cases with high coverage, have measured the recall that is being achieved by various static analysis algorithms and configurations, and investigated which language features lead to static analysis false negatives. We report that (1) the median recall is 0.884 suggesting that standard static analyses have significant gaps with respect to the proportion of the program modelled (2) built-in tests are significantly better to expose dynamic program behaviour than synthesised tests (3) adding precision to the static analysis has little impact on recall indicating that those are separate concerns (4) state-of-the-art support for dynamic language features can significantly improve recall (the median observed is 0.935), but it comes with a hefty performance penalty, and (5) the main sources of unsoundness are not reflective method invocations, but objects allocated or accessed via native methods, and invocations initiated by the JVM, without matching call sites in the program under analysis. These results provide some novel insights into the interaction between static and dynamic program analyses that can be used to assess the utility of static analysis results and to guide the development of future static and hybrid analyses. These results provide some novel insights into the interaction between static and dynamic program analyses that can be used to assess the utility of static analysis results and to guide the development of future static and hybrid analyses.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283958,static program analysis;testing;soundness;java;call graph construction;test case generation,Analytical models;Heuristic algorithms;Static analysis;Dynamic scheduling;Reflection;Performance analysis;Test pattern generators,dynamic programming;Java;program diagnostics;program verification,dynamic program behaviour;synthesised tests adding precision;separate concerns state-of-the-art support;static program analyses;dynamic program analyses;static analysis results;future static analyses;hybrid analyses;static call graph construction;problems modelling dynamic language;real-world programs;real-world Java programs;actual program;synthesised test cases;static analysis algorithms;static analysis false negatives;median recall;standard static analyses,
434,Static Analysis 2,Heaps'n Leaks: How Heap Snapshots Improve Android Taint Analysis,M. Benz; E. K. Kristensen; L. Luo; N. P. Borges; E. Bodden; A. Zeller,"Department of Computer Science, Paderborn University, Germany; Department of Computer Science, Aarhus University, Denmark; Department of Computer Science, Paderborn University, Germany; CISPA Helmholtz Center for Information Security, Germany; Paderborn University & Fraunhofer IEM, Germany; CISPA Helmholtz Center for Information Security, Germany",2020,"The assessment of information flows is an essential part of analyzing Android apps, and is frequently supported by static taint analysis. Its precision, however, can suffer from the analysis not being able to precisely determine what elements a pointer can (and cannot) point to. Recent advances in static analysis suggest that incorporating dynamic heap snapshots, taken at one point at runtime, can significantly improve general static analysis. In this paper, we investigate to what extent this also holds for taint analysis, and how various design decisions, such as when and how many snapshots are collected during execution, and how exactly they are used, impact soundness and precision. We have extended FlowDroid to incorporate heap snapshots, yielding our prototype Heapster, and evaluated it on DroidMacroBench, a novel benchmark comprising real-world Android apps that we also make available as an artifact. The results show (1) the use of heap snapshots lowers analysis time and memory consumption while increasing precision; (2) a very good trade-off between precision and recall is achieved by a mixed mode in which the analysis falls back to static points-to relations for objects for which no dynamic data was recorded; and (3) while a single heap snapshot (ideally taken at the end of the execution) suffices to improve performance and precision, a better trade-off can be obtained by using multiple snapshots.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284080,points-to analysis;heap snapshot;taint analysis;Soot,Runtime;Upper bound;Memory management;Prototypes;Static analysis;Benchmark testing;Resource management,Android (operating system);mobile computing;program diagnostics;security of data;smart phones;storage management,single heap snapshot;multiple snapshots;heaps;heap snapshots improve Android taint analysis;information flows;Android apps;static taint analysis;incorporating dynamic heap snapshots;general static analysis;real-world Android;static points-to relations,
435,Big Data,Big Code != Big Vocabulary: Open-Vocabulary Models for Source Code,R. -M. Karampatsis; H. Babii; R. Robbes; C. Sutton; A. Janes,"University of Edinburgh, Edinburgh, United Kingdom; Free University of Bozen-Bolzano, Bozen-Bolzano, Italy; Free University of Bozen-Bolzano, Bozen-Bolzano, Italy; Google Research and University of Edinburgh, Mountain View, CA, United States; Free University of Bozen-Bolzano, Bozen-Bolzano, Italy",2020,"Statistical language modeling techniques have successfully been applied to large source code corpora, yielding a variety of new software development tools, such as tools for code suggestion, improving readability, and API migration. A major issue with these techniques is that code introduces new vocabulary at a far higher rate than natural language, as new identifier names proliferate. Both large vocabularies and out-of-vocabulary issues severely affect Neural Language Models (NLMs) of source code, degrading their performance and rendering them unable to scale. In this paper, we address this issue by: 1) studying how various modelling choices impact the resulting vocabulary on a large-scale corpus of 13,362 projects; 2) presenting an open vocabulary source code NLM that can scale to such a corpus, 100 times larger than in previous work; and 3) showing that such models outperform the state of the art on three distinct code corpora (Java, C, Python). To our knowledge, these are the largest NLMs for code that have been reported. All datasets, code, and trained models used in this work are publicly available.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284032,Naturalness of code;Neural Language Models;Byte-Pair Encoding,Training;Vocabulary;Adaptation models;Computer bugs;Predictive models;Tools;Software engineering,application program interfaces;Java;natural language processing;natural languages;neurophysiology;programming languages;software engineering;software maintenance;source code (software);statistical analysis;vocabulary,big code;big vocabulary;open-vocabulary Models;statistical language modeling techniques;source code corpora;software development tools;code suggestion;natural language;out-of-vocabulary issues;Neural Language Models;resulting vocabulary;large-scale corpus;open vocabulary source code NLM;distinct code corpora;trained models,
436,Big Data,Improving Data Scientist Efficiency with Provenance,J. Hu; J. Joung; M. Jacobs; K. Z. Gajos; M. I. Seltzer,"Harvard University, Cambridge, MA, USA; University of Michigan, Ann Arbor, MI, USA; Harvard University, Cambridge, MA, USA; Harvard University, Cambridge, MA, USA; University of British Columbia, Vancouver, BC, Canada",2020,"Data scientists frequently analyze data by writing scripts. We conducted a contextual inquiry with interdisciplinary researchers, which revealed that parameter tuning is a highly iterative process and that debugging is time-consuming. As analysis scripts evolve and become more complex, analysts have difficulty conceptualizing their workflow. In particular, after editing a script, it becomes difficult to determine precisely which code blocks depend on the edit. Consequently, scientists frequently re-run entire scripts instead of re-running only the necessary parts. We present ProvBuild, a tool that leverages language-level provenance to streamline the debugging process by reducing programmer cognitive load and decreasing subsequent runtimes, leading to an overall reduction in elapsed debugging time. ProvBuild uses provenance to track dependencies in a script. When an analyst debugs a script, ProvBuild generates a simplified script that contains only the information necessary to debug a particular problem. We demonstrate that debugging the simplified script lowers a programmer's cognitive load and permits faster re-execution when testing changes. The combination of reduced cognitive load and shorter runtime reduces the time necessary to debug a script. We quantitatively and qualitatively show that even though ProvBuild introduces overhead during a script's first execution, it is a more efficient way for users to debug and tune complex workflows. ProvBuild demonstrates a novel use of language-level provenance, in which it is used to proactively improve programmer productively rather than merely providing a way to retroactively gain insight into a body of code.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284103,Provenance;incremental execution;dependency tracking;data analysis,Runtime;Pipelines;Debugging;Tools;Writing;Tuning;Testing,data analysis;data visualisation;human computer interaction;interactive systems;program debugging;user interfaces,ProvBuild;improving data scientist efficiency;data scientists;writing scripts;highly iterative process;analysis scripts evolve;precisely which code blocks;entire scripts;leverages language-level provenance;debugging process;programmer cognitive load;decreasing subsequent runtimes;elapsed debugging time;simplified script;reduced cognitive load,
437,Big Data,Managing data constraints in database-backed web applications,J. Yang; U. Sethi; C. Yan; A. Cheung; S. Lu,"University of Chicago, USA; University of Chicago, USA; University of Washington, USA; University of California, Berkeley, USA; University of Chicago, USA",2020,"Database-backed web applications manipulate large amounts of persistent data, and such applications often contain constraints that restrict data length, data value, and other data properties. Such constraints are critical in ensuring the reliability and usability of these applications. In this paper, we present a comprehensive study on where data constraints are expressed, what they are about, how often they evolve, and how their violations are handled. The results show that developers struggle with maintaining consistent data constraints and checking them across different components and versions of their web applications, leading to various problems. Guided by our study, we developed checking tools and API enhancements that can automatically detect such problems and improve the quality of such applications.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284009,data constraints;database backed web applications;data consistency,Tools;Reliability;Usability;Open source software;Software engineering,,,
438,Deep Learning Testing and Debugging 3,Taxonomy of Real Faults in Deep Learning Systems,N. Humbatova; G. Jahangirova; G. Bavota; V. Riccio; A. Stocco; P. Tonella,"UniversitÃ della Svizzera italiana, Lugano, Switzerland; UniversitÃ della Svizzera italiana, Lugano, Switzerland; UniversitÃ della Svizzera italiana, Lugano, Switzerland; UniversitÃ della Svizzera italiana, Lugano, Switzerland; UniversitÃ della Svizzera italiana, Lugano, Switzerland; UniversitÃ della Svizzera italiana, Lugano, Switzerland",2020,"The growing application of deep neural networks in safety-critical domains makes the analysis of faults that occur in such systems of enormous importance. In this paper we introduce a large taxonomy of faults in deep learning (DL) systems. We have manually analysed 1059 artefacts gathered from GitHub commits and issues of projects that use the most popular DL frameworks (TensorFlow, Keras and PyTorch) and from related Stack Overflow posts. Structured interviews with 20 researchers and practitioners describing the problems they have encountered in their experience have enriched our taxonomy with a variety of additional faults that did not emerge from the other two sources. Our final taxonomy was validated with a survey involving an additional set of 21 developers, confirming that almost all fault categories (13/15) were experienced by at least 50% of the survey participants.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284134,deep learning;real faults;software testing;taxonomy,Deep learning;System testing;Taxonomy;Neural networks;Interviews;Software development management;Software engineering,learning (artificial intelligence);neural nets;public domain software;safety-critical software,deep learning systems;deep neural networks;safety-critical domains;DL frameworks;Stack Overflow posts;fault categories;GitHub;TensorFlow;Keras;PyTorch,57
439,Deep Learning Testing and Debugging 3,Testing DNN Image Classifiers for Confusion & Bias Errors,Y. Tian; Z. Zhong; V. Ordonez; G. Kaiser; B. Ray,Columbia University; Columbia University; University of Virginia; Columbia University; Columbia University,2020,"Image classifiers are an important component of today's software, from consumer and business applications to safety-critical domains. The advent of Deep Neural Networks (DNNs) is the key catalyst behind such wide-spread success. However, wide adoption comes with serious concerns about the robustness of software systems dependent on DNNs for image classification, as several severe erroneous behaviors have been reported under sensitive and critical circumstances. We argue that developers need to rigorously test their software's image classifiers and delay deployment until acceptable. We present an approach to testing image classifier robustness based on class property violations. We found that many of the reported erroneous cases in popular DNN image classifiers occur because the trained models confuse one class with another or show biases towards some classes over others. These bugs usually violate some class properties of one or more of those classes. Most DNN testing techniques focus on per-image violations, so fail to detect class-level confusions or biases. We developed a testing technique to automatically detect class-based confusion and bias errors in DNN-driven image classification software. We evaluated our implementation, DeepInspect, on several popular image classifiers with precision up to 100% (avg. 72.6%) for confusion errors, and up to 84.3% (avg. 66.8%) for bias errors. DeepInspect found hundreds of classification mistakes in widely-used models, many exposing errors indicating confusion or bias.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284045,whitebox testing;deep learning;DNNs;image classifiers;bias,Neural networks;Tools;Software systems;Robustness;Testing;Image classification;Software engineering,,,
440,Deep Learning Testing and Debugging 3,Repairing Deep Neural Networks: Fix Patterns and Challenges,M. J. Islam; R. Pan; G. Nguyen; H. Rajan,"Dept. of Computer Science, Iowa State University, Ames, IA, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA",2020,"Significant interest in applying Deep Neural Network (DNN) has fueled the need to support engineering of software that uses DNNs. Repairing software that uses DNNs is one such unmistakable SE need where automated tools could be beneficial; however, we do not fully understand challenges to repairing and patterns that are utilized when manually repairing DNNs. What challenges should automated repair tools address? What are the repair patterns whose automation could help developers? Which repair patterns should be assigned a higher priority for building automated bug repair tools? This work presents a comprehensive study of bug fix patterns to address these questions. We have studied 415 repairs from Stack Overflow and 555 repairs from GitHub for five popular deep learning libraries Caffe, Keras, Tensorflow, Theano, and Torch to understand challenges in repairs and bug repair patterns. Our key findings reveal that DNN bug fix patterns are distinctive compared to traditional bug fix patterns; the most common bug fix patterns are fixing data dimension and neural network connectivity; DNN bug fixes have the potential to introduce adversarial vulnerabilities; DNN bug fixes frequently introduce new bugs; and DNN bug localization, reuse of trained model, and coping with frequent releases are major challenges faced by developers when fixing bugs. We also contribute a benchmark of 667 DNN (bug, repair) instances.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284050,deep neural networks;bugs;bug fix;bug fix patterns,Computer bugs;Neural networks;Maintenance engineering;Tools;Software;Software engineering;Software development management,learning (artificial intelligence);neural nets;program debugging;public domain software;software maintenance,neural network connectivity;DNN bug fixes;DNN bug localization;fixing bugs;667 DNN;Deep Neural networks;Deep Neural Network;repairing software;automated tools;repair tools address;building automated bug repair tools;bug repair patterns;DNN bug fix patterns;traditional bug;common bug fix patterns,
441,Deep Learning Testing and Debugging 3,Fuzz Testing based Data Augmentation to Improve Robustness of Deep Neural Networks,X. Gao; R. K. Saha; M. R. Prasad; A. Roychoudhury,"National University of Singapore, Singapore; Fujitsu Laboratories of America, Inc, USA; Fujitsu Laboratories of America, Inc, USA; National University of Singapore, Singapore",2020,"Deep neural networks (DNN) have been shown to be notoriously brittle to small perturbations in their input data. This problem is analogous to the over-fitting problem in test-based program synthesis and automatic program repair, which is a consequence of the incomplete specification, i.e., the limited tests or training examples, that the program synthesis or repair algorithm has to learn from. Recently, test generation techniques have been successfully employed to augment existing specifications of intended program behavior, to improve the generalizability of program synthesis and repair. Inspired by these approaches, in this paper, we propose a technique that re-purposes software testing methods, specifically mutation-based fuzzing, to augment the training data of DNNs, with the objective of enhancing their robustness. Our technique casts the DNN data augmentation problem as an optimization problem. It uses genetic search to generate the most suitable variant of an input data to use for training the DNN, while simultaneously identifying opportunities to accelerate training by skipping augmentation in many instances. We instantiate this technique in two tools, Sensei and Sensei-SA, and evaluate them on 15 DNN models spanning 5 popular image data-sets. Our evaluation shows that Sensei can improve the robust accuracy of the DNN, compared to the state of the art, on each of the 15 models, by upto 11.9% and 5.5% on average. Further, Sensei-SA can reduce the average DNN training time by 25%, while still improving robust accuracy.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283925,Genetic Algorithm;DNN;Robustness;Data Augmentation,Training;Neural networks;Maintenance engineering;Tools;Software systems;Robustness;Test pattern generators,automatic programming;genetic algorithms;learning (artificial intelligence);neural nets;program debugging;program diagnostics;program testing;program verification,incomplete specification;repair algorithm;test generation techniques;existing specifications;intended program behavior;mutation-based fuzzing;DNN data augmentation problem;optimization problem;Sensei-SA;DNN models;image datasets;robust accuracy;average DNN training time;fuzz testing;deep neural networks;over-fitting problem;test-based program synthesis;automatic program repair;software testing methods;training data,
442,Deep Learning Testing and Debugging,An Empirical Study on Program Failures of Deep Learning Jobs,R. Zhang; W. Xiao; H. Zhang; Y. Liu; H. Lin; M. Yang,Microsoft Research; Alibaba Group; The University of Newcastle; Microsoft Research; Microsoft Research; Microsoft Research,2020,"Deep learning has made significant achievements in many application areas. To train and test models more efficiently, enterprise developers submit and run their deep learning programs on a shared, multi-tenant platform. However, some of the programs fail after a long execution time due to code/script defects, which reduces the development productivity and wastes expensive resources such as GPU, storage, and network I/O. This paper presents the first comprehensive empirical study on program failures of deep learning jobs. 4960 real failures are collected from a deep learning platform in Microsoft. We manually examine their failure messages and classify them into 20 categories. In addition, we identify the common root causes and bug-fix solutions on a sample of 400 failures. To better understand the current testing and debugging practices for deep learning, we also conduct developer interviews. Our major findings include: (1) 48.0% of the failures occur in the interaction with the platform rather than in the execution of code logic, mostly due to the discrepancies between local and platform execution environments; (2) Deep learning specific failures (13.5%) are mainly caused by inappropriate model parameters/structures and framework API misunderstanding; (3) Current debugging practices are not efficient for fault localization in many cases, and developers need more deep learning specific tools. Based on our findings, we further suggest possible research topics and tooling support that could facilitate future deep learning development.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284109,Deep learning jobs;Program failures;Empirical study,Deep learning;Training;Debugging;Tools;Interviews;Testing;Software engineering,application program interfaces;computer aided instruction;learning (artificial intelligence);program debugging;program diagnostics;user interfaces,current testing;bug-fix solutions;common root causes;failure messages;deep learning platform;comprehensive empirical study;wastes expensive resources;development productivity;long execution time;multitenant platform;deep learning programs;enterprise developers submit;deep learning jobs;program failures;future deep learning development;deep learning specific tools;specific failures;developer interviews;debugging practices,22
443,Natural Language Artifacts in Software,Primers or Reminders? The Effects of Existing Review Comments on Code Review,D. Spadini; G. Ã‡alikli; A. Bacchelli,"Software Improvement Group & Delft University of Technology, Amsterdam & Delft, The Netherlands; Chalmers & University of Gothenburg, Gothenburg, Sweden; University of Zurich, Zurich, Switzerland",2020,"In contemporary code review, the comments put by reviewers on a specific code change are immediately visible to the other reviewers involved. Could this visibility prime new reviewers' attention (due to the human's proneness to availability bias), thus biasing the code review outcome? In this study, we investigate this topic by conducting a controlled experiment with 85 developers who perform a code review and a psychological experiment. With the psychological experiment, we find that â‰ˆ70% of participants are prone to availability bias. However, when it comes to the code review, our experiment results show that participants are primed only when the existing code review comment is about a type of bug that is not normally considered; when this comment is visible, participants are more likely to find another occurrence of this type of bug. Moreover, this priming effect does not influence reviewers' likelihood of detecting other types of bugs. Our findings suggest that the current code review practice is effective because existing review comments about bugs in code changes are not negative primers, rather positive reminders for bugs that would otherwise be overlooked during code review. Data and materials: https://doi.org/10.5281/zenodo.3653856.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284010,Code Review;Priming;Availability Heuristic,Computer bugs;Psychology;Robustness;Task analysis;Software engineering,data handling;psychology;reviews;software engineering,code review outcome;psychological experiment;availability bias;existing code review comment;reviewers;current code review practice;existing review comments;code changes;contemporary code review;specific code change,
444,Natural Language Artifacts in Software,"Mitigating Turnover with Code Review Recommendation: Balancing Expertise, Workload, and Knowledge Distribution",E. Mirsaeedi; P. C. Rigby,"Department of Computer Science and Software Engineering, Concordia University, MontrÃ©al, QuÃ©bec, Canada; Department of Computer Science and Software Engineering, Concordia University, MontrÃ©al, QuÃ©bec, Canada",2020,"Developer turnover is inevitable on software projects and leads to knowledge loss, a reduction in productivity, and an increase in defects. Mitigation strategies to deal with turnover tend to disrupt and increase workloads for developers. In this work, we suggest that through code review recommendation we can distribute knowledge and mitigate turnover with minimal impact on the development process. We evaluate review recommenders in the context of ensuring expertise during review, Expertise, reducing the review workload of the core team, CoreWorkload, and reducing the Files at Risk to turnover, FaR. We find that prior work that assigns reviewers based on file ownership concentrates knowledge on a small group of core developers increasing risk of knowledge loss from turnover by up to 65%. We propose learning and retention aware review recommenders that when combined are effective at reducing the risk of turnover by -29% but they unacceptably reduce the overall expertise during reviews by -26%. We develop the Sofia recommender that suggests experts when none of the files under review are hoarded by developers, but distributes knowledge when files are at risk. In this way, we are able to simultaneously increase expertise during review with a Î”Expertise of 6%, with a negligible impact on workload of Î”CoreWorkload of 0.09%, and reduce the files at risk by Î”FaR -28%. Sofia is integrated into GitHub pull requests allowing developers to select an appropriate expert or â€œlearnerâ€ based on the context of the review. We release the Sofia bot as well as the code and data for replication purposes.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284120,Turnover;Knowledge Distribution;Code Review;Recommenders;Tool Support,Productivity;Knowledge engineering;Software;Software engineering;Software development management,distributed processing;learning (artificial intelligence);organisational aspects;project management;public domain software;recommender systems;software engineering;software quality;text analysis,mitigating turnover;code review recommendation;knowledge distribution;developer turnover;knowledge loss;mitigation strategies;increase workloads;mitigate turnover;review workload;file ownership;core developers;retention aware review recommenders;reviews;Sofia recommender;increase expertise,14
445,Open Source Software,How Do Companies Collaborate in Open Source Ecosystems? An Empirical Study of OpenStack,Y. Zhang; M. Zhou; K. -J. Stol; J. Wu; Z. Jin,"Department of Computer Science and Technology, Peking University, Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China; Department of Computer Science and Technology, Peking University, Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China; School of Computer Science and Information Technology, University College Cork Lero, the Irish Software Research Centre, Cork, Ireland; Department of Computer Science and Technology, Peking University, Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China; Department of Computer Science and Technology, Peking University, Key Laboratory of High Confidence Software Technologies, Ministry of Education, Beijing, China",2020,"Open Source Software (OSS) has come to play a critical role in the software industry. Some large ecosystems enjoy the participation of large numbers of companies, each of which has its own focus and goals. Indeed, companies that otherwise compete, may become collaborators within the OSS ecosystem they participate in. Prior research has largely focused on commercial involvement in OSS projects, but there is a scarcity of research focusing on company collaborations within OSS ecosystems. Some of these ecosystems have become critical building blocks for organizations worldwide; hence, a clear understanding of how companies collaborate within large ecosystems is essential. This paper presents the results of an empirical study of the OpenStack ecosystem, in which hundreds of companies collaborate on thousands of project repositories to deliver cloud distributions. Based on a detailed analysis, we identify clusters of collaborations, and identify four strategies that companies adopt to engage with the OpenStack ecosystem. We alsofind that companies may engage in intentional or passive collaborations, or may work in an isolated fashion. Further, wefi nd that a com-pany's position in the collaboration network is positively associated with its productivity in OpenStack. Our study sheds light on howlarge OSS ecosystems work, and in particular on the patterns of collaboration within one such large ecosystem.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284137,Open source software;OpenStack;company participation;OSS ecosystem;open collaboration;software development,Industries;Productivity;Cloud computing;Ecosystems;Collaboration;Companies;Software engineering,cloud computing;DP industry;groupware;public domain software,howlarge OSS ecosystems work;collaboration network;passive collaborations;intentional collaborations;OpenStack ecosystem;critical building blocks;company collaborations;OSS projects;OSS ecosystem;software industry;Open Source Software;open source ecosystems,3
446,Open Source Software,How to Not Get Rich: An Empirical Study of Donations in Opâ‚¬n $ourÂ¢e,C. Overney; J. Meinicke; C. KÃ¤stner; B. Vasilescu,"Olin College, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA",2020,"Open source is ubiquitous and many projects act as critical infrastructure, yet funding and sustaining the whole ecosystem is challenging. While there are many different funding models for open source and concerted efforts through foundations, donation platforms like PayPal, Patreon, and OpenCollective are popular and low-bar platforms to raise funds for open-source development. With a mixed-method study, we investigate the emerging and largely unexplored phenomenon of donations in open source. Specifically, we quantify how commonly open-source projects ask for donations, statistically model characteristics of projects that ask for and receive donations, analyze for what the requested funds are needed and used, and assess whether the received donations achieve the intended outcomes. We find 25,885 projects asking for donations on GitHub, often to support engineering activities; however, we also find no clear evidence that donations influence the activity level of a project. In fact, we find that donations are used in a multitude of ways, raising new research questions about effective funding.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283983,open source ecosystem;npm;GitHub;donations;funding,Biological system modeling;Ecosystems;Critical infrastructure;Open source software;Stress;Software development management;Software engineering,project management;public domain software;software engineering;statistical analysis,received donations;funding models;open-source development;open-source projects;statistically model characteristics;engineering activities,
447,Open Source Software,Scaling Open Source Communities: An Empirical Study of the Linux Kernel,X. Tan; M. Zhou; B. Fitzgerald,"Department of Computer Science and Technology, Ministry of Education, Peking University, Key Laboratory of High Confidence Software Technologies, Beijing, China; Department of Computer Science and Technology, Ministry of Education, Peking University, Key Laboratory of High Confidence Software Technologies, Beijing, China; Leroâ€”the Irish Software Research Centre, University of Limerick, Limerick, Ireland",2020,"Large-scale open source communities, such as the Linux kernel, have gone through decades of development, substantially growing in scale and complexity. In the traditional workflow, maintainers serve as â€œgatekeepersâ€ for the subsystems that they maintain. As the number of patches and authors significantly increases, maintainers come under considerable pressure, which may hinder the operation and even the sustainability of the community. A few subsystems have begun to use new workflows to address these issues. However, it is unclear to what extent these new workflows are successful, or how to apply them. Therefore, we conduct an empirical study on the multiple-committer model (MCM) that has provoked extensive discussion in the Linux kernel community. We explore the effect of the model on the i915 subsystem with respect to four dimensions: pressure, latency, complexity, and quality assurance. We find that after this model was adopted, the burden of the i915 maintainers was significantly reduced. Also, the model scales well to allow more committers. After analyzing the online documents and interviewing the maintainers of i915, we propose that overloaded subsystems which have trustworthy candidate committers are suitable for adopting the model. We further suggest that the success of the model is closely related to a series of measures for risk mitigation-sufficient precommit testing, strict review process, and the use of tools to simplify work and reduce errors. We employ a network analysis approach to locate candidate committers for the target subsystems and validate this approach and contextual success factors through email interviews with their maintainers. To the best of our knowledge, this is the first study focusing on how to scale open source communities. We expect that our study will help the rapidly growing Linux kernel and other similar communities to adapt to changes and remain sustainable.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284035,Open source communities;Scalability;Sustainability;Multiple committers;Maintainer;Workload;Linux kernel,Linux;Tools;Complexity theory;Kernel;Sustainable development;Testing;Software engineering,Linux;operating system kernels;public domain software;software reliability,open source communities;workflows;multiple-committer model;Linux kernel community;overloaded subsystems;trustworthy candidate committers;target subsystems;network analysis;risk mitigation;online documents;MCM;i915 maintainers,
448,Symbolic Execution,SPECUSYM: Speculative Symbolic Execution for Cache Timing Leak Detection,S. Guo; Y. Chen; P. Li; Y. Cheng; H. Wang; M. Wu; Z. Zuo,"Baidu Security; Penn State University; Baidu Security; Baidu Security; Baidu Security; Ant Financial Services Group; State Key Lab. for Novel Software Technology, Nanjing University",2020,"CPU cache is a limited but crucial storage component in modern processors, whereas the cache timing side-channel may inadvertently leak information through the physically measurable timing variance. Speculative execution, an essential processor optimization, and a source of such variances, can cause severe detriment on deliberate branch mispredictions. Despite static analysis could qualitatively verify the timing-leakage-free property under speculative execution, it is incapable of producing endorsements including inputs and speculated flows to diagnose leaks in depth. This work proposes a new symbolic execution based method, Specusym, for precisely detecting cache timing leaks introduced by speculative execution. Given a program (leakage-free in non-speculative execution), SpecuSym systematically explores the program state space, models speculative behavior at conditional branches, and accumulates the cache side effects along with subsequent path explorations. During the dynamic execution, SpecuSym constructs leak predicates for memory visits according to the specified cache model and conducts a constraint-solving based cache behavior analysis to inspect the new cache behaviors. We have implemented SpecuSym atop KLEE and evaluated it against 15 open-source benchmarks. Experimental results show that SpecuSym successfully detected from 2 to 61 leaks in 6 programs under 3 different cache settings and identified false positives in 2 programs reported by recent work.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284110,Speculative execution;cache;timing;side-channel leak;symbolic execution,Analytical models;Program processors;Static analysis;Timing;Optimization;Open source software;Software engineering,cache storage;leak detection;program diagnostics,3 different cache settings;speculative symbolic execution;cache timing leak detection;CPU cache;crucial storage component;physically measurable timing variance;speculative execution;essential processor optimization;timing-leakage-free property;symbolic execution based method;cache timing leaks;nonspeculative execution;models speculative behavior;dynamic execution;SpecuSym constructs leak;specified cache model;conducts;constraint-solving based cache behavior analysis;cache behaviors,4
449,Symbolic Execution,Symbolic Verification of Message Passing Interface Programs,H. Yu; Z. Chen; X. Fu; J. Wang; Z. Su; J. Sun; C. Huang; W. Dong,"College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China; State Key Laboratory of High Performance Computing, National University of Defense Technology, Changsha, China; State Key Laboratory of High Performance Computing, National University of Defense Technology, Changsha, China; Department of Computer Science, ETH, Zurich, Switzerland; School of Information Systems, Singapore Management University, Singapore; College of Computer, National University of Defense Technology, Changsha, China; College of Computer, National University of Defense Technology, Changsha, China",2020,"Message passing is the standard paradigm of programming in high-performance computing. However, verifying Message Passing Interface (MPI) programs is challenging, due to the complex program features (such as non-determinism and non-blocking operations). In this work, we present MPI symbolic verifier (MPI-SV), the first symbolic execution based tool for automatically verifying MPI programs with non-blocking operations. MPI-SV combines symbolic execution and model checking in a synergistic way to tackle the challenges in MPI program verification. The synergy improves the scalability and enlarges the scope of verifiable properties. We have implemented MPI-SV and evaluated it with 111 real-world MPI verification tasks. The pure symbolic execution-based technique successfully verifies 61 out of the 111 tasks (55%) within one hour, while in comparison, MPI-SV verifies 100 tasks (90%). On average, compared with pure symbolic execution, MPI-SV achieves 19x speedups on verifying the satisfaction of the critical property and 5x speedups on finding violations.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283955,Symbolic Verification;Symbolic Execution;Model Checking;Message Passing Interface;Synergy,Message passing;Model checking;Tools;Task analysis;Tuning;Standards;Software engineering,application program interfaces;message passing;program verification,MPI program verification;verifiable properties;real-world MPI verification tasks;pure symbolic execution-based technique;symbolic verification;high-performance computing;complex program features;MPI symbolic verifier;symbolic execution based tool;automatically verifying MPI programs;MPI-SV combines symbolic execution;message passing interface program verification,
450,Symbolic Execution,Efficient Generation of Error-Inducing Floating-Point Inputs via Symbolic Execution,H. Guo; C. Rubio-GonzÃ¡lez,"Department of Computer Science, University of California, Davis, USA; Department of Computer Science, University of California, Davis, USA",2020,"Floating point is widely used in software to emulate arithmetic over reals. Unfortunately, floating point leads to rounding errors that propagate and accumulate during execution. Generating inputs to maximize the numerical error is critical when evaluating the accuracy of floating-point code. In this paper, we formulate the problem of generating high error-inducing floating-point inputs as a code coverage maximization problem solved using symbolic execution. Specifically, we define inaccuracy checks to detect large precision loss and cancellation. We inject these checks at strategic program locations to construct specialized branches that, when covered by a given input, are likely to lead to large errors in the result. We apply symbolic execution to generate inputs that exercise these specialized branches, and describe optimizations that make our approach practical. We implement a tool named FPGen and present an evaluation on 21 numerical programs including matrix computation and statistics libraries. We show that FPGen exposes errors for 20 of these programs and triggers errors that are, on average, over 2 orders of magnitude larger than the state of the art.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283956,floating-point;testing;catastrophic cancellation;roundoff errors;symbolic execution,Tools;Libraries;Software;Generators;Explosions;Optimization;Software engineering,floating point arithmetic;optimisation;program testing,symbolic execution;floating point;rounding errors;numerical error;floating-point code;high error-inducing floating-point inputs;code coverage maximization problem;specialized branches;21 numerical programs;triggers errors,
451,Symbolic Execution,HyDiff: Hybrid Differential Software Analysis,Y. Noller; C. S. PÄƒsÄƒreanu; M. BÃ¶hme; Y. Sun; H. L. Nguyen; L. Grunske,"Humboldt-UniversitÃ¤t zu Berlin, Germany; Carnegie Mellon University Silicon Valley, NASA Ames Research Center, USA; Monash University, Australia; Queen's University, Belfast, United Kingdom; Humboldt-UniversitÃ¤t zu Berlin, Germany; Humboldt-UniversitÃ¤t zu Berlin, Germany",2020,"Detecting regression bugs in software evolution, analyzing side-channels in programs and evaluating robustness in deep neural networks (DNNs) can all be seen as instances of differential software analysis, where the goal is to generate diverging executions of program paths. Two executions are said to be diverging if the observable program behavior differs, e.g., in terms of program output, execution time, or (DNN) classification. The key challenge of differential software analysis is to simultaneously reason about multiple program paths, often across program variants. This paper presents HyDiff, the first hybrid approach for differential software analysis. HyDiff integrates and extends two very successful testing techniques: Feedback-directed greybox fuzzing for efficient program testing and shadow symbolic execution for systematic program exploration. HyDiff extends greybox fuzzing with divergence-driven feedback based on novel cost metrics that also take into account the control flow graph of the program. Furthermore HyDiff extends shadow symbolic execution by applying four-way forking in a systematic exploration and still having the ability to incorporate concrete inputs in the analysis. HyDiff applies divergence revealing heuristics based on resource consumption and control-flow information to efficiently guide the symbolic exploration, which allows its efficient usage beyond regression testing applications. We introduce differential metrics such as output, decision and cost difference, as well as patch distance, to assist the fuzzing and symbolic execution components in maximizing the execution divergence. We implemented our approach on top of the fuzzer AFL and the symbolic execution framework Symbolic PathFinder. We illustrate HyDiff on regression and side-channel analysis for Java bytecode programs, and further show how to use HyDiff for robustness analysis of neural networks.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284021,Differential Program Analysis;Symbolic Execution;Fuzzing,Measurement;Systematics;Neural networks;Fuzzing;Software;Robustness;Testing,Java;neural nets;program debugging;program diagnostics;program testing;program verification;regression analysis;software maintenance,multiple program paths;program variants;HyDiff integrates;greybox fuzzing;efficient program testing;shadow symbolic execution;systematic program exploration;divergence-driven feedback;differential metrics;fuzzing execution components;symbolic execution components;execution divergence;symbolic execution framework Symbolic PathFinder;side-channel analysis;robustness analysis;hybrid differential software analysis;software evolution;evaluating robustness;diverging executions;program output;execution time,
452,Testing 1,Seenomaly: Vision-Based Linting of GUI Animation Effects Against Design-Don't Guidelines,D. Zhao; Z. Xing; C. Chen; X. Xu; L. Zhu; G. Li; J. Wang,"Research School of Computer Science, Australian National University; Research School of Computer Science, Australian National University; Faculty of Information Technology, Monash University, Australia; Data61, CSIRO, Australia; Data61, CSIRO, Australia; School of Software, Shanghai Jiao Tong University, Shanghai, China; Research School of Computer Science, Australian National University",2020,"GUI animations, such as card movement, menu slide in/out, snack-bar display, provide appealing user experience and enhance the usability of mobile applications. These GUI animations should not violate the platform's UI design guidelines (referred to as design-don't guideline in this work) regarding component motion and interaction, content appearing and disappearing, and elevation and shadow changes. However, none of existing static code analysis, functional GUI testing and GUI image comparison techniques can â€œseeâ€ the GUI animations on the scree, and thus they cannot support the linting of GUI animations against design-don't guidelines. In this work, we formulate this GUI animation linting problem as a multi-class screencast classification task, but we do not have sufficient labeled GUI animations to train the classifier. Instead, we propose an unsupervised, computer-vision based adversarial autoencoder to solve this linting problem. Our autoencoder learns to group similar GUI animations by â€œseeingâ€ lots of unlabeled real-application GUI animations and learning to generate them. As the first work of its kind, we build the datasets of synthetic and realworld GUI animations. Through experiments on these datasets, we systematically investigate the learning capability of our model and its effectiveness and practicality for linting GUI animations, and identify the challenges in this linting problem for future work.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284093,GUI animation;Design guidelines;Lint;Unsupervised learning,Animation;Feature extraction;Visual effects;Software;User experience;Graphical user interfaces;Guidelines,computer animation;computer vision;graphical user interfaces;mobile computing;neural nets;unsupervised learning;user interfaces,GUI animation linting problem;learning capability;adversarial autoencoder;computer vision;unsupervised autoencoder;multiclass screencast classification task;GUI image comparison;static code analysis;UI design;user experience;mobile applications,
453,Testing 1,Low-Overhead Deadlock Prediction,Y. Cai; R. Meng; J. Palsberg,"State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, and University of Chinese Academy of Sciences, Beijing, China; University of California, Los Angeles, USA",2020,"Multithreaded programs can have deadlocks, even after deployment, so users may want to run deadlock tools on deployed programs. However, current deadlock predictors such as MagicLock and UnDead have large overheads that make them impractical for end-user deployment and confine their use to development time. Such overhead stems from running an exponential-time algorithm on a large execution trace. In this paper, we present the first low-overhead deadlock predictor, called AirLock, that is fit for both in-house testing and deployed programs. AirLock maintains a small predictive lock reachability graph, searches the graph for cycles, and runs an exponential-time algorithm only for each cycle. This approach lets AirLock find the same deadlocks as MagicLock and UnDead but with much less overhead because the number of cycles is small in practice. Our experiments with real-world benchmarks show that the average time overhead of AirLock is 3.5%, which is three orders of magnitude less than that of MagicLock and UnDead. AirLock's low overhead makes it suitable for use with fuzz testers like AFL and on-the-fly after deployment.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284136,Deadlock detection;multithreaded programs;lock reachability graph,System recovery;Tools;Benchmark testing;Prediction algorithms;High frequency;Software engineering,computational complexity;concurrency control;multiprocessor interconnection networks;multi-threading;operating systems (computers);system recovery,predictive lock reachability graph;exponential-time algorithm;deadlocks;average time overhead;AirLock's low overhead;low-overhead deadlock prediction;multithreaded programs;deadlock tools;deployed programs;current deadlock predictors;overheads;end-user deployment;execution trace;low-overhead deadlock predictor;called AirLock,4
454,Android,An Empirical Assessment of Security Risks of Global Android Banking Apps,S. Chen; L. Fan; G. Meng; T. Su; M. Xue; Y. Xue; Y. Liu; L. Xu,"Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, China; ETH Zurich, Switzerland; The University of Adelaide, Australia; University of Science and Technology of China, China; Nanyang Technological University, Singapore; New York University Shanghai, China",2020,"Mobile banking apps, belonging to the most security-critical app category, render massive and dynamic transactions susceptible to security risks. Given huge potential financial loss caused by vulnerabilities, existing research lacks a comprehensive empirical study on the security risks of global banking apps to provide useful insights and improve the security of banking apps. Since data-related weaknesses in banking apps are critical and may directly cause serious financial loss, this paper first revisits the state-of-the-art available tools and finds that they have limited capability in identifying data-related security weaknesses of banking apps. To complement the capability of existing tools in data-related weakness detection, we propose a three-phase automated security risk assessment system, named Ausera, which leverages static program analysis techniques and sensitive keyword identification. By leveraging Ausera, we collect 2,157 weaknesses in 693 real-world banking apps across 83 countries, which we use as a basis to conduct a comprehensive empirical study from different aspects, such as global distribution and weakness evolution during version updates. We find that apps owned by subsidiary banks are always less secure than or equivalent to those owned by parent banks. In addition, we also track the patching of weaknesses and receive much positive feedback from banking entities so as to improve the security of banking apps in practice. We further find that weaknesses derived from outdated versions of banking apps or third-party libraries are highly prone to being exploited by attackers. To date, we highlight that 21 banks have confirmed the weaknesses we reported (including 126 weaknesses in total). We also exchange insights with 7 banks, such as HSBC in UK and OCBC in Singapore, via in-person or online meetings to help them improve their apps. We hope that the insights developed in this paper will inform the communities about the gaps among multiple stakeholders, including banks, academic researchers, and third-party security companies.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283969,Mobile Banking Apps;Vulnerability;Weakness;Empirical Study,Banking;Companies;Tools;Security;Stakeholders;Risk management;Software engineering,bank data processing;banking;financial management;mobile computing;program diagnostics;risk management;security of data;smart phones,security risks;global android banking apps;mobile banking apps;security-critical app category;comprehensive empirical study;global banking apps;data-related weaknesses;data-related security weaknesses;data-related weakness detection;three-phase automated security risk assessment system;real-world banking apps;subsidiary banks;parent banks;banking entities,
455,Android,"Accessibility Issues in Android Apps: State of Affairs, Sentiments, and Ways Forward",A. Alshayban; I. Ahmed; S. Malek,"University of California, Irvine, USA; University of California, Irvine, USA; University of California, Irvine, USA",2020,"Mobile apps are an integral component of our daily life. Ability to use mobile apps is important for everyone, but arguably even more so for approximately 15% of the world population with disabilities. This paper presents the results of a large-scale empirical study aimed at understanding accessibility of Android apps from three complementary perspectives. First, we analyze the prevalence of accessibility issues in over 1, 000 Android apps. We find that almost all apps are riddled with accessibility issues, hindering their use by disabled people. We then investigate the developer sentiments through a survey aimed at understanding the root causes of so many accessibility issues. We find that in large part developers are unaware of accessibility design principles and analysis tools, and the organizations in which they are employed do not place a premium on accessibility. We finally investigate user ratings and comments on app stores. We find that due to the disproportionately small number of users with disabilities, user ratings and app popularity are not indicative of the extent of accessibility issues in apps. We conclude the paper with several observations that form the foundation for future research and development.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284030,Empirical software engineering;Android accessibility;Mobile applications;Apps and app store analysis,Sociology;Organizations;Tools;Mobile applications;Statistics;Research and development;Software engineering,Android (operating system);handicapped aids;mobile computing,Android apps;app popularity;user ratings;app stores;accessibility design principles;mobile apps,
456,Android,Collaborative Bug Finding for Android Apps,S. H. Tan; Z. Li,"Southern University of Science and Technology, Shenzhen, Guangdong Province, China; Southern University of Science and Technology, Shenzhen, Guangdong Province, China",2020,"Many automated test generation techniques have been proposed for finding crashes in Android apps. Despite recent advancement in these approaches, a study shows that Android app developers prefer reading test cases written in natural language. Meanwhile, there exist redundancies in bug reports (written in natural language) across different apps that have not been previously reused. We propose collaborative bug finding, a novel approach that uses bugs in other similar apps to discover bugs in the app under test. We design three settings with varying degrees of interactions between programmers: (1) bugs from programmers who develop a different app, (2) bugs from manually searching for bug reports in GitHub repositories, (3) bugs from a bug recommendation system, Bugine. Our studies of the first two settings in a software testing course show that collaborative bug finding helps students who are novice Android app testers to discover 17 new bugs. As students admit that searching for relevant bug reports could be time-consuming, we introduce Bugine, an approach that automatically recommends relevant GitHub issues for a given app. Bugine uses (1) natural language processing to find GitHub issues that mention common UI components shared between the app under test and other apps in our database, and (2) a ranking algorithm to select GitHub issues that are of the best quality. Our results show that Bugine is able to find 34 new bugs. In total, collaborative bug finding helps us find 51 new bugs, in which eight have been confirmed and 11 have been fixed by the developers. These results confirm our intuition that our proposed technique is useful in discovering new bugs for Android apps.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284092,collaborative programming;test generation;recommendation system;Android apps,Software testing;Computer bugs;Redundancy;Collaboration;Test pattern generators;Software development management;Software engineering,mobile computing;natural language processing;program debugging;program testing;public domain software;recommender systems,relevant bug reports;Bugine;given app;GitHub issues;total bug finding;collaborative bug finding;bugs;Android apps;automated test generation techniques;Android app developers;different app;similar apps;bug recommendation system;novice Android app,4
457,Code Summarization and Analysis,POSIT: Simultaneously Tagging Natural and Programming Languages,P. -P. PÃ¢rÆ«achi; S. K. Dash; C. Treude; E. T. Barr,"University College London, London, United Kingdom; University of Surrey, Surrey, United Kingdom; University of Adelaide, Adelaide, South Australia, Australia; University College London, London, United Kingdom",2020,"Software developers use a mix of source code and natural language text to communicate with each other: Stack Overflow and Developer mailing lists abound with this mixed text. Tagging this mixed text is essential for making progress on two seminal software engineering problems - traceability, and reuse via precise extraction of code snippets from mixed text. In this paper, we borrow code-switching techniques from Natural Language Processing and adapt them to apply to mixed text to solve two problems: language identification and token tagging. Our technique, POSIT, simultaneously provides abstract syntax tree tags for source code tokens, part-of-speech tags for natural language words, and predicts the source language of a token in mixed text. To realize POSIT, we trained a biLSTM network with a Conditional Random Field output layer using abstract syntax tree tags from the CLANG compiler and part-of-speech tags from the Standard Stanford part-of-speech tagger. POSIT improves the state-of-the-art on language identification by 10.6% and PoS/Ast tagging by 23.7% in accuracy.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284084,part-of-speech Tagging;Mixed-Code;Code-Switching;Language Identification,Neural networks;Tagging;Syntactics;Software;Task analysis;Standards;Software engineering,C++ language;computational linguistics;natural language processing;natural languages;program compilers;programming languages;public domain software;software engineering;text analysis,POSIT;Natural languages;programming languages;natural language text;mixed text;seminal software engineering problems - traceability;code-switching techniques;Natural Language Processing;language identification;token tagging;abstract syntax tree tags;source code tokens;part-of-speech tags;natural language words;source language,
458,Code Summarization and Analysis,CPC: Automatically Classifying and Propagating Natural Language Comments via Program Analysis,J. Zhai; X. Xu; Y. Shi; G. Tao; M. Pan; S. Ma; L. Xu; W. Zhang; L. Tan; X. Zhang,Purdue University; Nanjing University; Purdue University; Purdue University; Nanjing University; Rutgers University; Nanjing University; Nanjing University of Posts and Telecommunications; Purdue University; Purdue University,2020,"Code comments provide abundant information that have been leveraged to help perform various software engineering tasks, such as bug detection, specification inference, and code synthesis. However, developers are less motivated to write and update comments, making it infeasible and error-prone to leverage comments to facilitate software engineering tasks. In this paper, we propose to leverage program analysis to systematically derive, refine, and propagate comments. For example, by propagation via program analysis, comments can be passed on to code entities that are not commented such that code bugs can be detected leveraging the propagated comments. Developers usually comment on different aspects of code elements like methods, and use comments to describe various contents, such as functionalities and properties. To more effectively utilize comments, a fine-grained and elaborated taxonomy of comments and a reliable classifier to automatically categorize a comment are needed. In this paper, we build a comprehensive taxonomy and propose using program analysis to propagate comments. We develop a prototype CPC, and evaluate it on 5 projects. The evaluation results demonstrate 41573 new comments can be derived by propagation from other code locations with 88% accuracy. Among them, we can derive precise functional comments for 87 native methods that have neither existing comments nor source code. Leveraging the propagated comments, we detect 37 new bugs in open source large projects, 30 of which have been confirmed and fixed by developers, and 304 defects in existing comments (by looking at inconsistencies between existing and propagated comments), including 12 incomplete comments and 292 wrong comments. This demonstrates the effectiveness of our approach. Our user study confirms propagated comments align well with existing comments in terms of quality.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284079,,Computer bugs;Taxonomy;Natural languages;Prototypes;Reliability;Task analysis;Software engineering,natural language processing;program debugging;program diagnostics;public domain software;software engineering;software maintenance,"leverage program analysis;systematically derive comments;refine, comments;propagate comments;propagated comments;precise functional comments;existing comments;12 incomplete comments;292 wrong comments;propagating natural language comments;code comments;software engineering tasks;leverage comments",
459,Code Summarization and Analysis,Suggesting Natural Method Names to Check Name Consistencies,S. Nguyen; H. Phan; T. Le; T. N. Nguyen,"Univ. of Texas, Dallas, USA; Iowa State Univ., USA; U. of Eng. & Tech., Vietnam; Univ. of Texas, Dallas, USA",2020,"Misleading names of the methods in a project or the APIs in a software library confuse developers about program functionality and API usages, leading to API misuses and defects. In this paper, we introduce MNire, a machine learning approach to check the consistency between the name of a given method and its implementation. MNire first generates a candidate name and compares the current name against it. If the two names are sufficiently similar, we consider the method as consistent. To generate the method name, we draw our ideas and intuition from an empirical study on the nature of method names in a large dataset. Our key finding is that high proportions of the tokens of method names can be found in the three contexts of a given method including its body, the interface (the method's parameter types and return type), and the enclosing class' name. Even when such tokens are not there, MNire uses the contexts to predict the tokens due to the high likelihoods of their co-occurrences. Our unique idea is to treat the name generation as an abstract summarization on the tokens collected from the names of the program entities in the three above contexts. We conducted several experiments to evaluate MNire in method name consistency checking and in method name recommending on large datasets with +14M methods. In detecting inconsistency method names, MNire improves the state-of-the-art approach by 10.4% and 11% relatively in recall and precision, respectively. In method name recommendation, MNire improves relatively over the state-of-the-art technique, code2vec, in both recall (18.2% higher) and precision (11.1% higher). To assess MNire's usefulness, we used it to detect inconsistent methods and suggest new names in several active, GitHub projects. We made 50 pull requests (PRs) and received 42 responses. Among them, five PRs were merged into the main branch, and 13 were approved for later merging. In total, in 31/42 cases, the developer teams agree that our suggested names are more meaningful than the current names, showing MNire'S usefulness.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284075,Naturalness of Source Code;Program Entity Name Suggestion;Deep Learning,Software libraries;Merging;Machine learning;Software development management,application program interfaces;learning (artificial intelligence);program diagnostics;public domain software;software engineering;software libraries;Web sites,suggesting natural method names;name consistencies;given method;candidate name;current name;enclosing class;name generation;method name consistency checking;inconsistency method names;method name recommendation;MNire's usefulness;inconsistent methods;suggested names;MNire'S usefulness,
460,Code Summarization and Analysis,Retrieval-based Neural Source Code Summarization,J. Zhang; X. Wang; H. Zhang; H. Sun; X. Liu,"SKLSDE Lab, Beihang University, China; SKLSDE Lab, Beihang University, China; The University of Newcastle, Australia; SKLSDE Lab, Beihang University, China; SKLSDE Lab, Beihang University, China",2020,"Source code summarization aims to automatically generate concise summaries of source code in natural language texts, in order to help developers better understand and maintain source code. Traditional work generates a source code summary by utilizing information retrieval techniques, which select terms from original source code or adapt summaries of similar code snippets. Recent studies adopt Neural Machine Translation techniques and generate summaries from code snippets using encoder-decoder neural networks. The neural-based approaches prefer the high-frequency words in the corpus and have trouble with the low-frequency ones. In this paper, we propose a retrieval-based neural source code summarization approach where we enhance the neural model with the most similar code snippets retrieved from the training set. Our approach can take advantages of both neural and retrieval-based techniques. Specifically, we first train an attentional encoder-decoder model based on the code snippets and the summaries in the training set; Second, given one input code snippet for testing, we retrieve its two most similar code snippets in the training set from the aspects of syntax and semantics, respectively; Third, we encode the input and two retrieved code snippets, and predict the summary by fusing them during decoding. We conduct extensive experiments to evaluate our approach and the experimental results show that our proposed approach can improve the state-of-the-art methods.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284039,Source code summarization;Information retrieval;Deep neural network,Training;Semantics;Neural networks;Syntactics;Predictive models;Testing;Software engineering,decoding;information retrieval;language translation;learning (artificial intelligence);natural language processing;natural languages;neural nets;query processing;software maintenance;text analysis,concise summaries;source code summary;information retrieval techniques;original source code;similar code snippets;Neural Machine Translation techniques;encoder-decoder neural networks;neural-based approaches;retrieval-based neural source code summarization approach;neural model;training set;input code snippet;retrieved code snippets,
461,Machine Learning and Models,On Learning Meaningful Assert Statements for Unit Test Cases,C. Watson; M. Tufano; K. Moran; G. Bavota; D. Poshyvanyk,"Washington and Lee University, Lexington, Virginia; Microsoft, Redmond, Washington; William & Mary, Williamsburg, Virginia; UniversitÃ della Svizzera italiana (USI), Lugano, Switzerland; William & Mary, Williamsburg, Virginia",2020,"Software testing is an essential part of the software lifecycle and requires a substantial amount of time and effort. It has been estimated that software developers spend close to 50% of their time on testing the code they write. For these reasons, a long standing goal within the research community is to (partially) automate software testing. While several techniques and tools have been proposed to automatically generate test methods, recent work has criticized the quality and usefulness of the assert statements they generate. Therefore, we employ a Neural Machine Translation (NMT) based approach called Atlas (AuTomatic Learning of Assert Statements) to automatically generate meaningful assert statements for test methods. Given a test method and a focal method (i.e., the main method under test), Atlas can predict a meaningful assert statement to assess the correctness of the focal method. We applied Atlas to thousands of test methods from GitHub projects and it was able to predict the exact assert statement manually written by developers in 31% of the cases when only considering the top-1 predicted assert. When considering the top-5 predicted assert statements, Atlas is able to predict exact matches in 50% of the cases. These promising results hint to the potential usefulness of our approach as (i) a complement to automatic test case generation techniques, and (ii) a code completion support for developers, who can benefit from the recommended assert statements while writing test code.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283916,Testing;Unit Tests;Deep Learning;Neural Machine Translation,Software testing;Writing;Tools;Software;Software engineering;Software development management,language translation;learning (artificial intelligence);neural nets;program testing,focal method;Atlas;test method;exact assert statement;automatic test case generation techniques;test code;unit test cases;software lifecycle;software developers;automate software testing;learning meaningful assert statement;neural machine translation based approach;NMT;automatic learning of assert statements,
462,Machine Learning and Models,Quickly Generating Diverse Valid Test Inputs with Reinforcement Learning,S. Reddy; C. Lemieux; R. Padhye; K. Sen,"University of California, Berkeley, USA; University of California, Berkeley, USA; University of California, Berkeley, USA; University of California, Berkeley, USA",2020,"Property-based testing is a popular approach for validating the logic of a program. An effective property-based test quickly generates many diverse valid test inputs and runs them through a parameterized test driver. However, when the test driver requires strict validity constraints on the inputs, completely random input generation fails to generate enough valid inputs. Existing approaches to solving this problem rely on whitebox or greybox information collected by instrumenting the input generator and/or test driver. However, collecting such information reduces the speed at which tests can be executed. In this paper, we propose and study a black-box approach for generating valid test inputs. We first formalize the problem of guiding random input generators towards producing a diverse set of valid inputs. This formalization highlights the role of a guide which governs the space of choices within a random input generator. We then propose a solution based on reinforcement learning (RL), using a tabular, on-policy RL approach to guide the generator. We evaluate this approach, RLCheck, against pure random input generation as well as a state-of-the-art greybox evolutionary algorithm, on four real-world benchmarks. We find that in the same time budget, RLCheck generates an order of magnitude more diverse valid inputs than the baselines.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284117,,Monte Carlo methods;Instruments;Reinforcement learning;Evolutionary computation;Benchmark testing;Generators;Software engineering,evolutionary computation;learning (artificial intelligence);program testing,completely random input generation;strict validity constraints;parameterized test driver;effective property-based test;property-based testing;generating diverse valid test inputs;magnitude more diverse valid inputs;pure random input generation;on-policy RL approach;reinforcement learning;random input generator;black-box approach,
463,Meta Studies,An Evidence-Based Inquiry into the Use of Grey Literature in Software Engineering,H. Zhang; X. Zhou; X. Huang; H. Huang; M. A. Babar,"State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, China; State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, China; State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, China; State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, China; School of Computer Science, University of Adelaide, Australia",2020,"Context: Following on other scientific disciplines, such as health sciences, the use of Grey Literature (GL) has become widespread in Software Engineering (SE) research. Whilst the number of papers incorporating GL in SE is increasing, there is little empirically known about different aspects of the use of GL in SE research. Method: We used a mixed-methods approach for this research. We carried out a Systematic Literature Review (SLR) of the use of GL in SE, and surveyed the authors of the selected papers included in the SLR (as GL users) and the invited experts in SE community on the use of GL in SE research. Results: We systematically selected and reviewed 102 SE secondary studies that incorporate GL in SE research, from which we identified two groups based on their reporting: 1) 76 reviews only claim their use of GL; 2) 26 reviews report the results by including GL. We also obtained 20 replies from the GL users and 24 replies from the invited SE experts. Conclusion: There is no common understanding of the meaning of GL in SE. Researchers define the scopes and the definitions of GL in a variety of ways. We found five main reasons of using GL in SE research. The findings have enabled us to propose a conceptual model for how GL works in SE research lifecycle. There is an apparent need for research to develop guidelines for using GL in SE and for assessing quality of GL. The current work can provide a panorama of the state-of-the-art of using GL in SE for the follow-up research, as to determine the important position of GL in SE research.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283930,Grey literature;systematic (literature) review;survey;evidence-based software engineering;empirical software engineering,Industries;Systematics;Quality control;Software;Task analysis;Software engineering;Guidelines,reviews;software engineering,GL users;SE research lifecycle;software engineering research;evidence-based inquiry;grey literature;health sciences;mixed-methods approach;systematic literature review;SLR;SE community;conceptual model,
464,Performance,Towards the Use of the Readily Available Tests from the Release Pipeline as Performance Tests. Are We There Yet?,Z. Ding; J. Chen; W. Shang,"Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada",2020,"Performance is one of the important aspects of software quality. Performance issues exist widely in software systems, and the process of fixing the performance issues is an essential step in the release cycle of software systems. Although performance testing is widely adopted in practice, it is still expensive and time-consuming. In particular, the performance testing is usually conducted after the system is built in a dedicated testing environment. The challenges of performance testing make it difficult to fit into the common DevOps process in software development. On the other hand, there exist a large number of tests readily available, that are executed regularly within the release pipeline during software development. In this paper, we perform an exploratory study to determine whether such readily available tests are capable of serving as performance tests. In particular, we would like to see whether the performance of these tests can demonstrate performance improvements obtained from fixing real-life performance issues. We collect 127 performance issues from Hadoop and Cassandra, and evaluate the performance of the readily available tests from the commits before and after the performance issue fixes. We find that most of the improvements from the fixes to performance issues can be demonstrated using the readily available tests in the release pipeline. However, only a very small portion of the tests can be used for demonstrating the improvements. By manually examining the tests, we identify eight reasons that a test cannot demonstrate performance improvements even though it covers the changed source code of the issue fix. Finally, we build random forest classifiers determining the important metrics influencing the readily available tests (not) being able to demonstrate performance improvements from issue fixes. We find that the test code itself and the source code covered by the test are important factors, while the factors related to the code changes in the performance issues fixes have a low importance. Practitioners may focus on designing and improving the tests, instead of fine-tuning tests for different performance issues fixes. Our findings can be used as a guideline for practitioners to reduce the amount of effort spent on leveraging and designing tests that run in the release pipeline for performance assurance activities.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284085,Performance testing;Performance issues;Software performance,Measurement;Pipelines;Software quality;Software systems;Testing;Guidelines;Software engineering,pipelines;program testing;software quality;source code (software),software systems;performance testing;dedicated testing environment;performance issue fixes;test code;performance assurance activities;performance issues fixes;software quality,
465,Software Verification,Verifying Object Construction,M. Kellogg; M. Ran; M. Sridharan; M. SchÃ¤f; M. D. Ernst,"U. of Washington, USA; UC Riverside, USA; UC Riverside, USA; Amazon Web Services, USA; U. of Washington, USA",2020,"In object-oriented languages, constructors often have a combination of required and optional formal parameters. It is tedious and inconvenient for programmers to write a constructor by hand for each combination. The multitude of constructors is error-prone for clients, and client code is difficult to read due to the large number of constructor arguments. Therefore, programmers often use design patterns that enable more flexible object construction-the builder pattern, dependency injection, or factory methods. However, these design patterns can be too flexible: not all combinations of logical parameters lead to the construction of well-formed objects. When a client uses the builder pattern to construct an object, the compiler does not check that a valid set of values was provided. Incorrect use of builders can lead to security vulnerabilities, run-time crashes, and other problems. This work shows how to statically verify uses of object construction, such as the builder pattern. Using a simple specification language, programmers specify which combinations of logical arguments are permitted. Our compile-time analysis detects client code that may construct objects unsafely. Our analysis is based on a novel special case of typestate checking, accumulation analysis, that modularly reasons about accumulations of method calls. Because accumulation analysis does not require precise aliasing information for soundness, our analysis scales to industrial programs. We evaluated it on over 9 million lines of code, discovering defects which included previously-unknown security vulnerabilities and potential null-pointer violations in heavily-used open-source codebases. Our analysis has a low false positive rate and low annotation burden. Our implementation and experimental data are publicly available.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284018,Pluggable type systems;AMI sniping;builder pattern;lightweight verification;Lombok;AutoValue,Annotations;Computer bugs;Writing;Production facilities;Specification languages;Security;Software engineering,data structures;formal specification;Java;object-oriented languages;object-oriented methods;object-oriented programming;program compilers;program debugging;program diagnostics;program verification;security of data;specification languages,verifying object construction;object-oriented languages;required parameters;optional formal parameters;error-prone;client code;constructor arguments;design patterns;flexible object construction-the;dependency injection;factory methods;logical parameters;well-formed objects;client uses;builder pattern;run-time crashes;simple specification language;logical arguments;compile-time analysis;typestate checking;accumulation analysis;method calls;analysis scales;previously-unknown security vulnerabilities;heavily-used open-source codebases,
466,Software Verification,Automatically Testing String Solvers,A. Bugariu; P. MÃ¼ller,"Department of Computer Science, ETH Zurich, Zurich, Switzerland; Department of Computer Science, ETH Zurich, Zurich, Switzerland",2020,"SMT solvers are at the basis of many applications, such as program verification, program synthesis, and test case generation. For all these applications to provide reliable results, SMT solvers must answer queries correctly. However, since they are complex, highly-optimized software systems, ensuring their correctness is challenging. In particular, state-of-the-art testing techniques do not reliably detect when an SMT solver is unsound. In this paper, we present an automatic approach for generating test cases that reveal soundness errors in the implementations of string solvers, as well as potential completeness and performance issues. We synthesize input formulas that are satisfiable or unsatisfiable by construction and use this ground truth as test oracle. We automatically apply satisfiability-preserving transformations to generate increasingly-complex formulas, which allows us to detect many errors with simple inputs and, thus, facilitates debugging. The experimental evaluation shows that our technique effectively reveals bugs in the implementation of widely-used SMT solvers and applies also to other types of solvers, such as automata-based solvers. We focus on strings here, but our approach carries over to other theories and their combinations.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284042,automatic testing;soundness testing;string solvers;SMT solvers,String theory;Computer bugs;Debugging;Software systems;Software reliability;Testing;Software engineering,computability;program debugging;program testing;query processing,string solvers;SMT solver;test case generation;highly-optimized software systems;testing techniques;test oracle;automata-based solvers,
467,Testing 2,A Study on the Lifecycle of Flaky Tests,W. Lam; K. MuÅŸlu; H. Sajnani; S. Thummalapenta,"University of Illinois at Urbana-Champaign, Urbana, Illinois, USA; Microsoft, Redmond, Washington, USA; Microsoft, Redmond, Washington, USA; Microsoft, Redmond, Washington, USA",2020,"During regression testing, developers rely on the pass or fail outcomes of tests to check whether changes broke existing functionality. Thus, flaky tests, which nondeterministically pass or fail on the same code, are problematic because they provide misleading signals during regression testing. Although flaky tests are the focus of several existing studies, none of them study (1) the reoccurrence, runtimes, and time-before-fix of flaky tests, and (2) flaky tests in-depth on proprietary projects. This paper fills this knowledge gap about flaky tests and investigates whether prior categorization work on flaky tests also apply to proprietary projects. Specifically, we study the lifecycle of flaky tests in six large-scale proprietary projects at Microsoft. We find, as in prior work, that asynchronous calls are the leading cause of flaky tests in these Microsoft projects. Therefore, we propose the first automated solution, called Flakiness and Time Balancer (FaTB), to reduce the frequency of flaky-test failures caused by asynchronous calls. Our evaluation of five such flaky tests shows that FaTB can reduce the running times of these tests by up to 78% without empirically affecting the frequency of their flaky-test failures. Lastly, our study finds several cases where developers claim they â€œfixedâ€ a flaky test but our empirical experiments show that their changes do not fix or reduce these tests' frequency of flaky-test failures. Future studies should be more cautious when basing their results on changes that developers claim to be â€œfixesâ€.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284119,flaky test;empirical study;lifecycle,Industries;Time-frequency analysis;Runtime;Computer bugs;Open source software;Testing;Message systems,failure analysis;program testing;project management;public domain software;regression analysis;software development management;software quality,regression testing;flaky test;flaky-test failures,35
468,Testing 2,Testing File System Implementations on Layered Models,D. Chen; Y. Jiang; C. Xu; X. Ma; J. Lu,"State Key Lab for Novel Software Technology and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Lab for Novel Software Technology and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Lab for Novel Software Technology and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Lab for Novel Software Technology and Department of Computer Science and Technology, Nanjing University, Nanjing, China; State Key Lab for Novel Software Technology and Department of Computer Science and Technology, Nanjing University, Nanjing, China",2020,"Generating high-quality system call sequences is not only important to testing file system implementations, but also challenging due to the astronomically large input space. This paper introduces a new approach to the workload generation problem by building layered models and abstract workloads refinement. This approach is instantiated as a three-layer file system model for file system workload generation. In a short-period experiment run, sequential workloads (system call sequences) manifested over a thousand crashes in mainline Linux Kernel file systems, with 12 previously unknown bugs being reported. We also provide evidence that such workloads benefit other domain-specific testing techniques including crash consistency testing and concurrency testing.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283972,Model-based testing;workload generation;file system,File systems;Buildings;Tools;Software reliability;Kernel;Complex systems;Testing,file organisation;Linux;operating system kernels;program debugging;program testing,abstract workloads refinement;three-layer file system model;file system workload generation;sequential workloads;system call sequences;mainline Linux Kernel file systems;domain-specific testing techniques;crash consistency testing;concurrency testing;testing file system implementations,
469,Code Generation,Co-Evolving Code with Evolving Metamodels,D. E. Khelladi; B. Combemale; M. Acher; O. Barais; J. -M. JÃ©zÃ©quel,"CNRS, IRISA, Univ. Rennes, Rennes, France; UniversitÃ© Toulouse & Inria Rennes, Rennes, France; Univ. Rennes, Inria, IRISA, Rennes, France; Univ. Rennes, Inria, IRISA, Rennes, France; Univ. Rennes, Inria, IRISA, Rennes, France",2020,"Metamodels play a significant role to describe and analyze the relations between domain concepts. They are also cornerstone to build a software language (SL) for a domain and its associated tooling. Metamodel definition generally drives code generation of a core API. The latter is further enriched by developers with additional code implementing advanced functionalities, e.g., checkers, recommenders, etc. When a SL is evolved to the next version, the metamodels are evolved as well before to re-generate the core API code. As a result, the developers added code both in the core API and the SL toolings may be impacted and thus may need to be co-evolved accordingly. Many approaches support the co-evolution of various artifacts when metamodels evolve. However, not the co-evolution of code. This paper fills this gap. We propose a semi-automatic co-evolution approach based on change propagation. The premise is that knowledge of the metamodel evolution changes can be propagated by means of resolutions to drive the code co-evolution. Our approach leverages on the abstraction level of metamodels where a given metamodel element has often different usages in the code. It supports alternative co-evaluations to meet different developers needs. Our work is evaluated on three Eclipse SL implementations, namely OCL, Modisco, and Papyrus over several evolved versions of metamodels and code. In response to five different evolved metamodels, we co-evolved 976 impacts over 18 projects. A comparison of our co-evolved code with the versioned ones shows the usefulness of our approach. Our approach was able to reach a weighted average of 87.4% and 88.9% respectively of precision and recall while supporting useful alternative co-evolution that developers have manually performed.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283913,,Automation;Buildings;Maintenance engineering;Software;Task analysis;Stress;Software engineering,application program interfaces;formal specification;object-oriented methods;object-oriented programming;reverse engineering;software architecture;software engineering;software maintenance;Unified Modeling Language,evolving code;evolving metamodels;metamodel definition;code generation;additional code;core API code;SL toolings;semiautomatic co-evolution approach;metamodel evolution changes;given metamodel element;Eclipse SL implementations;evolved versions;different evolved metamodels;useful alternative co-evolution,
470,Dependencies and Configuration,Lazy Product Discovery in Huge Configuration Spaces,M. Lienhardt; F. Damiani; E. B. Johnsen; J. Mauro,"ONERA - The French Aerospace Lab, France; University of Turin, Italy; University of Oslo Norway; University of Southern Denmark, Denmark",2020,"Highly-configurable software systems can have thousands of interdependent configuration options across different subsystems. In the resulting configuration space, discovering a valid product configuration for some selected options can be complex and error prone. The configuration space can be organized using a feature model, fragmented into smaller interdependent feature models reflecting the configuration options of each subsystem. We propose a method for lazy product discovery in large fragmented feature models with interdependent features. We formalize the method and prove its soundness and completeness. The evaluation explores an industrial-size configuration space. The results show that lazy product discovery has significant performance benefits compared to standard product discovery, which in contrast to our method requires all fragments to be composed to analyze the feature model. Furthermore, the method succeeds when more efficient, heuristics-based engines fail to find a valid configuration.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283998,Software Product Lines;Configurable Software;Variability Modeling;Feature Models;Composition;Linux Distribution,Analytical models;Software systems;Standards;Engines;Software engineering,configuration management;performance evaluation;program testing;telecommunication computing,fragmented feature models;smaller interdependent feature models;selected options;valid product configuration;resulting configuration space;different subsystems;interdependent configuration options;highly-configurable software systems;huge configuration spaces;valid configuration;feature model;standard product discovery;lazy product discovery;industrial-size configuration space;interdependent features,
471,Dependencies and Configuration,Reducing Run-Time Adaptation Space via Analysis of Possible Utility Bounds,C. Stevens; H. Bagheri,"Department of Computer Science and Engineering, University of Nebraska-Lincoln; Department of Computer Science and Engineering, University of Nebraska-Lincoln",2020,"Self-adaptive systems often employ dynamic programming or similar techniques to select optimal adaptations at run-time. These techniques suffer from the â€œcurse of dimensionalityâ€, increasing the cost of run-time adaptation decisions. We propose a novel approach that improves upon the state-of-the-art proactive self-adaptation techniques to reduce the number of possible adaptations that need be considered for each run-time adaptation decision. The approach, realized in a tool called Thallium, employs a combination of automated formal modeling techniques to (i) analyze a structural model of the system showing which configurations are reachable from other configurations and (ii) compute the utility that can be generated by the optimal adaptation over a bounded horizon in both the best- and worst-case scenarios. It then constructs triangular possibility values using those optimized bounds to automatically compare adjacent adaptations for each configuration, keeping only the alternatives with the best range of potential results. The experimental results corroborate Thallium's ability to significantly reduce the number of states that need to be considered with each adaptation decision, freeing up vital resources at run-time.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283948,formal methods;self-adaptive systems;run-time adaptation;multiobjective optimization,Thallium;Analytical models;Adaptation models;Computational modeling;Tools;Optimization;Software engineering,adaptive systems;computational complexity;dynamic programming;formal verification;optimisation;reachability analysis,run-time adaptation space;possible utility bounds;self-adaptive systems;dynamic programming;optimal adaptation;run-time adaptation decision;state-of-the-art proactive self-adaptation techniques;possible adaptations;automated formal modeling techniques;optimized bounds;adjacent adaptations,
472,Recommendations,Context-aware In-process Crowdworker Recommendation,J. Wang; Y. Yang; S. Wang; Y. Hu; D. Wang; Q. Wang,"Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; School of Systems and Enterprises, Stevens Institute of Technology, USA; Lassonde School of Engineering, York University, Canada; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China",2020,"Identifying and optimizing open participation is essential to the success of open software development. Existing studies highlighted the importance of worker recommendation for crowdtesting tasks in order to detect more bugs with fewer workers. However, these studies mainly focus on one-time recommendations with respect to the initial context at the beginning of a new task. This paper argues the need for in-process crowdtesting worker recommendation. We motivate this study through a pilot study, revealing the prevalence of long-sized non-yielding windows, i.e., no new bugs are revealed in consecutive test reports during the process of a crowdtesting task. This indicates the potential opportunity for accelerating crowdtesting by recommending appropriate workers in a dynamic manner, so that the non-yielding windows could be shortened. To that end, this paper proposes a context-aware in-process crowdworker recommendation approach, iRec, to detect more bugs earlier and potentially shorten the non-yielding windows. It consists of three main components: 1) the modeling of dynamic testing context, 2) the learning-based ranking component, and 3) the diversity-based re-ranking component. The evaluation is conducted on 636 crowdtesting tasks from one of the largest crowdtesting platforms, and results show the potential of iRec in improving the cost-effectiveness of crowdtesting by saving the cost and shortening the testing process.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283975,Crowdtesting;Crowdworker;Recommendation,Bridges;Computer bugs;Software;Task analysis;Testing;Context modeling;Software engineering,human computer interaction;human factors;labour resources;learning (artificial intelligence);program testing;recommender systems,context-aware in-process crowdworker recommendation approach;bugs;nonyielding windows;dynamic testing context;learning-based ranking component;diversity-based re-ranking component;636 crowdtesting tasks;largest crowdtesting platforms;testing process;optimizing open participation;open software development;crowdtesting task;fewer workers;one-time recommendations;initial context;in-process crowdtesting worker recommendation;consecutive test reports;appropriate workers,
473,Security,A Large-Scale Empirical Study on Vulnerability Distribution within Projects and the Lessons Learned,B. Liu; G. Meng; W. Zou; Q. Gong; F. Li; M. Lin; D. Sun; W. Huo; C. Zhang,"Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute for Network Science and Cyberspace, Tsinghua University; Institute of Information Engineering, Chinese Academy of Sciences; Institute of Information Engineering, Chinese Academy of Sciences; Institute for Network Science and Cyberspace, Tsinghua University",2020,"The number of vulnerabilities increases rapidly in recent years, due to advances in vulnerability discovery solutions. It enables a thorough analysis on the vulnerability distribution and provides support for correlation analysis and prediction of vulnerabilities. Previous research either focuses on analyzing bugs rather than vulnerabilities, or only studies general vulnerability distribution among projects rather than the distribution within each project. In this paper, we collected a large vulnerability dataset, consisting of all known vulnerabilities associated with five representative open source projects, by utilizing automated crawlers and spending months of manual efforts. We then analyzed the vulnerability distribution within each project over four dimensions, including files, functions, vulnerability types and responsible developers. Based on the results analysis, we presented 12 practical insights on the distribution of vulnerabilities. Finally, we applied such insights on several vulnerability discovery solutions (including static analysis and dynamic fuzzing), and helped them find 10 zero-day vulnerabilities in target projects, showing that our insights are useful.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283967,Empirical Study;Vulnerability Distribution,Semantics;Crawlers;Static analysis;Manuals;Fuzzing;Security;Software engineering,public domain software;security of data,general vulnerability distribution;zero-day vulnerabilities;vulnerability types;representative open source projects;vulnerability dataset;vulnerability discovery solutions,
474,Security,Unsuccessful Story about Few Shot Malware Family Classification and Siamese Network to the Rescue,Y. Bai; Z. Xing; X. Li; Z. Feng; D. Ma,"Tianjin Key Laboratory of Advanced Networking (TANK), School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; Research School of Computer Science, Australian National University, Data61 CSIRO, Australia; Tianjin Key Laboratory of Advanced Networking (TANK), School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China; School of Computer Software, College of Intelligence and Computing, Tianjin University, Tianjin, China; Tianjin Key Laboratory of Advanced Networking (TANK), School of Computer Science and Technology, College of Intelligence and Computing, Tianjin University, Tianjin, China",2020,"To battle the ever-increasing Android malware, malware family classification, which classifies malware with common features into a malware family, has been proposed as an effective malware analysis method. Several machine-learning based approaches have been proposed for the task of malware family classification. Our study shows that malware families suffer from several data imbalance, with many families with only a small number of malware applications (referred to as few shot malware families in this work). Unfortunately, this issue has been overlooked in existing approaches. Although existing approaches achieve high classification performance at the overall level and for large malware families, our experiments show that they suffer from poor performance and generalizability for few shot malware families, and traditionally downsampling method cannot solve the problem. To address the challenge in few shot malware family classification, we propose a novel siamese-network based learning method, which allows us to train an effective MultiLayer Perceptron (MLP) network for embedding malware applications into a real-valued, continuous vector space by contrasting the malware applications from the same or different families. In the embedding space, the performance of malware family classification can be significantly improved for all scales of malware families, especially for few shot malware families, which also leads to the significant performance improvement at the overall level.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9283961,Malware family classification;Few shot learning;Siamese network,Learning systems;Machine learning;Multilayer perceptrons;Malware;Task analysis;Software engineering,invasive software;learning (artificial intelligence);multilayer perceptrons;pattern classification,effective malware analysis method;malware applications;shot malware families;shot malware family classification;same families;Android malware,
475,Security,How Does Misconfiguration of Analytic Services Compromise Mobile Privacy?,X. Zhang; X. Wang; R. Slavin; T. Breaux; J. Niu,"University of Texas at San Antonio, San Antonio, TX, USA; University of Texas at San Antonio, San Antonio, TX, USA; University of Texas at San Antonio, San Antonio, TX, USA; Carnegie Mellon University, Pittsburgh, PA, USA; University of Texas at San Antonio, San Antonio, TX, USA",2020,"Mobile application (app) developers commonly utilize analytic services to analyze their app users' behavior to support debugging, improve service quality, and facilitate advertising. Anonymization and aggregation can reduce the sensitivity of such behavioral data, therefore analytic services often encourage the use of such protections. However, these protections are not directly enforced so it is possible for developers to misconfigure the analytic services and expose personal information, which may cause greater privacy risks. Since people use apps in many aspects of their daily lives, such misconfigurations may lead to the leaking of sensitive personal information such as a users' real-time location, health data, or dating preferences. To study this issue and identify potential privacy risks due to such misconfigurations, we developed a semiautomated approach, Privacy-Aware Analytics Misconfiguration Detector (PAMDroid), which enables our empirical study on mis-configurations of analytic services. This paper describes a study of 1,000 popular apps using top analytic services in which we found misconfigurations in 120 apps. In 52 of the 120 apps, misconfigurations lead to a violation of either the analytic service providers' terms of service or the app's own privacy policy.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284006,Privacy;Mobile Application;Program Analysis;Analytic Services;Configuration,Privacy;Sensitivity;Detectors;Real-time systems;Mobile applications;Encryption;Software engineering,data privacy;mobile computing,misconfigurations;analytic service providers;mobile privacy;mobile application;app users;service quality;privacy-aware analytics misconfiguration detector;PAMDroid,
476,Stack Overflow,Interpreting Cloud Computer Vision Pain-Points: A Mining Study of Stack Overflow,A. Cummaudo; R. Vasa; S. Barnett; J. Grundy; M. Abdelrazek,"Applied Artificial Intelligence Inst., Deakin University, Geelong, Victoria, Australia; Applied Artificial Intelligence Inst., Deakin University, Geelong, Victoria, Australia; Applied Artificial Intelligence Inst., Deakin University, Geelong, Victoria, Australia; Faculty of Information Technology, Monash University, Clayton, Victoria, Australia; School of Information Technology, Deakin University, Geelong, Victoria, Australia",2020,"Intelligent services are becoming increasingly more pervasive; application developers want to leverage the latest advances in areas such as computer vision to provide new services and products to users, and large technology firms enable this via RESTful APIs. While such APIs promise an easy-to-integrate on-demand machine intelligence, their current design, documentation and developer interface hides much of the underlying machine learning techniques that power them. Such APIs look and feel like conventional APIs but abstract away data-driven probabilistic behaviour-the implications of a developer treating these APIs in the same way as other, traditional cloud services, such as cloud storage, is of concern. The objective of this study is to determine the various pain-points developers face when implementing systems that rely on the most mature of these intelligent services, specifically those that provide computer vision. We use Stack Overflow to mine indications of the frustrations that developers appear to face when using computer vision services, classifying their questions against two recent classification taxonomies (documentation-related and general questions). We find that, unlike mature fields like mobile development, there is a contrast in the types of questions asked by developers. These indicate a shallow understanding of the underlying technology that empower such systems. We discuss several implications of these findings via the lens of learning taxonomies to suggest how the software engineering community can improve these services and comment on the nature by which developers use them.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9284118,intelligent services;computer vision;documentation;pain points;stack overflow;empirical study,Computer vision;Cloud computing;Taxonomy;Restful API;Documentation;Faces;Software engineering,application program interfaces;cloud computing;computer vision;data mining;learning (artificial intelligence);mobile computing;software engineering,cloud computer vision pain-points;mining study;Stack Overflow;intelligent services;application developers;technology firms;RESTful APIs;APIs promise;on-demand machine intelligence;documentation;underlying machine;APIs look;conventional APIs;abstract away data-driven probabilistic behaviour-the;traditional cloud services;cloud storage;pain-points developers;provide computer vision;mine indications;computer vision services;mobile development;underlying technology,5
477,Agile Methods,Playing Planning Poker in Crowds: Human Computation of Software Effort Estimates,M. Alhamed; T. Storer,"University of Glasgow, United Kingdom; University of Glasgow, United Kingdom",2021,"Reliable cost effective effort estimation remains a considerable challenge for software projects. Recent work has demonstrated that the popular Planning Poker practice can produce reliable estimates when undertaken within a software team of knowledgeable domain experts. However, the process depends on the availability of experts and can be time-consuming to perform, making it impractical for large scale or open source projects that may curate many thousands of outstanding tasks. This paper reports on a full study to investigate the feasibility of using crowd workers supplied with limited information about a task to provide comparably accurate estimates using Planning Poker. We describe the design of a Crowd Planning Poker (CPP) process implemented on Amazon Mechanical Turk and the results of a substantial set of trials, involving more than 5000 crowd workers and 39 diverse software tasks. Our results show that a carefully organised and selected crowd of workers can produce effort estimates that are of similar accuracy to those of a single expert.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402047,Software effort estimation;Planning poker;Agile development;Human Computation;Augmented intelligence;Software engineering;crowdsourcing;crowd planning poker;open source,Estimation;Team working;Software;Planning;Software reliability;Task analysis;Software engineering,project management;software cost estimation;software development management,human computation;software effort estimates;reliable cost effective effort estimation;software projects;software team;knowledgeable domain experts;open source projects;Crowd Planning Poker process;software tasks;Amazon Mechanical Turk,5
478,"Analysis System Properties: Correctness, Determinism, Realizability",JEST: N+1-Version Differential Testing of Both JavaScript Engines and Specification,J. Park; S. An; D. Youn; G. Kim; S. Ryu,"School of Computing KAIST, Daejeon, South Korea; School of Computing KAIST, Daejeon, South Korea; School of Computing KAIST, Daejeon, South Korea; School of Computing KAIST, Daejeon, South Korea; School of Computing KAIST, Daejeon, South Korea",2021,"Modern programming follows the continuous integration (CI) and continuous deployment (CD) approach rather than the traditional waterfall model. Even the development of modern programming languages uses the CI/CD approach to swiftly provide new language features and to adapt to new development environments. Unlike in the conventional approach, in the modern CI/CD approach, a language specification is no more the oracle of the language semantics because both the specification and its implementations (interpreters or compilers) can co-evolve. In this setting, both the specification and implementations may have bugs, and guaranteeing their correctness is non-trivial. In this paper, we propose a novel N+1-versiondifferentialtesting to resolve the problem. Unlike the traditional differential testing, our approach consists of three steps: (1) to automatically synthesize programs guided by the syntax and semantics from a given language specification, (2) to generate conformance tests by injecting assertions to the synthesized programs to check their final program states, (3) to detect bugs in the specification and implementations via executing the conformance tests on multiple implementations and (4) to localize bugs on the specification using statistical information. We actualize our approach for the JavaScript programming language via JEST, which performs N+1-version differential testing for modern JavaScript engines and ECMAScript, the language specification describing the syntax and semantics of JavaScript in a natural language. We evaluated JEST with four JavaScript engines that support all modern JavaScript language features and the latest version of ECMAScript (ES11, 2020). JEST automatically synthesized 1,700 programs that covered 97.78% of syntax and 87.70% of semantics from ES11. Using the assertion-injected JavaScript programs, it detected 44 engine bugs in four different engines and 27 specification bugs in ES11.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402086,JavaScript;conformance test generation;mechanized specification;differential testing,Computer languages;Computer bugs;Semantics;Syntactics;Engines;Testing;Software engineering,formal specification;Java;program compilers;program debugging;program testing;program verification;programming languages,conformance tests;multiple implementations;JavaScript programming language;specification bugs;engine bugs;CD;continuous deployment;CI;continuous integration;JavaScript specification;JavaScript engines;N+1-version differential testing;final program states;synthesized programs;language specification;traditional differential testing;language semantics;development environments;programming languages;traditional waterfall model;assertion-injected JavaScript programs;support all modern JavaScript language features;natural language;modern JavaScript engines;+1-version differential testing;JEST,2
479,"Analysis System Properties: Correctness, Determinism, Realizability",Unrealizable Cores for Reactive Systems Specifications,S. Maoz; R. Shalom,"Tel Aviv University, Tel Aviv, Israel; Tel Aviv University, Tel Aviv, Israel",2021,"One of the main challenges of reactive synthesis, an automated procedure to obtain a correct-by-construction reactive system, is to deal with unrealizable specifications. One means to deal with unrealizability, in the context of GR(1), an expressive assume-guarantee fragment of LTL that enables efficient synthesis, is the computation of an unrealizable core, which can be viewed as a fault-localization approach. Existing solutions, however, are computationally costly, are limited to computing a single core, and do not correctly support specifications with constructs beyond pure GR(1) elements. In this work we address these limitations. First, we present QuickCore, a novel algorithm that accelerates unrealizable core computations by relying on the monotonicity of unrealizability, on an incremental computation, and on additional properties of GR(1) specifications. Second, we present Punch, a novel algorithm to efficiently compute all unrealizable cores of a specification. Finally, we present means to correctly handle specifications that include higher-level constructs beyond pure GR(1) elements. We implemented our ideas on top of Spectra, an open-source language and synthesis environment. Our evaluation over benchmarks from the literature shows that QuickCore is in most cases faster than previous algorithms, and that its relative advantage grows with scale. Moreover, we found that most specifications include more than one core, and that Punch finds all the cores significantly faster than a competing naive algorithm.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402008,Reactive systems specifications;Unrealizability,Software algorithms;Benchmark testing;Computational efficiency;Acceleration;Software engineering,formal specification;program debugging;temporal logic,reactive systems specifications;reactive synthesis;correct-by-construction reactive system;unrealizable specifications;unrealizability;single core;unrealizable core computations;incremental computation;handle specifications;open-source language;synthesis environment,6
480,"Analysis System Properties: Correctness, Determinism, Realizability",Verifying Determinism in Sequential Programs,R. Mudduluru; J. Waataja; S. Millstein; M. Ernst,University of Washington University of Washington; University of Washington University of Washington; University of Washington; University of Washington,2021,"When a program is nondeterministic, it is difficult to test and debug. Nondeterminism occurs even in sequential programs: e.g., by iterating over the elements of a hash table. We have created a type system that expresses determinism specifications in a program. The key ideas in the type system are type qualifiers for nondeterminism, order-nondeterminism, and determinism; type well-formedness rules to restrict collection types; and enhancements to polymorphism that improve precision when analyzing collection operations. While state of-the-art nondeterminism detection tools rely on observing output from specific runs, our approach soundly verifies determinism at compile time. We implemented our type system for Java. Our type checker, the Determinism Checker, warns if a program is nondeterministic or verifies that the program is deterministic. In case studies of 90097 lines of code, the Determinism Checker found 87 previously-unknown nondeterminism errors, even in programs that had been heavily vetted by developers who were greatly concerned about nondeterminism errors. In experiments, the Determinism Checker found all of the non-concurrency-related nondeterminism that was found by state-of-the-art dynamic approaches for detecting flaky tests.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402110,nondeterminism;type system;verification;specification;hash table;flaky tests,Java;Computer bugs;Detectors;Tools;Software;Software engineering,,,1
481,API: Development,Domain-Specific Fixes for Flaky Tests with Wrong Assumptions on Underdetermined Specifications,P. Zhang; Y. Jiang; A. Wei; V. Stodden; D. Marinov; A. Shi,"University of Illinois at Urbana-Champaign, USA; Beijing Institute of Technology, China; Peking University, China; University of Illinois at Urbana-Champaign, USA; University of Illinois at Urbana-Champaign, USA; The University of Texas at Austin, USA",2021,"Library developers can provide classes and methods with underdetermined specifications that allow flexibility in future implementations. Library users may write code that relies on a specific implementation rather than on the specification, e.g., assuming mistakenly that the order of elements cannot change in the future. Prior work proposed the NonDex approach that detects such wrong assumptions. We present a novel approach, called DexFix, to repair wrong assumptions on underdetermined specifications in an automated way. We run the NonDex tool on 200 open-source Java projects and detect 275 tests that fail due to wrong assumptions. The majority of failures are from iterating over HashMap/HashSet collections and the getDeclaredFields method. We provide several new repair strategies that can fix these violations in both the test code and the main code. DexFix proposes fixes for 119 tests from the detected 275 tests. We have already reported fixes for 102 tests as GitHub pull requests: 74 have been merged, with only 5 rejected, and the remaining pending.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402009,,Java;Maintenance engineering;Tools;Libraries;Open source software;Software engineering;Software development management,Java;program testing;public domain software,domain-specific fixes;flaky tests;NonDex approach;open-source Java projects;test code;DexFix;HashMap-HashSet collections;DeclaredFields method;GitHub pull requests,18
482,API: Development,Studying Test Annotation Maintenance in the Wild,D. J. Kim; N. Tsantalis; T. -H. Chen; J. Yang,"Software PEformance, Analysis and Reliability (SPEAR) Lab, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Analysis and Reliability (SPEAR) Lab, Concordia University, Montreal, QC, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada",2021,"Since the introduction of annotations in Java 5, the majority of testing frameworks, such as JUnit, TestNG, and Mockito, have adopted annotations in their core design. This adoption affected the testing practices in every step of the test life-cycle, from fixture setup and test execution to fixture teardown. Despite the importance of test annotations, most research on test maintenance has mainly focused on test code quality and test assertions. As a result, there is little empirical evidence on the evolution and maintenance of test annotations. To fill this gap, we perform the first fine-grained empirical study on annotation changes. We developed a tool to mine 82,810 commits and detect 23,936 instances of test annotation changes from 12 open-source Java projects. Our main findings are: (1) Test annotation changes are more frequent than rename and type change refactorings. (2) We recover various migration efforts within the same testing framework or between different frameworks by analyzing common annotation replacement patterns. (3) We create a taxonomy by manually inspecting and classifying a sample of 368 test annotation changes and documenting the motivations driving these changes. Finally, we present a list of actionable implications for developers, researchers, and framework designers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402083,Software Quality;Empirical Study;Annotation;Software Evolution,Java;Annotations;Fixtures;Taxonomy;Maintenance engineering;Tools;Testing,Java;program testing;public domain software;software engineering;software maintenance,testing practices;test life-cycle;fixture setup;test execution;test annotations;test maintenance;test code quality;test assertions;testing framework;common annotation replacement patterns;368 test annotation changes;Test annotation maintenance,6
483,API: Evolution and Maintenance 1,Semantic Patches for Adaptation of JavaScript Programs to Evolving Libraries,B. B. Nielsen; M. T. Torp; A. MÃ¸ller,Aarhus University; Aarhus University; Aarhus University,2021,"JavaScript libraries are often updated and sometimes breaking changes are introduced in the process, resulting in the client developers having to adapt their code to the changes. In addition to locating the affected parts of their code, the client developers must apply suitable patches, which is a tedious, error-prone, and entirely manual process. To reduce the manual effort, we present JSFIX. Given a collection of semantic patches, which are formalized descriptions of the breaking changes, the tool detects the locations affected by breaking changes and then transforms those parts of the code to become compatible with the new library version. JSFIX relies on an existing static analysis to approximate the set of affected locations, and an interactive process where the user answers questions about the client code to filter away false positives. An evaluation involving 12 popular JavaScript libraries and 203 clients shows that our notion of semantic patches can accurately express most of the breaking changes that occur in practice, and that JSFIX can successfully adapt most of the clients to the changes. In particular, 31 clients have accepted pull requests made by JSFIX, indicating that the code quality is good enough for practical usage. It takes JSFIX only a few seconds to patch, on average, 3.8 source locations affected by breaking changes in each client, with only 2.7 questions to the user, which suggests that the approach can significantly reduce the manual effort required when adapting JavaScript programs to evolving libraries.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402106,,Semantics;Manuals;Transforms;Static analysis;Tools;Position measurement;Libraries,,,
484,API: Evolution and Maintenance 2,DepOwl: Detecting Dependency Bugs to Prevent Compatibility Failures,Z. Jia; S. Li; T. Yu; C. Zeng; E. Xu; X. Liu; J. Wang; X. Liao,"College of Computer Science, National University of Defense Technology Changsha, China; College of Computer Science, National University of Defense Technology Changsha, China; Department of Computer Science, University of Kentucky Lexington, USA; College of Computer Science, National University of Defense Technology Changsha, China; College of Computer Science, National University of Defense Technology Changsha, China; College of Computer Science, National University of Defense Technology Changsha, China; College of Computer Science, National University of Defense Technology Changsha, China; College of Computer Science, National University of Defense Technology Changsha, China",2021,"Applications depend on libraries to avoid reinventing the wheel. Libraries may have incompatible changes during evolving. As a result, applications will suffer from compatibility failures. There has been much research on addressing detecting incompatible changes in libraries, or helping applications co-evolve with the libraries. The existing solution helps the latest application version work well against the latest library version as an afterthought. However, end users have already been suffering from the failures and have to wait for new versions. In this paper, we propose DepOwl, a practical tool helping users prevent compatibility failures. The key idea is to avoid using incompatible versions from the very beginning. We evaluated DepOwl on 38 known compatibility failures from StackOverflow, and DepOwl can prevent 35 of them. We also evaluated DepOwl using the software repository shipped with Ubuntu-19.10. DepOwl detected 77 unknown dependency bugs, which may lead to compatibility failures.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402138,Software dependency;Library incompatibility;Compatibility failure,Computer bugs;Wheels;Tools;Libraries;Software;Software reliability;Software engineering,Internet;program debugging;public domain software;software libraries;software maintenance,compatibility failures;unknown dependency bugs;incompatible changes;incompatible versions;DepOwl;library version;application version;helping applications,3
485,API: Evolution and Maintenance 2,Hero: On the Chaos When PATH Meets Modules,Y. Wang; L. Qiao; C. Xu; Y. Liu; S. -C. Cheung; N. Meng; H. Yu; Z. Zhu,"Software College, Northeastern University, China; Software College, Northeastern University, China; State Key Laboratory for Novel Software Technology and Department of Computer Science and Technology, Nanjing University, China; Southern University of Science and Technology, China; The Hong Kong University of Science and Technology, China; Virginia Tech, USA; Software College, Northeastern University, China; Software College, Northeastern University, China",2021,"Ever since its first release in 2009, the Go programming language (Golang) has been well received by software communities. A major reason for its success is the powerful support of library-based development, where a Golang project can be conveniently built on top of other projects by referencing them as libraries. As Golang evolves, it recommends the use of a new library-referencing mode to overcome the limitations of the original one. While these two library modes are incompatible, both are supported by the Golang ecosystem. The heterogeneous use of library-referencing modes across Golang projects has caused numerous dependency management (DM) issues, incurring reference inconsistencies and even build failures. Motivated by the problem, we conducted an empirical study to characterize the DM issues, understand their root causes, and examine their fixing solutions. Based on our findings, we developed Hero, an automated technique to detect DM issues and suggest proper fixing solutions. We applied Hero to 19,000 popular Golang projects. The results showed that Hero achieved a high detection rate of 98.5% on a DM issue benchmark and found 2,422 new DM issues in 2,356 popular Golang projects. We reported 280 issues, among which 181 (64.6%) issues have been confirmed, and 160 of them (88.4%) have been fixed or are under fixing. Almost all the fixes have adopted our fixing suggestions.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401974,Golang Ecosystem;Dependency Management,Chaos;Computer languages;Ecosystems;Benchmark testing;Libraries;Software;Software engineering,data analysis;programming languages;public domain software;software libraries,Hero;Go programming language;software communities;powerful support;library-based development;Golang project;library-referencing mode;library modes;Golang ecosystem;DM issues;proper fixing solutions;DM issue benchmark;fixes;dependency management issues,
486,API: Usage and Refactoring,SOAR: A Synthesis Approach for Data Science API Refactoring,A. Ni; D. Ramos; A. Z. H. Yang; I. Lynce; V. Manquinho; R. Martins; C. Le Goues,"Yale University, New Haven, USA; INESC-ID/IST, U. Lisboa, Portugal Carnegie Mellon University, USA; Queen's University, Queensland, QC, Canada; INESC-ID/IST, U. Lisboa, Lisbon, Portugal; INESC-ID/IST, U. Lisboa, Lisboa, Portugal; School of Computer Science, Carnegie Mellon University, Pittsburgh, USA; Carnegie Mellon University, Mellon, MA, USA",2021,"With the growth of the open-source data science community, both the number of data science libraries and the number of versions for the same library are increasing rapidly. To match the evolving APIs from those libraries, open-source organizations often have to exert manual effort to refactor the APIs used in the code base. Moreover, due to the abundance of similar open-source libraries, data scientists working on a certain application may have an abundance of libraries to choose, maintain and migrate between. The manual refactoring between APIs is a tedious and error-prone task. Although recent research efforts were made on performing automatic API refactoring between different languages, previous work relies on statistical learning with collected pairwise training data for the API matching and migration. Using large statistical data for refactoring is not ideal because such training data will not be available for a new library or a new version of the same library. We introduce Synthesis for Open-Source API Refactoring (SOAR), a novel technique that requires no training data to achieve API migration and refactoring. SOAR relies only on the documentation that is readily available at the release of the library to learn API representations and mapping between libraries. Using program synthesis, SOAR automatically computes the correct configuration of arguments to the APIs and any glue code required to invoke those APIs. SOAR also uses the interpreter's error messages when running refactored code to generate logical constraints that can be used to prune the search space. Our empirical evaluation shows that SOAR can successfully refactor 80% of our benchmarks corresponding to deep learning models with up to 44 layers with an average run time of 97.23 seconds, and 90% of the data wrangling benchmarks with an average run time of 17.31 seconds.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402016,software maintenance;program translation;program synthesis,Training data;Documentation;Data science;Benchmark testing;Libraries;Task analysis;Open source software,application program interfaces;data analysis;learning (artificial intelligence);software libraries;software maintenance,SOAR;data science API Refactoring;open-source data science community;data science libraries;evolving APIs;library;open-source organizations;similar open-source libraries;data scientists;manual refactoring;automatic API refactoring;training data;statistical data;Open-Source API Refactoring;API migration,6
487,API: Usage and Refactoring,Are Machine Learning Cloud APIs Used Correctly?,C. Wan; S. Liu; H. Hoffmann; M. Maire; S. Lu,University of Chicago; University of Chicago; University of Chicago; University of Chicago; University of Chicago,2021,"Machine learning (ML) cloud APIs enable developers to easily incorporate learning solutions into software systems. Unfortunately, ML APIs are challenging to use correctly and efficiently, given their unique semantics, data requirements, and accuracy-performance tradeoffs. Much prior work has studied how to develop ML APIs or ML cloud services, but not how open-source applications are using ML APIs. In this paper, we manually studied 360 representative open-source applications that use Google or AWS cloud-based ML APIs, and found 70% of these applications contain API misuses in their latest versions that degrade functional, performance, or economical quality of the software. We have generalized 8 anti-patterns based on our manual study and developed automated checkers that identify hundreds of more applications that contain ML API misuses.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402073,machine learning;software engineering;cloud API,Semantics;Machine learning;Manuals;Software systems;Internet;Open source software;Software engineering,application program interfaces;cloud computing;learning (artificial intelligence),easily incorporate learning solutions;ML APIs;ML cloud services;360 representative open-source applications;ML API misuses;cloud APIs used;machine learning cloud APIs,5
488,Code Completion,"Siri, Write the Next Method",F. Wen; E. Aghajani; C. Nagy; M. Lanza; G. Bavota,"Software Institute USI UniversitÃ della Svizzera italiana, Switzerland; Software Institute USI UniversitÃ della Svizzera italiana, Switzerland; Software Institute USI UniversitÃ della Svizzera italiana, Switzerland; Software Institute USI UniversitÃ della Svizzera italiana, Switzerland; Software Institute USI UniversitÃ della Svizzera italiana, Switzerland",2021,"Code completion is one of the killer features of Integrated Development Environments (IDEs), and researchers have proposed different methods to improve its accuracy. While these techniques are valuable to speed up code writing, they are limited to recommendations related to the next few tokens a developer is likely to type given the current context. In the best case, they can recommend a few APIs that a developer is likely to use next. We present FeaRS, a novel retrieval-based approach that, given the current code a developer is writing in the IDE, can recommend the next complete method (i.e., signature and method body) that the developer is likely to implement. To do this, FeaRS exploits ""implementation patterns"" (i.e., groups of methods usually implemented within the same task) learned by mining thousands of open source projects. We instantiated our approach to the specific context of Android apps. A large-scale empirical evaluation we performed across more than 20k apps shows encouraging preliminary results, but also highlights future challenges to overcome.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402018,Code Recommender;Empirical Software Engineering;Mining Software Repositories,Virtual assistants;Writing;Tools;Software;Encoding;Task analysis;Software engineering,application program interfaces;data mining;information retrieval;programming environments;public domain software;source code (software),code completion;killer features;integrated development environments;IDE;code writing;FeaRS;retrieval-based approach;implementation patterns;Siri;API;open source projects,7
489,Code Completion,Code Prediction by Feeding Trees to Transformers,S. Kim; J. Zhao; Y. Tian; S. Chandra,"Facebook Inc., U.S.A.; University of Wisconsin-Madison, U.S.A.; Columbia University, U.S.A.; Facebook Inc., U.S.A.",2021,"Code prediction, more specifically autocomplete, has become an essential feature in modern IDEs. Autocomplete is more effective when the desired next token is at (or close to) the top of the list of potential completions offered by the IDE at cursor position. This is where the strength of the underlying machine learning system that produces a ranked order of potential completions comes into play. We advance the state-of-the-art in the accuracy of code prediction (next token prediction) used in autocomplete systems. Our work uses Transformers as the base neural architecture. We show that by making the Transformer architecture aware of the syntactic structure of code, we increase the margin by which a Transformer-based system outperforms previous systems. With this, it outperforms the accuracy of several state-of-the-art next token prediction systems by margins ranging from 14% to 18%. We present in the paper several ways of communicating the code structure to the Transformer, which is fundamentally built for processing sequence data. We provide a comprehensive experimental evaluation of our proposal, along with alternative design choices, on a standard Python dataset, as well as on Facebook internal Python corpus. Our code and data preparation pipeline will be available in open source.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402114,code embedding;code prediction;autocomplete,Pipelines;Computer architecture;Syntactics;Proposals;Standards;Python;Software engineering,learning (artificial intelligence);neural net architecture;parallel architectures;Python;social networking (online);trees (mathematics),code prediction;IDEs;machine learning system;autocomplete systems;neural architecture;token prediction systems;code structure;data preparation pipeline;transformer-based system;transformer architecture aware;Facebook internal Python corpus;sequence data,38
490,Code Review: Automation,Towards Automating Code Review Activities,R. Tufano; L. Pascarella; M. Tufano; D. Poshyvanyk; G. Bavota,"SEART @ Software Institute, UniversitÃ della Svizzera Italiana, Lugano, Switzerland; SEART@Software Institute UniversitÃ della Svizzera italiana (USI), Switzerland; Microsoft; SEMEIRU @ Computer Science Department, William and Mary, Williamsburg, VA, USA; SEART@Software Institute UniversitÃ della Svizzera italiana (USI), Switzerland",2021,"Code reviews are popular in both industrial and open source projects. The benefits of code reviews are widely recognized and include better code quality and lower likelihood of introducing bugs. However, since code review is a manual activity it comes at the cost of spending developers' time on reviewing their teammates' code. Our goal is to make the first step towards partially automating the code review process, thus, possibly reducing the manual costs associated with it. We focus on both the contributor and the reviewer sides of the process, by training two different Deep Learning architectures. The first one learns code changes performed by developers during real code review activities, thus providing the contributor with a revised version of her code implementing code transformations usually recommended during code review before the code is even submitted for review. The second one automatically provides the reviewer commenting on a submitted code with the revised code implementing her comments expressed in natural language. The empirical evaluation of the two models shows that, on the contributor side, the trained model succeeds in replicating the code transformations applied during code reviews in up to 16% of cases. On the reviewer side, the model can correctly implement a comment provided in natural language in up to 31% of cases. While these results are encouraging, more research is needed to make these models usable by developers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402025,Code Review;Empirical Software Engineering;Deep Learning,Training;Deep learning;Natural languages;Manuals;Data models;Software;Software engineering,learning (artificial intelligence);program diagnostics;public domain software;software maintenance;software quality,code changes;code transformations;submitted code;revised code;reviewer side;code quality;teammates;code review process;code review activities automation;natural language;open source projects,29
491,Configuration of Software Systems: Optimization,Resource-Guided Configuration Space Reduction for Deep Learning Models,Y. Gao; Y. Zhu; H. Zhang; H. Lin; M. Yang,"Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; The University of Newcastle, NSW, Australia; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China",2021,"Deep learning models, like traditional software systems, provide a large number of configuration options. A deep learning model can be configured with different hyperparameters and neural architectures. Recently, AutoML (Automated Machine Learning) has been widely adopted to automate model training by systematically exploring diverse configurations. However, current AutoML approaches do not take into consideration the computational constraints imposed by various resources such as available memory, computing power of devices, or execution time. The training with non-conforming configurations could lead to many failed AutoML trial jobs or inappropriate models, which cause significant resource waste and severely slow down development productivity. In this paper, we propose DnnSAT, a resource-guided AutoML approach for deep learning models to help existing AutoML tools efficiently reduce the configuration space ahead of time. DnnSAT can speed up the search process and achieve equal or even better model learning performance because it excludes trial jobs not satisfying the constraints and saves resources for more trials. We formulate the resource-guided configuration space reduction as a constraint satisfaction problem. DnnSAT includes a unified analytic cost model to construct common constraints with respect to the model weight size, number of floating-point operations, model inference time, and GPU memory consumption. It then utilizes an SMT solver to obtain the satisfiable configurations of hyperparameters and neural architectures. Our evaluation results demonstrate the effectiveness of DnnSAT in accelerating state-of-the-art AutoML methods (Hyperparameter Optimization and Neural Architecture Search) with an average speedup from 1.19X to 3.95X on public benchmarks. We believe that DnnSAT can make AutoML more practical in a real-world environment with constrained resources.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402095,configurable systems;deep learning;AutoML;constraint solving,Deep learning;Training;Analytical models;Computational modeling;Tools;Software systems;Software engineering,deep learning (artificial intelligence);neural net architecture;optimisation;search problems,resource-guided configuration space reduction;deep learning model;configuration options;automate model training;diverse configurations;nonconforming configurations;failed AutoML trial jobs;inappropriate models;DnnSAT;resource-guided AutoML approach;model learning performance;unified analytic cost model;model weight size;model inference time;SMT solver;satisfiable configurations;neural architectures;floating-point operations;GPU memory consumption,5
492,Configuration of Software Systems: Testing,An Evolutionary Study of Configuration Design and Implementation in Cloud Systems,Y. Zhang; H. He; O. Legunsen; S. Li; W. Dong; T. Xu,"National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China; Cornell University, Ithaca, NY, USA; National University of Defense Technology, Changsha, Hunan, China; National University of Defense Technology, Changsha, Hunan, China; University of Illinois at Urbana-Champaign, Urbana, IL, USA",2021,"Many techniques were proposed for detecting software misconfigurations in cloud systems and for diagnosing unintended behavior caused by such misconfigurations. Detection and diagnosis are steps in the right direction: misconfigurations cause many costly failures and severe performance issues. But, we argue that continued focus on detection and diagnosis is symptomatic of a more serious problem: configuration design and implementation are not yet first-class software engineering endeavors in cloud systems. Little is known about how and why developers evolve configuration design and implementation, and the challenges that they face in doing so. This paper presents a source-code level study of the evolution of configuration design and implementation in cloud systems. Our goal is to understand the rationale and developer practices for revising initial configuration design/implementation decisions, especially in response to consequences of misconfigurations. To this end, we studied 1178 configuration-related commits from a 2.5 year version-control history of four large-scale, actively-maintained open-source cloud systems (HDFS, HBase, Spark, and Cassandra). We derive new insights into the software configuration engineering process. Our results motivate new techniques for proactively reducing misconfigurations by improving the configuration design and implementation process in cloud systems. We highlight a number of future research directions.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401962,,Runtime;Sparks;History;Open source software;Faces,cloud computing;configuration management;public domain software;software maintenance;software reliability;system recovery,software misconfigurations;first-class software engineering endeavors;source-code level study;actively-maintained open-source cloud systems;software configuration engineering process;configuration-related commits;time 2.5 year,4
493,Configuration of Software Systems: Testing,AutoCCAG: An Automated Approach to Constrained Covering Array Generation,C. Luo; J. Lin; S. Cai; X. Chen; B. He; B. Qiao; P. Zhao; Q. Lin; H. Zhang; W. Wu; S. Rajmohan; D. Zhang,"Microsoft Research, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, China; Microsoft Research, China; Microsoft Research, China; Microsoft Research, China; Microsoft Research, China; Microsoft Research, China; The University of Newcastle, Australia; L3S Research Center, Leibniz University Hannover, Germany; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, China; Microsoft Research, China",2021,"Combinatorial interaction testing (CIT) is an important technique for testing highly configurable software systems with demonstrated effectiveness in practice. The goal of CIT is to generate test cases covering the interactions of configuration options, under certain hard constraints. In this context, constrained covering arrays (CCAs) are frequently used as test cases in CIT. Constrained Covering Array Generation (CCAG) is an NP-hard combinatorial optimization problem, solving which requires an effective method for generating small CCAs. In particular, effectively solving t-way CCAG with t>=4 is even more challenging. Inspired by the success of automated algorithm configuration and automated algorithm selection in solving combinatorial optimization problems, in this paper, we investigate the efficacy of automated algorithm configuration and automated algorithm selection for the CCAG problem, and propose a novel, automated CCAG approach called AutoCCAG. Extensive experiments on public benchmarks show that AutoCCAG can find much smaller-sized CCAs than current state-of-the-art approaches, indicating the effectiveness of AutoCCAG. More encouragingly, to our best knowledge, our paper reports the first results for CCAG with a high coverage strength (i.e., 5-way CCAG) on public benchmarks. Our results demonstrate that AutoCCAG can bring considerable benefits in testing highly configurable software systems.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402109,Constrained Covering Array Generation;Automated Algorithm Optimization,Software algorithms;Benchmark testing;Software systems;Problem-solving;Optimization;Software engineering,combinatorial mathematics;computational complexity;optimisation;program testing,AutoCCAG;constrained Covering Array Generation;combinatorial interaction testing;CIT;highly configurable software systems;configuration options;Constrained Covering Array Generation;NP-hard combinatorial optimization problem;automated algorithm configuration;automated algorithm selection;combinatorial optimization problems;CCAG problem;automated CCAG approach,4
494,Continuous Integration,"What Helped, and what did not? An Evaluation of the Strategies to Improve Continuous Integration",X. Jin; F. Servant,"Virginia Tech, Computer Science; Virginia Tech, Computer Science",2021,"Continuous integration (CI) is a widely used practice in modern software engineering. Unfortunately, it is also an expensive practice - Google and Mozilla estimate their CI systems in millions of dollars. There are a number of techniques and tools designed to or having the potential to save the cost of CI or expand its benefit - reducing time to feedback. However, their benefits in some dimensions may also result in drawbacks in others. They may also be beneficial in other scenarios where they are not designed to help. In this paper, we perform the first exhaustive comparison of techniques to improve CI, evaluating 14 variants of 10 techniques using selection and prioritization strategies on build and test granularity. We evaluate their strengths and weaknesses with 10 different cost and time-tofeedback saving metrics on 100 real-world projects. We analyze the results of all techniques to understand the design decisions that helped different dimensions of benefit. We also synthesized those results to lay out a series of recommendations for the development of future research techniques to advance this area.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401965,continuous integration;software maintenance;empirical software engineering,Measurement;Tools;Internet;Software engineering,program testing;software development management,continuous integration;modern software engineering;Google;CI systems;exhaustive comparison;prioritization strategies;test granularity;design decisions;future research techniques,6
495,Deep Neural Networks: Data Selection,Distribution-Aware Testing of Neural Networks Using Generative Models,S. Dola; M. B. Dwyer; M. L. Soffa,"Department of Computer Engineering, University of Virginia, Charlottesville, USA; Department of Computer Science, University of Virginia, Charlottesville, USA; Department of Computer Science, University of Virginia, Charlottesville, USA",2021,"The reliability of software that has a Deep Neural Network (DNN) as a component is urgently important today given the increasing number of critical applications being deployed with DNNs. The need for reliability raises a need for rigorous testing of the safety and trustworthiness of these systems. In the last few years, there have been a number of research efforts focused on testing DNNs. However the test generation techniques proposed so far lack a check to determine whether the test inputs they are generating are valid, and thus invalid inputs are produced. To illustrate this situation, we explored three recent DNN testing techniques. Using deep generative model based input validation, we show that all the three techniques generate significant number of invalid test inputs. We further analyzed the test coverage achieved by the test inputs generated by the DNN testing techniques and showed how invalid test inputs can falsely inflate test coverage metrics. To overcome the inclusion of invalid inputs in testing, we propose a technique to incorporate the valid input space of the DNN model under test in the test generation process. Our technique uses a deep generative model-based algorithm to generate only valid inputs. Results of our empirical studies show that our technique is effective in eliminating invalid tests and boosting the number of valid test inputs generated.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402100,deep Neural Networks;deep learning;input validation;test generation;test coverage,Neural networks;Computer architecture;Lead;Software;Software reliability;Test pattern generators;Testing,neural nets;program testing,distribution-aware testing;Deep Neural Network;rigorous testing;test generation techniques;DNN testing techniques;deep generative model based input validation;test coverage metrics;test generation process,7
496,Deep Neural Networks: Data Selection,An Empirical Study of Refactorings and Technical Debt in Machine Learning Systems,Y. Tang; R. Khatchadourian; M. Bagherzadeh; R. Singh; A. Stewart; A. Raja,CUNY Graduate Center; CUNY Hunter College; Oakland University; CUNY Macaulay Honors College; CUNY Hunter College; CUNY Hunter College,2021,"Machine Learning (ML), including Deep Learning (DL), systems, i.e., those with ML capabilities, are pervasive in today's data-driven society. Such systems are complex; they are comprised of ML models and many subsystems that support learning processes. As with other complex systems, ML systems are prone to classic technical debt issues, especially when such systems are long-lived, but they also exhibit debt specific to these systems. Unfortunately, there is a gap of knowledge in how ML systems actually evolve and are maintained. In this paper, we fill this gap by studying refactorings, i.e., source-to-source semantics-preserving program transformations, performed in real-world, open-source software, and the technical debt issues they alleviate. We analyzed 26 projects, consisting of 4.2 MLOC, along with 327 manually examined code patches. The results indicate that developers refactor these systems for a variety of reasons, both specific and tangential to ML, some refactorings correspond to established technical debt categories, while others do not, and code duplication is a major cross-cutting theme that particularly involved ML configuration and model code, which was also the most refactored. We also introduce 14 and 7 new ML-specific refactorings and technical debt categories, respectively, and put forth several recommendations, best practices, and anti-patterns. The results can potentially assist practitioners, tool developers, and educators in facilitating long-term ML system usefulness.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401990,empirical studies;refactoring;machine learning systems;technical debt;software repository mining;software evolution,Deep learning;Tools;Data mining;Complex systems;Open source software;Best practices;Software engineering,learning (artificial intelligence);public domain software;software maintenance,ML-specific refactorings;machine learning systems;data-driven society;learning processes;complex systems;ML systems;source-to-source semantics-preserving program transformations;open-source software;technical debt categories;manually examined code patches,15
497,Deep Neural Networks: Data Selection,DeepLocalize: Fault Localization for Deep Neural Networks,M. Wardat; W. Le; H. Rajan,"Department of Computer Science, Iowa State University, 2434 Osborn Dr, Ames, IA, USA; Department of Computer Science, Iowa State University, 2434 Osborn Dr, Ames, IA, USA; Department of Computer Science, Iowa State University, 2434 Osborn Dr, Ames, IA, USA",2021,"Deep Neural Networks (DNNs) are becoming an integral part of most software systems. Previous work has shown that DNNs have bugs. Unfortunately, existing debugging techniques don't support localizing DNN bugs because of the lack of understanding of model behaviors. The entire DNN model appears as a black box. To address these problems, we propose an approach and a tool that automatically determines whether the model is buggy or not, and identifies the root causes for DNN errors. Our key insight is that historic trends in values propagated between layers can be analyzed to identify faults, and also localize faults. To that end, we first enable dynamic analysis of deep learning applications: by converting it into an imperative representation and alternatively using a callback mechanism. Both mechanisms allows us to insert probes that enable dynamic analysis over the traces produced by the DNN while it is being trained on the training data. We then conduct dynamic analysis over the traces to identify the faulty layer or hyperparameter that causes the error. We propose an algorithm for identifying root causes by capturing any numerical error and monitoring the model during training and finding the relevance of every layer/parameter on the DNN outcome. We have collected a benchmark containing 40 buggy models and patches that contain real errors in deep learning applications from Stack Overflow and GitHub. Our benchmark can be used to evaluate automated debugging tools and repair techniques. We have evaluated our approach using this DNN bug-and-patch benchmark, and the results showed that our approach is much more effective than the existing debugging approach used in the state-of-the-practice Keras library. For 34/40 cases, our approach was able to detect faults whereas the best debugging approach provided by Keras detected 32/40 faults. Our approach was able to localize 21/40 bugs whereas Keras did not localize any faults.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402065,Deep Neural Networks;Fault Location;Debugging;Program Analysis;Deep learning bugs,Fault diagnosis;Computer bugs;Neural networks;Debugging;Benchmark testing;Tools;Numerical models,neural nets;program debugging,deep learning applications;fault localization;deep neural networks;black box;dynamic analysis;callback mechanism;DNN bug-and-patch benchmark,23
498,Deep Neural Networks: Hacking,DeepPayload: Black-box Backdoor Attack on Deep Learning Models through Neural Payload Injection,Y. Li; J. Hua; H. Wang; C. Chen; Y. Liu,"Microsoft Research, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Monash University, Melbourne, Australia; Microsoft Research, Beijing, China",2021,"Deep learning models are increasingly used in mobile applications as critical components. Unlike the program bytecode whose vulnerabilities and threats have been widely-discussed, whether and how the deep learning models deployed in the applications can be compromised are not well-understood since Neural Networks are usually viewed as a black box. In this paper, we introduce a highly practical backdoor attack achieved with a set of reverse-engineering techniques over compiled deep learning models. The core of the attack is a neural conditional branch constructed with a trigger detector and several operators and injected into the victim model as a malicious payload. The attack is effective as the conditional logic can be flexibly customized by the attacker, and scalable as it does not require any prior knowledge from the original model. We evaluated the attack effectiveness using 5 state-of-the-art deep learning models and real-world samples collected from 30 users. The results demonstrated that the injected backdoor can be triggered with a success rate of 93.5%, while only brought less than 2ms latency overhead and no more than 1.4% accuracy decrease. We further conducted an empirical study on real-world mobile deep learning apps collected from Google Play. We found 54 apps that were vulnerable to our attack, including popular and security-critical ones. The results call for the awareness of deep learning application developers and auditors to enhance the protection of deployed models.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402020,deep learning;backdoor attack;mobile applications;reverse engineering;malicious payload,Deep learning;Training;Neural networks;Internet;Mobile applications;Payloads;Software engineering,learning (artificial intelligence);mobile computing;neural nets;reverse engineering;security of data,black-box backdoor attack;backdoor attack;compiled deep learning models;neural conditional branch;attack effectiveness;5 state-of-the-art deep learning models;injected backdoor;real-world mobile deep learning apps;deep learning application developers;auditors;DeepPayload;black box backdoor attack;neural payload;neural payload injection;mobile applications;neural networks;reverse engineering technique;Google Play,12
499,Deep Neural Networks: Hacking,Reducing DNN Properties to Enable Falsification with Adversarial Attacks,D. Shriver; S. Elbaum; M. B. Dwyer,"Department of Computer Science, University of Virginia, Charlottesville, Virginia, USA; Department of Computer Science, University of Virginia, Charlottesville, Virginia, USA; Department of Computer Science, University of Virginia, Charlottesville, Virginia, USA",2021,"Deep Neural Networks (DNN) are increasingly being deployed in safety-critical domains, from autonomous vehicles to medical devices, where the consequences of errors demand techniques that can provide stronger guarantees about behavior than just high test accuracy. This paper explores broadening the application of existing adversarial attack techniques for the falsification of DNN safety properties. We contend and later show that such attacks provide a powerful repertoire of scalable algorithms for property falsification. To enable the broad application of falsification, we introduce a semantics-preserving reduction of multiple safety property types, which subsume prior work, into a set of equivalid correctness problems amenable to adversarial attacks. We evaluate our reduction approach as an enabler of falsification on a range of DNN correctness problems and show its cost-effectiveness and scalability.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402125,falsification;formal methods;neural nets,Systematics;Scalability;Software algorithms;Neural networks;Safety;Portfolios;Software engineering,neural nets;program verification;safety-critical software,adversarial attack techniques;DNN safety properties;scalable algorithms;property falsification;semantics-preserving reduction;multiple safety property types;adversarial attacks;DNN correctness problems;deep neural networks;safety-critical domains;autonomous vehicles;medical devices;errors demand techniques,6
500,Deep Neural Networks: Quality Assurance,Graph-Based Fuzz Testing for Deep Learning Inference Engines,W. Luo; D. Chai; X. Ruan; J. Wang; C. Fang; Z. Chen,"HiSilicon, Huawei, China; HiSilicon, Huawei, China; HiSilicon, Huawei, China; HiSilicon, Huawei, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",2021,"With the wide use of Deep Learning (DL) systems, academy and industry begin to pay attention to their quality. Testing is one of the major methods of quality assurance. However, existing testing techniques focus on the quality of DL models but lacks attention to the core underlying inference engines (i.e., frameworks and libraries). Inspired by the success stories of fuzz testing, we design a graph-based fuzz testing method to improve the quality of DL inference engines. This method is naturally followed by the graph structure of DL models. A novel operator-level coverage criterion based on graph theory is introduced and six different mutations are implemented to generate diversified DL models by exploring combinations of model structures, parameters, and data inputs. The Monte Carlo Tree Search (MCTS) is used to drive DL model generation without a training process. The experimental results show that the MCTS outperforms the random method in boosting operator-level coverage and detecting exceptions. Our method has discovered more than 40 different exceptions in three types of undesired behaviors: model conversion failure, inference failure, output comparison failure. The mutation strategies are useful to generate new valid test inputs, by up to an 8.2% more operator-level coverage on average and 8.6 more exceptions captured.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401995,Deep Learning Inference Engine;Graph Theory;Deep Learning Models;Operator-Level Coverage;Monte Carlo Tree Search,Deep learning;Industries;Fuzzing;Graph theory;Data models;Engines;Testing,fuzzy set theory;graph theory;inference mechanisms;learning (artificial intelligence);Monte Carlo methods;tree searching,deep learning inference engines;academy;quality assurance;DL models;graph-based fuzz testing method;DL inference engines;graph structure;graph theory;DL model generation;random method;model conversion failure;inference failure;DL systems;Monte Carlo tree search;MCTS,5
501,Deep Neural Networks: Quality Assurance,RobOT: Robustness-Oriented Testing for Deep Learning Systems,J. Wang; J. Chen; Y. Sun; X. Ma; D. Wang; J. Sun; P. Cheng,Zhejiang University; Zhejiang University; Queenâ€™s University Belfast; Deakin University; Zhejiang University; Singapore Management University; Zhejiang University,2021,"Recently, there has been a significant growth of interest in applying software engineering techniques for the quality assurance of deep learning (DL) systems. One popular direction is deep learning testing, where adversarial examples (a.k.a.~bugs) of DL systems are found either by fuzzing or guided search with the help of certain testing metrics. However, recent studies have revealed that the commonly used neuron coverage metrics by existing DL testing approaches are not correlated to model robustness. It is also not an effective measurement on the confidence of the model robustness after testing. In this work, we address this gap by proposing a novel testing framework called Robustness-Oriented Testing (RobOT). A key part of RobOT is a quantitative measurement on 1) the value of each test case in improving model robustness (often via retraining), and 2) the convergence quality of the model robustness improvement. RobOT utilizes the proposed metric to automatically generate test cases valuable for improving model robustness. The proposed metric is also a strong indicator on how well robustness improvement has converged through testing. Experiments on multiple benchmark datasets confirm the effectiveness and efficiency of RobOT in improving DL model robustness, with 67.02% increase on the adversarial robustness that is 50.65% higher than the state-of-the-art work DeepGini.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402039,,Measurement;Deep learning;Benchmark testing;Robustness;Robots;Software engineering;Convergence,learning (artificial intelligence);program testing;software engineering,software engineering techniques;deep learning systems;DL systems;testing metrics;DL testing approaches;novel testing framework;model robustness improvement;DL model robustness;adversarial robustness;robustness oriented testing;neuron coverage metrics;RobOT;quantitative measurment,23
502,Deep Neural Networks: Quality Assurance,Scalable Quantitative Verification for Deep Neural Networks,T. Baluta; Z. L. Chua; K. S. Meel; P. Saxena,National University of Singapore; Independent Researcher; National University of Singapore; National University of Singapore,2021,"Despite the functional success of deep Neural Networks, their trustworthiness remains a crucial open challenge. To address this challenge, both testing and verification techniques have been proposed. But these existing techniques provide either scalability to large networks or formal guarantees, not both. In this paper, we propose a scalable quantitative verification framework for deep Neural Networks, i.e., a test-driven approach that comes with formal guarantees that a desired probabilistic property is satisfied. Our technique performs enough tests until soundness of a formal probabilistic property can be proven. It can be used to certify properties of both deterministic and randomized DNNs. We implement our approach in a tool called PROVERO and apply it in the context of certifying adversarial robustness of DNNs. In this context, we first show a new attack-agnostic measure of robustness which offers an alternative to purely attack-based methodology of evaluating robustness being reported today. Second, PROVERO provides certificates of robustness for large DNNs, where existing state-of-the-art verification tools fail to produce conclusive results. Our work paves the way forward for verifying properties of distributions captured by real-world deep Neural Networks, with provable guarantees, even where testers only have black-box access to the neural network.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402111,deep Neural Networks;quantitative verification;probabilistic;robustness,Neural networks;Software algorithms;Tools;Probabilistic logic;Robustness;Testing;Software engineering,,,3
503,Deep Neural Networks: Supporting SE Tasks 1,Traceability Transformed: Generating More Accurate Links with Pre-Trained BERT Models,J. Lin; Y. Liu; Q. Zeng; M. Jiang; J. Cleland-Huang,"Computer Science And Engineering, University Of Notre Dame, Notre Dame, IN, USA; Computer Science And Engineering, University Of Notre Dame, Notre Dame, IN, USA; Computer Science And Engineering, University Of Notre Dame, Notre Dame, IN, USA; Computer Science And Engineering, University Of Notre Dame, Notre Dame, IN, USA; Computer Science And Engineering, University Of Notre Dame, Notre Dame, IN, USA",2021,"Software traceability establishes and leverages associations between diverse development artifacts. Researchers have proposed the use of deep learning trace models to link natural language artifacts, such as requirements and issue descriptions, to source code; however, their effectiveness has been restricted by availability of labeled data and efficiency at runtime. In this study, we propose a novel framework called Trace BERT (T-BERT) to generate trace links between source code and natural language artifacts. To address data sparsity, we leverage a three-step training strategy to enable trace models to transfer knowledge from a closely related Software Engineering challenge, which has a rich dataset, to produce trace links with much higher accuracy than has previously been achieved. We then apply the T-BERT framework to recover links between issues and commits in Open Source Projects. We comparatively evaluated accuracy and efficiency of three BERT architectures. Results show that a Single-BERT architecture generated the most accurate links, while a Siamese-BERT architecture produced comparable results with significantly less execution time. Furthermore, by learning and transferring knowledge, all three models in the framework outperform classical IR trace models. On the three evaluated real-word OSS projects, the best T-BERT stably outperformed the VSM model with average improvements of 60.31% measured using Mean Average Precision (MAP). RNN severely underperformed on these projects due to insufficient training data, while T-BERT overcame this problem by using pretrained language models and transfer learning.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402118,software traceability;deep learning;langauge model,Bit error rate;Natural languages;Transfer learning;Training data;Computer architecture;Data models;Software,deep learning (artificial intelligence);natural language processing;public domain software;recurrent neural nets;software maintenance;source code (software),three-step training strategy;closely related Software Engineering challenge;trace links;T-BERT framework;open source projects;Single-BERT architecture;Siamese-BERT architecture;learning transferring knowledge;VSM model;pretrained language models;transfer learning;traceability transformed;pre-trained BERT models;software traceability;deep learning trace models;natural language artifacts;source code;data sparsity;IR trace models;trace BERT;RNN,37
504,Deep Neural Networks: Supporting SE Tasks 2,Studying the Usage of Text-To-Text Transfer Transformer to Support Code-Related Tasks,A. Mastropaolo; S. Scalabrino; N. Cooper; D. Nader Palacio; D. Poshyvanyk; R. Oliveto; G. Bavota,"SEART @ Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland; University of Molise, Italy; SEMERU @ Computer Science Department, William and Mary, USA; Computer Science Department, William and Mary, Williamsburg, VA, USA; SEMERU @ Computer Science Department, William and Mary, USA; SEMERU @ Computer Science Department, William and Mary, USA; SEART @ Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland",2021,"Deep learning (DL) techniques are gaining more and more attention in the software engineering community. They have been used to support several code-related tasks, such as automatic bug fixing and code comments generation. Recent studies in the Natural Language Processing (NLP) field have shown that the Text-To-Text Transfer Transformer (T5) architecture can achieve state-of-the-art performance for a variety of NLP tasks. The basic idea behind T5 is to first pre-train a model on a large and generic dataset using a self-supervised task (e.g., filling masked words in sentences). Once the model is pre-trained, it is fine-tuned on smaller and specialized datasets, each one related to a specific task (e.g., language translation, sentence classification). In this paper, we empirically investigate how the T5 model performs when pre-trained and fine-tuned to support code-related tasks. We pre-train a T5 model on a dataset composed of natural language English text and source code. Then, we fine-tune such a model by reusing datasets used in four previous works that used DL techniques to: (i) fix bugs, (ii) inject code mutants, (iii) generate assert statements, and (iv) generate code comments. We compared the performance of this single model with the results reported in the four original papers proposing DL-based solutions for those four tasks. We show that our T5 model, exploiting additional data for the self-supervised pre-training phase, can achieve performance improvements over the four baselines.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401982,Empirical software engineering;Deep Learning,Deep learning;Computer bugs;Natural language processing;Software;Filling;Task analysis;Software engineering,learning (artificial intelligence);natural language processing;pattern classification;program debugging;public domain software;software engineering;software maintenance,code-related tasks;source code;inject code mutants;code comments;self-supervised pre-training phase;Natural Language Processing field;Text-To-Text Transfer Transformer architecture;NLP tasks;self-supervised task;T5 model performs,42
505,Deep Neural Networks: Validation 1,Operation is the Hardest Teacher: Estimating DNN Accuracy Looking for Mispredictions,A. Guerriero; R. Pietrantuono; S. Russo,"DIETI, UniversitÃ degli Studi di Napoli Federico II Via Claudio 21, Napoli, Italy; DIETI, UniversitÃ degli Studi di Napoli Federico II Via Claudio 21, Napoli, Italy; DIETI, UniversitÃ degli Studi di Napoli Federico II Via Claudio 21, Napoli, Italy",2021,"Deep Neural Networks (DNN) are typically tested for accuracy relying on a set of unlabelled real world data (operational dataset), from which a subset is selected, manually labelled and used as test suite. This subset is required to be small (due to manual labelling cost) yet to faithfully represent the operational context, with the resulting test suite containing roughly the same proportion of examples causing misprediction (i.e., failing test cases) as the operational dataset. However, while testing to estimate accuracy, it is desirable to also learn as much as possible from the failing tests in the operational dataset, since they inform about possible bugs of the DNN. A smart sampling strategy may allow to intentionally include in the test suite many examples causing misprediction, thus providing this way more valuable inputs for DNN improvement while preserving the ability to get trustworthy unbiased estimates. This paper presents a test selection technique (DeepEST) that actively looks for failing test cases in the operational dataset of a DNN, with the goal of assessing the DNN expected accuracy by a small and ""informative"" test suite (namely with a high number of mispredictions) for subsequent DNN improvement. Experiments with five subjects, combining four DNN models and three datasets, are described. The results show that DeepEST provides DNN accuracy estimates with precision close to (and often better than) those of existing sampling-based DNN testing techniques, while detecting from 5 to 30 times more mispredictions, with the same test suite size.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402144,Software testing;Artificial Neural Networks,Adaptation models;Neural networks;Computer bugs;Software;Labeling;Testing;Software engineering,learning (artificial intelligence);neural nets;program testing;sampling methods,operational dataset;informative test suite;DNN models;deep neural networks;trustworthy unbiased estimates;test selection technique;sampling-based DNN testing techniques,5
506,Deep Neural Networks: Validation 1,AUTOTRAINER: An Automatic DNN Training Problem Detection and Repair System,X. Zhang; J. Zhai; S. Ma; C. Shen,"School of Cyber Science and Engineering, Xiâ€™an Jiaotong University, Xiâ€™an, China; Rutgers University, United States; Rutgers University, United States; School of Cyber Science and Engineering, Xiâ€™an Jiaotong University, Xiâ€™an, China",2021,"With machine learning models especially Deep Neural Network (DNN) models becoming an integral part of the new intelligent software, new tools to support their engineering process are in high demand. Existing DNN debugging tools are either post-training which wastes a lot of time training a buggy model and requires expertises, or limited on collecting training logs without analyzing the problem not even fixing them. In this paper, we propose AUTOTRAINER, a DNN training monitoring and automatic repairing tool which supports detecting and auto repairing five commonly seen training problems. During training, it periodically checks the training status and detects potential problems. Once a problem is found, AUTOTRAINER tries to fix it by using built-in state-of-the-art solutions. It supports various model structures and input data types, such as Convolutional Neural Networks (CNNs) for image and Recurrent Neural Networks (RNNs) for texts. Our evaluation on 6 datasets, 495 models show that AUTOTRAINER can effectively detect all potential problems with 100% detection rate and no false positives. Among all models with problems, it can fix 97.33% of them, increasing the accuracy by 47.08% on average.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402077,software engineering;software tools;deep learning training,Training;Recurrent neural networks;Tools;Maintenance engineering;Software;Monitoring;Software engineering,convolutional neural nets;deep learning (artificial intelligence);program debugging;recurrent neural nets;software engineering,AUTOTRAINER;automatic DNN training problem detection;machine learning models;intelligent software;engineering process;DNN debugging tools;buggy model;training logs;DNN training monitoring;automatic repairing tool;training status;model structures;convolutional neural networks;deep neural network models;recurrent neural networks,10
507,Deep Neural Networks: Validation 1,Self-Checking Deep Neural Networks in Deployment,Y. Xiao; I. Beschastnikh; D. S. Rosenblum; C. Sun; S. Elbaum; Y. Lin; J. S. Dong,"School of Computing, National University of Singapore, Singapore; Department of Computer Science, University of British Columbia, Vancouver, BC, Canada; Department of Computer Science, George Mason University, Fairfax, VA, USA; School of Computing, National University of Singapore, Singapore; Department of Computer Science, University of Virginia, Charlottesville, VA, USA; School of Computing, National University of Singapore, Singapore; School of Computing, National University of Singapore, Singapore",2021,"The widespread adoption of Deep Neural Networks (DNNs) in important domains raises questions about the trustworthiness of DNN outputs. Even a highly accurate DNN will make mistakes some of the time, and in settings like self-driving vehicles these mistakes must be quickly detected and properly dealt with in deployment. Just as our community has developed effective techniques and mechanisms to monitor and check programmed components, we believe it is now necessary to do the same for DNNs. In this paper we present DNN self-checking as a process by which internal DNN layer features are used to check DNN predictions. We detail SelfChecker, a self-checking system that monitors DNN outputs and triggers an alarm if the internal layer features of the model are inconsistent with the final prediction. SelfChecker also provides advice in the form of an alternative prediction. We evaluated SelfChecker on four popular image datasets and three DNN models and found that SelfChecker triggers correct alarms on 60.56% of wrong DNN predictions, and false alarms on 2.04% of correct DNN predictions. This is a substantial improvement over prior work (SelfOracle, Dissector, and ConfidNet). In experiments with self-driving car scenarios, SelfChecker triggers more correct alarms than SelfOracle for two DNN models (DAVE-2 and Chauffeur) with comparable false alarms. Our implementation is available as open source.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402003,deep learning;trustworthiness;deployment,Neural networks;Predictive models;Autonomous automobiles;Monitoring;Software engineering,deep learning (artificial intelligence);mobile robots;object detection;road vehicles,DNN self-checking;internal DNN layer features;DNN outputs;internal layer features;alternative prediction;correct alarms;DNN predictions;comparable false alarms;important domains;self-checking deep neural networks;self-driving vehicles;SelfChecker;image datasets;self-driving car scenarios;components programming,5
508,Deep Neural Networks: Validation 2,Measuring Discrimination to Boost Comparative Testing for Multiple Deep Learning Models,L. Meng; Y. Li; L. Chen; Z. Wang; D. Wu; Y. Zhou; B. Xu,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; Momenta, Suzhou, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",2021,"The boom of DL technology leads to massive DL models built and shared, which facilitates the acquisition and reuse of DL models. For a given task, we encounter multiple DL models available with the same functionality, which are considered as candidates to achieve this task. Testers are expected to compare multiple DL models and select the more suitable ones w.r.t. the whole testing context. Due to the limitation of labeling effort, testers aim to select an efficient subset of samples to make an as precise rank estimation as possible for these models. To tackle this problem, we propose Sample Discrimination based Selection (SDS) to select efficient samples that could discriminate multiple models, i.e., the prediction behaviors (right/wrong) of these samples would be helpful to indicate the trend of model performance. To evaluate SDS, we conduct an extensive empirical study with three widely-used image datasets and 80 real world DL models. The experiment results show that, compared with state-of-the-art baseline methods, SDS is an effective and efficient sample selection method to rank multiple DL models.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402140,Testing;Deep Learning;Comparative Testing;Discrimination,Voting;Predictive models;Labeling;Task analysis;Testing;Context modeling;Software engineering,deep learning (artificial intelligence);program testing;sampling methods;set theory,boost comparative;multiple deep learning models;DL models;testing context;efficient subset;precise rank estimation;SDS;sample selection method;sample discrimination based selection;image datasets,6
509,Deep Neural Networks: Validation 2,Prioritizing Test Inputs for Deep Neural Networks via Mutation Analysis,Z. Wang; H. You; J. Chen; Y. Zhang; X. Dong; W. Zhang,"College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Information and Network Center, Tianjin University, Tianjin, China; Information and Network Center, Tianjin University, Tianjin, China",2021,"Deep Neural Network (DNN) testing is one of the most widely-used ways to guarantee the quality of DNNs. However, labeling test inputs to check the correctness of DNN prediction is very costly, which could largely affect the efficiency of DNN testing, even the whole process of DNN development. To relieve the labeling-cost problem, we propose a novel test input prioritization approach (called PRIMA) for DNNs via intelligent mutation analysis in order to label more bug-revealing test inputs earlier for a limited time, which facilitates to improve the efficiency of DNN testing. PRIMA is based on the key insight: a test input that is able to kill many mutated models and produce different prediction results with many mutated inputs, is more likely to reveal DNN bugs, and thus it should be prioritized higher. After obtaining a number of mutation results from a series of our designed model and input mutation rules for each test input, PRIMA further incorporates learning-to-rank (a kind of supervised machine learning to solve ranking problems) to intelligently combine these mutation results for effective test input prioritization. We conducted an extensive study based on 36 popular subjects by carefully considering their diversity from five dimensions (i.e., different domains of test inputs, different DNN tasks, different network structures, different types of test inputs, and different training scenarios). Our experimental results demonstrate the effectiveness of PRIMA, significantly outperforming the state-of-the-art approaches (with the average improvement of 8.50%~131.01% in terms of prioritization effectiveness). In particular, we have applied PRIMA to the practical autonomous-vehicle testing in a large motor company, and the results on 4 real-world scene-recognition models in autonomous vehicles further confirm the practicability of PRIMA.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402064,Test Prioritization;Deep Neural Network;Mutation;Label;Deep Learning Testing,Training;Neural networks;Computer bugs;Companies;Predictive models;Labeling;Testing,deep learning (artificial intelligence);program debugging;program testing,PRIMA;bug-revealing test inputs;DNN testing;mutated inputs;mutation results;effective test input;DNN tasks;autonomous-vehicle testing;Deep Neural Network testing;labeling test inputs;test input prioritization approach,31
510,Deep Neural Networks: Validation 2,Testing Machine Translation via Referential Transparency,P. He; C. Meister; Z. Su,"Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland; Department of Computer Science, ETH Zurich, Switzerland",2021,"Machine translation software has seen rapid progress in recent years due to the advancement of deep Neural Networks. People routinely use machine translation software in their daily lives for tasks such as ordering food in a foreign restaurant, receiving medical diagnosis and treatment from foreign doctors, and reading international political news online. However, due to the complexity and intractability of the underlying Neural Networks, modern machine translation software is still far from robust and can produce poor or incorrect translations; this can lead to misunderstanding, financial loss, threats to personal safety and health, and political conflicts. To address this problem, we introduce referentially transparent inputs (RTIs), a simple, widely applicable methodology for validating machine translation software. A referentially transparent input is a piece of text that should have similar translations when used in different contexts. Our practical implementation, Purity, detects when this property is broken by a translation. To evaluate RTI, we use Purity to test Google Translate and Bing Microsoft Translator with 200 unlabeled sentences, which detected 123 and 142 erroneous translations with high precision (79.3% and 78.3%). The translation errors are diverse, including examples of under-translation, over-translation, word/phrase mistranslation, incorrect modification, and unclear logic.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402091,Testing;Machine translation;Referential transparency;Metamorphic testing,Neural networks;Software;Internet;Machine translation;Task analysis;Testing;Software engineering,language translation;learning (artificial intelligence);neural nets;politics,deep Neural Networks;underlying Neural Networks;modern machine translation software;poor translations;incorrect translations;referentially transparent input;similar translations;Google Translate;Bing Microsoft Translator;123 translations;142 erroneous translations;translation errors;under-translation;over-translation,13
511,Defect Prediction: Automation 1,Automatic Web Testing Using Curiosity-Driven Reinforcement Learning,Y. Zheng; Y. Liu; X. Xie; Y. Liu; L. Ma; J. Hao; Y. Liu,"Tianjin University, Tianjin, China; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Dept. of Comp. Sci. and Engr, Southern University of Science and Technology, Shenzhen, China; Kyushu University, Fukuoka, Japan; Tianjin University, Tianjin, China; Nanyang Technological University, Singapore",2021,"Web testing has long been recognized as a notoriously difficult task. Even nowadays, web testing still mainly relies on manual efforts in many cases while automated web testing is still far from achieving human-level performance. Key challenges include dynamic content update and deep bugs hiding under complicated user interactions and specific input values, which can only be triggered by certain action sequences in the huge space of all possible sequences. In this paper, we propose WebExplor, an automatic end-to-end web testing framework, to achieve an adaptive exploration of web applications. WebExplor adopts a curiosity-driven reinforcement learning to generate high-quality action sequences (test cases) with temporal logical relations. Besides, WebExplor incrementally builds an automaton during the online testing process, which acts as the high-level guidance to further improve the testing efficiency. We have conducted comprehensive evaluations on six real-world projects, a commercial SaaS web application, and performed an in-the-wild study of the top 50 web applications in the world. The results demonstrate that in most cases WebExplor can achieve significantly higher failure detection rate, code coverage and efficiency than existing state-of-the-art web testing techniques. WebExplor also detected 12 previously unknown failures in the commercial web application, which have been confirmed and fixed by the developers. Furthermore, our in-the-wild study further uncovered 3,466 exceptions and errors.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402046,Web testing;Reinforcement Learning;Curiosity;Exploration;Software Engineering,Location awareness;Software as a service;Reinforcement learning;Manuals;Task analysis;Testing;Software engineering,cloud computing;Internet;learning (artificial intelligence);program testing;temporal logic;Web services,dynamic content update;human-level performance;automated web testing;notoriously difficult task;automatic web;commercial web application;state-of-the-art web testing techniques;cases WebExplor;50 web applications;commercial SaaS web application;testing efficiency;high-level guidance;online testing process;high-quality action sequences;curiosity-driven reinforcement learning;automatic end-to-end web testing framework;possible sequences,17
512,Defect Prediction: Automation 2,Evaluating SZZ Implementations Through a Developer-Informed Oracle,G. Rosa; L. Pascarella; S. Scalabrino; R. Tufano; G. Bavota; M. Lanza; R. Oliveto,"University of Molise, Italy; Software Institute @ USI Universita della Svizzera italiana, Switzerland; University of Molise, Italy; Software Institute @ USI Universita della Svizzera italiana, Switzerland; Software Institute @ USI Universita della Svizzera italiana, Switzerland; Software Institute @ USI Universita della Svizzera italiana, Switzerland; University of Molise, Italy",2021,"The SZZ algorithm for identifying bug-inducing changes has been widely used to evaluate defect prediction techniques and to empirically investigate when, how, and by whom bugs are introduced. Over the years, researchers have proposed several heuristics to improve the SZZ accuracy, providing various implementations of SZZ. However, fairly evaluating those implementations on a reliable oracle is an open problem: SZZ evaluations usually rely on (i) the manual analysis of the SZZ output to classify the identified bug-inducing commits as true or false positives; or (ii) a golden set linking bug-fixing and bug-inducing commits. In both cases, these manual evaluations are performed by researchers with limited knowledge of the studied subject systems. Ideally, there should be a golden set created by the original developers of the studied systems. We propose a methodology to build a ""developer-informed"" oracle for the evaluation of SZZ variants. We use Natural Language Processing (NLP) to identify bug-fixing commits in which developers explicitly reference the commit(s) that introduced a fixed bug. This was followed by a manual filtering step aimed at ensuring the quality and accuracy of the oracle. Once built, we used the oracle to evaluate several variants of the SZZ algorithm in terms of their accuracy. Our evaluation helped us to distill a set of lessons learned to further improve the SZZ algorithm.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402150,SZZ;Defect Prediction;Empirical Study,Software algorithms;Computer bugs;Manuals;Prediction algorithms;Natural language processing;Software;Software engineering,learning (artificial intelligence);natural language processing;program debugging;public domain software;software maintenance,SZZ implementations;developer-informed oracle;SZZ algorithm;bug-inducing changes;defect prediction techniques;bugs;SZZ accuracy;reliable oracle;SZZ evaluations;manual analysis;SZZ output;identified bug-inducing commits;golden set linking bug-fixing;manual evaluations;studied subject systems;original developers;SZZ variants;bug-fixing commits;fixed bug,9
513,Defect Prediction: Data Issues and Bug Classification,Early Life Cycle Software Defect Prediction. Why? How?,S. N.C.; S. Majumder; T. Menzies,"North Carolina State University, USA; Department of Computer Science, North Carolina State University, Raleigh, USA; Department of Computer Science, North Carolina State University, Raleigh, USA",2021,"Many researchers assume that, for software analytics, ""more data is better."" We write to show that, at least for learning defect predictors, this may not be true. To demonstrate this, we analyzed hundreds of popular GitHub projects. These projects ran for 84 months and contained 3,728 commits (median values). Across these projects, most of the defects occur very early in their life cycle. Hence, defect predictors learned from the first 150 commits and four months perform just as well as anything else. This means that, at least for the projects studied here, after the first few months, we need not continually update our defect prediction models. We hope these results inspire other researchers to adopt a ""simplicity-first"" approach to their work. Some domains require a complex and data-hungry analysis. But before assuming complexity, it is prudent to check the raw data looking for ""short cuts"" that can simplify the analysis.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401968,sampling;early;defect prediction;analytics,Transfer learning;Predictive models;Software;Task analysis;Stress;Software engineering;Software development management,learning (artificial intelligence);project management;public domain software;software engineering;software metrics,defect predictors;popular GitHub projects;median values;defect prediction models;complex data-hungry analysis;life cycle software defect prediction;software analytics;data hungry analysis;software metrics,2
514,Defect Prediction: Data Issues and Bug Classification,IoT Bugs and Development Challenges,A. Makhshari; A. Mesbah,"University of British Columbia, Vancouver, BC, Canada; University of British Columbia, Vancouver, BC, Canada",2021,"IoT systems are rapidly adopted in various domains, from embedded systems to smart homes. Despite their growing adoption and popularity, there has been no thorough study to understand IoT development challenges from the practitioners' point of view. We provide the first systematic study of bugs and challenges that IoT developers face in practice, through a large-scale empirical investigation. We collected 5,565 bug reports from 91 representative IoT project repositories and categorized a random sample of 323 based on the observed failures, root causes, and the locations of the faulty components. In addition, we conducted nine interviews with IoT experts to uncover more details about IoT bugs and to gain insight into IoT developers' challenges. Lastly, we surveyed 194 IoT developers to validate our findings and gain further insights. We propose the first bug taxonomy for IoT systems based on our results. We highlight frequent bug categories and their root causes, correlations between them, and common pitfalls and challenges that IoT developers face. We recommend future directions for IoT areas that require research and development attention.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402092,Internet of Things;Software Engineering;Mining Software Repositories;Empirical Study,Correlation;Systematics;Computer bugs;Taxonomy;Tools;Internet of Things;Faces,embedded systems;Internet of Things;program debugging;public domain software,research;development attention;IoT bugs;IoT systems;embedded systems;growing adoption;IoT development challenges;IoT developers face;565 bug reports;91 representative IoT project repositories;IoT experts;194 IoT developers;bug taxonomy;highlight frequent bug categories;common pitfalls;IoT areas,18
515,Defect Prediction: Modeling and Performance,How Developers Optimize Virtual Reality Applications: A Study of Optimization Commits in Open Source Unity Projects,F. Nusrat; F. Hassan; H. Zhong; X. Wang,"Department of Computer Science, University of Texas at San Antonio; Department of Computer and Information Science, University of Michigan-Dearborn; Department of Computer Science and Engineering, Shanghai Jiao Tong University; Department of Computer Science, University of Texas at San Antonio",2021,"Virtual Reality (VR) is an emerging technique that provides immersive experience for users. Due to the high computation cost of rendering real-time animation twice (for both eyes) and the resource limitation of wearable devices, VR applications often face performance bottlenecks and performanceoptimization plays an important role in VR software develop-ment. Performance optimizations of VR applications can be very different from those in traditional software as VR involves more elements such as graphics rendering and real-time animation. In this paper, we present the first empirical study on 183 real-world performance optimizations from 45 VR software projects. In particular, we manually categorized the optimizations in to 11 categories, and applied static analysis to identify how they affect different life-cycle phases of VR applications. Furthermore, we studied the complexity and design / behavior effects of performance optimizations, and how optimizations are different between large organizational software projects and smaller personal software projects. Our major findings include: (1) graphics simplification (24.0%), rendering optimization (16.9%), language / API optimization (15.3%), heap avoidance (14.8%), and valuecaching (12.0%) are the most common categories of performance optimization in VR applications; (2) game logic updates (30.4%) and before-scene initialization (20.0%) are the most common life-cycle phases affected by performance issues; (3) 45.9% of the optimizations have behavior and design effects and 39.3% of the optimizations are systematic changes; (4) the distributionsof optimization classes are very different between organizational VR projects and personal VR projects.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402052,Empirical Study;Virtual Reality;Performance Optimization,Rendering (computer graphics);Animation;Software;Real-time systems;Complexity theory;Optimization;Software engineering,application program interfaces;program diagnostics;rendering (computer graphics);virtual reality,optimization commits;open source unity projects;real-time animation;VR applications;performance bottlenecks;VR software development;performance optimization;real-world performance optimizations;VR software projects;life-cycle phases;organizational software projects;organizational VR projects;personal VR projects;virtual reality applications;optimization classes;personal software projects,9
516,Developers: Behavior,"â€œDo this! Do that!, and Nothing will Happenâ€ Do Specifications Lead to Securely Stored Passwords?",J. Hallett; N. Patnaik; B. Shreeve; A. Rashid,University of Bristol; University of Bristol; University of Bristol; University of Bristol,2021,"Does the act of writing a specification (how the code should behave) for a piece of security sensitive code lead to developers producing more secure code? We asked 138 developers to write a snippet of code to store a password: Half of them were asked to write down a specification of how the code should behave before writing the program, the other half were asked to write the code but without being prompted to write a specification first. We find that explicitly prompting developers to write a specification has a small positive effect on the security of password storage approaches implemented. However, developers often fail to store passwords securely, despite claiming to be confident and knowledgeable in their approaches, and despite considering an appropriate range of threats. We find a need for developer-centered usable mechanisms for telling developers how to store passwords: lists of what they must do are not working.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402024,passwords;security;developer centered security,Writing;Software;Requirements engineering;Task analysis;Secure storage;Standards;Password,formal specification;security of data,security sensitive code;password storage approaches;developer-centered usable mechanisms,5
517,Developers: Behavior,Why Donâ€™t Developers Detect Improper Input Validation? '; DROP TABLE Papers; --,L. Braz; E. Fregnan; G. Ã‡alikli; A. Bacchelli,"University of Zurich; University of Zurich; University of Zurich, Zurich, Switzerland; University of Zurich",2021,"Improper Input Validation (IIV) is a software vulnerability that occurs when a system does not safely handle input data. Even though IIV is easy to detect and fix, it still commonly happens in practice. In this paper, we study to what extent developers can detect IIV and investigate underlying reasons. This knowledge is essential to better understand how to support developers in creating secure software systems. We conduct an online experiment with 146 participants, of which 105 report at least three years of professional software development experience. Our results show that the existence of a visible attack scenario facilitates the detection of IIV vulnerabilities and that a significant portion of developers who did not find the vulnerability initially could identify it when warned about its existence. Yet, a total of 60 participants could not detect the vulnerability even after the warning. Other factors, such as the frequency with which the participants perform code reviews, influence the detection of IIV. Preprint: https://arxiv.org/abs/2102.06251. Data and materials: https://doi.org/10.5281/zenodo.3996696.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402102,software vulnerabilities;code review;empirical software engineering;improper input validation;sql injection,SQL injection;Software systems;Security;Software engineering,computer crime;software engineering,professional software development experience;IIV;improper input validation;DROP TABLE papers;software vulnerability;extent developers;secure software systems;online experiment;attack scenario,10
518,Developers: Experiments,The Mind Is a Powerful Place: How Showing Code Comprehensibility Metrics Influences Code Understanding,M. Wyrich; A. Preikschat; D. Graziotin; S. Wagner,"Institute of Software Engineering, University of Stuttgart, Stuttgart, Germany; Institute of Software Engineering, University of Stuttgart, Stuttgart, Germany; Institute of Software Engineering, University of Stuttgart, Stuttgart, Germany; Institute of Software Engineering, University of Stuttgart, Stuttgart, Germany",2021,"Static code analysis tools and integrated development environments present developers with quality-related software metrics, some of which describe the understandability of source code. Software metrics influence overarching strategic decisions that impact the future of companies and the prioritization of everyday software development tasks. Several software metrics, however, lack in validation: we just choose to trust that they reflect what they are supposed to measure. Some of them were even shown to not measure the quality aspects they intend to measure. Yet, they influence us through biases in our cognitive-driven actions. In particular, they might anchor us in our decisions. Whether the anchoring effect exists with software metrics has not been studied yet. We conducted a randomized and double-blind experiment to investigate the extent to which a displayed metric value for source code comprehensibility anchors developers in their subjective rating of source code comprehensibility, whether performance is affected by the anchoring effect when working on comprehension tasks, and which individual characteristics might play a role in the anchoring effect. We found that the displayed value of a comprehensibility metric has a significant and large anchoring effect on a developer's code comprehensibility rating. The effect does not seem to affect the time or correctness when working on comprehension questions related to the code snippets under study. Since the anchoring effect is one of the most robust cognitive biases, and we have limited understanding of the consequences of the demonstrated manipulation of developers by non-validated metrics, we call for an increased awareness of the responsibility in code quality reporting and for corresponding tools to be based on scientific evidence.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402040,behavioral software engineering;code comprehension;placebo effect;cognitive bias;anchoring effect;metrics,Measurement;Software metrics;Atmospheric measurements;Tools;Particle measurements;Software;Task analysis,anchors;cognition;program diagnostics;software engineering;software maintenance;software metrics;software quality;software tools,code comprehensibility metrics influences code understanding;static code analysis tools;integrated development environments;quality-related software metrics;everyday software development tasks;anchoring effect;metric value;source code comprehensibility anchors developers;comprehension tasks;comprehensibility metric;developer;comprehension questions;code snippets;nonvalidated metrics;code quality,10
519,Developers: Experiments,Program Comprehension and Code Complexity Metrics: An fMRI Study,N. Peitek; S. Apel; C. Parnin; A. Brechmann; J. Siegmund,"Leibniz Institute for Neurobiology, Magdeburg, Germany; Saarland Informatics Campus, Saarland University, SaarbrÃ¼cken, Germany; NC State University, Raleigh, North Carolina, USA; Leibniz Institute for Neurobiology, Magdeburg, Germany; Chemnitz University of Technology, Chemnitz, Germany",2021,"Background: Researchers and practitioners have been using code complexity metrics for decades to predict how developers comprehend a program. While it is plausible and tempting to use code metrics for this purpose, their validity is debated, since they rely on simple code properties and rarely consider particularities of human cognition. Aims: We investigate whether and how code complexity metrics reflect difficulty of program comprehension. Method: We have conducted a functional magnetic resonance imaging (fMRI) study with 19 participants observing program comprehension of short code snippets at varying complexity levels. We dissected four classes of code complexity metrics and their relationship to neuronal, behavioral, and subjective correlates of program comprehension, overall analyzing more than 41 metrics. Results: While our data corroborate that complexity metrics can-to a limited degree-explain programmers' cognition in program comprehension, fMRI allowed us to gain insights into why some code properties are difficult to process. In particular, a code's textual size drives programmers' attention, and vocabulary size burdens programmers' working memory. Conclusion: Our results provide neuro-scientific evidence supporting warnings of prior research questioning the validity of code complexity metrics and pin down factors relevant to program comprehension. Future Work: We outline several follow-up experiments investigating fine-grained effects of code complexity and describe possible refinements to code complexity metrics.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402005,program comprehension;code complexity metrics;cognitive load;functional magnetic resonance imaging,Measurement;Vocabulary;Correlation;Functional magnetic resonance imaging;Cognition;Complexity theory;Time factors,biomedical MRI;brain;cognition;medical image processing;neurophysiology;software metrics,program comprehension;code complexity metrics;code metrics,19
520,Developers: General Issues,Do you Really Code? Designing and Evaluating Screening Questions for Online Surveys with Programmers,A. Danilova; A. Naiakshina; S. Horstmann; M. Smith,"University of Bonn, Bonn, Germany; University of Bonn, Bonn, Germany; University of Bonn, Bonn, Germany; Fraunhofer FKIE, University of Bonn, Bonn, Germany",2021,"Recruiting professional programmers in sufficient numbers for research studies can be challenging because they often cannot spare the time, or due to their geographical distribution and potentially the cost involved. Online platforms such as Clickworker or Qualtrics do provide options to recruit participants with programming skill; however, misunderstandings and fraud can be an issue. This can result in participants without programming skill taking part in studies and surveys. If these participants are not detected, they can cause detrimental noise in the survey data. In this paper, we develop screener questions that are easy and quick to answer for people with programming skill but difficult to answer correctly for those without. In order to evaluate our questionnaire for efficacy and efficiency, we recruited several batches of participants with and without programming skill and tested the questions. In our batch 42% of Clickworkers stating that they have programming skill did not meet our criteria and we would recommend filtering these from studies. We also evaluated the questions in an adversarial setting. We conclude with a set of recommended questions which researchers can use to recruit participants with programming skill from online platforms.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402043,Developer Studies;Programmer Studies;Screening Questions Programmers,Filtering;Instruments;Robustness;Task analysis;Programming profession;Testing;Software engineering,Internet;software engineering,programming skill;online platforms;online surveys;professional programmers;screening questions evaluation;geographical distribution;Clickworker;Qualtrics,10
521,Developers: General Issues,How Gamification Affects Software Developers: Cautionary Evidence from a Natural Experiment on GitHub,L. Moldon; M. Strohmaier; J. Wachs,"RWTH Aachen University, Aachen, Germany; RWTH Aachen University & GESIS-Leibniz Institute for the Social Sciences, Cologne, Germany; Vienna Uni. of Econ. and Business & Complexity Science Hub Vienna, Vienna, Austria",2021,"We examine how the behavior of software developers changes in response to removing gamification elements from GitHub, an online platform for collaborative programming and software development. We find that the unannounced removal of daily activity streak counters from the user interface (from user profile pages) was followed by significant changes in behavior. Long-running streaks of activity were abandoned and became less common. Weekend activity decreased and days in which developers made a single contribution became less common. Synchronization of streaking behavior in the platform's social network also decreased, suggesting that gamification is a powerful channel for social influence. Focusing on a set of software developers that were publicly pursuing a goal to make contributions for 100 days in a row, we find that some of these developers abandon this quest following the removal of the public streak counter. Our findings provide evidence for the significant impact of gamification on the behavior of developers on large collaborative programming and software development platforms. They urge caution: gamification can steer the behavior of software developers in unexpected and unwanted directions.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402097,gamification;behavior;software engineering;natural experiment;GitHub,Collaboration;Programming;User interfaces;Software;Synchronization;Software development management;Software engineering,computer aided instruction;computer games;human computer interaction;social networking (online);software engineering;user interfaces,GitHub;software developers;gamification elements;online platform;collaborative programming;daily activity streak counters;user profile pages;public streak counter;software development platforms;social network,9
522,Developers: Naming Methods and Variables,IdBench: Evaluating Semantic Representations of Identifier Names in Source Code,Y. Wainakh; M. Rauf; M. Pradel,"Department of Computer Science, TU Darmstadt, Darmstadt, Germany; Department of Computer Science, University of Stuttgart, Stuttgart, Germany; Department of Computer Science, University of Stuttgart, Stuttgart, Germany",2021,"Identifier names convey useful information about the intended semantics of code. Name-based program analyses use this information, e.g., to detect bugs, to predict types, and to improve the readability of code. At the core of name-based analyses are semantic representations of identifiers, e.g., in the form of learned embeddings. The high-level goal of such a representation is to encode whether two identifiers, e.g., len and size, are semantically similar. Unfortunately, it is currently unclear to what extent semantic representations match the semantic relatedness and similarity perceived by developers. This paper presents IdBench, the first benchmark for evaluating semantic representations against a ground truth created from thousands of ratings by 500 software developers. We use IdBench to study state-of-the-art embedding techniques proposed for natural language, an embedding technique specifically designed for source code, and lexical string distance functions. Our results show that the effectiveness of semantic representations varies significantly and that the best available embeddings successfully represent semantic relatedness. On the downside, no existing technique provides a satisfactory representation of semantic similarities, among other reasons because identifiers with opposing meanings are incorrectly considered to be similar, which may lead to fatal mistakes, e.g., in a refactoring tool. Studying the strengths and weaknesses of the different techniques shows that they complement each other. As a first step toward exploiting this complementarity, we present an ensemble model that combines existing techniques and that clearly outperforms the best available semantic representation.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401986,source code;Neural Networks;embeddings;identifiers;benchmark,Semantics;Natural languages;Benchmark testing;Tools;Software;Software engineering;Lenses,natural language processing;program diagnostics;software maintenance;source code (software);text analysis,IdBench;semantic representations;identifier names;source code;name-based program analyses;embedding technique;semantic similarities;natural language;lexical string distance functions,10
523,Developers: Naming Methods and Variables,A Context-Based Automated Approach for Method Name Consistency Checking and Suggestion,Y. Li; S. Wang; T. Nguyen,"Department of Informatics, New Jersey Institute of Technology, New Jersey, USA; Department of Informatics, New Jersey Institute of Technology, New Jersey, USA; University of Texas at Dallas, Dallas, TX, USA",2021,"Misleading method names in software projects can confuse developers, which may lead to software defects and affect code understandability. In this paper, we present DeepName, a context-based, deep learning approach to detect method name inconsistencies and suggest a proper name for a method. The key departure point is the philosophy of ""Show Me Your Friends, I'll Tell You Who You Are"". Unlike the state-of-the-art approaches, in addition to the method's body, we also consider the interactions of the current method under study with the other ones including the caller and callee methods, and the sibling methods in the same enclosing class. The sequences of sub-tokens in the program entities' names in the contexts are extracted and used as the input for an RNN-based encoder-decoder to produce the representations for the current method. We modify that RNN model to integrate the copy mechanism and our newly developed component, called the non-copy mechanism, to emphasize on the possibility of a certain sub-token not to be copied to follow the current sub-token in the currently generated method name. We conducted several experiments to evaluate DeepName on large datasets with +14M methods. For consistency checking, DeepName improves the state-of-the-art approach by 2.1%, 19.6%, and 11.9% relatively in recall, precision, and F-score, respectively. For name suggestion, DeepName improves relatively over the state-of-the-art approaches in precision (1.8%â€“30.5%), recall (8.8%â€“46.1%), and F-score (5.2%â€“38.2%). To assess DeepName's usefulness, we detected inconsistent methods and suggested new method names in active projects. Among 50 pull requests, 12 were merged into the main branch. In total, in 30/50 cases, the team members agree that our suggested method names are more meaningful than the current names.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402103,Naturalness of Software;Deep Learning;Entity Name Suggestion;Inconsistent Method Name Checking,Deep learning;Software;Software engineering,learning (artificial intelligence);recurrent neural nets;software maintenance,noncopy mechanism;DeepName;software projects;software defects;deep learning approach;program entities;RNN-based encoder-decoder;context-based automated approach;name consistency checking,13
524,Developers: Naming Methods and Variables,On the Naming of Methods: A Survey of Professional Developers,R. Alsuhaibani; C. Newman; M. Decker; M. Collard; J. Maletic,"Kent State University, Kent, OH, USA; Rochester Institute of Technology, Rochester, NY, USA; Bowling Green State University; University of Akron, Akron, OH, USA; Kent State University, Kent, OH, USA",2021,"This paper describes the results of a large (+1100 responses) survey of professional software developers concerning standards for naming source code methods. The various standards for source code method names are derived from and supported in the software engineering literature. The goal of the survey is to determine if there is a general consensus among developers that the standards are accepted and used in practice. Additionally, the paper examines factors such as years of experience and programming language knowledge in the context of survey responses. The survey results show that participants very much agree about the importance of various standards and how they apply to names and that years of experience and the programming language has almost no effect on their responses. The results imply that the given standards are both valid and to a large degree complete. The work provides a foundation for automated method name assessment during development and code reviews.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401967,method names;coding standards;styling;naming conventions,Computer languages;Tools;Software;Natural language processing;Standards;Software engineering;Software development management,software engineering;source coding,professional software developers;source code methods;software engineering literature;programming language knowledge;automated method name assessment,7
525,Developers: Observational Studies,"Relating Reading, Visualization, and Coding for New Programmers: A Neuroimaging Study",M. Endres; Z. Karas; X. Hu; I. Kovelman; W. Weimer,"Computer Science and Engineering, University of Michigan; Department of Psychology, University of Michigan; Department of Psychology, University of Michigan; Computer Science and Engineering, University of Michigan; Computer Science and Engineering, University of Michigan",2021,"Understanding how novices reason about coding at a neurological level has implications for training the next generation of software engineers. In recent years, medical imaging has been increasingly employed to investigate patterns of neural activity associated with coding activity. However, such studies have focused on advanced undergraduates and professionals. In a human study of 31 participants, we use functional near-infrared spectroscopy to measure the neural activity associated with introductory programming. In a controlled, contrast-based experiment, we relate brain activity when coding to that of reading natural language or mentally rotating objects (a spatial visualization task). Our primary result is that all three tasks-coding, prose reading, and mental rotation-are mentally distinct for novices. However, while those tasks are neurally distinct, we find more significant differences between prose and coding than between mental rotation and coding. Intriguingly, we generally find more activation in areas of the brain associated with spatial ability and task difficulty for novice coding compared to that reported in studies with more expert developers. Finally, in an exploratory analysis, we also find a neural activation pattern predictive of programming performance 11 weeks later. While preliminary, these findings both expand on previous results (e.g., relating expertise to a similarity between coding and prose reading) and also provide a new understanding of the cognitive processes underlying novice programming.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402035,novice software engineers;program comprehension;spatial ability;reading ability;fNIRS;cognition,Training;Cognitive processes;Neural activity;Encoding;Software;Task analysis;Programming profession,brain;cognitive systems;medical image processing;neurophysiology;software engineering,functional near-infrared spectroscopy;novice programming;programming performance;neural activation pattern predictive;mental rotation;prose reading;spatial visualization task;mentally rotating objects;reading natural language;brain activity;controlled contrast-based experiment;coding activity;medical imaging;software engineers;neuroimaging study,3
526,Developers: Onboarding,A Case Study of Onboarding in Software Teams: Tasks and Strategies,A. Ju; H. Sajnani; S. Kelly; K. Herzig,"University of California, Berkeley, Berkeley, CA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA; Microsoft, Redmond, WA, USA",2021,"Developers frequently move into new teams or environments across software companies. Their onboarding experience is correlated with productivity, job satisfaction, and other short-term and long-term outcomes. The majority of the onboarding process comprises engineering tasks such as fixing bugs or implementing small features. Nevertheless, we do not have a systematic view of how tasks influence onboarding. In this paper, we present a case study of Microsoft, where we interviewed 32 developers moving into a new team and 15 engineering managers onboarding a new developer into their team - to understand and characterize developers' onboarding experience and expectations in relation to the tasks performed by them while onboarding. We present how tasks interact with new developers through three representative themes: learning, confidence building, and socialization. We also discuss three onboarding strategies as inferred from the interviews that managers commonly use unknowingly, and discuss their pros and cons and offer situational recommendations. Furthermore, we triangulate our interview findings with a developer survey (N = 189) and a manager survey (N = 37) and find that survey results suggest that our findings are representative and our recommendations are actionable. Practitioners could use our findings to improve their onboarding processes, while researchers could find new research directions from this study to advance the understanding of developer onboarding. Our research instruments and anonymous data are available at https://zenodo.org/record/4455937#.YCOQCs 0lFd.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401978,onboarding;software development teams;teams;learning;confidence;socialization,Productivity;Systematics;Instruments;Software;Task analysis;Interviews;Software engineering,Internet;program debugging;software development management,tasks interact;interview findings;developer survey;manager survey;software teams;software companies;job satisfaction;long-term outcomes;engineering tasks;systematic view;developers onboarding experience;engineering managers onboarding,6
527,Developers: Well-Being and Productivity,"""How Was Your Weekend?"" Software Development Teams Working From Home During COVID-19",C. Miller; P. Rodeghero; M. -A. Storey; D. Ford; T. Zimmermann,"New College of Florida, FL, USA; Clemson University, SC, USA; University of Victoria, BC, Canada; Microsoft Research, WA, USA; Microsoft Research, WA, USA",2021,"The mass shift to working at home during the COVID-19 pandemic radically changed the way many software development teams collaborate and communicate. To investigate how team culture and team productivity may also have been affected, we conducted two surveys at a large software company. The first, an exploratory survey during the early months of the pandemic with 2,265 developer responses, revealed that many developers faced challenges reaching milestones and that their team productivity had changed. We also found through qualitative analysis that important team culture factors such as communication and social connection had been affected. For example, the simple phrase ""How was your weekend?"" had become a subtle way to show peer support. In our second survey, we conducted a quantitative analysis of the team cultural factors that emerged from our first survey to understand the prevalence of the reported changes. From 608 developer responses, we found that 74% of these respondents missed social interactions with colleagues and 51% reported a decrease in their communication ease with colleagues. We used data from the second survey to build a regression model to identify important team culture factors for modeling team productivity. We found that the ability to brainstorm with colleagues, difficulty communicating with colleagues, and satisfaction with interactions from social activities are important factors that are associated with how developers report their software development team's productivity. Our findings inform how managers and leaders in large software companies can support sustained team productivity during times of crisis and beyond.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401956,,Productivity;COVID-19;Pandemics;Companies;Brain modeling;Software;Data models,epidemics;regression analysis;software development management;team working,team cultural factors;colleagues;modeling team productivity;software companies;COVID-19 pandemic;software development teams working from home;communication ease;social connection;regression model,40
528,Fault Localization 1,FLACK: Counterexample-Guided Fault Localization for Alloy Models,G. Zheng; T. Nguyen; S. GutiÃ©rrez Brida; G. Regis; M. F. Frias; N. Aguirre; H. Bagheri,"University of Nebraska-Lincoln; University of Nebraska, Lincoln; University of Rio Cuarto and CONICET; University of Rio Cuarto and CONICET; Software Engineering Instituto TecnolÃ³gico de Buenos Aires; University of Rio Cuarto and CONICET; University of Nebraska-Lincoln",2021,"Fault localization is a practical research topic that helps developers identify code locations that might cause bugs in a program. Most existing fault localization techniques are designed for imperative programs (e.g., C and Java) and rely on analyzing correct and incorrect executions of the program to identify suspicious statements. In this work, we introduce a fault localization approach for models written in a declarative language, where the models are not ""executed,"" but rather converted into a logical formula and solved using backend constraint solvers. We present FLACK, a tool that takes as input an Alloy model consisting of some violated assertion and returns a ranked list of suspicious expressions contributing to the assertion violation. The key idea is to analyze the differences between counterexamples, i.e., instances of the model that do not satisfy the assertion, and instances that do satisfy the assertion to find suspicious expressions in the input model. The experimental results show that FLACK is efficient (can handle complex, real-world Alloy models with thousand lines of code within 5 seconds), accurate (can consistently rank buggy expressions in the top 1.9% of the suspicious list), and useful (can often narrow down the error to the exact location within the suspicious expressions).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402006,Debugging;Fault Localization;Formal Method;Alloy,Location awareness;Fault diagnosis;Analytical models;Java;Metals;Tools;Software engineering,Java;program debugging;program diagnostics;program testing;software fault tolerance,counterexample-guided fault localization;Alloy model;code locations;fault localization techniques;imperative programs;correct executions;incorrect executions;suspicious statements;backend constraint solvers;FLACK;violated assertion;suspicious expressions;assertion violation;input model;suspicious list,4
529,Fault Localization 1,Improving Fault Localization by Integrating Value and Predicate Based Causal Inference Techniques,Y. KÃ¼Ã§Ã¼k; T. A. D. Henderson; A. Podgurski,"Case Western Reserve University; Google Inc, Mountain View, CA, USA; Department of Computer and Data Sciences, Case Western Reserve University, Cleveland, OH, USA",2021,"Statistical fault localization (SFL) techniques use execution profiles and success/failure information from software executions, in conjunction with statistical inference, to automatically score program elements based on how likely they are to be faulty. SFL techniques typically employ one type of profile data: either coverage data, predicate outcomes, or variable values. Most SFL techniques actually measure correlation, not causation, between profile values and success/failure, and so they are subject to confounding bias that distorts the scores they produce. This paper presents a new SFL technique, named UniVal, that uses causal inference techniques and machine learning to integrate information about both predicate outcomes and variable values to more accurately estimate the true failure-causing effect of program statements. UniVal was empirically compared to several coverage-based, predicate-based, and value-based SFL techniques on 800 program versions with real faults.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402143,Software Engineering;Software Fault Localization;Causal Inference;Fault Localization;Statistical Fault Localization,Location awareness;Correlation;Machine learning;Software;Numerical models;Distortion measurement;Software engineering,fault diagnosis;inference mechanisms;learning (artificial intelligence);program debugging;program testing;software fault tolerance;statistical analysis,statistical fault localization techniques;integrating value;value-based SFL techniques;coverage-based;causal inference techniques;profile values;variable values;predicate outcomes;coverage data;profile data;SFL technique;statistical inference;software executions;execution profiles,9
530,Fault Localization 2,Fault Localization with Code Coverage Representation Learning,Y. Li; S. Wang; T. Nguyen,"Department of Informatics, New Jersey Institute of Technology, New Jersey, USA; Department of Informatics, New Jersey Institute of Technology, New Jersey, USA; University of Texas at Dallas, Dallas, TX, USA",2021,"In this paper, we propose DeepRL4FL, a deep learning fault localization (FL) approach that locates the buggy code at the statement and method levels by treating FL as an image pattern recognition problem. DeepRL4FL does so via novel code coverage representation learning (RL) and data dependencies RL for program statements. Those two types of RL on the dynamic information in a code coverage matrix are also combined with the code representation learning on the static information of the usual suspicious source code. This combination is inspired by crime scene investigation in which investigators analyze the crime scene (failed test cases and statements) and related persons (statements with dependencies), and at the same time, examine the usual suspects who have committed a similar crime in the past (similar buggy code in the training data). For the code coverage information, DeepRL4FL first orders the test cases and marks error-exhibiting code statements, expecting that a model can recognize the patterns discriminating between faulty and non-faulty statements/methods. For dependencies among statements, the suspiciousness of a statement is seen taking into account the data dependencies to other statements in execution and data flows, in addition to the statement by itself. Finally, the vector representations for code coverage matrix, data dependencies among statements, and source code are combined and used as the input of a classifier built from a Convolution Neural Network to detect buggy statements/methods. Our empirical evaluation shows that DeepRL4FL improves the top-1 results over the state-of-the-art statement-level FL baselines from 173.1% to 491.7%. It also improves the top-1 results over the existing method-level FL baselines from 15.0% to 206.3%.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402079,Fault Localization;Code Coverage;Representation Learning;Machine Learning;Deep Learning,Location awareness;Deep learning;Training;Image recognition;Neural networks;Pattern recognition;Software engineering,image recognition;learning (artificial intelligence);neural nets;pattern classification;program debugging;program diagnostics;program testing;software fault tolerance,image pattern recognition;existing method-level FL baselines;state-of-the-art statement-level FL baselines;marks error-exhibiting code statements;code coverage information;buggy code;crime scene investigation;source code;code coverage matrix;program statements;data dependencies;code coverage representation learning;deep learning fault localization approach;DeepRL4FL,29
531,Fault Localization 3,An Empirical Study on Deployment Faults of Deep Learning Based Mobile Applications,Z. Chen; H. Yao; Y. Lou; Y. Cao; Y. Liu; H. Wang; X. Liu,"Ministry of Education, Key Lab of High Confidence Software Technologies (Peking University), Beijing, China; Ministry of Education, Key Lab of High Confidence Software Technologies (Peking University), Beijing, China; Ministry of Education, Key Lab of High Confidence Software Technologies (Peking University), Beijing, China; Ministry of Education, Key Lab of High Confidence Software Technologies (Peking University), Beijing, China; Ministry of Education, Key Lab of High Confidence Software Technologies (Peking University), Beijing, China; Beijing University of Posts and Telecommunications, Beijing, China; Ministry of Education, Key Lab of High Confidence Software Technologies (Peking University), Beijing, China",2021,"Deep learning (DL) is moving its step into a growing number of mobile software applications. These software applications, named as DL based mobile applications (abbreviated as mobile DL apps) integrate DL models trained using large-scale data with DL programs. A DL program encodes the structure of a desirable DL model and the process by which the model is trained using training data. Due to the increasing dependency of current mobile apps on DL, software engineering (SE) for mobile DL apps has become important. However, existing efforts in SE research community mainly focus on the development of DL models and extensively analyze faults in DL programs. In contrast, faults related to the deployment of DL models on mobile devices (named as deployment faults of mobile DL apps) have not been well studied. Since mobile DL apps have been used by billions of end users daily for various purposes including for safety-critical scenarios, characterizing their deployment faults is of enormous importance. To fill in the knowledge gap, this paper presents the first comprehensive study to date on the deployment faults of mobile DL apps. We identify 304 real deployment faults from Stack Overflow and GitHub, two commonly used data sources for studying software faults. Based on the identified faults, we construct a fine-granularity taxonomy consisting of 23 categories regarding to fault symptoms and distill common fix strategies for different fault symptoms. Furthermore, we suggest actionable implications and research avenues that can potentially facilitate the deployment of DL models on mobile devices.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401981,deep learning;mobile applications;deployment faults,Taxonomy;Software;Mobile handsets;Data models;Mobile applications;Software engineering;Software development management,deep learning (artificial intelligence);mobile computing;software fault tolerance,deployment faults;mobile software applications;DL based mobile applications;mobile DL apps;DL program;mobile devices;software faults;deep learning based mobile applications;software engineering;stack overflow;GitHub;fine-granularity taxonomy,19
532,Fault Localization 3,Extracting Concise Bug-Fixing Patches from Human-Written Patches in Version Control Systems,Y. Jiang; H. Liu; N. Niu; L. Zhang; Y. Hu,"School of Computer Science and Technology, Beijing Institute of Technology, China; School of Computer Science and Technology, Beijing Institute of Technology, China; Department of Electrical Engineering and Computer Science, University of Cincinnati, USA; Key Laboratory of High Confidence Software Technologies, Peking University, China; School of Computer Science and Technology, Beijing Institute of Technology, China",2021,"High-quality and large-scale repositories of real bugs and their concise patches collected from real-world applications are critical for research in software engineering community. In such a repository, each real bug is explicitly associated with its fix. Therefore, on one side, the real bugs and their fixes may inspire novel approaches for finding, locating, and repairing software bugs; on the other side, the real bugs and their fixes are indispensable for rigorous and meaningful evaluation of approaches for software testing, fault localization, and program repair. To this end, a number of such repositories, e.g., Defects4J, have been proposed. However, such repositories are rather small because their construction involves expensive human intervention. Although bug-fixing code commits as well as associated test cases could be retrieved from version control systems automatically, existing approaches could not yet automatically extract concise bug-fixing patches from bug-fixing commits because such commits often involve bug-irrelevant changes. In this paper, we propose an automatic approach, called BugBuilder, to extracting complete and concise bug-fixing patches from human-written patches in version control systems. It excludes refactorings by detecting refactorings involved in bug-fixing commits, and reapplying detected refactorings on the faulty version. It enumerates all subsets of the remaining part and validates them on test cases. If none of the subsets has the potential to be a complete bug-fixing patch, the remaining part as a whole is taken as a complete and concise bug-fixing patch. Evaluation results on 809 real bug-fixing commits in Defects4J suggest that BugBuilder successfully generated complete and concise bug-fixing patches for forty percent of the bug-fixing commits, and its precision (99%) was even higher than human experts.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401985,Defect;Bug;Testing;Patch;Repository;Dataset,Software testing;Location awareness;Computer bugs;Maintenance engineering;Control systems;Software;Software engineering,program debugging;program testing;software engineering;software maintenance;software quality,concise bug-fixing patches;human-written patches;version control systems;fix;repairing software bugs;bug-fixing code commits;bug-fixing commits;bug-irrelevant changes;complete bug-fixing patch,8
533,Fuzzing,Input Algebras,R. Gopinath; H. Nemati; A. Zeller,"CISPA Helmholtz Center for Information Security, SaarbrÃ¼cken, Germany; CISPA Helmholtz Center for Information Security, SaarbrÃ¼cken, Germany; CISPA Helmholtz Center for Information Security, SaarbrÃ¼cken, Germany",2021,"Grammar-based test generators are highly efficient in producing syntactically valid test inputs, and give their user precise control over which test inputs should be generated. Adapting a grammar or a test generator towards a particular testing goal can be tedious, though. We introduce the concept of a grammar transformer, specializing a grammar towards inclusion or exclusion of specific patterns: ""The phone number must not start with 011 or +1"". To the best of our knowledge, ours is the first approach to allow for arbitrary Boolean combinations of patterns, giving testers unprecedented flexibility in creating targeted software tests. The resulting specialized grammars can be used with any grammar-based fuzzer for targeted test generation, but also as validators to check whether the given specialization is met or not, opening up additional usage scenarios. In our evaluation on real-world bugs, we show that specialized grammars are accurate both in producing and validating targeted inputs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402010,testing;debuggin;faults;oracles,Computer bugs;Generators;Software;Grammar;Test pattern generators;Testing;Software engineering,Boolean functions;grammars;program testing,input algebras;grammar-based test generators;syntactically valid test inputs;user precise control;particular testing goal;grammar transformer;targeted software tests;resulting specialized grammars;targeted test generation;targeted inputs;arbitrary Boolean combinations,1
534,Fuzzing,Fuzzing Symbolic Expressions,L. Borzacchiello; E. Coppa; C. Demetrescu,"DIAG Department, Sapienza University of Rome, Rome, Italy; DIAG Department, Sapienza University of Rome, Rome, Italy; DIAG Department, Sapienza University of Rome, Rome, Italy",2021,"Recent years have witnessed a wide array of results in software testing, exploring different approaches and methodologies ranging from fuzzers to symbolic engines, with a full spectrum of instances in between such as concolic execution and hybrid fuzzing. A key ingredient of many of these tools is Satisfiability Modulo Theories (SMT) solvers, which are used to reason over symbolic expressions collected during the analysis. In this paper, we investigate whether techniques borrowed from the fuzzing domain can be applied to check whether symbolic formulas are satisfiable in the context of concolic and hybrid fuzzing engines, providing a viable alternative to classic SMT solving techniques. We devise a new approximate solver, FUZZY-SAT, and show that it is both competitive with and complementary to state-of-the-art solvers such as Z3 with respect to handling queries generated by hybrid fuzzers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402056,concolic execution;fuzzing testing;SMT solver,Switches;Fuzzing;Tools;Hybrid power systems;Distance measurement;Engines;Software engineering,computability;program diagnostics;program testing,symbolic expressions;software testing;symbolic engines;concolic execution;hybrid fuzzing;Satisfiability Modulo Theories solvers;fuzzing domain;symbolic formulas;classic SMT solving techniques;approximate solver;state-of-the-art solvers;hybrid fuzzers,6
535,Fuzzing,Growing a Test Corpus with Bonsai Fuzzing,V. Vikram; R. Padhye; K. Sen,"University of California, Berkeley, Berkeley, CA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; University of California, Berkeley, Berkeley, CA, USA",2021,"This paper presents a coverage-guided grammar-based fuzzing technique for automatically synthesizing a corpus of concise test inputs. We walk-through a case study of a compiler designed for education and the corresponding problem of generating meaningful test cases to provide to students. The prior state-of-the-art solution is a combination of fuzzing and test-case reduction techniques such as variants of delta-debugging. Our key insight is that instead of attempting to minimize convoluted fuzzer-generated test inputs, we can instead grow concise test inputs by construction using a form of iterative deepening. We call this approach bonsai fuzzing. Experimental results show that bonsai fuzzing can generate test corpora having inputs that are 16-45% smaller in size on average as compared to a fuzz-then-reduce approach, while achieving approximately the same code coverage and fault-detection capability.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402015,test-case generation;grammar-based testing;fuzz testing;small scope hypothesis;test-case reduction,Education;Fuzzing;Software engineering,fuzzy set theory;iterative methods;natural language processing;program compilers;program debugging;program testing,test corpus;coverage-guided grammar-based fuzzing technique;concise test inputs;test-case reduction techniques;bonsai fuzzing;fuzz-then-reduce approach;convoluted fuzzer-generated test input minimization;delta-debugging;iterative deepening;code coverage;fault-detection capability,1
536,Games,We'll Fix It in Post: What Do Bug Fixes in Video Game Update Notes Tell Us?,A. Truelove; E. Santana de Almeida; I. Ahmed,"University of California, USA; Federal University of Bahia, Brazil; University of California, USA",2021,"Bugs that persist into releases of video games can have negative impacts on both developers and users, but particular aspects of testing in game development can lead to difficulties in effectively catching these missed bugs. It has become common practice for developers to apply updates to games in order to fix missed bugs. These updates are often accompanied by notes that describe the changes to the game included in the update. However, some bugs reappear even after an update attempts to fix them. In this paper, we develop a taxonomy for bug types in games that is based on prior work. We examine 12,122 bug fixes from 723 updates for 30 popular games on the Steam platform. We label the bug fixes included in these updates to identify the frequency of these different bug types, the rate at which bug types recur over multiple updates, and which bug types are treated as more severe. Additionally, we survey game developers regarding their experience with different bug types and what aspects of game development they most strongly associate with bug appearance. We find that Information bugs appear the most frequently in updates, while Crash bugs recur the most frequently and are often treated as more severe than other bug types. Finally, we find that challenges in testing, code quality, and bug reproduction have a close association with bug persistence. These findings should help developers identify which aspects of game development could benefit from greater attention in order to prevent bugs. Researchers can use our results in devising tools and methods to better identify and address certain bug types.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402141,video games;bugs;bug fix,Computer bugs;Taxonomy;Games;Tools;Testing;Software engineering,computer games;program debugging,video game update notes;missed bugs;multiple updates;game developers;bug appearance;information bugs;crash bugs;bug reproduction;bug persistence;game development aspects,1
537,GUI Design,GUIGAN: Learning to Generate GUI Designs Using Generative Adversarial Networks,T. Zhao; C. Chen; Y. Liu; X. Zhu,"Jilin University, Changchun, China; Monash University, Melbourne, Australia; Jilin University, Changchun, China; Jilin University, Changchun, China",2021,"Graphical User Interface (GUI) is ubiquitous in almost all modern desktop software, mobile applications and online websites. A good GUI design is crucial to the success of the software in the market, but designing a good GUI which requires much innovation and creativity is difficult even to well-trained designers. In addition, the requirement of rapid development of GUI design also aggravates designers' working load. So, the availability of various automated generated GUIs can help enhance the design personalization and specialization as they can cater to the taste of different designers. To assist designers, we develop a model tool to automatically generate GUI designs. Different from conventional image generation models based on image pixels, our tool is to reuse GUI components collected from existing mobile app GUIs for composing a new design which is similar to natural-language generation. Our tool is based on SeqGAN by modelling the GUI component style compatibility and GUI structure. The evaluation demonstrates that our model significantly outperforms the best of the baseline methods by 30.77% in Fr'echet Inception distance (FID) and 12.35% in 1-Nearest Neighbor Accuracy (1-NNA). Through a pilot user study, we provide initial evidence of the usefulness of our approach for generating acceptable brand new GUI designs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402029,Graphical User Interface;mobile application;GUI design;deep learning;Generative Adversarial Network,Technological innovation;Generative adversarial networks;Software;Mobile applications;Creativity;Graphical user interfaces;Load modeling,graphical user interfaces;mobile computing;natural language processing;nearest neighbour methods;neural nets,GUI designs;image generation models;mobile app GUIs;natural-language generation;GUI component style compatibility;GUI structure;generative adversarial networks;design personalization;graphical user interface;desktop software;mobile applications;online websites;image pixels;SeqGAN;Fr'echet Inception distance;1-nearest neighbor accuracy,18
538,GUI Design,Donâ€™t Do That! Hunting Down Visual Design Smells in Complex UIs Against Design Guidelines,B. Yang; Z. Xing; X. Xia; C. Chen; D. Ye; S. Li,"Zhejiang University, China; Australian National University, Australia; Monash University, Australia; Monash University, Australia; Tencent AI Lab, China; Zhejiang University, China",2021,"Just like code smells in source code, UI design has visual design smells. We study 93 don't-do-that guidelines in the Material Design, a complex design system created by Google. We find that these don't-guidelines go far beyond UI aesthetics, and involve seven general design dimensions (layout, typography, iconography, navigation, communication, color, and shape) and four component design aspects (anatomy, placement, behavior, and usage). Violating these guidelines results in visual design smells in UIs (or UI design smells). In a study of 60,756 UIs of 9,286 Android apps, we find that 7,497 UIs of 2,587 apps have at least one violation of some Material Design guidelines. This reveals the lack of developer training and tool support to avoid UI design smells. To fill this gap, we design an automated UI design smell detector (UIS-Hunter) that extracts and validates multi-modal UI information (component metadata, typography, iconography, color, and edge) for detecting the violation of diverse don't-guidelines in Material Design. The detection accuracy of UIS-Hunter is high (precision=0.81, recall=0.90) on the 60,756 UIs of 9,286 apps. We build a guideline gallery with real-world UI design smells that UIS-Hunter detects for developers to learn the best Material Design practices. Our user studies show that UIS-Hunter is more effective than manual detection of UI design smells, and the UI design smells that are detected by UIS-Hunter have severely negative impacts on app users.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402139,GUI testing;UI design smell;Violation detection;Material design,Training;Visualization;Design methodology;Color;Tools;Guidelines;Software engineering,graphical user interfaces;software maintenance,visual design smells;complex design system;material design guidelines;automated UI design smell detector;UIS-Hunter;real-world UI design,15
539,Handling Ecosystems of Forked Projects,"Same File, Different Changes: The Potential of Meta-Maintenance on GitHub",H. Hata; R. G. Kula; T. Ishio; C. Treude,Nara Institute of Science and Technology; Nara Institute of Science and Technology; Nara Institute of Science and Technology; University of Adelaide,2021,"Online collaboration platforms such as GitHub have provided software developers with the ability to easily reuse and share code between repositories. With clone-and-own and forking becoming prevalent, maintaining these shared files is important, especially for keeping the most up-to-date version of reused code. Different to related work, we propose the concept of meta-maintenance-i.e., tracking how the same files evolve in different repositories with the aim to provide useful maintenance opportunities to those files. We conduct an exploratory study by analyzing repositories from seven different programming languages to explore the potential of meta-maintenance. Our results indicate that a majority of active repositories on GitHub contains at least one file which is also present in another repository, and that a significant minority of these files are maintained differently in the different repositories which contain them. We manually analyzed a representative sample of shared files and their variants to understand which changes might be useful for meta-maintenance. Our findings support the potential of meta-maintenance and open up avenues for future work to capitalize on this potential.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401959,,Computer languages;Statistical analysis;Tools;Maintenance engineering;Software;Software development management;Software engineering,Internet;programming languages;software maintenance,GitHub;online collaboration platforms;shared files;reused code;useful maintenance opportunities;programming languages;meta-maintenance potential,5
540,Handling Ecosystems of Forked Projects,Can Program Synthesis be Used to Learn Merge Conflict Resolutions? An Empirical Analysis,R. Pan; V. Le; N. Nagappan; S. Gulwani; S. Lahiri; M. Kaufman,"Iowa State University, Ames, IA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA; Microsoft Corporation, Redmond, WA, USA",2021,"Forking structure is widespread in the open-source repositories and that causes a significant number of merge conflicts. In this paper, we study the problem of textual merge conflicts from the perspective of Microsoft Edge, a large, highly collaborative fork off the main Chromium branch with significant merge conflicts. Broadly, this study is divided into two sections. First, we empirically evaluate textual merge conflicts in Microsoft Edge and classify them based on the type of files, location of conflicts in a file, and the size of conflicts. We found that ~28% of the merge conflicts are 1-2 line changes, and many resolutions have frequent patterns. Second, driven by these findings, we explore Program Synthesis (for the first time) to learn patterns and resolve structural merge conflicts. We propose a novel domain-specific language (DSL) that captures many of the repetitive merge conflict resolution patterns and learn resolution strategies as programs in this DSL from example resolutions. We found that the learned strategies can resolve 11.4% of the conflicts (~41% of 1-2 line changes) that arise in the C++ files with 93.2% accuracy.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402036,merge conflict;program synthesis;automated fixing,Collaboration;C++ languages;Chromium;DSL;Open source software;Software engineering;Domain specific languages,C++ language;configuration management;learning (artificial intelligence);pattern classification;public domain software;specification languages;text analysis,program synthesis;conflict resolution patterns;merge conflict resolutions;Microsoft Edge;textual merge conflict problem;domain-specific language;DSL;C++ files,6
541,Identifying Information Leaks,Abacus: Precise Side-Channel Analysis,Q. Bao; Z. Wang; X. Li; J. R. Larus; D. Wu,The Pennsylvania State University; The Pennsylvania State University; The Pennsylvania State University; EPFL; The Pennsylvania State University,2021,"Side-channel attacks allow adversaries to infer sensitive information from non-functional characteristics. Prior side-channel detection work is able to identify numerous potential vulnerabilities. However, in practice, many such vulnerabilities leak a negligible amount of sensitive information, and thus developers are often reluctant to address them. Existing tools do not provide information to evaluate a leak's severity, such as the number of leaked bits. To address this issue, we propose a new program analysis method to precisely quantify the leaked information in a single-trace attack through side-channels. It can identify covert information flows in programs that expose confidential information and can reason about security flaws that would otherwise be difficult, if not impossible, for a developer to find. We model an attacker's observation of each leakage site as a constraint. We use symbolic execution to generate these constraints and then run Monte Carlo sampling to estimate the number of leaked bits for each leakage site. By applying the Central Limit Theorem, we provide an error bound for these estimations. We have implemented the technique in a tool called Abacus, which not only finds very fine-grained side-channel vulnerabilities but also estimates how many bits are leaked. Abacus outperforms existing dynamic side-channel detection tools in performance and accuracy. We evaluate Abacus on OpenSSL, mbedTLS, Libgcrypt, and Monocypher. Our results demonstrate that most reported vulnerabilities are difficult to exploit in practice and should be de-prioritized by developers. We also find several sensitive vulnerabilities that are missed by the existing tools. We confirm those vulnerabilities with manual checks and by contacting the developers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402127,,Monte Carlo methods;Prototypes;Side-channel attacks;Manuals;Tools;Security;Software engineering,data privacy;Monte Carlo methods;program diagnostics,Abacus;precise side-channel analysis;side-channel attacks;sensitive information;nonfunctional characteristics;side-channel detection work;program analysis method;single-trace attack;confidential information;Monte Carlo sampling;fine-grained side-channel vulnerabilities;dynamic side-channel detection tools;sensitive vulnerabilities,2
542,Identifying Information Leaks,Data-Driven Synthesis of Provably Sound Side Channel Analyses,J. Wang; C. Sung; M. Raghothaman; C. Wang,University of Southern California; University of Southern California; University of Southern California; University of Southern California,2021,"We propose a data-driven method for synthesizing static analyses to detect side-channel information leaks in cryptographic software. Compared to the conventional way of manually crafting such static analyzers, which can be tedious, error prone and suboptimal, our learning-based technique is not only automated but also provably sound. Our analyzer consists of a set of type-inference rules learned from the training data, i.e., example code snippets annotated with the ground truth. Internally, we use syntax-guided synthesis (SyGuS) to generate new recursive features and decision tree learning (DTL) to generate analysis rules based on these features. We guarantee soundness by proving each learned analysis rule via a technique called query containment checking. We have implemented our technique in the LLVM compiler and used it to detect power side channels in C programs that implement cryptographic protocols. Our results show that, in addition to being automated and provably sound during synthesis, our analyzer can achieve the same empirical accuracy as two state-of-the-art, manually-crafted analyzers while being 300X and 900X faster, respectively.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402113,,Software algorithms;Training data;Static analysis;Tools;Software;Cryptography;Cryptographic protocols,cryptographic protocols;decision trees;inference mechanisms;learning (artificial intelligence);program compilers;program diagnostics;query processing,data-driven synthesis;provably sound side channel analyses;data-driven method;static analyses;side-channel information leaks;cryptographic software;static analyzers;learning-based technique;type-inference rules;training data;example code snippets;ground truth;syntax-guided synthesis;recursive features;decision tree learning;learned analysis rule;technique called query containment checking;power side channels;cryptographic protocols;manually-crafted analyzers,3
543,Image Processing,IMGDroid: Detecting Image Loading Defects in Android Applications,W. Song; M. Han; J. Huang,"School of Computer Sci. & Eng., Nanjing University of Sci. & Tech., Nanjing, China; School of Computer Sci. & Eng., Nanjing University of Sci. & Tech., Nanjing, China; Parasol Laboratory, Texas A&M University, College Station, TX, USA",2021,"Images are essential for many Android applications or apps. Although images play a critical role in app functionalities and user experience, inefficient or improper image loading and displaying operations may severely impact the app performance and quality. Additionally, since these image loading defects may not be manifested by immediate failures, e.g., app crashes, existing GUI testing approaches cannot detect them effectively. In this paper, we identify five anti-patterns of such image loading defects, including image passing by intent, image decoding without resizing, local image loading without permission, repeated decoding without caching, and image decoding in UI thread. Based on these anti-patterns, we propose a static analysis technique, IMGDroid, to automatically and effectively detect such defects. We have applied IMGDroid to a benchmark of 21 open-source Android apps, and found that it not only successfully detects the 45 previously-known image loading defects but also finds 15 new such defects. Our empirical study on 1,000 commercial Android apps demonstrates that the image loading defects are prevalent.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402123,Android app;image loading;defect analysis,Loading;Static analysis;Benchmark testing;User experience;Decoding;Open source software;Software engineering,Android (operating system);graphical user interfaces;mobile computing;power aware computing;program diagnostics;program testing;smart phones,image decoding;image loading defects;Android applications;inefficient image loading;improper image loading;image passing;local image loading,2
544,Model Checking,Fast Parametric Model Checking through Model Fragmentation,X. Fang; R. Calinescu; S. Gerasimou; F. Alhwikem,"University of York, UK; University of York, UK; University of York, UK; University of York, UK",2021,"Parametric model checking (PMC) computes algebraic formulae that express key non-functional properties of a system (reliability, performance, etc.) as rational functions of the system and environment parameters. In software engineering, PMC formulae can be used during design, e.g., to analyse the sensitivity of different system architectures to parametric variability, or to find optimal system configurations. They can also be used at runtime, e.g., to check if non-functional requirements are still satisfied after environmental changes, or to select new configurations after such changes. However, current PMC techniques do not scale well to systems with complex behaviour and more than a few parameters. Our paper introduces a fast PMC (fPMC) approach that overcomes this limitation, extending the applicability of PMC to a broader class of systems than previously possible. To this end, fPMC partitions the Markov models that PMC operates with into fragments whose reachability properties are analysed independently, and obtains PMC reachability formulae by combining the results of these fragment analyses. To demonstrate the effectiveness of fPMC, we show how our fPMC tool can analyse three systems (taken from the research literature, and belonging to different application domains) with which current PMC techniques and tools struggle.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402032,Parametric model checking;discrete time Markov chains;non-functional properties,Analytical models;Sensitivity;Runtime;Systems architecture;Tools;Markov processes;Parametric statistics,formal verification;Markov processes;reachability analysis,fragment analyses;fPMC tool;fast parametric model checking;algebraic formulae;nonfunctional properties;rational functions;software engineering;parametric variability;optimal system configurations;nonfunctional requirements;fPMC partitions;Markov models;PMC reachability formulae,7
545,Model Checking,Trace-Checking CPS Properties: Bridging the Cyber-Physical Gap,C. Menghi; E. ViganÃ²; D. Bianculli; L. C. Briand,"University of Luxembourg Luxembourg, Luxembourg; University of Luxembourg, Luxembourg, Luxembourg; University of Luxembourg Luxembourg, Luxembourg; University of Luxembourg Luxembourg, Luxembourg University of Ottawa, Ottawa, Canada",2021,"Cyber-physical systems combine software and physical components. Specification-driven trace-checking tools for CPS usually provide users with a specification language to express the requirements of interest, and an automatic procedure to check whether these requirements hold on the execution traces of a CPS. Although there exist several specification languages for CPS, they are often not sufficiently expressive to allow the specification of complex CPS properties related to the software and the physical components and their interactions. In this paper, we propose (i) the Hybrid Logic of Signals (HLS), a logic-based language that allows the specification of complex CPS requirements, and (ii) ThEodorE, an efficient SMT-based trace-checking procedure. This procedure reduces the problem of checking a CPS requirement over an execution trace, to checking the satisfiability of an SMT formula. We evaluated our contributions by using a representative industrial case study in the satellite domain. We assessed the expressiveness of HLS by considering 212 requirements of our case study. HLS could express all the 212 requirements. We also assessed the applicability of ThEodorE by running the trace-checking procedure for 747 trace-requirement combinations. ThEodorE was able to produce a verdict in 74.5% of the cases. Finally, we compared HLS and ThEodorE with other specification languages and trace-checking tools from the literature. Our results show that, from a practical standpoint, our approach offers a better trade-off between expressiveness and performance.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402030,Monitors;Languages;Specification;Validation;Formal methods;Semantics,Satellites;Tools;Cyber-physical systems;Software;Specification languages;Monitoring;Software engineering,cyber-physical systems;formal specification;formal verification;program verification;specification languages,specification language;complex CPS properties;physical components;HLS;logic-based language;complex CPS requirements;ThEodorE;CPS requirement;trade-off between expressiveness;trace-checking CPS properties;cyber-physical gap;cyber-physical systems;trace requirement combinations;SMT-based trace checking procedure;specification driven trace checking tools;hybrid logic of signals,4
546,Modularization and Reusability,CENTRIS: A Precise and Scalable Approach for Identifying Modified Open-Source Software Reuse,S. Woo; S. Park; S. Kim; H. Lee; H. Oh,Korea University; Korea University; Georgia Institute of Technology; Korea University; Korea University,2021,"Open-source software (OSS) is widely reused as it provides convenience and efficiency in software development. Despite evident benefits, unmanaged OSS components can introduce threats, such as vulnerability propagation and license violation. Unfortunately, however, identifying reused OSS components is a challenge as the reused OSS is predominantly modified and nested. In this paper, we propose CENTRIS, a precise and scalable approach for identifying modified OSS reuse. By segmenting an OSS code base and detecting the reuse of a unique part of the OSS only, CENTRIS is capable of precisely identifying modified OSS reuse in the presence of nested OSS components. For scalability, CENTRIS eliminates redundant code comparisons and accelerates the search using hash functions. When we applied CENTRIS on 10,241 widely-employed GitHub projects, comprising 229,326 versions and 80 billion lines of code, we observed that modified OSS reuse is a norm in software development, occurring 20 times more frequently than exact reuse. Nonetheless, CENTRIS identified reused OSS components with 91% precision and 94% recall in less than a minute per application on average, whereas a recent clone detection technique, which does not take into account modified and nested OSS reuse, hardly reached 10% precision and 40% recall.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402128,Open-Source Software;Software Composition Analysis;Software Security,Scalability;Pressing;Security;Software reusability;Open source software;Software engineering;Software development management,public domain software;software maintenance;software reusability,CENTRIS;identifying modified open-source software reuse;software development;unmanaged OSS components;reused OSS components;precise approach;scalable approach;modified OSS reuse;OSS code base;nested OSS components;exact reuse,10
547,Modularization and Reusability,Interpretation-Enabled Software Reuse Detection Based on a Multi-level Birthmark Model,X. Xu; Q. Zheng; Z. Yan; M. Fan; A. Jia; T. Liu,"Key Laboratory of Intelligent Networks and Network Security, Ministry of Education, China; Key Laboratory of Intelligent Networks and Network Security, Ministry of Education, China; State Key Lab on Integrated Services Networks, School of Cyber Engineering, Xidian University, China; Key Laboratory of Intelligent Networks and Network Security, Ministry of Education, China; Key Laboratory of Intelligent Networks and Network Security, Ministry of Education, China; Key Laboratory of Intelligent Networks and Network Security, Ministry of Education, China",2021,"Software reuse, especially partial reuse, poses legal and security threats to software development. Since its source codes are usually unavailable, software reuse is hard to be detected with interpretation. On the other hand, current approaches suffer from poor detection accuracy and efficiency, far from satisfying practical demands. To tackle these problems, in this paper, we propose ISRD, an interpretation-enabled software reuse detection approach based on a multi-level birthmark model that contains function level, basic block level, and instruction level. To overcome obfuscation caused by cross-compilation, we represent function semantics with Minimum Branch Path (MBP) and perform normalization to extract core semantics of instructions. For efficiently detecting reused functions, a process for ""intent search based on anchor recognition"" is designed to speed up reuse detection. It uses strict instruction match and identical library call invocation check to find anchor functions (in short anchors) and then traverses neighbors of the anchors to explore potentially matched function pairs. Extensive experiments based on two real-world binary datasets reveal that ISRD is interpretable, effective, and efficient, which achieves 97.2% precision and 94.8% recall. Moreover, it is resilient to cross-compilation, outperforming state-of-the-art approaches.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402081,Binary Similarity Analysis;Software Reuse Detection;Multi-Level Software Birthmark;Interpretation,Law;Semantics;Libraries;Security;Software reusability;Software engineering;Resilience,feature extraction;program compilers;security of data;software libraries;software reusability;source code (software),multilevel birthmark model;partial reuse;software development;poor detection accuracy;satisfying practical demands;interpretation-enabled software reuse detection approach;function level;basic block level;instruction level;cross-compilation;function semantics;reused functions;strict instruction match;anchor functions;potentially matched function pairs,6
548,Monitoring Cloud-Based Services,Fast Outage Analysis of Large-Scale Production Clouds with Service Correlation Mining,Y. Wang; G. Li; Z. Wang; Y. Kang; Y. Zhou; H. Zhang; F. Gao; J. Sun; L. Yang; P. Lee; Z. Xu; P. Zhao; B. Qiao; L. Li; X. Zhang; Q. Lin,"School of Computer Science, Fudan University, China; School of Electronics Engineering and Computer Science, Peking University, Beijing, China; School of Computer Science, Fudan University, China; Microsoft Research, Beijing, China; School of Computer Science, Fudan University, China; School of Electrical Engineering and Computing, The University of Newcastle, Australia; Microsoft Azure, Redmond, USA; Microsoft Azure, Redmond, USA; Microsoft Azure, Redmond, USA; Microsoft Azure, Redmond, USA; Microsoft Azure, Redmond, USA; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China",2021,"Cloud-based services are surging into popularity in recent years. However, outages, i.e., severe incidents that always impact multiple services, can dramatically affect user experience and incur severe economic losses. Locating the root-cause service, i.e., the service that contains the root cause of the outage, is a crucial step to mitigate the impact of the outage. In current industrial practice, this is generally performed in a bootstrap manner and largely depends on human efforts: the service that directly causes the outage is identified first, and the suspected root cause is traced back manually from service to service during diagnosis until the actual root cause is found. Unfortunately, production cloud systems typically contain a large number of interdependent services. Such a manual root cause analysis is often time-consuming and labor-intensive. In this work, we propose COT, the first outage triage approach that considers the global view of service correlations. COT mines the correlations among services from outage diagnosis data. After learning from historical outages, COT can infer the root cause of emerging ones accurately. We implement COT and evaluate it on a real-world dataset containing one year of data collected from Microsoft Azure, one of the representative cloud computing platforms in the world. Our experimental results show that COT can reach a triage accuracy of 82.1%-83.5%, which outperforms the state-of-the-art triage approach by 28.0%-29.7%.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402074,cloud computing;root cause analysis;outage triage;machine learning,Cloud computing;Root cause analysis;Correlation;Systematics;Production;User experience;Software engineering,cloud computing;data mining;learning (artificial intelligence);statistical analysis,fast outage analysis;large-scale production clouds;service correlation mining;cloud-based services;severe incidents;impact multiple services;severe economic losses;root-cause service;current industrial practice;suspected root cause;actual root cause;production cloud systems;interdependent services;manual root cause analysis;outage triage approach;service correlations;COT mines;outage diagnosis data;historical outages;representative cloud,7
549,Mutation Testing: General Issues,MuDelta: Delta-Oriented Mutation Testing at Commit Time,W. Ma; T. Titcheu Chekam; M. Papadakis; M. Harman,"SnT, University of Luxembourg, Luxembourg; SnT, University of Luxembourg, Esch-sur-Alzette, Luxembourg; SnT, University of Luxembourg, Luxembourg; Facebook and University College London, UK",2021,"To effectively test program changes using mutation testing, one needs to use mutants that are relevant to the altered program behaviours. In view of this, we introduce MuDelta, an approach that identifies commit-relevant mutants; mutants that affect and are affected by the changed program behaviours. Our approach uses machine learning applied on a combined scheme of graph and vector-based representations of static code features. Our results, from 50 commits in 21 Coreutils programs, demonstrate a strong prediction ability of our approach; yielding 0.80 (ROC) and 0.50 (PR Curve) AUC values with 0.63 and 0.32 precision and recall values. These predictions are significantly higher than random guesses, 0.20 (PR-Curve) AUC, 0.21 and 0.21 precision and recall, and subsequently lead to strong relevant tests that kill 45%more relevant mutants than randomly sampled mutants (either sampled from those residing on the changed component(s) or from the changed lines). Our results also show that MuDelta selects mutants with 27% higher fault revealing ability in fault introducing commits. Taken together, our results corroborate the conclusion that commit-based mutation testing is suitable and promising for evolving software.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402071,mutation testing;commit-relevant mutants;continuous integration;regression testing;machine learning,Machine learning;Software;Testing;Software engineering,learning (artificial intelligence);program diagnostics;program testing;software fault tolerance;software quality,fault introducing commits;commit-based mutation testing;Coreutils programs;higher fault revealing ability;randomly sampled mutants;strong relevant tests;random guesses;recall values;strong prediction ability;static code features;vector-based representations;commit-relevant mutants;altered program behaviours;commit time;delta-oriented mutation;MuDelta,6
550,Mutation Testing: General Issues,Does Mutation Testing Improve Testing Practices?,G. PetroviÄ‡; M. IvankoviÄ‡; G. Fraser; R. Just,"GrnbH, Google Switzerland, ZÃ¼rich, Switzerland; GrnbH, Google Switzerland, ZÃ¼rich, Switzerland; University of Passau, Passau, Germany; University of Washington, Seattle, WA, USA",2021,"Various proxy metrics for test quality have been defined in order to guide developers when writing tests. Code coverage is particularly well established in practice, even though the question of how coverage relates to test quality is a matter of ongoing debate. Mutation testing offers a promising alternative: Artificial defects can identify holes in a test suite, and thus provide concrete suggestions for additional tests. Despite the obvious advantages of mutation testing, it is not yet well established in practice. Until recently, mutation testing tools and techniques simply did not scale to complex systems. Although they now do scale, a remaining obstacle is lack of evidence that writing tests for mutants actually improves test quality. In this paper we aim to fill this gap: By analyzing a large dataset of almost 15 million mutants, we investigate how these mutants influenced developers over time, and how these mutants relate to real faults. Our analyses suggest that developers using mutation testing write more tests, and actively improve their test suites with high quality tests such that fewer mutants remain. By analyzing a dataset of past fixes of real high-priority faults, our analyses further provide evidence that mutants are indeed coupled with real faults. In other words, had mutation testing been used for the changes introducing the faults, it would have reported a live mutant that could have prevented the bug.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402038,mutation testing;code coverage;fault coupling,Scalability;Computer bugs;Software quality;Writing;Tools;Testing;Software engineering,program testing,test quality;test suite;mutation testing tools;high quality tests;proxy metrics;code coverage,11
551,Obtaining Information from App User Reviews 1,Identifying Key Features from App User Reviews,H. Wu; W. Deng; X. Niu; C. Nie,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",2021,"Due to the rapid growth and strong competition of mobile application (app) market, app developers should not only offer users with attractive new features, but also carefully maintain and improve existing features based on users' feedbacks. User reviews indicate a rich source of information to plan such feature maintenance activities, and it could be of great benefit for developers to evaluate and magnify the contribution of specific features to the overall success of their apps. In this study, we refer to the features that are highly correlated to app ratings as key features, and we present KEFE, a novel approach that leverages app description and user reviews to identify key features of a given app. The application of KEFE especially relies on natural language processing, deep machine learning classifier, and regression analysis technique, which involves three main steps: 1) extracting feature-describing phrases from app description; 2) matching each app feature with its relevant user reviews; and 3) building a regression model to identify features that have significant relationships with app ratings. To train and evaluate KEFE, we collect 200 app descriptions and 1,108,148 user reviews from Chinese Apple App Store. Experimental results demonstrate the effectiveness of KEFE in feature extraction, where an average F-measure of 78.13% is achieved. The key features identified are also likely to provide hints for successful app releases, as for the releases that receive higher app ratings, 70% of features improvements are related to key features.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402119,key features;feature extraction;user reviews;app store analysis,Matched filters;Bit error rate;Feature extraction;Natural language processing;Regression analysis;Mobile applications;Software engineering,feature extraction;learning (artificial intelligence);mobile computing;natural language processing;regression analysis,App user reviews;mobile application market;app developers;users;feature maintenance activities;KEFE;app description;given app;app feature;relevant user reviews;148 user reviews;Chinese Apple App Store;feature extraction;successful app releases;higher app ratings;features improvements,10
552,Obtaining Information from App User Reviews 1,CHAMP: Characterizing Undesired App Behaviors from User Comments Based on Market Policies,Y. Hu; H. Wang; T. Ji; X. Xiao; X. Luo; P. Gao; Y. Guo,"Chongqing University of Posts and Telecommunications, Chongqing, China; Beijing University of Posts and Telecommunications, Beijing, China; Case Western Reserve University, USA; Case Western Reserve University, USA; The Hong Kong Polytechnic University, Hong Kong, China; University of California, Berkeley, USA; Peking University, Beijing, China",2021,"Millions of mobile apps have been available through various app markets. Although most app markets have enforced a number of automated or even manual mechanisms to vet each app before it is released to the market, thousands of low-quality apps still exist in different markets, some of which violate the explicitly specified market policies. In order to identify these violations accurately and timely, we resort to user comments, which can form an immediate feedback for app market maintainers, to identify undesired behaviors that violate market policies, including security-related user concerns. Specifically, we present the first large-scale study to detect and characterize the correlations between user comments and market policies. First, we propose CHAMP, an approach that adopts text mining and natural language processing (NLP) techniques to extract semantic rules through a semi-automated process, and classifies comments into 26 pre-defined types of undesired behaviors that violate market policies. Our evaluation on real-world user comments shows that it achieves both high precision and recall (>0.9) in classifying comments for undesired behaviors. Then, we curate a large-scale comment dataset (over 3 million user comments) from apps in Google Play and 8 popular alternative Android app markets, and apply CHAMP to understand the characteristics of undesired behavior comments in the wild. The results confirm our speculation that user comments can be used to pinpoint suspicious apps that violate policies declared by app markets. The study also reveals that policy violations are widespread in many app markets despite their extensive vetting efforts. CHAMP can be a whistle blower that assigns policy-violation scores and identifies most informative comments for apps.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402070,User comment;app market;undesired behavior,Text mining;Correlation;Semantics;Manuals;Natural language processing;Mobile applications;Software engineering,Android (operating system);data mining;medical computing;mobile computing;natural language processing;security of data;smart phones,3 million user comments;8 popular alternative Android app markets;CHAMP;undesired behavior comments;suspicious apps;policy violations;policy-violation scores;app behaviors;mobile apps;low-quality apps;different markets;explicitly specified market policies;app market maintainers;undesired behaviors;violate market policies;real-world user comments,3
553,Obtaining Information from App User Reviews 1,Prioritize Crowdsourced Test Reports via Deep Screenshot Understanding,S. Yu; C. Fang; Z. Cao; X. Wang; T. Li; Z. Chen,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",2021,"Crowdsourced testing is increasingly dominant in mobile application (app) testing, but it is a great burden for app developers to inspect the incredible number of test reports. Many researches have been proposed to deal with test reports based only on texts or additionally simple image features. However, in mobile app testing, texts contained in test reports are condensed and the information is inadequate. Many screenshots are included as complements that contain much richer information beyond texts. This trend motivates us to prioritize crowdsourced test reports based on a deep screenshot understanding. In this paper, we present a novel crowdsourced test report prioritization approach, namely DeepPrior. We fifirstrst represent the crowdsourced test reports with a novelly introduced feature, namely DeepFeature, that includes all the widgets along with their texts, coordinates, types, and even intents based on the deep analysis of the app screenshots, and the textual descriptions in the crowdsourced test reports. DeepFeature includes the BugFeature, which directly describes the bugs, and the ContextFeature, which depicts the thorough context of the bug. The similarity of the DeepFeature is used to represent the test reports' similarity and prioritize the crowdsourced test reports. We formally define the similarity as DeepSimilarity. We also conduct an empirical experiment to evaluate the effectiveness of the proposed technique with a large dataset group. The results show that DeepPrior is promising, and it outperforms the state-of-the-art approach with less than half the overhead.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401980,Crowdsourced testing;Mobile App Testing;Deep Screenshot Understanding,Computer bugs;Transforms;Market research;Mobile applications;Testing;Software engineering,mobile computing;program debugging;program testing;text analysis,prioritize crowdsourced test reports;deep screenshot understanding;crowdsourced testing;mobile application testing;mobile app testing;crowdsourced test report prioritization approach;app screenshots,7
554,Obtaining Information from App User Reviews 1,It Takes Two to Tango: Combining Visual and Textual Information for Detecting Duplicate Video-Based Bug Reports,N. Cooper; C. Bernal-CÃ¡rdenas; O. Chaparro; K. Moran; D. Poshyvanyk,"College of William & Mary, Williamsburg, VA, USA; College of William and Mary; College of William & Mary, Williamsburg, VA, USA; George Mason University, Fairfax, VA, USA; College of William & Mary, Williamsburg, VA, USA",2021,"When a bug manifests in a user-facing application, it is likely to be exposed through the graphical user interface (GUI). Given the importance of visual information to the process of identifying and understanding such bugs, users are increasingly making use of screenshots and screen-recordings as a means to report issues to developers. However, when such information is reported en masse, such as during crowd-sourced testing, managing these artifacts can be a time-consuming process. As the reporting of screen-recordings in particular becomes more popular, developers are likely to face challenges related to manually identifying videos that depict duplicate bugs. Due to their graphical nature, screen-recordings present challenges for automated analysis that preclude the use of current duplicate bug report detection techniques. To overcome these challenges and aid developers in this task, this paper presents Tango, a duplicate detection technique that operates purely on video-based bug reports by leveraging both visual and textual information. Tango combines tailored computer vision techniques, optical character recognition, and text retrieval. We evaluated multiple configurations of Tango in a comprehensive empirical evaluation on 4,860 duplicate detection tasks that involved a total of 180 screen-recordings from six Android apps. Additionally, we conducted a user study investigating the effort required for developers to manually detect duplicate video-based bug reports and compared this to the effort required to use Tango. The results reveal that Tango's optimal configuration is highly effective at detecting duplicate video-based bug reports, accurately ranking target duplicate videos in the top-2 returned results in 83% of the tasks. Additionally, our user study shows that, on average, Tango can reduce developer effort by over 60%, illustrating its practicality.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401992,Bug Reporting;Screen Recordings;Duplicate Detection,Visualization;Computer bugs;Task analysis;Videos;Graphical user interfaces;Testing;Software engineering,computer vision;graphical user interfaces;information retrieval;mobile computing;optical character recognition;program debugging;text analysis;video signal processing,Tango;visual textual information;duplicate video-based bug reports;bug manifests;user-facing application;graphical user interface;visual information;duplicate bugs;current duplicate bug report detection techniques;duplicate detection technique;4 duplicate detection tasks;860 duplicate detection tasks;180 screen-recordings;target duplicate videos,3
555,Obtaining Information from App User Reviews 2,Automatically Matching Bug Reports With Related App Reviews,M. Haering; C. Stanik; W. Maalej,"University of Hamburg, Hamburg, Germany; University of Hamburg, Hamburg, Germany; University of Hamburg, Hamburg, Germany",2021,"App stores allow users to give valuable feedback on apps, and developers to find this feedback and use it for the software evolution. However, finding user feedback that matches existing bug reports in issue trackers is challenging as users and developers often use a different language. In this work, we introduce DeepMatcher, an automatic approach using state-of-the-art deep learning methods to match problem reports in app reviews to bug reports in issue trackers. We evaluated DeepMatcher with four open-source apps quantitatively and qualitatively. On average, DeepMatcher achieved a hit ratio of 0.71 and a Mean Average Precision of 0.55. For 91 problem reports, DeepMatcher did not find any matching bug report. When manually analyzing these 91 problem reports and the issue trackers of the studied apps, we found that in 47 cases, users actually described a problem before developers discovered and documented it in the issue tracker. We discuss our findings and different use cases for DeepMatcher.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402053,app store analytics;natural language processing;deep learning;mining software repositories;software evolution,Deep learning;Computer bugs;Transforms;Manuals;Open source software;Software engineering,feature extraction;formal specification;information filtering;learning (artificial intelligence);natural language processing;program debugging;public domain software;software maintenance,DeepMatcher;open-source apps;91 problem reports;matching bug report;issue tracker;studied apps;related app reviews;app stores;valuable feedback;user feedback;bug reports;state-of-the-art deep learning methods,18
556,Open Source: Developer's Skills,What Makes a Great Maintainer of Open Source Projects?,E. Dias; P. Meirelles; F. Castor; I. Steinmacher; I. Wiese; G. Pinto,"UFPA, Brazil; UNIFESP, Brazil; UFPE, Brazil; UTFPR, Brazil; UTFPR, Brazil; UFPA, Brazil",2021,"Although Open Source Software (OSS) maintainers devote a significant proportion of their work to coding tasks, great maintainers must excel in many other activities beyond coding. Maintainers should care about fostering a community, helping new members to find their place, while also saying ""no"" to patches that although are well-coded and well-tested, do not contribute to the goal of the project. To perform all these activities masterfully, maintainers should exercise attributes that software engineers (working on closed source projects) do not always need to master. This paper aims to uncover, relate, and prioritize the unique attributes that great OSS maintainers might have. To achieve this goal, we conducted 33 semi-structured interviews with well-experienced maintainers that are the gatekeepers of notable projects such as the Linux Kernel, the Debian operating system, and the GitLab coding platform. After we analyzed the interviews and curated a list of attributes, we created a conceptual framework to explain how these attributes are connected. We then conducted a rating survey with 90 OSS contributors. We noted that ""technical excellence"" and ""communication"" are the most recurring attributes. When grouped, these attributes fit into four broad categories: management, social, technical, and personality. While we noted that ""sustain a long term vision of the project"" and being ""extremely careful"" seem to form the basis of our framework, we noted through our survey that the communication attribute was perceived as the most essential one.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402023,Open source software;open source maintainers;great attributes,Quality assurance;Statistical analysis;Encoding;Interviews;Task analysis;Sustainable development;Software engineering,Linux;project management;public domain software;software engineering,open source projects;coding tasks;software engineers;closed source projects;OSS maintainers;GitLab coding platform;OSS contributors;recurring attributes;communication attribute;open source software maintainers;Debian operating system,8
557,Open Source: Developer's Skills,Representation of Developer Expertise in Open Source Software,T. Dey; A. Karnauch; A. Mockus,"The University of Tennessee, Knoxville, TN, USA; The University of Tennessee, Knoxville, TN, USA; The University of Tennessee, Knoxville, TN, USA",2021,"Background: Accurate representation of developer expertise has always been an important research problem. While a number of studies proposed novel methods of representing expertise within individual projects, these methods are difficult to apply at an ecosystem level. However, with the focus of software development shifting from monolithic to modular, a method of representing developers' expertise in the context of the entire OSS development becomes necessary when, for example, a project tries to find new maintainers and look for developers with relevant skills. Aim: We aim to address this knowledge gap by proposing and constructing the Skill Space where each API, developer, and project is represented and postulate how the topology of this space should reflect what developers know (and projects need). Method: we use the World of Code infrastructure to extract the complete set of APIs in the files changed by open source developers and, based on that data, employ Doc2Vec embeddings for vector representations of APIs, developers, and projects. We then evaluate if these embeddings reflect the postulated topology of the Skill Space by predicting what new APIs/projects developers use/join, and whether or not their pull requests get accepted. We also check how the developers' representations in the Skill Space align with their self-reported API expertise. Result: Our results suggest that the proposed embeddings in the Skill Space appear to satisfy the postulated topology and we hope that such representations may aid in the construction of signals that increase trust (and efficiency) of open source ecosystems at large and may aid investigations of other phenomena related to developer proficiency and learning.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401957,Expertise;Developer Expertise;Vector Embedding;Doc2Vec;API;API embedding;Project embedding;Developer embedding;Skill Space;Machine Learning;Open Source;World of Code,Ecosystems;Topology;Trajectory;Software measurement;Task analysis;Open source software;Software engineering,application program interfaces;document handling;natural language processing;public domain software;software engineering,API expertise;open source ecosystems;developer expertise;open source software;individual projects;software development;OSS development;open source developers;Skill Space;Doc2Vec embeddings;vector representations,7
558,Open Source: General Issues,Extracting Rationale for Open Source Software Development Decisions â€” A Study of Python Email Archives,P. N. Sharma; B. T. R. Savarimuthu; N. Stanger,"Department of Information Science, University of Otago, Dunedin, New Zealand; Department of Information Science, University of Otago, Dunedin, New Zealand; Department of Information Science, University of Otago, Dunedin, New Zealand",2021,"A sound Decision-Making (DM) process is key to the successful governance of software projects. In many Open Source Software Development (OSSD) communities, DM processes lie buried amongst vast amounts of publicly available data. Hidden within this data lie the rationale for decisions that led to the evolution and maintenance of software products. While there have been some efforts to extract DM processes from publicly available data, the rationale behind 'how' the decisions are made have seldom been explored. Extracting the rationale for these decisions can facilitate transparency (by making them known), and also promote accountability on the part of decision-makers. This work bridges this gap by means of a large-scale study that unearths the rationale behind decisions from Python development email archives comprising about 1.5 million emails. This paper makes two main contributions. First, it makes a knowledge contribution by unearthing and presenting the rationale behind decisions made. Second, it makes a methodological contribution by presenting a heuristics-based rationale extraction system called Rationale Miner that employs multiple heuristics, and follows a data-driven, bottom-up approach to infer the rationale behind specific decisions (e.g., whether a new module is implemented based on core developer consensus or benevolent dictator's pronouncement). Our approach can be applied to extract rationale in other OSSD communities that have similar governance structures.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402135,Open Source Software Development (OSSD);decision making;Python;rationale;causal extraction;heuristics;Rationale Miner,Bridges;Decision making;Tools;Electronic mail;Open source software;Python;Software engineering,decision making;public domain software;software engineering;software maintenance,heuristics-based rationale extraction system;Rationale Miner;specific decisions;Open Source Software Development decisions;Python email archives;software projects;Open Source Software Development communities;DM processes;software products;decision-makers;Python development email archives,4
559,Open Source: Participant's Motivations,Leaving My Fingerprints: Motivations and Challenges of Contributing to OSS for Social Good,Y. Huang; D. Ford; T. Zimmermann,"University of Michigan, Ann Arbor, MI; Microsoft Research, Redmond, WA, USA; Microsoft Research, Redmond, WA, USA",2021,"When inspiring software developers to contribute to open source software, the act is often referenced as an opportunity to build tools to support the developer community. However, that is not the only charge that propels contributions-growing interest in open source has also been attributed to software developers deciding to use their technical skills to benefit a common societal good. To understand how developers identify these projects, their motivations for contributing, and challenges they face, we conducted 21 semi-structured interviews with OSS for Social Good (OSS4SG) contributors. From our interview analysis, we identified themes of contribution styles that we wanted to understand at scale by deploying a survey to over 5765 OSS and Open Source Software for Social Good contributors. From our quantitative analysis of 517 responses, we find that the majority of contributors demonstrate a distinction between OSS4SG and OSS. Likewise, contributors described definitions based on what societal issue the project was to mitigate and who the outcomes of the project were going to benefit. In addition, we find that OSS4SG contributors focus less on benefiting themselves by padding their resume with new technology skills and are more interested in leaving their mark on society at statistically significant levels. We also find that OSS4SG contributors evaluate the owners of the project significantly more than OSS contributors. These findings inform implications to help contributors identify high societal impact projects, help project maintainers reduce barriers to entry, and help organizations understand why contributors are drawn to these projects to sustain active participation.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402112,Social Good;OSS4SG;OSS;GitHub;open source software,Statistical analysis;Publishing;Organizations;Tools;Propulsion;Fingerprint recognition;Interviews,industrial psychology;organisational aspects;personnel;project management;public domain software;software development management,inspiring software developers;developer community;propels contributions-growing interest;common societal good;contributing;Social Good contributors;interview analysis;contribution styles;5765 OSS;Open Source Software;OSS4SG contributors;OSS contributors;high societal impact projects,10
560,Open Source: Participant's Motivations,"Onboarding vs. Diversity, Productivity and Quality â€” Empirical Study of the OpenStack Ecosystem",A. Foundjem; E. Eghan; B. Adams,"MCIS Laboratory, Queenâ€™s University, Canada; Polytechnique Montreal, Montreal, QC, Canada; MCIS Laboratory, Queenâ€™s University, Canada",2021,"Despite the growing success of open-source software ecosystems (SECOs), their sustainability depends on the recruitment and involvement of ever-larger contributors. As such, onboarding, i.e., the socio-technical adaptation of new contributors to a SECO, forms a significant aspect of a SECO's growth that requires substantial resources. Unfortunately, despite theoretical models and initial user studies to examine the potential benefits of onboarding, little is known about the process of SECO onboarding, nor about the socio-technical benefits and drawbacks of contributors' onboarding experience in a SECO. To address these, we first carry out an observational study of 72 new contributors during an OpenStack onboarding event to provide a catalog of teaching content, teaching strategies, onboarding challenges, and expected benefits. Next, we empirically validate the extent to which diversity, productivity, and quality benefits are achieved by mining code changes, reviews, and contributors' issues with(out) OpenStack onboarding experience. Among other findings, our study shows a significant correlation with increasing gender diversity (65% for both females and non-binary contributors) and patch acceptance rates (13.5%). Onboarding also has a significant negative correlation with the time until a contributor's first commit and bug-proneness of contributions.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402037,Onboarding;Mentoring;Collaboration;contributors;knowledge transfer;Software ecosystems;Open source,Productivity;Correlation;Education;Ecosystems;Sustainable development;Software engineering;Gender issues,data mining;gender issues;human factors;program debugging;public domain software;teaching,openstack ecosystem;open-source software ecosystems;socio-technical adaptation;SECO growth;socio-technical benefits;openstack onboarding event;teaching content;teaching strategies;quality benefits;OpenStack onboarding experience;gender diversity;nonbinary contributors;code mining;patch acceptance rates;bug-proneness,2
561,Open Source: Participant's Motivations,The Shifting Sands of Motivation: Revisiting What Drives Contributors in Open Source,M. Gerosa; I. Wiese; B. Trinkenreich; G. Link; G. Robles; C. Treude; I. Steinmacher; A. Sarma,"Northern Arizona University, USA; Universidade TecnolÃ³gica Federal do ParanÃ¡, Brazil; Northern Arizona University, USA; Bitergia, USA; Universidad Rey Juan Carlos, Spain; University of Adelaide, Australia; Northern Arizona University, USA; Oregon State University, USA",2021,"Open Source Software (OSS) has changed drastically over the last decade, with OSS projects now producing a large ecosystem of popular products, involving industry participation, and providing professional career opportunities. But our field's understanding of what motivates people to contribute to OSS is still fundamentally grounded in studies from the early 2000s. With the changed landscape of OSS, it is very likely that motivations to join OSS have also evolved. Through a survey of 242 OSS contributors, we investigate shifts in motivation from three perspectives: (1) the impact of the new OSS landscape, (2) the impact of individuals' personal growth as they become part of OSS communities, and (3) the impact of differences in individuals' demographics. Our results show that some motivations related to social aspects and reputation increased in frequency and that some intrinsic and internalized motivations, such as learning and intellectual stimulation, are still highly relevant. We also found that contributing to OSS often transforms extrinsic motivations to intrinsic, and that while experienced contributors often shift toward altruism, novices often shift toward career, fun, kinship, and learning. OSS projects can leverage our results to revisit current strategies to attract and retain contributors, and researchers and tool builders can better support the design of new studies and tools to engage and support OSS development.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402044,open source;motivation;incentive,Industries;Engineering profession;Ecosystems;Transforms;Tools;Teamwork;Open source software,human factors;project management;public domain software;software engineering,open source software;OSS projects;intrinsic motivations;internalized motivations;extrinsic motivations;OSS development,35
562,Performance Modeling of Highly Configurable Software Systems,White-Box Performance-Influence Models: A Profiling and Learning Approach,M. Weber; S. Apel; N. Siegmund,"Leipzig University, Germany; Saarland Informatics Campus, Saarland University, Germany; Leipzig University, Germany",2021,"Many modern software systems are highly configurable, allowing the user to tune them for performance and more. Current performance modeling approaches aim at finding performance-optimal configurations by building performance models in a black-box manner. While these models provide accurate estimates, they cannot pinpoint causes of observed performance behavior to specific code regions. This does not only hinder system understanding, but it also complicates tracing the influence of configuration options to individual methods. We propose a white-box approach that models configuration-dependent performance behavior at the method level. This allows us to predict the influence of configuration decisions on individual methods, supporting system understanding and performance debugging. The approach consists of two steps: First, we use a coarse-grained profiler and learn performance-influence models for all methods, potentially identifying some methods that are highly configuration-and performance-sensitive, causing inaccurate predictions. Second, we re-measure these methods with a fine-grained profiler and learn more accurate models, at higher cost, though. By means of 9 real-world Java software systems, we demonstrate that our approach can efficiently identify configuration-relevant methods and learn accurate performance-influence models.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401975,configuration management;performance;software variability;software product lines,Java;Debugging;Predictive models;Software systems;Software measurement;Testing;Software engineering,Java;learning (artificial intelligence);program debugging;software performance evaluation,system understanding;configuration options;configuration decisions;performance debugging;coarse-grained profiler;fine-grained profiler;real-world Java software systems;configuration-relevant methods;white-box performance-influence models;modern software systems;performance-optimal configurations;building performance models;black-box manner;configuration-dependent performance behavior,10
563,Performance Modeling of Highly Configurable Software Systems,White-Box Analysis over Machine Learning: Modeling Performance of Configurable Systems,M. Velez; P. Jamshidi; N. Siegmund; S. Apel; C. KÃ¤stner,"Carnegie Mellon University; University of South Carolina; Leipzig University; Saarland Informatics Campus, Saarland University; Carnegie Mellon University, Pittsburgh, PA, USA",2021,"Performance-influence models can help stakeholders understand how and where configuration options and their interactions influence the performance of a system. With this understanding, stakeholders can debug performance behavior and make deliberate configuration decisions. Current black-box techniques to build such models combine various sampling and learning strategies, resulting in tradeoffs between measurement effort, accuracy, and interpretability. We present Comprex, a white-box approach to build performance-influence models for configurable systems, combining insights of local measurements, dynamic taint analysis to track options in the implementation, compositionality, and compression of the configuration space, without relying on machine learning to extrapolate incomplete samples. Our evaluation on 4 widely-used, open-source projects demonstrates that Comprex builds similarly accurate performance-influence models to the most accurate and expensive black-box approach, but at a reduced cost and with additional benefits from interpretable and local models.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401991,Performance influence modeling;Software configuration;Dynamic analysis,Analytical models;Current measurement;Buildings;Machine learning;Stakeholders;Open source software;Software engineering,learning (artificial intelligence);program debugging;program testing;security of data,white-box analysis;machine learning;configurable systems;stakeholders;configuration options;performance behavior;deliberate configuration decisions;black-box techniques;learning strategies;white-box approach;dynamic taint analysis;similarly accurate performance-influence models;expensive black-box approach;interpretable models,14
564,Privacy in Apps: Cases from COVID-19,An Empirical Assessment of Global COVID-19 Contact Tracing Applications,R. Sun; W. Wang; M. Xue; G. Tyson; S. Camtepe; D. C. Ranasinghe,"The University of Adelaide, Australia; The University of Adelaide, Australia; The University of Adelaide, Australia; Queen Mary University of London, United Kingdom; CSIRO-Data61, Australia; The University of Adelaide, Australia",2021,"The rapid spread of COVID-19 has made manual contact tracing difficult. Thus, various public health authorities have experimented with automatic contact tracing using mobile applications (or ""apps""). These apps, however, have raised security and privacy concerns. In this paper, we propose an automated security and privacy assessment tool - COVIDGUARDIAN - which combines identification and analysis of Personal Identification Information (PII), static program analysis and data flow analysis, to determine security and privacy weaknesses. Furthermore, in light of our findings, we undertake a user study to investigate concerns regarding contact tracing apps. We hope that COVIDGUARDIAN, and the issues raised through responsible disclosure to vendors, can contribute to the safe deployment of mobile contact tracing. As part of this, we offer concrete guidelines, and highlight gaps between user requirements and app performance.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402131,Mobile application;Software security privacy;contact tracing;COVID-19,COVID-19;Privacy;Data privacy;Telecommunication traffic;Manuals;Tools;Mobile applications,data flow analysis;data privacy;mobile computing;program diagnostics;security of data,COVIDGUARDIAN;mobile contact tracing;app performance;empirical assessment;Global COVID-19 contact;rapid spread;manual contact;public health authorities;automatic contact;mobile applications;privacy concerns;automated security;static program analysis;data flow analysis;privacy weaknesses;contact tracing apps,14
565,Privacy in Apps: Cases from COVID-19,Sustainable Solving: Reducing the Memory Footprint of IFDS-Based Data Flow Analyses Using Intelligent Garbage Collection,S. Arzt,"Secure Software Engineering Group, Fraunhofer Institute for Secure Information Technology, Darmstadt, Germany",2021,"Static data flow analysis is an integral building block for many applications, ranging from compile-time code optimization to security and privacy analysis. When assessing whether a mobile app is trustworthy, for example, analysts need to identify which of the user's personal data is sent to external parties such as the app developer or cloud providers. Since accessing and sending data is usually done via API calls, tracking the data flow between source and sink API is often the method of choice. Precise algorithms such as IFDS help reduce the number of false positives, but also introduce significant performance penalties. With its fixpoint iteration over the program's entire exploded supergraph, IFDS is particularly memory-intensive, consuming hundreds of megabytes or even several gigabytes for medium-sized apps. In this paper, we present a technique called CleanDroid for reducing the memory footprint of a precise IFDS-based data flow analysis and demonstrate its effectiveness in the popular FlowDroid open-source data flow solver. CleanDroid efficiently removes edges from the path edge table used for the IFDS fixpoint iteration without affecting termination. As we show on 600 realworld Android apps from the Google Play Store, CleanDroid reduces the average per-app memory consumption by around 63% to 78%. At the same time, CleanDroid speeds up the analysis by up to 66%.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402149,Static Analysis;Performance;IFDS;IDE;Data Flow,Privacy;Memory management;Mobile applications;Security;Optimization;Open source software;Software engineering,Android (operating system);application program interfaces;cloud computing;data flow analysis;data privacy;iterative methods;mobile computing;optimisation;public domain software;storage management,memory footprint;IFDS-based data flow analyses;intelligent garbage collection;static data flow analysis;integral building block;compile-time code optimization;security analysis;privacy analysis;mobile app;users personal data;external parties;app developer;medium-sized apps;CleanDroid;precise IFDS-based data flow analysis;IFDS fixpoint iteration;average per-app memory consumption;cloud providers;API calls;FlowDroid open-source data flow solver;Android apps,1
566,Program Repair: Automatic Patching,Synthesizing Object State Transformers for Dynamic Software Updates,Z. Zhao; Y. Jiang; C. Xu; T. Gu; X. Ma,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; Alibaba Group, Sunnyvale, CA, USA; State Key Laboratory for Novel Software Technology, Nanjing University, China",2021,"There is an increasing demand for evolving software systems to deliver continuous services of no restart. Dynamic software update (DSU) aims to achieve this goal by patching the system state on the fly but is currently hindered from practice due to non-trivial cross-version object state transformations. This paper revisits this problem through an in-depth empirical study of over 190 class changes from Tomcat 8. The study produced an important finding that most non-trivial object state transformers can be constructed by reassembling existing old/new version code snippets. This paper presents a domain-specific language and an efficient algorithm for synthesizing non-trivial object transformers over code reuse. We experimentally evaluated our tool implementation PASTA with real-world software systems, reporting PASTA's effectiveness in succeeding in 7.5X non-trivial object transformation tasks compared with the best existing DSU techniques.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402027,Software maintenance and evolution;dynamic software update;object transformation;program synthesis,Software maintenance;Heuristic algorithms;Software algorithms;Tools;Software systems;Task analysis;Software engineering,public domain software;software development management;software maintenance;software reusability;specification languages,evolving software systems;dynamic software update;system state;nontrivial cross-version object state transformations;nontrivial object state transformers;real-world software systems;nontrivial object transformation tasks,2
567,Program Repair: Automatic Patching,Fast and Precise On-the-Fly Patch Validation for All,L. Chen; Y. Ouyang; L. Zhang,"Department of Computer Science, The University of Texas at Dallas; Department of Computer Science, The University of Texas at Dallas; Department of Computer Science, University of Illinois at Urbana-Champaign",2021,"Generate-and-validate (G&V) automated program repair (APR) techniques have been extensively studied during the past decade. Meanwhile, such techniques can be extremely time-consuming due to the manipulation of program code to fabricate a large number of patches and also the repeated test executions on patches to identify potential fixes. PraPR, a recentG furthermore, UniAPR addresses the imprecise patch validation issue by resetting the JVM global state via runtime bytecode transformation. We have implemented UniAPR as a publicly available fully automated Maven Plugin. Our study demonstrates for the first time that on-the-fly patch validation can often speed up state-of-the-art source-code-level APR by over an order of magnitude, enabling all existing APR techniques to explore a larger search space to fix more bugs in the near future. Furthermore, our study shows the first empirical evidence that vanilla on-the-fly patch validation can be imprecise/unsound, while UniAPR with JVM reset is able to mitigate such issues with negligible overhead.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402121,,Runtime;Computer bugs;Maintenance engineering;Tools,Java;program compilers;program debugging;program testing;program verification;search problems;software maintenance;source code (software),program code;repeated test executions;potential fixes;UniAPR;imprecise patch validation issue;JVM global state;runtime bytecode transformation;APR techniques;vanilla on-the-fly patch validation;JVM reset;Maven Plugin;source-code-level APR,4
568,Program Repair: General Issues,Bounded Exhaustive Search of Alloy Specification Repairs,S. GutiÃ©rrez Brida; G. Regis; G. Zheng; H. Bagheri; T. Nguyen; N. Aguirre; M. Frias,"University of Rio Cuarto, Rio Cuarto, Argentina; Department of Computer Science, University of Rio Cuarto, Argentina; Department of Computer Science & Engineering, University of Nebraska-Lincoln, USA; Department of Computer Science & Engineering, University of Nebraska-Lincoln, USA; Department of Computer Science & Engineering, University of Nebraska-Lincoln, USA; Department of Computer Science, University of Rio Cuarto, Argentina; National Council for Scientific and Technical Research (CONICET), Argentina",2021,"The rising popularity of declarative languages and the hard to debug nature thereof have motivated the need for applicable, automated repair techniques for such languages. However, despite significant advances in the program repair of imperative languages, there is a dearth of repair techniques for declarative languages. This paper presents BeAFix, an automated repair technique for faulty models written in Alloy, a declarative language based on first-order relational logic. BeAFix is backed with a novel strategy for bounded exhaustive, yet scalable, exploration of the spaces of fix candidates and a formally rigorous, sound pruning of such spaces. Moreover, different from the state-of-the-art in Alloy automated repair, that relies on the availability of unit tests, BeAFix does not require tests and can work with assertions that are naturally used in formal declarative languages. Our experience with using BeAFix to repair thousands of real-world faulty models, collected by other researchers, corroborates its ability to effectively generate correct repairs and outperform the state-of-the-art.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402059,Alloy;Automated Repair;Formal Specification;Bounded exhaustive analysis,Metals;Maintenance engineering;Tools;Syntactics;Software;Space exploration;Task analysis,formal logic;formal specification;program debugging;program testing,BeAFix;automated repair technique;declarative language;bounded exhaustive;Alloy automated repair;formal declarative languages;correct repairs;Alloy specification repairs;applicable repair techniques;automated repair techniques;program repair;imperative languages;first-order relational logic,
569,Program Repair: General Issues,Shipwright: A Human-in-the-Loop System for Dockerfile Repair,J. Henkel; D. Silva; L. Teixeira; M. dâ€™Amorim; T. Reps,"University of Wisconsinâ€“Madison, Madison, WI, USA; Federal University of Pernambuco, Recife, PE, Brazil; Federal University of Pernambuco, Recife, PE, Brazil; Federal University of Pernambuco, Recife, Brazil; University of Wisconsinâ€“Madison, Madison, WI, USA",2021,"Docker is a tool for lightweight OS-level virtualization. Docker images are created by performing a build, controlled by a source-level artifact called a Dockerfile. We studied Dockerfiles on GitHub, and-to our great surprise-found that over a quarter of the examined Dockerfiles failed to build (and thus to produce images). To address this problem, we propose SHIPWRIGHT, a human-in-the-loop system for finding repairs to broken Dockerfiles. SHIPWRIGHT uses a modified version of the BERT language model to embed build logs and to cluster broken Dockerfiles. Using these clusters and a search-based procedure, we were able to design 13 rules for making automated repairs to Dockerfiles. With the aid of SHIPWRIGHT, we submitted 45 pull requests (with a 42.2% acceptance rate) to GitHub projects with broken Dockerfiles. Furthermore, in a ""time-travel"" analysis of broken Dockerfiles that were later fixed, we found that SHIPWRIGHT proposed repairs that were equivalent to human-authored patches in 22.77% of the cases we studied. Finally, we compared our work with recent, state-of-the-art, static Dockerfile analyses, and found that, while static tools detected possible build-failure-inducing issues in 20.6-33.8% of the files we examined, SHIPWRIGHT was able to detect possible issues in 73.25% of the files and, additionally, provide automated repairs for 18.9% of the files.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402069,Docker;DevOps;Repair,Bit error rate;Maintenance engineering;Tools;Virtualization;Software development management;Software engineering,operating systems (computers);parallel processing;pattern clustering;program diagnostics;public domain software;search problems;software maintenance;virtual machines;virtualisation,shipwright;human-in-the-loop system;Dockerfile repair;lightweight OS-level virtualization;docker images;source-level artifact;SHIPWRIGHT;build logs;cluster broken Dockerfiles;automated repairs;human-authored patches;static Dockerfile analyses;build-failure-inducing issues,4
570,Program Repair: General Issues,CURE: Code-Aware Neural Machine Translation for Automatic Program Repair,N. Jiang; T. Lutellier; L. Tan,"Purdue University, West Lafayette, USA; University of Waterloo, Waterloo, Canada; Purdue University, West Lafayette, USA",2021,"Automatic program repair (APR) is crucial to improve software reliability. Recently, neural machine translation (NMT) techniques have been used to automatically fix software bugs. While promising, these approaches have two major limitations. Their search space often does not contain the correct fix, and their search strategy ignores software knowledge such as strict code syntax. Due to these limitations, existing NMT-based techniques underperform the best template-based approaches. We propose CURE, a new NMT-based APR technique with three major novelties. First, CURE pre-trains a programming language (PL) model on a large software codebase to learn developer-like source code before the APR task. Second, CURE designs a new code-aware search strategy that finds more correct fixes by focusing on searching for compilable patches and patches that are close in length to the buggy code. Finally, CURE uses a subword tokenization technique to generate a smaller search space that contains more correct fixes. Our evaluation on two widely-used benchmarks shows that CURE correctly fixes 57 Defects4J bugs and 26 QuixBugs bugs, outperforming all existing APR techniques on both benchmarks.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401997,automatic program repair;software reliability,Computer bugs;Maintenance engineering;Benchmark testing;Search problems;Software;Software reliability;Machine translation,language translation;learning (artificial intelligence);neural nets;program compilers;program debugging;program testing;software maintenance;software reliability,code-aware neural machine translation;automatic program repair;software reliability;neural machine translation techniques;software bugs;correct fix;software knowledge;strict code syntax;NMT-based techniques;NMT-based APR technique;programming language model;software codebase;developer-like source code;APR task;code-aware search strategy;compilable patches;buggy code;existing APR techniques;smaller search space;subword tokenization technique,61
571,Programming: Code Analysis Algorithms,A Differential Testing Approach for Evaluating Abstract Syntax Tree Mapping Algorithms,Y. Fan; X. Xia; D. Lo; A. E. Hassan; Y. Wang; S. Li,"Zhejiang University, China; Monash University, Australia; Singapore Management University, Singapore; Queenâ€™s University, Canada; Huawei Sweden Research Center; Zhejiang University, China",2021,"Abstract syntax tree (AST) mapping algorithms are widely used to analyze changes in source code. Despite the foundational role of AST mapping algorithms, little effort has been made to evaluate the accuracy of AST mapping algorithms, i.e., the extent to which an algorithm captures the evolution of code. We observe that a program element often has only one best-mapped program element. Based on this observation, we propose a hierarchical approach to automatically compare the similarity of mapped statements and tokens by different algorithms. By performing the comparison, we determine if each of the compared algorithms generates inaccurate mappings for a statement or its tokens. We invite 12 external experts to determine if three commonly used AST mapping algorithms generate accurate mappings for a statement and its tokens for 200 statements. Based on the experts' feedback, we observe that our approach achieves a precision of 0.98-1.00 and a recall of 0.65-0.75. Furthermore, we conduct a large-scale study with a dataset of ten Java projects containing a total of 263,165 file revisions. Our approach determines that GumTree, MTDiff and IJM generate inaccurate mappings for 20%-29%, 25%-36% and 21%-30% of the file revisions, respectively. Our experimental results show that state-of-the-art AST mapping algorithms still need improvements.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401960,program element mapping;abstract syntax tree;software evolution,Java;Software algorithms;Syntactics;Testing;Software engineering,Java;program debugging;public domain software;software metrics;source code (software);trees (mathematics),abstract syntax tree mapping algorithms;best-mapped program element;mapped statements;inaccurate mappings;accurate mappings;state-of-the-art AST mapping algorithms,3
572,Programming: Code Analysis Algorithms,InferCode: Self-Supervised Learning of Code Representations by Predicting Subtrees,N. D. Q. Bui; Y. Yu; L. Jiang,"School of Computing & Information Systems, Singapore Management Univerity; School of Computing & Communications, The Open University, UK; School of Computing & Information Systems, Singapore Management Univerity",2021,"Learning code representations has found many uses in software engineering, such as code classification, code search, comment generation, and bug prediction, etc. Although representations of code in tokens, syntax trees, dependency graphs, paths in trees, or the combinations of their variants have been proposed, existing learning techniques have a major limitation that these models are often trained on datasets labeled for specific downstream tasks, and as such the code representations may not be suitable for other tasks. Even though some techniques generate representations from unlabeled code, they are far from being satisfactory when applied to the downstream tasks. To overcome the limitation, this paper proposes InferCode, which adapts the self-supervised learning idea from natural language processing to the abstract syntax trees (ASTs) of code. The novelty lies in the training of code representations by predicting subtrees automatically identified from the contexts of ASTs. With InferCode, subtrees in ASTs are treated as the labels for training the code representations without any human labelling effort or the overhead of expensive graph construction, and the trained representations are no longer tied to any specific downstream tasks or code units. We have trained an instance of InferCode model using Tree-Based Convolutional Neural Network (TBCNN) as the encoder of a large set of Java code. This pre-trained model can then be applied to downstream unsupervised tasks such as code clustering, code clone detection, cross-language code search, or be reused under a transfer learning scheme to continue training the model weights for supervised tasks such as code classification and method name prediction. Compared to prior techniques applied to the same downstream tasks, such as code2vec, code2seq, ASTNN, using our pre-trained InferCode model higher performance is achieved with a significant margin for most of the tasks, including those involving different programming languages. The implementation of InferCode and the trained embeddings are available at the link: https://github.com/bdqnghi/infercode.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402028,code search;self supervised;code clone detection;cross language;fine tuning;code retrieval;unlabel data;unlabelled data,Training;Computer bugs;Cloning;Predictive models;Syntactics;Task analysis;Software engineering,computational linguistics;convolutional neural nets;Java;supervised learning;trees (mathematics),pre-trained InferCode model;subtrees prediction;trained representations;unlabeled code;specific downstream tasks;code representations;code2seq;code classification;cross-language code search;code clone detection;code clustering;Java code;code units,28
573,Programming: General Issues,Efficient Compiler Autotuning via Bayesian Optimization,J. Chen; N. Xu; P. Chen; H. Zhang,"College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; The University of Newcastle, Callaghan, NSW, Australia",2021,"A typical compiler such as GCC supports hundreds of optimizations controlled by compilation flags for improving the runtime performance of the compiled program. Due to the large number of compilation flags and the exponential number of flag combinations, it is impossible for compiler users to manually tune these optimization flags in order to achieve the required runtime performance of the compiled programs. Over the years, many compiler autotuning approaches have been proposed to automatically tune optimization flags, but they still suffer from the efficiency problem due to the huge search space. In this paper, we propose the first Bayesian optimization based approach, called BOCA, for efficient compiler autotuning. In BOCA, we leverage a tree-based model for approximating the objective function in order to make Bayesian optimization scalable to a large number of optimization flags. Moreover, we design a novel searching strategy to improve the efficiency of Bayesian optimization by incorporating the impact of each optimization flag measured by the tree-based model and a decay function to strike a balance between exploitation and exploration. We conduct extensive experiments to investigate the effectiveness of BOCA on two most popular C compilers (i.e., GCC and LLVM) and two widely-used C benchmarks (i.e., cBench and PolyBench). The results show that BOCA significantly outperforms the state-of-the-art compiler autotuning approaches and Bayesion optimization methods in terms of the time spent on achieving specified speedups, demonstrating the effectiveness of BOCA.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401979,Compiler Autotuning;Bayesian Optimization;Compiler Optimization;Configuration,Runtime;Program processors;Optimization methods;Benchmark testing;Search problems;Bayes methods;Optimization,Bayes methods;optimisation;program compilers;trees (mathematics),Bayesian optimization compiler autotuning;compiler autotuning approaches;decay function;C benchmarks;C compilers;automatically tune optimization flag;compiler users;flag combinations;compilation flags;typical compiler;popular C compilers;Bayesian optimization;tree-based model;BOCA;efficient compiler autotuning;compiled program,10
574,Programming: General Issues,TransRegex: Multi-modal Regular Expression Synthesis by Generate-and-Repair,Y. Li; S. Li; Z. Xu; J. Cao; Z. Chen; Y. Hu; H. Chen; S. -C. Cheung,"State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; School of Computer Science and Technology, University of Chinese Academy of Sciences, Beijing, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China; The Hong Kong University of Science and Technology, Hong Kong, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; Science #x0026; Technology on Integrated Infomation System Laboratory, Institute of Software, Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China; College of Computer Science and Software Engineering, Shenzhen University, Shenzhen, China",2021,"Since regular expressions (abbrev. regexes) are difficult to understand and compose, automatically generating regexes has been an important research problem. This paper introduces TransRegex, for automatically constructing regexes from both natural language descriptions and examples. To the best of our knowledge, TransRegex is the first to treat the NLP-and-example-based regex synthesis problem as the problem of NLP-based synthesis with regex repair. For this purpose, we present novel algorithms for both NLP-based synthesis and regex repair. We evaluate TransRegex with ten relevant state-of-the-art tools on three publicly available datasets. The evaluation results demonstrate that the accuracy of our TransRegex is 17.4%, 35.8% and 38.9% higher than that of NLP-based approaches on the three datasets, respectively. Furthermore, TransRegex can achieve higher accuracy than the state-of-the-art multi-modal techniques with 10% to 30% higher accuracy on all three datasets. The evaluation results also indicate TransRegex utilizing natural language and examples in a more effective way.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401951,regex synthesis;regex repair;programming by natural languages;programming by example,Natural languages;Maintenance engineering;Tools;Software engineering,natural language processing;software maintenance;text analysis,TransRegex;multimodal regular expression synthesis;regex repair;multimodal techniques;NLP-based regex synthesis problem;example-based regex synthesis problem,4
575,Programming: General Issues,EvoSpex: An Evolutionary Algorithm for Learning Postconditions,F. Molina; P. Ponzio; N. Aguirre; M. Frias,"Department of Computer Science, University of RÃ­o Cuarto, Argentina; Department of Computer Science, University of RÃ­o Cuarto, Argentina; Department of Computer Science, University of RÃ­o Cuarto, Argentina; National Council for Scientific and Technical Research (CONICET), Argentina",2021,"Software reliability is a primary concern in the construction of software, and thus a fundamental component in the definition of software quality. Analyzing software reliability requires a specification of the intended behavior of the software under analysis, and at the source code level, such specifications typically take the form of assertions. Unfortunately, software many times lacks such specifications, or only provides them for scenario-specific behaviors, as assertions accompanying tests. This issue seriously diminishes the analyzability of software with respect to its reliability. In this paper, we tackle this problem by proposing a technique that, given a Java method, automatically produces a specification of the method's current behavior, in the form of postcondition assertions. This mechanism is based on generating executions of the method under analysis to obtain valid pre/post state pairs, mutating these pairs to obtain (allegedly) invalid ones, and then using a genetic algorithm to produce an assertion that is satisfied by the valid pre/post pairs, while leaving out the invalid ones. The technique, which targets in particular methods of reference-based class implementations, is assessed on a benchmark of open source Java projects, showing that our genetic algorithm is able to generate post-conditions that are stronger and more accurate, than those generated by related automated approaches, as evaluated by an automated oracle assessment tool. Moreover, our technique is also able to infer an important part of manually written rich postconditions in verified classes, and reproduce contracts for methods whose class implementations were automatically synthesized from specifications.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402148,Oracle Problem;Specification Synthesis;Evolutionary Computation;Program Assertions,Java;Tools;Genetics;Software reliability;Contracts;Genetic algorithms;Software engineering,formal specification;genetic algorithms;Java;public domain software;software quality;software reliability,source code level;genetic algorithm;reference-based class implementations;open source Java projects;evolutionary algorithm;software quality;software reliability;oracle assessment tool;contracts,4
576,Programming: Low Level,"Interface Compliance of Inline Assembly: Automatically Check, Patch and Refine",F. Recoules; S. Bardin; R. Bonichon; M. Lemerre; L. Mounier; M. -L. Potet,"List, UniversitÃ© Paris-Saclay, Saclay, France; List, UniversitÃ© Paris-Saclay, Saclay, France; Tweag I/O, Paris, France; Univ. Paris-Saclay, CEA, List, Saclay, France; Univ. Grenoble Alpes, VERIMAG, Grenoble, France; Univ. Grenoble Alpes, VERIMAG, Grenoble, France",2021,"Inline assembly is still a common practice in low-level C programming, typically for efficiency reasons or for accessing specific hardware resources. Such embedded assembly codes in the GNU syntax (supported by major compilers such as GCC, Clang and ICC) have an interface specifying how the assembly codes interact with the C environment. For simplicity reasons, the compiler treats GNU inline assembly codes as blackboxes and relies only on their interface to correctly glue them into the compiled C code. Therefore, the adequacy between the assembly chunk and its interface (named compliance) is of primary importance, as such compliance issues can lead to subtle and hard-to-find bugs. We propose RUSTInA, the first automated technique for formally checking inline assembly compliance, with the extra ability to propose (proven) patches and (optimization) refinements in certain cases. RUSTInA is based on an original formalization of the inline assembly compliance problem together with novel dedicated algorithms. Our prototype has been evaluated on 202 Debian packages with inline assembly (2656 chunks), finding 2183 issues in 85 packages â€“ 986 significant issues in 54 packages (including major projects such as ffmpeg or ALSA), and proposing patches for 92% of them. Currently, 38 patches have already been accepted (solving 156 significant issues), with positive feedback from development teams.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402072,low level programming;inline assembly;compilation issues;program analysis,Prototypes;Tools;Syntactics;Programming;Hardware;Optimization;Software engineering,C language;embedded systems;optimising compilers;program assemblers;program debugging;program verification;public domain software,compliance issues;inline assembly compliance problem;interface compliance;low-level C programming;embedded assembly codes;GNU syntax;GNU inline assembly codes;compiled C code;assembly chunk,3
577,Programming: Low Level,Enabling Software Resilience in GPGPU Applications via Partial Thread Protection,L. Yang; B. Nie; A. Jog; E. Smirni,"William & Mary, Williamsburg, VA; William & Mary, Williamsburg, VA; William & Mary, Williamsburg, VA; William & Mary, Williamsburg, VA",2021,"Graphics Processing Units (GPUs) are widely used by various applications in a broad variety of fields to accelerate their computation but remain susceptible to transient hardware faults (soft errors) that can easily compromise application output. By taking advantage of a general purpose GPU application hierarchical organization in threads, warps, and cooperative thread arrays, we propose a methodology that identifies the resilience of threads and aims to map threads with the same resilience characteristics to the same warp. This allows to engage partial replication mechanisms for error detection/correction at the warp level. By exploring 12 benchmarks (17 kernels) from 4 benchmark suites, we illustrate that threads can be remapped into reliable or unreliable warps with only 1.63% introduced overhead (on average), and then enable selective protection via replication to those groups of threads that truly need it. Furthermore, we show that thread remapping to different warps does not sacrifice application performance. We show how this remapping facilitates warp replication for error detection and/or correction and achieves average reduction of 20.61% and 27.15% execution cycles, respectively comparing to standard duplication/triplication.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401987,Reliability;GPGPU application resilience;Transient faults;Thread remapping,Instruction sets;Standards organizations;Graphics processing units;Benchmark testing;Kernel;Transient analysis;Resilience,error correction;error detection;graphics processing units;software reliability,software resilience;GPGPU applications;partial thread protection;Graphics Processing Units;soft errors;general purpose GPU application hierarchical organization;cooperative thread arrays;map threads;resilience characteristics;partial replication mechanisms;warp level;thread remapping;application performance;warp replication;transient hardware faults;error detection;error correction,5
578,Q&A in Online Platforms: Stack Overflow 1,Automatic Extraction of Opinion-Based Q&A from Online Developer Chats,P. Chatterjee; K. Damevski; L. Pollock,"University of Delaware, Newark, DE, USA; Virginia Commonwealth University, Richmond, VA, USA; University of Delaware, Newark, DE, USA",2021,"Virtual conversational assistants designed specifically for software engineers could have a huge impact on the time it takes for software engineers to get help. Research efforts are focusing on virtual assistants that support specific software development tasks such as bug repair and pair programming. In this paper, we study the use of online chat platforms as a resource towards collecting developer opinions that could potentially help in building opinion Q&A systems, as a specialized instance of virtual assistants and chatbots for software engineers. Opinion Q&A has a stronger presence in chats than in other developer communications, thus mining them can provide a valuable resource for developers in quickly getting insight about a specific development topic (e.g., What is the best Java library for parsing JSON?). We address the problem of opinion Q&A extraction by developing automatic identification of opinion-asking questions and extraction of participants' answers from public online developer chats. We evaluate our automatic approaches on chats spanning six programming communities and two platforms. Our results show that a heuristic approach to opinion-asking questions works well (.87 precision), and a deep learning approach customized to the software domain outperforms heuristics-based, machine-learning-based and deep learning for answer extraction in community question answering.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402078,opinion question-answering system;public chats;opinion-asking question;answer extraction,Deep learning;Virtual assistants;Programming;Maintenance engineering;Software;Task analysis;Software engineering,data mining;deep learning (artificial intelligence);human factors;Internet;Java;knowledge based systems;question answering (information retrieval);software agents;software maintenance;user interfaces,virtual conversational assistants;software engineers;specific software development tasks;bug repair;pair programming;online chat platforms;opinion Q&A systems;developer communications;opinion-asking questions;public online developer chats;software domain;opinion-based Q&A,9
579,Q&A in Online Platforms: Stack Overflow 2,Automated Query Reformulation for Efficient Search Based on Query Logs From Stack Overflow,K. Cao; C. Chen; S. Baltes; C. Treude; X. Chen,"Software Institute Nanjing University, China; Faculty of Information Technology, Monash University, Australia; School of Computer Science, University of Adelaide, Australia; School of Computer Science, University of Adelaide, Australia; School of Information Science and Technology, Nantong University, China",2021,"As a popular Q&A site for programming, Stack Overflow is a treasure for developers. However, the amount of questions and answers on Stack Overflow make it difficult for developers to efficiently locate the information they are looking for. There are two gaps leading to poor search results: the gap between the user's intention and the textual query, and the semantic gap between the query and the post content. Therefore, developers have to constantly reformulate their queries by correcting misspelled words, adding limitations to certain programming languages or platforms, etc. As query reformulation is tedious for developers, especially for novices, we propose an automated software-specific query reformulation approach based on deep learning. With query logs provided by Stack Overflow, we construct a large-scale query reformulation corpus, including the original queries and corresponding reformulated ones. Our approach trains a Transformer model that can automatically generate candidate reformulated queries when given the user's original query. The evaluation results show that our approach outperforms five state-of-the-art baselines, and achieves a 5.6% to 33.5% boost in terms of ExactMatch and a 4.8% to 14.4% boost in terms of GLEU.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402151,Stack Overflow;Data Mining;Query Reformulation;Deep Learning;Query Logs,Deep learning;Computer languages;Semantics;Programming;History;Task analysis;Software engineering,learning (artificial intelligence);query formulation;query processing,textual query;semantic gap;poor search results;user;candidate reformulated queries;corresponding reformulated ones;original queries;large-scale query reformulation corpus;Stack Overflow;query logs;automated software-specific query reformulation approach;programming languages,20
580,Q&A in Online Platforms: Stack Overflow 2,Automatic Solution Summarization for Crash Bugs,H. Wang; X. Xia; D. Lo; J. Grundy; X. Wang,"College of Computer Science and Technology, Zhejiang University; Faculty of Information Technology, Monash University; School of Information Systems, Singapore Management University; Faculty of Information Technology, Monash University; College of Computer Science and Technology, Zhejiang University",2021,"The causes of software crashes can be hidden anywhere in the source code and development environment. When encountering software crashes, recurring bugs that are discussed on Q&A sites could provide developers with solutions to their crashing problems. However, it is difficult for developers to accurately search for relevant content on search engines, and developers have to spend a lot of manual effort to find the right solution from the returned results. In this paper, we present CRASOLVER, an approach that takes into account both the structural information of crash traces and the knowledge of crash-causing bugs to automatically summarize solutions from crash traces. Given a crash trace, CRASOLVER retrieves relevant questions from Q&A sites by combining a proposed position dependent similarity - based on the structural information of the crash trace - with an extra knowledge similarity, based on the knowledge from official documentation sites. After obtaining the answers to these questions from the Q&A site, CRASOLVER summarizes the final solution based on a multi-factor scoring mechanism. To evaluate our approach, we built two repositories of Java and Android exception-related questions from Stack Overflow with size of 69,478 and 33,566 questions respectively. Our user study results using 50 selected Java crash traces and 50 selected Android crash traces show that our approach significantly outperforms four baselines in terms of relevance, usefulness, and diversity. The evaluation also confirms the effectiveness of the relevant question retrieval component in our approach for crash traces.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401996,,Java;Computer bugs;Diversity reception;Documentation;Search engines;Software;Software engineering,Android (operating system);information retrieval;Java;program debugging;question answering (information retrieval);search engines;source code (software),crash trace;CRASOLVER;Q&A site;automatic solution summarization;source code;development environment;crashing problems;crash-causing bugs;Android crash traces;software crashes;Java crash;search engines;structural information,5
581,Q&A in Online Platforms: Stack Overflow 2,Supporting Quality Assurance with Automated Process-Centric Quality Constraints Checking,C. Mayr-Dorn; M. Vierhauser; S. Bichler; F. Keplinger; J. Cleland-Huang; A. Egyed; T. Mehofer,"Johannes Kepler University, Linz, Austria; Johannes Kepler University, Linz, Austria; Johannes Kepler University, Linz, Austria; Johannes Kepler University, Linz, Austria; University of Notre Dame, Notre Dame, USA; Johannes Kepler University, Linz, Austria; Frequentis AG, Vienna, Austria",2021,"Regulations, standards, and guidelines for safety-critical systems stipulate stringent traceability but do not prescribe the corresponding, detailed software engineering process. Given the industrial practice of using only semi-formal notations to describe engineering processes, processes are rarely ""executable"" and developers have to spend significant manual effort in ensuring that they follow the steps mandated by quality assurance. The size and complexity of systems and regulations makes manual, timely feedback from Quality Assurance (QA) engineers infeasible. In this paper we propose a novel framework for tracking processes in the background, automatically checking QA constraints depending on process progress, and informing the developer of unfulfilled QA constraints. We evaluate our approach by applying it to two different case studies; one open source community system and a safety-critical system in the air-traffic control domain. Results from the analysis show that trace links are often corrected or completed after the fact and thus timely and automated constraint checking support has significant potential on reducing rework.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402034,software engineering process;traceability;developer support,Quality assurance;Manuals;Regulation;Complexity theory;Standards;Software engineering;Guidelines,air traffic control;formal specification;program verification;quality assurance;safety-critical software;software quality,Quality Assurance engineers;process progress;unfulfilled QA constraints;open source community system;safety-critical system;constraint checking support;automated process-centric Quality constraints;safety-critical systems stipulate stringent traceability;corresponding software engineering process;industrial practice;semiformal notations;engineering processes;software engineering process,5
582,Quality Assurance,Understanding Bounding Functions in Safety-Critical UAV Software,X. Liang; J. H. Burns; J. Sanchez; K. Dantu; L. Ziarek; Y. D. Liu,"SUNY Binghamton, Binghamton, New York; SUNY Binghamton, Binghamton, New York; SUNY Binghamton, Binghamton, New York; SUNY Buffalo, Buffalo, New York; SUNY Buffalo, Buffalo, New York; SUNY Binghamton, Binghamton, New York",2021,"Unmanned Aerial Vehicles (UAVs) are an emerging computation platform known for their safety-critical need. In this paper, we conduct an empirical study on a widely used open-source UAV software framework, Paparazzi, with the goal of understanding the safety-critical concerns of UAV software from a bottom-up developer-in-the-field perspective. We set our focus on the use of Bounding Functions (BFs), the runtime checks injected by Paparazzi developers on the range of variables. Through an in-depth analysis on BFs in the Paparazzi autopilot software, we found a large number of them (109 instances) are used to bound safety-critical variables essential to the cyber-physical nature of the UAV, such as its thrust, its speed, and its sensor values. The novel contributions of this study are two fold. First, we take a static approach to classify all BF instances, presenting a novel datatype-based 5-category taxonomy with fine-grained insight on the role of BFs in ensuring the safety of UAV systems. Second, we dynamically evaluate the impact of the BF uses through a differential approach, establishing the UAV behavioral difference with and without BFs. The two-pronged static and dynamic approach together illuminates a rarely studied design space of safety-critical UAV software systems.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401993,unmanned aerial vehicles;bounding functions;safety,Runtime;Taxonomy;Software systems;Unmanned aerial vehicles;Safety;Vehicle dynamics;Open source software,aerospace computing;aerospace control;autonomous aerial vehicles;control engineering computing;public domain software;remotely operated vehicles;safety-critical software;telerobotics;vehicle dynamics,Unmanned Aerial Vehicles;open-source UAV software framework;developer-in-the-field perspective;Paparazzi developers;Paparazzi autopilot software;bound safety-critical variables;datatype-based 5-category taxonomy;UAV systems;UAV behavioral difference;safety-critical UAV software systems;safety critical concerns,1
583,Search-Based SE & Genetic Operations,Enhancing Genetic Improvement of Software with Regression Test Selection,G. Guizzo; J. Petke; F. Sarro; M. Harman,"Department of Computer Science, University College London (UCL), London, United Kingdom; Department of Computer Science, University College London (UCL), London, United Kingdom; Department of Computer Science, University College London (UCL), London, United Kingdom; Department of Computer Science, University College London (UCL), London, United Kingdom",2021,"Genetic improvement uses artificial intelligence to automatically improve software with respect to non-functional properties (AI for SE). In this paper, we propose the use of existing software engineering best practice to enhance Genetic Improvement (SE for AI). We conjecture that existing Regression Test Selection (RTS) techniques (which have been proven to be efficient and effective) can and should be used as a core component of the GI search process for maximising its effectiveness. To assess our idea, we have carried out a thorough empirical study assessing the use of both dynamic and static RTS techniques with GI to improve seven real-world software programs. The results of our empirical evaluation show that incorporation of RTS within GI significantly speeds up the whole GI process, making it up to 78% faster on our benchmark set, being still able to produce valid software improvements. Our findings are significant in that they can save hours to days of computational time, and can facilitate the uptake of GI in an industrial setting, by significantly reducing the time for the developer to receive feedback from such an automated technique. Therefore, we recommend the use of RTS in future test-based automated software improvement work. Finally, we hope this successful application of SE for AI will encourage other researchers to investigate further applications in this area.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401972,Genetic Improvement;Regression Test Selection;Search Based Software Engineering;Genetic Programming,Runtime;Computer bugs;Genetics;Software;Artificial intelligence;Task analysis;Standards,artificial intelligence;genetic algorithms;program testing;program verification;regression analysis;software maintenance,dynamic RTS techniques;static RTS techniques;real-world software programs;GI process;valid software improvements;software improvement work;Genetic Improvement;Genetic improvement;nonfunctional properties;AI for SE;existing software engineering best practice;SE for AI;Regression Test Selection techniques;GI search process,2
584,Security Vulnerabilities: Different Domains,Containing Malicious Package Updates in npm with a Lightweight Permission System,G. Ferreira; L. Jia; J. Sunshine; C. KÃ¤stner,"Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University, Pittsburgh, PA, USA",2021,"The large amount of third-party packages available in fast-moving software ecosystems, such as Node.js/npm, enables attackers to compromise applications by pushing malicious updates to their package dependencies. Studying the npm repository, we observed that many packages in the npm repository that are used in Node.js applications perform only simple computations and do not need access to filesystem or network APIs. This offers the opportunity to enforce least-privilege design per package, protecting applications and package dependencies from malicious updates. We propose a lightweight permission system that protects Node.js applications by enforcing package permissions at runtime. We discuss the design space of solutions and show that our system makes a large number of packages much harder to be exploited, almost for free.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402108,security;malicious package updates;supplychain security;package management;permission system;sandboxing;design trade-offs,Runtime;Ecosystems;Software;Security,Java;security of data;software packages,lightweight permission system;Node.js applications;package permissions;malicious package updates;third-party packages;fast-moving software ecosystems;package dependencies;npm repository;filesystem;network APIs,8
585,Security Vulnerabilities: Different Domains,Too Quiet in the Library: An Empirical Study of Security Updates in Android Apps' Native Code,S. Almanee; A. Ãœnal; M. Payer; J. Garcia,"University of California, Irvine; University of California Irvine, Irvine, CA, USA; EPFL; University of California, Irvine",2021,"Android apps include third-party native libraries to increase performance and to reuse functionality. Native code is directly executed from apps through the Java Native Interface or the Android Native Development Kit. Android developers add precompiled native libraries to their projects, enabling their use. Unfortunately, developers often struggle or simply neglect to update these libraries in a timely manner. This results in the continuous use of outdated native libraries with unpatched security vulnerabilities years after patches became available. To further understand such phenomena, we study the security updates in native libraries in the most popular 200 free apps on Google Play from Sept. 2013 to May 2020. A core difficulty we face in this study is the identification of libraries and their versions. Developers often rename or modify libraries, making their identification challenging. We create an approach called LibRARIAN (LibRAry veRsion IdentificAtioN) that accurately identifies native libraries and their versions as found in Android apps based on our novel similarity metric bin2sim. LibRARIAN leverages different features extracted from libraries based on their metadata and identifying strings in read-only sections. We discovered 53/200 popular apps (26.5%) with vulnerable versions with known CVEs between Sept. 2013 and May 2020, with 14 of those apps remaining vulnerable. We find that app developers took, on average, 528.71Â±40.20 days to apply security patches, while library developers release a security patch after 54.59 Â± 8.12 days-a 10 times slower rate of update.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402004,Apps and App Store Analysis;Empirical Software Engineering;Evolution and maintenance;Mining Software Repositories;Mobile applications;Software Security,Measurement;Feature extraction;Libraries;Internet;Security;Task analysis;Software engineering,Android (operating system);feature extraction;Java;mobile computing;program compilers;security of data;software libraries,third-party native libraries;Java native interface;Android native development kit;Android developers;security updates;app developers;security patch;library developers;Android apps native code;unpatched security vulnerabilities;library version identification;LibRARIAN;features extraction,2
586,Security Vulnerabilities: Different Domains,"If Itâ€™s Not Secure, It Should Not Compile: Preventing DOM-Based XSS in Large-Scale Web Development with API Hardening",P. Wang; J. Bangert; C. Kern,Security Engineering Research Google; Security Engineering Research Google; Security Engineering Research Google,2021,"With tons of efforts spent on its mitigation, Cross-site scripting (XSS) remains one of the most prevalent security threats on the internet. Decades of exploitation and remediation demonstrated that code inspection and testing alone does not eliminate XSS vulnerabilities in complex web applications with a high degree of confidence. This paper introduces Google's secure-by-design engineering paradigm that effectively prevents DOM-based XSS vulnerabilities in large-scale web development. Our approach, named API hardening, enforces a series of company-wide secure coding practices. We provide a set of secure APIs to replace native DOM APIs that are prone to XSS vulnerabilities. Through a combination of type contracts and appropriate validation and escaping, the secure APIs ensure that applications based thereon are free of XSS vulnerabilities. We deploy a simple yet capable compile-time checker to guarantee that developers exclusively use our hardened APIs to interact with the DOM. We make various of efforts to scale this approach to tens of thousands of engineers without significant productivity impact. By offering rigorous tooling and consultant support, we help developers adopt the secure coding practices as seamlessly as possible. We present empirical results showing how API hardening has helped reduce the occurrences of XSS vulnerabilities in Google's enormous code base over the course of two-year deployment.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401952,web security;cross-site scripting;language based security;empirical software engineering,Cross-site scripting;Writing;Encoding;Software;Security;Software engineering;Testing,application program interfaces;authoring languages;Internet;security of data,large-scale web development;Cross-site scripting;prevalent security threats;code inspection;complex web applications;DOM-based XSS vulnerabilities;company-wide secure coding practices;secure APIs;native DOM APIs;API hardening;compile-time checker;Google enormous code base;Google secure-by-design engineering paradigm,1
587,Security Vulnerabilities: From 3rd Parties' Code,Why Security Defects Go Unnoticed During Code Reviews? A Case-Control Study of the Chromium OS Project,R. Paul; A. K. Turzo; A. Bosu,"Department of Computer Science, Wayne State University, Detroit, Michigan, USA; Department of Computer Science, Wayne State University, Detroit, Michigan, USA; Department of Computer Science, Wayne State University, Detroit, Michigan, USA",2021,"Peer code review has been found to be effective in identifying security vulnerabilities. However, despite practicing mandatory code reviews, many Open Source Software (OSS) projects still encounter a large number of post-release security vulnerabilities, as some security defects escape those. Therefore, a project manager may wonder if there was any weakness or inconsistency during a code review that missed a security vulnerability. Answers to this question may help a manager pinpointing areas of concern and taking measures to improve the effectiveness of his/her project's code reviews in identifying security defects. Therefore, this study aims to identify the factors that differentiate code reviews that successfully identified security defects from those that missed such defects. With this goal, we conduct a case-control study of Chromium OS project. Using multi-stage semi-automated approaches, we build a dataset of 516 code reviews that successfully identified security defects and 374 code reviews where security defects escaped. The results of our empirical study suggest that the are significant differences between the categories of security defects that are identified and that are missed during code reviews. A logistic regression model fitted on our dataset achieved an AUC score of 0.91 and has identified nine code review attributes that influence identifications of security defects. While time to complete a review, the number of mutual reviews between two developers, and if the review is for a bug fix have positive impacts on vulnerability identification, opposite effects are observed from the number of directories under review, the number of total reviews by a developer, and the total number of prior commits for the file under review.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402130,security;code review;vulnerability,Quality assurance;Computer bugs;Chromium;Security;Open source software;Logistics;Software engineering,program debugging;project management;public domain software;regression analysis;security of data;software maintenance;software management;software quality,post-release security vulnerabilities;mandatory code reviews;security vulnerability;peer code review;Chromium OS project;case-control study;374 code reviews;successfully identified security defects,11
588,Security Vulnerabilities: General Issues 1,Technical Leverage in a Software Ecosystem: Development Opportunities and Security Risks,F. Massacci; I. Pashchenko,"University of Trento (IT), Vrije Universiteit Amsterdam (NL); University of Trento (IT)",2021,"In finance, leverage is the ratio between assets borrowed from others and one's own assets. A matching situation is present in software: by using free open-source software (FOSS) libraries a developer leverages on other people's code to multiply the offered functionalities with a much smaller own codebase. In finance as in software, leverage magnifies profits when returns from borrowing exceed costs of integration, but it may also magnify losses, in particular in the presence of security vulnerabilities. We aim to understand the level of technical leverage in the FOSS ecosystem and whether it can be a potential source of security vulnerabilities. Also, we introduce two metrics change distance and change direction to capture the amount and the evolution of the dependency on third-party libraries. The application of the proposed metrics on 8494 distinct library versions from the FOSS Maven-based Java libraries shows that small and medium libraries (less than 100KLoC) have disproportionately more leverage on FOSS dependencies in comparison to large libraries. We show that leverage pays off as leveraged libraries only add a 4% delay in the time interval between library releases while providing four times more code than their own. However, libraries with such leverage (i.e., 75% of libraries in our sample) also have 1.6 higher odds of being vulnerable in comparison to the libraries with lower leverage. We provide an online demo for computing the proposed metrics for real-world software libraries available under the following URL: https://techleverage.eu/.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402061,software security;dependencies;vulnerabilities;leverage;technical debt;empirical analysis;Maven;free open source software,Measurement;Uniform resource locators;Ecosystems;Finance;Libraries;Security;Open source software,Internet;Java;public domain software;security of data;software libraries,distinct library versions;real-world software libraries;lower leverage;library releases;leveraged libraries;FOSS dependencies;FOSS Maven-based Java libraries;third-party libraries;change direction;FOSS ecosystem;security vulnerabilities;free open-source software libraries;finance;software ecosystem;technical leverage,3
589,Security Vulnerabilities: General Issues 2,RAICC: Revealing Atypical Inter-Component Communication in Android Apps,J. Samhi; A. Bartel; T. F. BissyandÃ©; J. Klein,"SnT, University of Luxembourg; SnT, University of Luxembourg; University of Luxembourg, Luxembourg, Luxembourg; SnT, University of Luxembourg",2021,"Inter-Component Communication (ICC) is a key mechanism in Android. It enables developers to compose rich functionalities and explore reuse within and across apps. Unfortunately, as reported by a large body of literature, ICC is rather ""complex and largely unconstrained"", leaving room to a lack of precision in apps modeling. To address the challenge of tracking ICCs within apps, state of the art static approaches such as EPICC, ICCTA and AMANDROID have focused on the documented framework ICC methods (e.g., startActivity) to build their approaches. In this work we show that ICC models inferred in these state of the art tools may actually be incomplete: the framework provides other atypical ways of performing ICCs. To address this limitation in the state of the art, we propose RAICC a static approach for modeling new ICC links and thus boosting previous analysis tasks such as ICC vulnerability detection, privacy leaks detection, malware detection, etc. We have evaluated RAICC on 20 benchmark apps, demonstrating that it improves the precision and recall of uncovered leaks in state of the art tools. We have also performed a large empirical investigation showing that Atypical ICC methods are largely used in Android apps, although not necessarily for data transfer. We also show that RAICC increases the number of ICC links found by 61.6% on a dataset of real-world malicious apps, and that RAICC enables the detection of new ICC vulnerabilities.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402001,Static Analysis;Android Security,Analytical models;Tools;Security;Leak detection;Task analysis;Standards;Software engineering,Android (operating system);data privacy;invasive software;mobile computing;smart phones,RAICC;revealing atypical inter-component communication;Android apps;art static approaches;documented framework ICC methods;ICC models;art tools;ICC links;ICC vulnerability detection;privacy leaks detection;malware detection;Atypical ICC methods;real-world malicious apps;tracking ICC,11
590,Smart Contracts,Smart Contract Security: A Practitioners' Perspective,Z. Wan; X. Xia; D. Lo; J. Chen; X. Luo; X. Yang,"College of Computer Science and Technology, Zhejiang University; Faculty of Information Technology, Monash University; School of Information Systems, Singapore Management University; Faculty of Information Technology, Monash University; Department of Computing, Hong Kong Polytechnic University; College of Computer Science and Technology, Zhejiang University",2021,"Smart contracts have been plagued by security incidents, which resulted in substantial financial losses. Given numerous research efforts in addressing the security issues of smart contracts, we wondered how software practitioners build security into smart contracts in practice. We performed a mixture of qualitative and quantitative studies with 13 interviewees and 156 survey respondents from 35 countries across six continents to understand practitioners' perceptions and practices on smart contract security. Our study uncovers practitioners' motivations and deterrents of smart contract security, as well as how security efforts and strategies fit into the development lifecycle. We also find that blockchain platforms have a statistically significant impact on practitioners' security perceptions and practices of smart contract development. Based on our findings, we highlight future research directions and provide recommendations for practitioners.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402082,Security;Empirical study;Smart contract;Practitioner,Smart contracts;Blockchain;Tools;Software;Security;Continents;Software engineering,blockchains;contracts;knowledge management;organisational aspects,smart contract security;study uncovers practitioners;security efforts;smart contract development;security incidents;security issues;blockchain platforms,10
591,Social Equality and Fairness 1,AID: An Automated Detector for Gender-Inclusivity Bugs in OSS Project Pages,A. Chatterjee; M. Guizani; C. Stevens; J. Emard; M. E. May; M. Burnett; I. Ahmed,"Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; Oregon State University, Corvallis, OR, USA; University of California, Irvine, Irvine, CA",2021,"The tools and infrastructure used in tech, including Open Source Software (OSS), can embed ""inclusivity bugs""- features that disproportionately disadvantage particular groups of contributors. To see whether OSS developers have existing practices to ward off such bugs, we surveyed 266 OSS developers. Our results show that a majority (77%) of developers do not use any inclusivity practices, and 92% of respondents cited a lack of concrete resources to enable them to do so. To help fill this gap, this paper introduces AID, a tool that automates the GenderMag method to systematically find gender-inclusivity bugs in software. We then present the results of the tool's evaluation on 20 GitHub projects. The tool achieved precision of 0.69, recall of 0.92, an F-measure of 0.79 and even captured some inclusivity bugs that human GenderMag teams missed.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402060,Gender inclusivity;automation;open source;information processing,Computer bugs;Detectors;Tools;Open source software;Software engineering;Software development management,gender issues;project management;public domain software;social aspects of automation;software engineering,GitHub projects;automated detector;gender-inclusivity bugs;OSS project pages;open source software;OSS developers;inclusivity practices;GenderMag method,9
592,Social Equality and Fairness 2,"""Ignorance and Prejudice"" in Software Fairness",J. M. Zhang; M. Harman,"University College London, London, UK; University College London, London, UK",2021,"Machine learning software can be unfair when making human-related decisions, having prejudices over certain groups of people. Existing work primarily focuses on proposing fairness metrics and presenting fairness improvement approaches. It remains unclear how key aspect of any machine learning system, such as feature set and training data, affect fairness. This paper presents results from a comprehensive study that addresses this problem. We find that enlarging the feature set plays a significant role in fairness (with an average effect rate of 38%). Importantly, and contrary to widely-held beliefs that greater fairness often corresponds to lower accuracy, our findings reveal that an enlarged feature set has both higher accuracy and fairness. Perhaps also surprisingly, we find that a larger training data does not help to improve fairness. Our results suggest a larger training data set has more unfairness than a smaller one when feature sets are insufficient; an important cautionary finding for practising software engineers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402057,software fairness;machine learning fairness,Measurement;Training data;Psychology;Machine learning;Software;Software engineering,learning (artificial intelligence);software engineering,fairness improvement approaches;fairness metrics;machine learning software;software fairness;ignorance prejudice,16
593,Software Log Analysis,Semi-Supervised Log-Based Anomaly Detection via Probabilistic Label Estimation,L. Yang; J. Chen; Z. Wang; W. Wang; J. Jiang; X. Dong; W. Zhang,"College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; Information and Network Center, Tianjin University, Tianjin, China; Information and Network Center, Tianjin University, Tianjin, China",2021,"With the growth of software systems, logs have become an important data to aid system maintenance. Log-based anomaly detection is one of the most important methods for such purpose, which aims to automatically detect system anomalies via log analysis. However, existing log-based anomaly detection approaches still suffer from practical issues due to either depending on a large amount of manually labeled training data (supervised approaches) or unsatisfactory performance without learning the knowledge on historical anomalies (unsupervised and semi-supervised approaches). In this paper, we propose a novel practical log-based anomaly detection approach, PLELog, which is semi-supervised to get rid of time-consuming manual labeling and incorporates the knowledge on historical anomalies via probabilistic label estimation to bring supervised approaches' superiority into play. In addition, PLELog is able to stay immune to unstable log data via semantic embedding and detect anomalies efficiently and effectively by designing an attention-based GRU neural network. We evaluated PLELog on two most widely-used public datasets, and the results demonstrate the effectiveness of PLELog, significantly outperforming the compared approaches with an average of 181.6% improvement in terms of F1-score. In particular, PLELog has been applied to two real-world systems from our university and a large corporation, further demonstrating its practicability",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401970,Log Analysis;Anomaly Detection;Deep Learning;Probabilistic Estimation;Label,Semantics;Neural networks;Estimation;Training data;Manuals;Probabilistic logic;Anomaly detection,probability;recurrent neural nets;supervised learning;system monitoring,real-world systems;semisupervised log-based anomaly detection;probabilistic label estimation;software systems;system maintenance;system anomalies;log analysis;manually labeled training data;PLELog;time-consuming manual labeling;unstable log data;attention-based GRU neural network;practical log-based anomaly detection approach,48
594,Software Log Analysis,DeepLV: Suggesting Log Levels Using Ordinal Based Neural Networks,Z. Li; H. Li; T. -H. Chen; W. Shang,"Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Engineering and Software Engineering, Polytechnique MontrÃ©al, Montreal, Canada; Concordia University, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada",2021,"Developers write logging statements to generate logs that provide valuable runtime information for debugging and maintenance of software systems. Log level is an important component of a logging statement, which enables developers to control the information to be generated at system runtime. However, due to the complexity of software systems and their runtime behaviors, deciding a proper log level for a logging statement is a challenging task. For example, choosing a higher level (e.g., error) for a trivial event may confuse end users and increase system maintenance overhead, while choosing a lower level (e.g., trace) for a critical event may prevent the important execution information to be conveyed opportunely. In this paper, we tackle the challenge by first conducting a preliminary manual study on the characteristics of log levels. We find that the syntactic context of the logging statement and the message to be logged might be related to the decision of log levels, and log levels that are further apart in order (e.g., trace and error) tend to have more differences in their characteristics. Based on this, we then propose a deep-learning based approach that can leverage the ordinal nature of log levels to make suggestions on choosing log levels, by using the syntactic context and message features of the logging statements extracted from the source code. Through an evaluation on nine large-scale open source projects, we find that: 1) our approach outperforms the state-of-the-art baseline approaches; 2) we can further improve the performance of our approach by enlarging the training data obtained from other systems; 3) our approach also achieves promising results on cross-system suggestions that are even better than the baseline approaches on within-system suggestions. Our study highlights the potentials in suggesting log levels to help developers make informed logging decisions.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402068,logs;deep learning;log level;empirical study,Runtime;Manuals;Syntactics;Maintenance engineering;Software systems;Feature extraction;Task analysis,learning (artificial intelligence);program debugging;public domain software;software maintenance,logging statement;software systems;informed logging decisions;DeepLV;ordinal based neural networks;log levels;deep-learning based approach;large-scale open source projects;valuable runtime information;software maintenance,19
595,Software Requirements,How to Identify Boundary Conditions with Contrasty Metric?,W. Luo; H. Wan; X. Song; B. Yang; H. Zhong; Y. Chen,"School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-sen University, Guangzhou, China; School of Computer Science, South China Normal University, Guangzhou, China",2021,"The boundary conditions (BCs) have shown great potential in requirements engineering because a BC captures the particular combination of circumstances, i.e., divergence, in which the goals of the requirement cannot be satisfied as a whole. Existing researches have attempted to automatically identify lots of BCs. Unfortunately, a large number of identified BCs make assessing and resolving divergences expensive. Existing methods adopt a coarse-grained metric, generality, to filter out less general BCs. However, the results still retain a large number of redundant BCs since a general BC potentially captures redundant circumstances that do not lead to a divergence. Furthermore, the likelihood of BC can be misled by redundant BCs resulting in costly repeatedly assessing and resolving divergences. In this paper, we present a fine-grained metric to filter out the redundant BCs. We first introduce the concept of contrasty of BC. Intuitively, if two BCs are contrastive, they capture different divergences. We argue that a set of contrastive BCs should be recommended to engineers, rather than a set of general BCs that potentially only indicates the same divergence. Then we design a post-processing framework (PPFc) to produce a set of contrastive BCs after identifying BCs. Experimental results show that the contrasty metric dramatically reduces the number of BCs recommended to engineers. Results also demonstrate that lots of BCs identified by the state-of-the-art method are redundant in most cases. Besides, to improve efficiency, we propose a joint framework (JFc) to interleave assessing based on the contrasty metric with identifying BCs. The primary intuition behind JFc is that it considers the search bias toward contrastive BCs during identifying BCs, thereby pruning the BCs capturing the same divergence. Experiments confirm the improvements of JFc in identifying contrastive BCs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401976,Goal Oriented Requirement Engineering;Boundary Conditions;Goal Conflict Identification,Measurement;Boundary conditions;Requirements engineering;Software engineering,formal specification;search problems;software metrics;software performance evaluation;statistical analysis,identify boundary conditions;contrasty metric;identified BCs;coarse-grained metric generality;general BC;redundant BCs;contrastive BCs,1
596,Software Requirements,Using Domain-Specific Corpora for Improved Handling of Ambiguity in Requirements,S. Ezzini; S. Abualhaija; C. Arora; M. Sabetzadeh; L. C. Briand,"SnT Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; SnT Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Deakin University, Geelong, Australia; School of Electrical Engineering and Computer Science, University of Ottawa, Canada; SnT Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg",2021,"Ambiguity in natural-language requirements is a pervasive issue that has been studied by the requirements engineering community for more than two decades. A fully manual approach for addressing ambiguity in requirements is tedious and time-consuming, and may further overlook unacknowledged ambiguity â€“ the situation where different stakeholders perceive a requirement as unambiguous but, in reality, interpret the requirement differently. In this paper, we propose an automated approach that uses natural language processing for handling ambiguity in requirements. Our approach is based on the automatic generation of a domain-specific corpus from Wikipedia. Integrating domain knowledge, as we show in our evaluation, leads to a significant positive improvement in the accuracy of ambiguity detection and interpretation. We scope our work to coordination ambiguity (CA) and prepositional-phrase attachment ambiguity (PAA) because of the prevalence of these types of ambiguity in natural-language requirements [1]. We evaluate our approach on 20 industrial requirements documents. These documents collectively contain more than 5000 requirements from seven distinct application domains. Over this dataset, our approach detects CA and PAA with an average precision of 80% and an average recall of 89% (90% for cases of unacknowledged ambiguity). The automatic interpretations that our approach yields have an average accuracy of 85%. Compared to baselines that use generic corpora, our approach, which uses domain-specific corpora, has 33% better accuracy in ambiguity detection and 16% better accuracy in interpretation.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402055,Requirements Engineering;Natural-language Requirements;Ambiguity;Natural Language Processing;Corpus Generation;Wikipedia,Electronic publishing;Encyclopedias;Software;Internet;Stakeholders;Requirements engineering;Software engineering,formal specification;natural language processing;natural languages;systems analysis;text analysis,20 industrial requirements documents;prepositional-phrase attachment ambiguity;natural language processing;automated approach;fully manual approach;requirements engineering community;natural-language requirements;improved handling;ambiguity detection;domain-specific corpora;approach yields;unacknowledged ambiguity,15
597,Source Code Histories and Documentations,On Indirectly Dependent Documentation in the Context of Code Evolution: A Study,D. Sondhi; A. Gupta; S. Purandare; A. Rana; D. Kaushal; R. Purandare,"IIIT-Delhi, New Delhi, India; IIIT-Delhi, New Delhi, India; IIIT-Delhi, New Delhi, India; IIIT-Delhi, New Delhi, India; IIIT-Delhi, New Delhi, India; IIIT-Delhi, New Delhi, India",2021,"A software system evolves over time due to factors such as bug-fixes, enhancements, optimizations and deprecation. As entities interact in a software repository, the alterations made at one point may require the changes to be reflected at various other points to maintain consistency. However, often less attention is given to making appropriate changes to the documentation associated with the functions. Inconsistent documentation is undesirable, since documentation serves as a useful source of information about the functionality. This paper presents a study on the prevalence of function documentations that are indirectly or implicitly dependent on entities other than the associated function. We observe a substantial presence of such documentations, with 62% of the studied Javadoc comments being dependent on other entities, as studied in 11 open-source repositories implemented in Java. We comprehensively analyze the nature of documentation updates made in 1288 commit logs and study patterns to reason about the cause of dependency in the documentation. Our findings from the observed patterns may be applied to suggest documentations that should be updated on making a change in the repository.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401958,code evolution;GitHub repositories;documentation;commits,Target tracking;Documentation;Feature extraction;Software systems;Open source software;Optimization;Software engineering,document handling;Java;program debugging;public domain software;software maintenance;system documentation,documentation updates;indirectly dependent documentation;software system;optimizations;deprecation;entities interact;software repository;appropriate changes;inconsistent documentation;function documentations;associated function;studied Javadoc comments;11 open-source repositories,
598,Source Code Histories and Documentations,CodeShovel: Constructing Method-Level Source Code Histories,F. Grund; S. A. Chowdhury; N. C. Bradley; B. Hall; R. Holmes,"Department of Computer Science, University of British Columbia, Vancouver, Canada; University of British Columbia; Department of Computer Science, University of British Columbia, Vancouver, Canada; Department of Computer Science, University of British Columbia, Vancouver, Canada; Department of Computer Science, University of British Columbia, Vancouver, Canada",2021,"Source code histories are commonly used by developers and researchers to reason about how software evolves. Through a survey with 42 professional software developers, we learned that developers face significant mismatches between the output provided by developers' existing tools for examining source code histories and what they need to successfully complete their historical analysis tasks. To address these shortcomings, we propose CodeShovel, a tool for uncovering method histories that quickly produces complete and accurate change histories for 90% methods (including 97% of all method changes) outperforming leading tools from both research (e.g, FinerGit) and practice (e.g., IntelliJ / git log). CodeShovel helps developers to navigate the entire history of source code methods so they can better understand how the method evolved. A field study on industrial code bases with 16 industrial developers confirmed our empirical findings of CodeShovel's correctness, low runtime overheads, and additionally showed that the approach can be useful for a wide range of industrial development tasks.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402063,source code history;maintenance;history slicing,Runtime;Navigation;Tools;History;Task analysis;Faces;Open source software,history;program compilers;program testing;software engineering;source code (software),method-level source code;source code histories;professional software developers;uncovering method histories;source code methods;industrial code bases;industrial development tasks;CodeShovel correctness,8
599,Testing: 3rd Party Software,Evaluating Unit Testing Practices in R Packages,M. Vidoni,"School of Computing Technologies, RMIT University, Melbourne, Australia",2021,"Testing Technical Debt (TTD) occurs due to shortcuts (non-optimal decisions) taken about testing; it is the test dimension of technical debt. R is a package-based programming ecosystem that provides an easy way to install third-party code, datasets, tests, documentation and examples. This structure makes it especially vulnerable to TTD because errors present in a package can transitively affect all packages and scripts that depend on it. Thus, TTD can effectively become a threat to the validity of all analysis written in R that rely on potentially faulty code. This two-part study provides the first analysis in this area. First, 177 systematically-selected, open-source R packages were mined and analysed to address quality of testing, testing goals, and identify potential TTD sources. Second, a survey addressed how R package developers perceive testing and face its challenges (response rate of 19.4%). Results show that testing in R packages is of low quality; the most common smells are inadequate and obscure unit testing, improper asserts, inexperienced testers and improper test design. Furthermore, skilled R developers still face challenges such as time constraints, emphasis on development rather than testing, poor tool documentation and a steep learning curve.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402000,R programming;mining software repositories;testing technical debt;unit testing;developers survey,Documentation;Programming;Tools;Time factors;Faces;Open source software;Testing,program testing;software fault tolerance;software maintenance;software packages,R packages;obscure unit testing;improper test design;unit testing practices;Testing Technical Debt;nonoptimal decisions;test dimension;package-based programming ecosystem;third-party code;documentation;potentially faulty code;testing goals;potential TTD sources,4
600,Testing: 3rd Party Software,Data-Oriented Differential Testing of Object-Relational Mapping Systems,T. Sotiropoulos; S. Chaliasos; V. Atlidakis; D. Mitropoulos; D. Spinellis,"Athens University of Economics and Business, Athens, Greece; Athens University of Economics and Business, Athens, Greece; Columbia University, New York, USA; National Infrastructures for Research and Technology - GRNET, Athens University of Economics and Business, Athens, Greeece; Athens University of Economics and Business, Athens, Greece",2021,"We introduce, what is to the best of our knowledge, the first approach for systematically testing Object-Relational Mapping (ORM) systems. Our approach leverages differential testing to establish a test oracle for ORM-specific bugs. Specifically, we first generate random relational database schemas, set up the respective databases, and then, we query these databases using the APIs of the ORM systems under test. To tackle the challenge that ORMs lack a common input language, we generate queries written in an abstract query language. These abstract queries are translated into concrete, executable ORM queries, which are ultimately used to differentially test the correctness of target implementations. The effectiveness of our method heavily relies on the data inserted to the underlying databases. Therefore, we employ a solver-based approach for producing targeted database records with respect to the constraints of the generated queries. We implement our approach as a tool, called CYNTHIA, which found 28 bugs in five popular ORM systems. The vast majority of these bugs are confirmed (25 / 28), more than half were fixed (20 / 28), and three were marked as release blockers by the corresponding developers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401963,Object-Relational Mapping;Differential Testing;Automated Testing,Computer bugs;Relational databases;Tools;Open source software;Testing;Software engineering;IEEE transactions,application program interfaces;program debugging;program testing;query languages;relational databases,executable ORM queries;target implementations;underlying databases;solver-based approach;targeted database records;generated queries;data-oriented differential testing;test oracle;ORM-specific bugs;random relational database schemas;respective databases;common input language;abstract query language;concrete ORM queries;ORM systems;object-relational mapping systems,3
601,Testing: Automatic Test Generation,Automatic Unit Test Generation for Machine Learning Libraries: How Far Are We?,S. Wang; N. Shrestha; A. K. Subburaman; J. Wang; M. Wei; N. Nagappan,"York University, Toronto, Canada; York University, Toronto, Canada; York University, Toronto, Canada; Chinese Academy of Sciences, Beijing, China; York University, Toronto, Canada; Microsoft Research, Redmond, USA",2021,"Automatic unit test generation that explores the input space and produces effective test cases for given programs have been studied for decades. Many unit test generation tools that can help generate unit test cases with high structural coverage over a program have been examined. However, the fact that existing test generation tools are mainly evaluated on general software programs calls into question about its practical effectiveness and usefulness for machine learning libraries, which are statistically orientated and have fundamentally different nature and construction from general software projects. In this paper, we set out to investigate the effectiveness of existing unit test generation techniques on machine learning libraries. To investigate this issue, we conducted an empirical study on five widely used machine learning libraries with two popular unit testcase generation tools, i.e., EVOSUITE and Randoop. We find that (1) most of the machine learning libraries do not maintain a high-quality unit test suite regarding commonly applied quality metrics such as code coverage (on average is 34.1%) and mutation score (on average is 21.3%), (2) unit test case generation tools, i.e., EVOSUITE and Randoop, lead to clear improvements in code coverage and mutation score, however, the improvement is limited, and (3) there exist common patterns in the uncovered code across the five machine learning libraries that can be used to improve unit test case generation tasks.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402041,empirical software engineering;test case generation;testing machine learning libraries,Machine learning;Tools;Libraries;Software;Space exploration;Test pattern generators;Task analysis,learning (artificial intelligence);program testing,general software programs;machine learning libraries;high-quality unit test suite;automatic unit test generation,6
602,Testing: Automation,Layout and Image Recognition Driving Cross-Platform Automated Mobile Testing,S. Yu; C. Fang; Y. Yun; Y. Feng,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",2021,"The fragmentation problem has extended from Android to different platforms, such as iOS, mobile web, and even mini-programs within some applications (app), like WeChat. In such a situation, recording and replaying test scripts is one of the most popular automated mobile app testing approaches. However, such approach encounters severe problems when crossing platforms. Different versions of the same app need to be developed to support different platforms relying on different platform supports. Therefore, mobile app developers need to develop and maintain test scripts for multiple platforms aimed at completely the same test requirements, greatly increasing testing costs. However, we discover that developers adopt highly similar user interface layouts for versions of the same app on different platforms. Such a phenomenon inspires us to replay test scripts from the perspective of similar UI layouts. In this paper, we propose an image-driven mobile app testing framework, utilizing Widget Feature Matching and Layout Characterization Matching to analyze app UIs. We use computer vision (CV) technologies to perform UI feature comparison and layout hierarchy extraction on mobile app screenshots to obtain UI structures containing rich contextual information of app widgets, including coordinates, relative relationship, etc. Based on acquired UI structures, we can form a platform-independent test script, and then locate the target widgets under test. Thus, the proposed framework non-intrusively replays test scripts according to a novel platform-independent test script model. We also design and implement a tool named LIRAT to devote the proposed framework into practice, based on which, we conduct an empirical study to evaluate the effectiveness and usability of the proposed testing framework. The results show that the overall replay accuracy reaches around 65.85% on Android (8.74% improvement over state-of-the-art approaches) and 35.26% on iOS (35% improvement over state-of-the-art approaches).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401983,Cross-Platform Testing;Mobile Testing;Image Analysis;Record and Replay,Layout;User interfaces;Tools;Feature extraction;Mobile applications;Usability;Testing,computer vision;graphical user interfaces;image recognition;mobile computing;program testing;software tools;user interfaces,mobile testing;mobile web;applications;replaying test scripts;popular automated mobile app testing approaches;crossing platforms;app need;different platform supports;mobile app developers;testing costs;highly similar user interface layouts;similar UI layouts;image-driven mobile app testing framework;Widget Feature Matching;Layout Characterization Matching;app UIs;layout hierarchy extraction;mobile app screenshots;app widgets;acquired UI structures;novel platform-independent test script model,7
603,Testing: Flaky Tests,FlakeFlagger: Predicting Flakiness Without Rerunning Tests,A. Alshammari; C. Morris; M. Hilton; J. Bell,"George Mason University, Fairfax, VA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Northeastern University, Boston, MA, USA",2021,"When developers make changes to their code, they typically run regression tests to detect if their recent changes (re) introduce any bugs. However, many tests are flaky, and their outcomes can change non-deterministically, failing without apparent cause. Flaky tests are a significant nuisance in the development process, since they make it more difficult for developers to trust the outcome of their tests, and hence, it is important to know which tests are flaky. The traditional approach to identify flaky tests is to rerun them multiple times: if a test is observed both passing and failing on the same code, it is definitely flaky. We conducted a very large empirical study looking for flaky tests by rerunning the test suites of 24 projects 10,000 times each, and found that even with this many reruns, some previously identified flaky tests were still not detected. We propose FlakeFlagger, a novel approach that collects a set of features describing the behavior of each test, and then predicts tests that are likely to be flaky based on similar behavioral features. We found that FlakeFlagger correctly labeled as flaky at least as many tests as a state-of-the-art flaky test classifier, but that FlakeFlagger reported far fewer false positives. This lower false positive rate translates directly to saved time for researchers and developers who use the classification result to guide more expensive flaky test detection processes. Evaluated on our dataset of 23 projects with flaky tests, FlakeFlagger outperformed the prior approach (by F1 score) on 16 projects and tied on 4 projects. Our results indicate that this approach can be effective for identifying likely flaky tests prior to running time-consuming flaky test detectors.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402098,regression testing;flaky tests;reliability,Computer bugs;Detectors;Testing;Software engineering,pattern classification;program debugging;program testing,regression tests;FlakeFlagger;running time-consuming flaky test detectors;flaky test classifier;flakiness prediction,12
604,Testing: Flaky Tests,An Empirical Analysis of UI-Based Flaky Tests,A. Romano; Z. Song; S. Grandhi; W. Yang; W. Wang,"University at Buffalo, SUNY; University of Texas at Dallas; University of Texas at Dallas; University of Texas at Dallas; University at Buffalo, SUNY",2021,"Flaky tests have gained attention from the research community in recent years and with good reason. These tests lead to wasted time and resources, and they reduce the reliability of the test suites and build systems they affect. However, most of the existing work on flaky tests focus exclusively on traditional unit tests. This work ignores UI tests that have larger input spaces and more diverse running conditions than traditional unit tests. In addition, UI tests tend to be more complex and resource-heavy, making them unsuited for detection techniques involving rerunning test suites multiple times. In this paper, we perform a study on flaky UI tests. We analyze 235 flaky UI test samples found in 62 projects from both web and Android environments. We identify the common underlying root causes of flakiness in the UI tests, the strategies used to manifest the flaky behavior, and the fixing strategies used to remedy flaky UI tests. The findings made in this work can provide a foundation for the development of detection and prevention techniques for flakiness arising in UI tests.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402129,flaky test;flaky UI test;web;Android,Reliability;Software engineering;Software development management,program testing;user interfaces,UI-based flaky tests;unit tests;Android environment;Web environment,15
605,Testing: General Issues,GenTree: Using Decision Trees to Learn Interactions for Configurable Software,K. Nguyen; T. Nguyen,"University of Nebraska-Lincoln, USA; University of Nebraska-Lincoln, USA",2021,"Modern software systems are increasingly designed to be highly configurable, which increases flexibility but can make programs harder to develop, test, and analyze, e.g., how configuration options are set to reach certain locations, what characterizes the configuration space of an interesting or buggy program behavior? We introduce GenTree, a new dynamic analysis that automatically learns a program's interactions - logical formulae that describe how configuration option settings map to code coverage. GenTree uses an iterative refinement approach that runs the program under a small sample of configurations to obtain coverage data; uses a custom classifying algorithm on these data to build decision trees representing interaction candidates; and then analyzes the trees to generate new configurations to further refine the trees and interactions in the next iteration. Our experiments on 17 configurable systems spanning 4 languages show that GenTree efficiently finds precise interactions using a tiny fraction of the configuration space.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401966,interaction;decision tree;gentree;interaction learning;highly configurable;software;coverage;iterative;interaction generation;features;line coverage;coreutils;software systems;c50;classifying;tree,Heuristic algorithms;Software systems;Iterative algorithms;Performance analysis;Classification algorithms;Decision trees;Software engineering,decision trees;iterative methods;learning (artificial intelligence);program debugging;program testing;software engineering;software maintenance;solid modelling;trees (mathematics),configuration options;configuration space;interesting buggy program behavior;GenTree;dynamic analysis;logical formulae;configuration option settings map;code coverage;iterative refinement approach;coverage data;decision trees;interaction candidates;17 configurable systems;precise interactions;configurable software;modern software systems,
606,Testing: General Issues,Semantic Web Accessibility Testing via Hierarchical Visual Analysis,M. Bajammal; A. Mesbah,"University of British Columbia, Canada; University of British Columbia, Canada",2021,"Web accessibility, the design of web apps to be usable by users with disabilities, impacts millions of people around the globe. Although accessibility has traditionally been a marginal afterthought that is often ignored in many software products, it is increasingly becoming a legal requirement that must be satisfied. While some web accessibility testing tools exist, most only perform rudimentary syntactical checks that do not assess the more important high-level semantic aspects that users with disabilities rely on. Accordingly, assessing web accessibility has largely remained a laborious manual process requiring human input. In this paper, we propose an approach, called AXERAY, that infers semantic groupings of various regions of a web page and their semantic roles. We evaluate our approach on 30 real-world websites and assess the accuracy of semantic inference as well as the ability to detect accessibility failures. The results show that AXERAY achieves, on average, an F-measure of 87% for inferring semantic groupings, and is able to detect accessibility failures with 85% accuracy.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402094,web accessibility;web testing;accessibility testing;visual analysis,Visualization;Law;Semantics;Web pages;Tools;Software;Testing,program testing;semantic Web,semantic inference;rudimentary syntactical checks;semantic Web accessibility testing;hierarchical visual analysis;AXERAY,8
607,Tools for the Python Language,Restoring Execution Environments of Jupyter Notebooks,J. Wang; L. Li; A. Zeller,"Faculty of Information Technology, Monash University, Melbourne, Australia; Faculty of Information Technology, Monash University, Melbourne, Australia; CISPA Helmholtz Center for Information Security, SaarbrÃ¼cken, Germany",2021,"More than ninety percent of published Jupyternotebooks do not state dependencies on external packages. This makes them non-executable and thus hinders reproducibility of scientific results. We present SnifferDog, an approach that1) collects the APIs of Python packages and versions, creating a database of APIs; 2) analyzes notebooks to determine candidates for required packages and versions; and 3) checks which packages are required to make the notebook executable(and ideally, reproduce its stored results). In its evaluation, we show thatSnifferDogprecisely restores execution environments for the largest majority of notebooks, making them immediately executable for end users.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402133,Jupyter Notebook;Environment;Python;API,Databases;Reproducibility of results;Software engineering;Python,application program interfaces;document handling;interactive systems;public domain software;software packages,jupyter notebooks;published Jupyternotebooks;external packages;SnifferDog;API;Python packages;notebook executable,17
608,Tools for the Python Language,PyART: Python API Recommendation in Real-Time,X. He; L. Xu; X. Zhang; R. Hao; Y. Feng; B. Xu,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",2021,"API recommendation in real-time is challenging for dynamic languages like Python. Many existing API recommendation techniques are highly effective, but they mainly support static languages. A few Python IDEs provide API recommendation functionalities based on type inference and training on a large corpus of Python libraries and third-party libraries. As such, they may fail to recommend or make poor recommendations when type information is missing or target APIs are project-specific. In this paper, we propose a novel approach, PyART, to recommend APIs for Python programs in real-time. It features a light-weight analysis to derives so-called optimistic data-flow, which is neither sound nor complete, but simulates the local data-flow information humans can derive. It extracts three kinds of features: data-flow, token similarity, and token co-occurrence, in the context of the program point where a recommendation is solicited. A predictive model is trained on these features using the Random Forest algorithm. Evaluation on 8 popular Python projects demonstrates that PyART can provide effective API recommendations. When historic commits can be leveraged, which is the target scenario of a state-of-the-art tool ARIREC, our average top-1 accuracy is over 50% and average top-10 accuracy over 70%, outperforming APIREC and Intellicode (i.e., the recommendation component in Visual Studio) by 28.48%-39.05% for top-1 accuracy and 24.41%-30.49% for top-10 accuracy. In other applications such as when historic comments are not available and cross-project recommendation, PyART also shows better overall performance. The time to make a recommendation is less than a second on average, satisfying the real-time requirement.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402054,API recommendation;context analysis;data flow analysis;real time recommendation;Python,Training;Visualization;Tools;Feature extraction;Real-time systems;Libraries;Python,application program interfaces;programming environments;Python;random forests;software libraries,PyART;Python API recommendation;dynamic languages;static languages;Python IDEs;API recommendation functionalities;type inference;Python libraries;third-party libraries;type information;Python programs;light-weight analysis;optimistic data-flow;Python projects;recommendation component;cross-project recommendation;real-time requirement;local data-flow information;ARIREC;APIREC;random forest algorithm,8
609,Tools for the Python Language,PyCG: Practical Call Graph Generation in Python,V. Salis; T. Sotiropoulos; P. Louridas; D. Spinellis; D. Mitropoulos,Athens University of Economics and Business; National Technical University of Athens; Athens University of Economics and Business; Athens University of Economics and Business; Athens University of Economics and Business; Athens University of Economics and Business; National Infrastructures for Research and Technology - GRNET,2021,"Call graphs play an important role in different contexts, such as profiling and vulnerability propagation analysis. Generating call graphs in an efficient manner can be a challenging task when it comes to high-level languages that are modular and incorporate dynamic features and higher-order functions. Despite the language's popularity, there have been very few tools aiming to generate call graphs for Python programs. Worse, these tools suffer from several effectiveness issues that limit their practicality in realistic programs. We propose a pragmatic, static approach for call graph generation in Python. We compute all assignment relations between program identifiers of functions, variables, classes, and modules through an inter-procedural analysis. Based on these assignment relations, we produce the resulting call graph by resolving all calls to potentially invoked functions. Notably, the underlying analysis is designed to be efficient and scalable, handling several Python features, such as modules, generators, function closures, and multiple inheritance. We have evaluated our prototype implementation, which we call PyCG, using two benchmarks: a micro-benchmark suite containing small Python programs and a set of macro-benchmarks with several popular real-world Python packages. Our results indicate that PyCG can efficiently handle thousands of lines of code in less than a second (0.38 seconds for 1k LoC on average). Further, it outperforms the state-of-the-art for Python in both precision and recall: PyCG achieves high rates of precision ~99.2% and adequate recall ~69.9%. Finally, we demonstrate how PyCG can aid dependency impact analysis by showcasing a potential enhancement to GitHub's ""security advisory"" notification service using a real-world example.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402076,Call Graph;Program Analysis;Inter procedural Analysis;Vulnerability Propagation,Prototypes;Tools;Benchmark testing;Task analysis;Standards;Python;Software engineering,data flow analysis;graph theory;inheritance;object-oriented programming;program compilers;public domain software;security of data,dependency impact analysis;real-world Python packages;microbenchmark suite;function closures;Python features;resulting call graph;inter-procedural analysis;assignment relations;realistic programs;Python programs;higher-order functions;dynamic features;high-level languages;vulnerability propagation analysis;practical call graph generation;PyCG,16
610,Variability and Product Lines,Seamless Variability Management with the Virtual Platform,W. Mahmood; D. StrÃ¼ber; T. Berger; R. LÃ¤mmel; M. Mukelabai,"Chalmers | University of Gothenburg, Sweden; Radboud University, Nijmegen, Netherlands; Chalmers | University of Gothenburg, Sweden; University of Koblenz-Landau, Mainz, Germany; Chalmers | University of Gothenburg, Sweden",2021,"Customization is a general trend in software engineering, demanding systems that support variable stakeholder requirements. Two opposing strategies are commonly used to create variants: software clone&own and software configuration with an integrated platform. Organizations often start with the former, which is cheap, agile, and supports quick innovation, but does not scale. The latter scales by establishing an integrated platform that shares software assets between variants, but requires high up-front investments or risky migration processes. So, could we have a method that allows an easy transition or even combine the benefits of both strategies? We propose a method and tool that supports a truly incremental development of variant rich systems, exploiting a spectrum between both opposing strategies. We design, formalize, and prototype the variability management framework virtual platform. It bridges clone&own and platform-oriented development. Relying on programming language independent conceptual structures representing software assets, it offers operators for engineering and evolving a system, comprising: traditional, asset-oriented operators and novel, feature-oriented operators for incrementally adopting concepts of an integrated platform. The operators record meta-data that is exploited by other operators to support the transition. Among others, they eliminate expensive feature-location effort or the need to trace clones. Our evaluation simulates the evolution of a real-world, clone-based system, measuring its costs and benefits.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401953,variability management;variant rich systems;feature location;change propagation;clone&own,Technological innovation;Cloning;Organizations;User interfaces;Tools;Software;Investment,formal specification;formal verification;programming languages;software engineering;software maintenance;systems analysis,feature oriented operators;platform oriented development;variability management framework;variant rich systems;truly incremental development;risky migration processes;software assets;software configuration;software clone;variable stakeholder requirements;software engineering;virtual platform;clone-based system;operators record meta-data;integrated platform;traditional asset-oriented operators,6
611,Vulnerabilities in Andriod 1,Fine with â€œ1234â€? An Analysis of SMS One-Time Password Randomness in Android Apps,S. Ma; J. Li; H. Kim; E. Bertino; S. Nepal; D. Ostry; C. Sun,The University of Queensland; Shanghai Jiao Tong University; Sungkyunwan University; Purdue University; Data61 CSIRO; Data61 CSIRO; Xidian University,2021,"A fundamental premise of SMS One-Time Password (OTP) is that the used pseudo-random numbers (PRNs) are uniquely unpredictable for each login session. Hence, the process of generating PRNs is the most critical step in the OTP authentication. An improper implementation of the pseudo-random number generator (PRNG) will result in predictable or even static OTP values, making them vulnerable to potential attacks. In this paper, we present a vulnerability study against PRNGs implemented for Android apps. A key challenge is that PRNGs are typically implemented on the server-side, and thus the source code is not accessible. To resolve this issue, we build an analysis tool, OTP-Lint, to assess implementations of the PRNGs in an automated manner without the source code requirement. Through reverse engineering, OTP-Lint identifies the apps using SMS OTP and triggers each app's login functionality to retrieve OTP values. It further assesses the randomness of the OTP values to identify vulnerable PRNGs. By analyzing 6,431 commercially used Android apps downloaded from Google Play and Tencent Myapp, OTP-Lint identified 399 vulnerable apps that generate predictable OTP values. Even worse, 194 vulnerable apps use the OTP authentication alone without any additional security mechanisms, leading to insecure authentication against guessing attacks and replay attacks.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402042,OTP Authentication Protocol;Mobile Application Security;Pseudo-Random Number Generator;Vulnerability Detection;Randomness Evaluation,Reverse engineering;Authentication;Tools;Prediction algorithms;Generators;Password;Software engineering,authorisation;cryptography;electronic messaging;mobile computing;random number generation;reverse engineering;smart phones;source code (software);telecommunication security,login session;PRNs;OTP authentication;improper implementation;pseudorandom number generator;predictable OTP values;even static OTP values;vulnerability study;Android apps;analysis tool;OTP-Lint;source code requirement;SMS OTP;SMS One-Time Password randomness;fundamental premise;pseudorandom numbers;vulnerable PRNG,4
612,Vulnerabilities in Andriod 1,Appâ€™s Auto-Login Function Security Testing via Android OS-Level Virtualization,W. Song; J. Ming; L. Jiang; H. Yan; Y. Xiang; Y. Chen; J. Fu; G. Peng,"Wuhan University, Wuhan, China; The University of Texas at Arlington, Arlington, TX, USA; Independent Researcher, Xian, China; Wuhan University, Wuhan, China; Wuhan University, Wuhan, China; Wuhan University, Wuhan, China; Wuhan University, Wuhan, China; Wuhan University, Wuhan, China",2021,"Limited by the small keyboard, most mobile apps support the automatic login feature for better user experience. Therefore, users avoid the inconvenience of retyping their ID and password when an app runs in the foreground again. However, this auto-login function can be exploited to launch the so-called ""data-clone attack"": once the locally-stored, auto-login depended data are cloned by attackers and placed into their own smartphones, attackers can break through the login-device number limit and log in to the victim's account stealthily. A natural countermeasure is to check the consistency of device-specific attributes. As long as the new device shows different device fingerprints with the previous one, the app will disable the auto-login function and thus prevent data-clone attacks. In this paper, we develop VPDroid, a transparent Android OS-level virtualization platform tailored for security testing. With VPDroid, security analysts can customize different device artifacts, such as CPU model, Android ID, and phone number, in a virtual phone without user-level API hooking. VPDroid's isolation mechanism ensures that user-mode apps in the virtual phone cannot detect device-specific discrepancies. To assess Android apps' susceptibility to the data-clone attack, we use VPDroid to simulate data-clone attacks with 234 most-downloaded apps. Our experiments on five different virtual phone environments show that VPDroid's device attribute customization can deceive all tested apps that perform device-consistency checks, such as Twitter, WeChat, and PayPal. 19 vendors have confirmed our report as a zero-day vulnerability. Our findings paint a cautionary tale: only enforcing a device-consistency check at client side is still vulnerable to an advanced data-clone attack.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9401988,,Social networking (online);User experience;Security;Virtualization;Smart phones;Testing;Software engineering,Android (operating system);application program interfaces;mobile computing;program testing;security of data;smart phones;social networking (online);user experience;virtualisation,mobile apps;automatic login feature;user experience;auto-login function;auto-login depended data;login-device number limit;device-specific attributes;different device fingerprints;transparent Android OS-level virtualization platform;security testing;Android ID;user-level API hooking;user-mode apps;device-specific discrepancies;Android apps;VPDroid's device attribute customization;tested apps;device-consistency check;advanced data-clone attack;device artifacts;virtual phone environments,1
613,Vulnerabilities in Andriod 1,ATVHunter: Reliable Version Detection of Third-Party Libraries for Vulnerability Identification in Android Applications,X. Zhan; L. Fan; S. Chen; F. We; T. Liu; X. Luo; Y. Liu,"Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; College of Cyber Science, Nankai University, China; College of Intelligence and Computing, Tianjin University, China; Nanyang Technological University, Singapore, Singapore; Faculty of Information Technology, Monash University, Australia; Department of Computing, The Hong Kong Polytechnic University, Hong Kong, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore",2021,"Third-party libraries (TPLs) as essential parts in the mobile ecosystem have become one of the most significant contributors to the huge success of Android, which facilitate the fast development of Android applications. Detecting TPLs in Android apps is also important for downstream tasks, such as malware and repackaged apps identification. To identify in-app TPLs, we need to solve several challenges, such as TPL dependency, code obfuscation, precise version representation. Unfortunately, existing TPL detection tools have been proved that they have not solved these challenges very well, let alone specify the exact TPL versions. To this end, we propose a system, named ATVHunter, which can pinpoint the precise vulnerable in-app TPL versions and provide detailed information about the vulnerabilities and TPLs. We propose a two-phase detection approach to identify specific TPL versions. Specifically, we extract the Control Flow Graphs as the coarse-grained feature to match potential TPLs in the pre-defined TPL database, and then extract opcode in each basic block of CFG as the fine-grained feature to identify the exact TPL versions. We build a comprehensive TPL database (189,545 unique TPLs with 3,006,676 versions) as the reference database. Meanwhile, to identify the vulnerable in-app TPL versions, we also construct a comprehensive and known vulnerable TPL database containing 1,180 CVEs and 224 security bugs. Experimental results show AtVHunter outperforms state-of-the-art TPL detection tools, achieving 90.55% precision and 88.79% recall with high efficiency, and is also resilient to widely-used obfuscation techniques and scalable for large-scale TPL detection. Furthermore, to investigate the ecosystem of the vulnerable TPLs used by apps, we exploit newtool to conduct a large-scale analysis on 104,446 apps and find that 9,050 apps include vulnerable TPL versions with 53,337 vulnerabilities and 7,480 security bugs, most of which are with high risks and are not recognized by app developers.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402031,Android;Third party libraries;vulnerability;version detection;ecosystem,Databases;Ecosystems;Computer bugs;Tools;Feature extraction;Libraries;Security,Android (operating system);flow graphs;program debugging;software libraries,third-party libraries;vulnerability identification;Android applications;in-app TPL versions;two-phase detection approach;AtVHunter;large-scale TPL detection;mobile ecosystem;control flow graph,27
614,Vulnerabilities in Andriod 2,JUSTGen: Effective Test Generation for Unspecified JNI Behaviors on JVMs,S. Hwang; S. Lee; J. Kim; S. Ryu,"School of Computing, KAIST, Daejeon, South Korea; Department of Computer Science and Engineering, Chungnam National University, Daejeon, South Korea; School of Computing, KAIST, Daejeon, South Korea; School of Computing, KAIST, Daejeon, South Korea",2021,"Java Native Interface (JNI) provides a way for Java applications to access native libraries, but it is difficult to develop correct JNI programs. By leveraging native code, the JNI enables Java developers to implement efficient applications and to reuse code written in other programming languages such as C and C++. Besides, the core Java libraries already use the JNI to provide system features like a graphical user interface. As a result, many mainstream Java Virtual Machines (JVMs) support the JNI. However, due to the complex interoperation semantics between different programming languages, implementing correct JNI programs is not trivial. Moreover, because of the performance overhead, JVMs do not validate erroneous JNI interoperations by default, but they validate them only when the debug feature, the -Xcheck:jni option, is enabled. Therefore, the correctness of JNI programs highly relies on the checks by the -Xcheck:jni option of JVMs. Questions remain, however, on the quality of the checks provided by the feature. Are there any properties that the -Xcheck:jni option fails to validate? If so, what potential issues can arise due to the lack of such validation? To the best of our knowledge, no research has explored these questions in-depth. In this paper, we empirically study the validation quality and impacts of the -Xcheck:jni option on mainstream JVMs using unspecified corner cases in the JNI specification. Such unspecified cases may lead to unexpected run-time behaviors because their semantics is not defined in the specification. For a systematic study, we propose JUSTGEN, a semi-automated approach to identify unspecified cases from a specification and generate test programs. JUSTGEN receives the JNI specification written in our domain specific language (DSL), and automatically discovers unspecified cases using an SMT solver. It then generates test programs that trigger the behaviors of unspecified cases. Using the generated tests, we empirically study the validation ability of the -Xcheck:jni option. Our experimental result shows that the JNI debug feature does not validate thousands of unspecified cases on JVMs, and they can cause critical run-time errors such as violation of the Java type system and memory corruption. We reported 792 unspecified cases that are not validated by JVMs to their corresponding JVM vendors. Among them, 563 cases have been fixed and the remaining cases will be fixed in near future. Based on our empirical study, we believe that the JNI specification should specify the semantics of the missing cases clearly and the debug feature should be supported completely.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9402099,Java Native Interface;Java Virtual Machine;Testing;Debugging;Empirical Study;Fuzzing,Java;Computer languages;Semantics;Computer bugs;Libraries;Virtual machining;DSL,,,
615,Not Mentioned,$\mu AFL$: Non-intrusive Feedback-driven Fuzzing for Microcontroller Firmware,W. Li; J. Shi; F. Li; J. Lin; W. Wang; L. Guan,"State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, School of Cyber Security, UCAS, Beijing, China; Department of Computer Science, The University of Georgia, Athens, Georgia, USA; Department of Electrical Engineering and Computer Science, The University of Kansas, Lawrence, Kansas, USA; School of Cyber Security, University of Science and Technology of China, Hefei, Anhui, China; State Key Laboratory of Information Security, Institute of Information Engineering, Chinese Academy of Sciences, Beijing, China; Department of Computer Science, The University of Georgia, Athens, Georgia, USA",2022,"Fuzzing is one of the most effective approaches to finding software flaws. However, applying it to microcontroller firmware incurs many challenges. For example, rehosting-based solutions cannot accurately model peripheral behaviors and thus cannot be used to fuzz the corresponding driver code. In this work, we present $\mu$ AFL, a hardware-in-the-loop approach to fuzzing microcontroller firmware. It leverages debugging tools in existing embedded system development to construct an AFL-compatible fuzzing framework. Specifically, we use the debug dongle to bridge the fuzzing environment on the PC and the target firmware on the microcontroller device. To collect code coverage information without costly code instrumentation, $\mu$ AFL relies on the ARM ETM hardware debugging feature, which transparently collects the instruction trace and streams the results to the PC. However, the raw ETM data is obscure and needs enormous computing resources to recover the actual instruction flow. We therefore propose an alternative representation of code coverage, which retains the same path sensitivity as the original AFL algorithm, but can directly work on the raw ETM data without matching them with disassembled instructions. To further reduce the workload, we use the DWT hardware feature to selectively collect runtime information of interest. We evaluated $\mu$ AFL on two real evaluation boards from two major vendors: NXP and STMicroelectronics. With our prototype, we discovered ten zero-day bugs in the driver code shipped with the SDK of STMicroelectronics and three zero-day bugs in the SDK of NXP. Eight CVEs have been allocated for them. Considering the wide adoption of vendor SDKs in real products, our results are alarming.",10.1145/3510003.3510208,firmware security;fuzzing;microcontroller;IoT;ETM,Codes;Embedded systems;Microcontrollers;Computer bugs;Prototypes;Fuzzing;Hardware,embedded systems;firmware;microcontrollers;peripheral interfaces;program debugging;program testing;program verification;security of data,rehosting-based solutions;corresponding driver code;$\mu$ AFL;hardware-in-the-loop approach;fuzzing microcontroller firmware;debugging tools;AFL-compatible fuzzing framework;fuzzing environment;target firmware;microcontroller device;code coverage information;costly code instrumentation;ARM ETM hardware debugging feature;raw ETM data;original AFL algorithm;DWT hardware feature;\mu AFL;nonintrusive feedback-driven fuzzing;finding software flaws,
616,Not Mentioned,A Grounded Theory Based Approach to Characterize Software Attack Surfaces,S. Moshtari; A. Okutan; M. Mirakhorli,"Rochester Institute of Technology, Rochester, NY, USA; Rochester Institute of Technology, Rochester, NY, USA; Rochester Institute of Technology, Rochester, NY, USA",2022,"The notion of Attack Surface refers to the critical points on the boundary of a software system which are accessible from outside or contain valuable content for attackers. The ability to identify attack surface components of software system has a significant role in effectiveness of vulnerability analysis approaches. Most prior works focus on vulnerability techniques that use an approximation of attack surfaces and there have not been many attempts to create a comprehensive list of attack surface components. Although limited number of studies have focused on attack surface analysis, they defined attack surface components based on project specific hypotheses to evaluate security risk of specific types of software applications. In this study, we leverage a qualitative analysis approach to empirically identify an extensive list of attack surface components. To this end, we conduct a Grounded Theory (GT) analysis on 1444 previously published vulnerability reports and weaknesses with a team of three software developers and security experts. We extract vulnerability information from two publicly available repositories: 1) Common Vulnerabilities and Exposures (CVE) and 2) Common Weakness Enumeration (CWE). We ask three key questions: where the attacks come from, what they target, and how they emerge, and to help answer these questions we define three core categories for attack surface components: Entry points, Targets, and Mechanisms. We extract attack surface concepts related to each category from collected vulnerability information using the GT analysis and provide a comprehensive categorization that represents attack surface components of software systems from various perspectives. The paper introduces 254 new attack surface components that did not exist in the literature. The comparison of the proposed attack surface model with prior works indicates that only 6.7% of the identified Code level attack surface components are studied before.",10.1145/3510003.3510210,Software Security;Attack Surface;Grounded Theory;Qualitative Analysis,Analytical models;Codes;Systematics;Bibliographies;Software systems;Security;Data mining,computer crime;program diagnostics;software engineering,attack surface model;software attack surfaces;software system;attack surface analysis;attack surface concepts;identified code level attack surface components;vulnerability techniques;project specific hypotheses;grounded theory analysis;GT;common vulnerabilities and exposures;CVE;common weakness enumeration;CWE;code level attack surface components,
617,Not Mentioned,A Grounded Theory of Coordination in Remote-First and Hybrid Software Teams,R. E. de Souza Santos; P. Ralph,"Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada; Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada",2022,"While the long-term effects of the COVID-19 pandemic on software professionals and organizations are difficult to predict, it seems likely that working from home, remote-first teams, distributed teams, and hybrid (part-remote/part-office) teams will be more common. It is therefore important to investigate the challenges that software teams and organizations face with new remote and hybrid work. Consequently, this paper reports a year-long, participant-observation, constructivist grounded theory study investigating the impact of working from home on software development. This study resulted in a theory of software team coordination. Briefly, shifting from in-office to at-home work fundamentally altered coordination within software teams. While group cohesion and more effective communication appear protective, coordination is under-mined by distrust, parenting and communication bricolage. Poor coordination leads to numerous problems including misunderstandings, help requests, lower job satisfaction among team members, and more illdefined tasks. These problems, in turn, reduce overall project success and prompt professionals to alter their software development processes (in this case, from Scrum to Kanban). Our findings suggest that software organizations with many remote employees can improve performance by encouraging greater engagement within teams and supporting employees with family and childcare responsibilities.",10.1145/3510003.3510105,software development;COVID-19;remote work;work-from-home;coordination;agile methods;grounded theory,COVID-19;South America;Industries;Pandemics;Lead;Market research;Software,human resource management;organisational aspects;personnel;project management;software development management;software engineering,software team coordination;at-home work;poor coordination;team members;software development processes;software organizations;remote employees;supporting employees;grounded theory;hybrid software teams;COVID-19 pandemic;software professionals;distributed teams;hybrid teams;hybrid work;year-long;theory study,2
618,Not Mentioned,A Scalable t-wise Coverage Estimator,E. Baranov; S. Chakraborty; A. Legay; K. S. Meel; V. N. Variyam,"Universite catholique de Louvain, Belgium; Indian Statistical Institute, India; Universite catholique de Louvain, Belgium; National University of Singapore, Singapore; University of Nebraska-Lincoln, USA",2022,"Owing to the pervasiveness of software in our modern lives, software systems have evolved to be highly configurable. Combinatorial testing has emerged as a dominant paradigm for testing highly configurable systems. Often constraints are employed to define the environments where a given system under test (SUT) is expected to work. Therefore, there has been a sustained interest in designing constraint-based test suite generation techniques. A significant goal of test suite generation techniques is to achieve $t$-wise coverage for higher values of $t$. Therefore, designing scalable techniques that can estimate $t$-wise coverage for a given set of tests and/or the estimation of maximum achievable $t$-wise coverage under a given set of constraints is of crucial importance. The existing estimation techniques face significant scalability hurdles. The primary scientific contribution of this work is the design of scalable algorithms with mathematical guarantees to estimate (i) $t$-wise coverage for a given set of tests, and (ii) maximum $t$-wise coverage for a given set of constraints. In particular, we design a scalable framework ApproxCov that takes in a test set $\mathcal{U}$, a coverage parameter $t$, a tolerance parameter $\varepsilon$, and a confidence parameter $\delta$, and returns an estimate of the t-wise coverage of $\mathcal{U}$ that is guaranteed to be within ($1\pm \varepsilon$) -factor of the ground truth with probability at least $1-\delta$. We design a scalable framework ApproxMaxCov that, for a given formula $\mathsf{F}$, a coverage parameter $t$, a tolerance parameter $\varepsilon$, and a confidence parameter $\delta$, outputs an approximation which is guaranteed to be within ($1\pm\varepsilon$) factor of the maximum achievable $t$-wise coverage under $\mathsf{F}$, with probability $\geq 1-\delta$. Our comprehensive evaluation demonstrates that ApproxCov and ApproxMaxCov can handle benchmarks that are beyond the reach of current state-of-the-art approaches. We believe that the availability of ApproxCov and ApproxMaxCov will enable test suite designers to evaluate the effectiveness of their generators and thereby significantly impact the development of combinatorial testing techniques.",10.1145/3510003.3510218,Configurable software;$t$-wise coverage;Approximation,Monte Carlo methods;Combinatorial testing;Scalability;Estimation;Benchmark testing;Software systems;Generators,probability;program testing,confidence parameter;combinatorial testing techniques;t-wise coverage estimator;software systems;constraint-based test suite generation techniques;system under test;SUT;ApproxMaxCov framework;coverage parameter;tolerance parameter;probability,
619,Not Mentioned,A Universal Data Augmentation Approach for Fault Localization,H. Xie; Y. Lei; M. Yan; Y. Yu; X. Xia; X. Mao,"School of Big Data & Software Engineering, Chongqing University, Chongqing, China; School of Big Data & Software Engineering, Chongqing University, Chongqing, China; School of Big Data & Software Engineering, Chongqing University, Chongqing, China; National University of Defense Technology, Changsha, China; Software Engineering Application Technology Lab, Huawei, China; National University of Defense Technology, Changsha, China",2022,"Data is the fuel to models, and it is still applicable in fault localization (FL). Many existing elaborate FL techniques take the code coverage matrix and failure vector as inputs, expecting the techniques could find the correlation between program entities and failures. However, the input data is high-dimensional and extremely imbalanced since the real-world programs are large in size and the number of failing test cases is much less than that of passing test cases, which are posing severe threats to the effectiveness of FL techniques. To overcome the limitations, we propose Aeneas, a universal data augmentation approach that generAtes synthesized failing test cases from reduced feature sace for more precise fault localization. Specifically, to improve the effectiveness of data augmentation, Aeneas applies a revised principal component analysis (PCA) first to generate reduced feature space for more concise representation of the original coverage matrix, which could also gain efficiency for data synthesis. Then, Aeneas handles the imbalanced data issue through generating synthesized failing test cases from the reduced feature space through conditional variational autoencoder (CVAE). To evaluate the effectiveness of Aeneas, we conduct large-scale experiments on 458 versions of 10 programs (from ManyBugs, SIR, and Defects4J) by six state-of-the-art FL techniques. The experimental results clearly show that Aeneas is statistically more effective than baselines, e.g., our approach can improve the six original methods by 89% on average under the Top-1 accuracy.",10.1145/3510003.3510136,Fault Localization;Imbalanced Data;Data Augmentation,Location awareness;Correlation;Codes;Pipelines;Feature extraction;Data models;Fuels,fault diagnosis;feature extraction;learning (artificial intelligence);matrix algebra;pattern classification;principal component analysis;program testing,universal data augmentation approach;existing elaborate FL techniques;code coverage matrix;failure vector;program entities;real-world programs;Aeneas;reduced feature sace;precise fault localization;reduced feature space;original coverage matrix;data synthesis;imbalanced data issue;state-of-the-art FL techniques,5
620,Not Mentioned,Adaptive Performance Anomaly Detection for Online Service Systems via Pattern Sketching,Z. Chen; J. Liu; Y. Su; H. Zhang; X. Ling; M. R. Lyu,"The Chinese University of Hong Kong, Hong Kong, China; The Chinese University of Hong Kong, Hong Kong, China; School of Software Engineering, Sun Yat-sen University, Zhuhai, China; The University of Newcastle, NSW, Australia; Yongqiang Yang Huawei Cloud BU, Beijing, China; The Chinese University of Hong Kong, Hong Kong, China",2022,"To ensure the performance of online service systems, their status is closely monitored with various software and system metrics. Performance anomalies represent the performance degradation issues (e.g., slow response) of the service systems. When performing anomaly detection over the metrics, existing methods often lack the merit of interpretability, which is vital for engineers and analysts to take remediation actions. Moreover, they are unable to effectively accommodate the ever-changing services in an online fashion. To address these limitations, in this paper, we propose ADSketch, an interpretable and adaptive performance anomaly detection approach based on pattern sketching. ADSketch achieves interpretability by identifying groups of anomalous metric patterns, which represent particular types of performance issues. The underlying issues can then be immediately recognized if similar patterns emerge again. In addition, an adaptive learning algorithm is designed to embrace unprecedented patterns induced by service updates or user behavior changes. The proposed approach is evaluated with public data as well as industrial data collected from a representative online service system in Huawei Cloud. The experimental results show that ADSketch outperforms state-of-the-art approaches by a significant margin, and demonstrate the effectiveness of the online algorithm in new pattern discovery. Furthermore, our approach has been successfully deployed in industrial practice.",10.1145/3510003.3510085,Cloud computing;performance anomaly detection;online learning,Measurement;Adaptive learning;Adaptation models;Time series analysis;Software algorithms;Production;Software,data handling;learning (artificial intelligence);object-oriented methods;object-oriented programming;software metrics;Web services,ADSketch;pattern discovery;adaptive performance anomaly detection;online service systems;pattern sketching;system metrics;performance degradation issues;interpretable performance anomaly detection;anomalous metric patterns;adaptive learning algorithm;unprecedented patterns;service updates;representative online service system,1
621,Not Mentioned,Adaptive Test Selection for Deep Neural Networks,X. Gao; Y. Feng; Y. Yin; Z. Liu; Z. Chen; B. Xu,"State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",2022,"Deep neural networks (DNN) have achieved tremendous development in the past decade. While many DNN-driven software applications have been deployed to solve various tasks, they could also produce incorrect behaviors and result in massive losses. To reveal the incorrect behaviors and improve the quality of DNN-driven applications, developers often need rich labeled data for the testing and optimization of DNN models. However, in practice, collecting diverse data from application scenarios and labeling them properly is often a highly expensive and time-consuming task. In this paper, we proposed an adaptive test selection method, namely ATS, for deep neural networks to alleviate this problem. ATS leverages the difference between the model outputs to measure the behavior diversity of DNN test data. And it aims at selecting a subset with diverse tests from a massive unlabelled dataset. We experiment ATS with four well-designed DNN models and four widely-used datasets in comparison with various kinds of neuron coverage (NC). The results demonstrate that ATS can significantly outperform all test selection methods in assessing both fault detection and model improvement capability of test suites. It is promising to save the data labeling and model retraining costs for deep neural networks.",1O.1145/3510003.3510232,deep learning testing;deep neural networks;adaptive random testing;test selection,Deep learning;Adaptation models;Adaptive systems;Computational modeling;Fault detection;Neural networks;Data models,deep learning (artificial intelligence);optimisation;program testing,optimization;DNN models;adaptive test selection method;ATS;deep neural networks;behavior diversity;DNN test data;diverse tests;test selection methods;test suites;data labeling;DNN-driven software applications;incorrect behaviors;DNN-driven applications;rich labeled data;neuron coverage;NC,
622,Not Mentioned,An Exploratory Study of Deep learning Supply Chain,X. Tan; K. Gao; M. Zhou; L. Zhang,"State Key Laboratory of Software Development Environment, School of Computer Science and Engineering Beihang University, Beijing, China; School of Software & Microelectronics, Peking University, Beijing, China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, School of Computer Science, Peking University, Beijing, China; State Key Laboratory of Software Development Environment, School of Computer Science and Engineering Beihang University, Beijing, China",2022,"Deep learning becomes the driving force behind many contemporary technologies and has been successfully applied in many fields. Through software dependencies, a multi-layer supply chain (SC) with a deep learning framework as the core and substantial down-stream projects as the periphery has gradually formed and is constantly developing. However, basic knowledge about the structure and characteristics of the SC is lacking, which hinders effective support for its sustainable development. Previous studies on software SC usually focus on the packages in different registries without paying attention to the SCs derived from a single project. We present an empirical study on two deep learning SCs: TensorFlow and PyTorch SCs. By constructing and analyzing their SCs, we aim to understand their structure, application domains, and evolutionary factors. We find that both SCs exhibit a short and sparse hierarchy structure. Overall, the relative growth of new projects increases month by month. Projects have a tendency to attract downstream projects shortly after the release of their packages, later the growth becomes faster and tends to stabilize. We propose three criteria to identify vulnerabilities and identify 51 types of packages and 26 types of projects involved in the two SCs. A comparison reveals their similarities and differences, e.g., TensorFlow SC provides a wealth of packages in experiment result analysis, while PyTorch SC contains more specific framework packages. By fitting the GAM model, we find that the number of dependent packages is significantly negatively associated with the number of downstream projects, but the relationship with the number of authors is nonlinear. Our findings can help further open the â€œblack boxâ€ of deep learning SCs and provide insights for their healthy and sustainable development.",10.1145/3510003.3510199,software supply chain;deep learning;open source;software structure;software evolution,Deep learning;Industries;Supply chains;Force;Fitting;Ecosystems;Software,manufacturing data processing;supply chain management;supply chains;sustainable development,sustainable development;deep learning supply chain;software dependencies;multilayer supply chain;deep learning framework;substantial down-stream projects;hierarchy structure;PyTorch SC;TensorFlow SC;deep learning SC,4
623,Not Mentioned,An Exploratory Study of Productivity Perceptions in Software Teams,A. Ruvimova; A. Lill; J. Gugler; L. Howe; E. Huang; G. Murphy; T. Fritz,"University of Zurich, Zurich, Switzerland; University of Zurich, Zurich, Switzerland; University of Zurich, Zurich, Switzerland; University of Zurich, Zurich, Switzerland; University of Zurich, Zurich, Switzerland; University of British Columbia, Vancouver, Canada; University of Zurich, Zurich, Switzerland",2022,"Software development is a collaborative process requiring a careful balance of focused individual effort and team coordination. Though questions of individual productivity have been widely examined in past literature, less is known about the interplay between developers' perceptions of their own productivity as opposed to their team's. In this paper, we present an analysis of 624 daily surveys and 2899 self-reports from 25 individuals across five software teams in North America and Europe, collected over the course of three months. We found that developers tend to operate in fluid team constructs, which impacts team awareness and complicates gauging team productivity. We also found that perceived individual productivity most strongly predicted perceived team productivity, even more than the amount of team interactions, unplanned work, and time spent in meetings. Future research should explore how fluid team structures impact individual and organizational productivity.",10.1145/3510003.3510081,Team;Productivity;Software Developer;User Study,Productivity;Fluids;Collaboration;Europe;Software;North America;Software engineering,groupware;project management;software development management;team working,complicates gauging team productivity;perceived individual productivity;strongly predicted perceived team productivity;team interactions;fluid team structures;organizational productivity;productivity perceptions;software teams;software development;collaborative process;careful balance;focused individual effort;developers;624 daily surveys;fluid team constructs;team awareness,1
624,Not Mentioned,Analyzing User Perspectives on Mobile App Privacy at Scale,P. Nema; P. Anthonysamy; N. Taft; S. T. Peddinti,"Google, Bangalore, India; Google, Zurich, Switzerland; Google, Mountain View, USA; Google, Mountain View, USA",2022,"In this paper we present a methodology to analyze usersâ€˜ con-cerns and perspectives about privacy at scale. We leverage NLP techniques to process millions of mobile app reviews and extract privacy concerns. Our methodology is composed of a binary clas-sifier that distinguishes between privacy and non-privacy related reviews. We use clustering to gather reviews that discuss similar privacy concerns, and employ summarization metrics to extract representative reviews to summarize each cluster. We apply our methods on 287M reviews for about 2M apps across the 29 cate-gories in Google Play to identify top privacy pain points in mobile apps. We identified approximately 440K privacy related reviews. We find that privacy related reviews occur in all 29 categories, with some issues arising across numerous app categories and other issues only surfacing in a small set of app categories. We show empirical evidence that confirms dominant privacy themes - concerns about apps requesting unnecessary permissions, collection of personal information, frustration with privacy controls, tracking and the selling of personal data. As far as we know, this is the first large scale analysis to confirm these findings based on hundreds of thousands of user inputs. We also observe some unexpected findings such as users warning each other not to install an app due to privacy issues, users uninstalling apps due to privacy reasons, as well as positive reviews that reward developers for privacy friendly apps. Finally we discuss the implications of our method and findings for developers and app stores.",10.1145/3510003.3510079,privacy;nlp;mobile apps;empirical,Privacy;Data privacy;Systematics;Pain;Ecosystems;Sociology;Mobile applications,data privacy;mobile computing;natural language processing;social aspects of automation,positive reviews;privacy friendly apps;app stores;user perspectives;mobile app privacy;mobile app reviews;nonprivacy related reviews;representative reviews;privacy pain points;privacy related reviews;app categories;privacy controls;dominant privacy themes;Google Play;NLP techniques,2
625,Not Mentioned,APER: Evolution-Aware Runtime Permission Misuse Detection for Android Apps,S. Wang; Y. Wang; X. Zhan; Y. Wang; Y. Liu; X. Luo; S. -C. Cheung,"Southern University of Science and Technology, Shenzhen, China; Northeastern University, Shenyang, China; The Hong Kong Polytechnic University, Hong Kong, China; Northeastern University, Shenyang, China; Southern University of Science and Technology, Shenzhen, China; The Hong Kong Polytechnic University, Hong Kong, China; The Hong Kong University of Science and Technology and Guangzhou HKUST Fok Ying Tung Research Institute, Hong Kong, China",2022,"The Android platform introduces the runtime permission model in version 6.0. The new model greatly improves data privacy and user experience, but brings new challenges for app developers. First, it allows users to freely revoke granted permissions. Hence, developers cannot assume that the permissions granted to an app would keep being granted. Instead, they should make their apps carefully check the permission status before invoking dangerous APIs. Second, the permission specification keeps evolving, bringing new types of compatibility issues into the ecosystem. To understand the impact of the challenges, we conducted an empirical study on 13,352 popular Google Play apps. We found that 86.0% apps used dangerous APIs asynchronously after permission management and 61.2% apps used evolving dangerous APIs. If an app does not properly handle permission revocations or platform differences, unexpected runtime issues may happen and even cause app crashes. We call such Android Runtime Permission issues as ARP bugs. Unfortunately, existing runtime permission issue detection tools cannot effectively deal with the ARP bugs induced by asynchronous permission management and permission specification evolution. To fill the gap, we designed a static analyzer, Aper, that performs reaching definition and dominator analysis on Android apps to detect the two types of ARP bugs. To compare Aper with existing tools, we built a benchmark, ARPFIX, from 60 real ARP bugs. Our experiment results show that Aper significantly outperforms two academic tools, ARPDROID and Revdroid, and an industrial tool, Lint, on ARPFIX, with an average improvement of 46.3% on F1-score. In addition, Aper successfully found 34 ARP bugs in 214 open-source Android apps, most of which can result in abnormal app behaviors (such as app crashes) according to our manual validation. We reported these bugs to the app developers. So far, 17 bugs have been confirmed and seven have been fixed.",10.1145/3510003.3510074,Android Runtime Permission;Compatibility Issues;Static Analysis,Data privacy;Runtime;Computer bugs;Ecosystems;Benchmark testing;User experience;Data models,Android (operating system);application program interfaces;data privacy;mobile computing;program debugging;program diagnostics;security of data;smart phones,APER;evolution-aware Runtime Permission misuse detection;Android platform;runtime permission model;data privacy;user experience;app developers;granted permissions;permission status;dangerous APIs;permission specification;compatibility issues;13 Google Play apps;352 popular Google Play apps;permission revocations;unexpected runtime issues;app crashes;Android Runtime Permission issues;runtime permission issue detection tools;asynchronous permission management;Aper;34 ARP bugs;214 open-source Android apps;abnormal app behaviors,
626,Not Mentioned,ARCLIN: Automated API Mention Resolution for Unformatted Texts,Y. Huo; Y. Su; H. Zhang; M. R. Lyu,"The Chinese University of Hong Kong, Hong Kong, China; School of Software Engineering, Sun Yat-sen University, Zhuhai, China; The Hong Kong University of Science and Technology, Hong Kong, China; The Chinese University of Hong Kong, Hong Kong, China",2022,"Online technical forums (e.g., StackOverflow) are popular platforms for developers to discuss technical problems such as how to use a specific Application Programming Interface (API), how to solve the programming tasks, or how to fix bugs in their code. These discussions can often provide auxiliary knowledge of how to use the software that is not covered by the official documents. The automatic extraction of such knowledge may support a set of down-stream tasks like API searching or indexing. However, unlike official documentation written by experts, discussions in open forums are made by regular developers who write in short and informal texts, including spelling errors or abbreviations. There are three major challenges for the accurate APIs recognition and linking mentioned APIs from unstructured natural language documents to an entry in the API repository: (1) distinguishing API mentions from common words; (2) identifying API mentions without a fully qualified name; and (3) disambiguating API mentions with similar method names but in a different library. In this paper, to tackle these challenges, we propose an ARCLIN tool, which can effectively distinguish and link APIs without using human annotations. Specifically, we first design an API recognizer to automatically extract API mentions from natural language sentences by a Conditional Random Field (CRF) on the top of a Bi-directional Long Short-Term Memory (Bi-LSTM) module, then we apply a context-aware scoring mechanism to compute the mention-entry similarity for each entry in an API repository. Compared to previous approaches with heuristic rules, our proposed tool without manual inspection outperforms by 8% in a high-quality dataset Py-mention, which contains 558 mentions and 2,830 sentences from five popular Python libraries. To our best knowledge, ARCLIN is the first approach to achieve full automation of API mention resolution from unformatted text without manually collected labels.",10.1145/3510003.3510158,API;API disambiguation;text mining,Training;Text recognition;Natural languages;Manuals;Programming;Libraries;Software,application program interfaces;natural language processing;Python;recurrent neural nets;software libraries;text analysis;ubiquitous computing,unstructured natural language documents;API repository;bi-directional long short-term memory;mention-entry similarity;high-quality dataset Py-mention;unformatted text;automated API mention resolution;online technical forums;application programming interface;official documentation;APIs recognition;ARCLIN tool;API mentions;natural language sentences;conditional random field;CRF;bi-LSTM;context-aware scoring mechanism;Python libraries,1
627,Not Mentioned,AST-Trans: Code Summarization with Efficient Tree-Structured Attention,Z. Tang; X. Shen; C. Li; J. Ge; L. Huang; Z. Zhu; B. Luo,"State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Alexa AI, Amazon, Berlin, Germany; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science, Southern Methodist University, Dallas, Texas, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",2022,"Code summarization aims to generate brief natural language descriptions for source codes. The state-of-the-art approaches follow a transformer-based encoder-decoder architecture. As the source code is highly structured and follows strict grammars, its Abstract Syntax Tree (AST) is widely used for encoding structural information. However, ASTs are much longer than the corresponding source code. Existing approaches ignore the size constraint and simply feed the whole linearized AST into the encoders. We argue that such a simple process makes it difficult to extract the truly useful dependency relations from the overlong input sequence. It also incurs significant computational overhead since each node needs to apply self-attention to all other nodes in the AST. To encode the AST more effectively and efficiently, we propose AST-Trans in this paper which exploits two types of node relationships in the AST: ancestor-descendant and sibling relationships. It applies the tree-structured attention to dynamically allocate weights for relevant nodes and exclude irrelevant nodes based on these two relationships. We further propose an efficient implementation to support fast parallel computation for tree-structure attention. On the two code summarization datasets, experimental results show that AST-Trans significantly outperforms the state-of-the-arts while being times more efficient than standard transformers 11All the codes and data are available at https://github.com/zetang94/ICSE2022_AST_Trans.git.",10.1145/3510003.3510224,tree-based neural network;source code summarization,Codes;Natural languages;Syntactics;Transformers;Computational efficiency;Grammar;Flow graphs,computational linguistics;decoding;encoding;grammars;image coding;natural language processing;query processing;tree data structures;trees (mathematics);XML,node relationships;tree-structure attention;code summarization datasets;AST-Trans;efficient Tree-structured attention;brief natural language;transformer-based encoder-decoder architecture;abstract Syntax Tree;structural information;corresponding source code;linearized AST;encoders,4
628,Not Mentioned,Automated Assertion Generation via Information Retrieval and Its Integration with Deep learning,H. Yu; Y. Lou; K. Sun; D. Ran; T. Xie; D. Hao; Y. Li; G. Li; Q. Wang,"School of Software and Microelectronics, Peking University, China; Department of Computer Science, Purdue University, USA; Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, China; Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, China; Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, China; Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, China; National Research Center of Software Engineering, Peking University, China; Key Laboratory of High Confidence Software Technologies (Peking University), Ministry of Education, China; Huawei Technologies CO., LTD., China",2022,"Unit testing could be used to validate the correctness of basic units of the software system under test. To reduce manual efforts in conducting unit testing, the research community has contributed with tools that automatically generate unit test cases, including test inputs and test oracles (e.g., assertions). Recently, ATLAS, a deep learning (DL) based approach, was proposed to generate assertions for a unit test based on other already written unit tests. Despite promising, the effectiveness of ATLAS is still limited. To improve the effectiveness, in this work, we make the first attempt to leverage Information Retrieval (IR) in assertion generation and propose an IR-based approach, including the technique of IR-based assertion retrieval and the technique of retrieved-assertion adaptation. In addition, we propose an integration approach to combine our IR-based approach with a DL-based approach (e.g., ATLAS) to further improve the effectiveness. Our experimental results show that our IR-based approach outperforms the state-of-the-art DL-based ap-proach, and integrating our IR-based approach with the DL-based approach can further achieve higher accuracy. Our results convey an important message that information retrieval could be competitive and worthwhile to pursue for software engineering tasks such as assertion generation, and should be seriously considered by the research community given that in recent years deep learning solutions have been over-popularly adopted by the research community for software engineering tasks.",10.1145/3510003.3510149,Unit Testing;Information Retrieval;Test Assertion;Deep Learning,Deep learning;Manuals;Information retrieval;Software systems;Task analysis;Software engineering;Testing,deep learning (artificial intelligence);information retrieval;program testing;software engineering,assertion generation;unit testing;basic units;research community;unit test cases;test inputs;test oracles;assertions;ATLAS;deep learning based approach;written unit tests;information retrieval;IR-based assertion retrieval;retrieved-assertion adaptation;integration approach;deep learning,2
629,Not Mentioned,Automated Detection of Password Leakage from Public GitHub Repositories,R. Feng; Z. Yan; S. Peng; Y. Zhang,"Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China",2022,"The prosperity of the GitHub community has raised new concerns about data security in public repositories. Practitioners who manage authentication secrets such as textual passwords and API keys in the source code may accidentally leave these texts in the public repositories, resulting in secret leakage. If such leakage in the source code can be automatically detected in time, potential damage would be avoided. With existing approaches focusing on detecting secrets with distinctive formats (e.g., API keys, cryptographic keys in PEM format), textual passwords, which are ubiquitously used for authentication, fall through the crack. Given that textual passwords could be virtually any strings, a naive detection scheme based on regular expression performs poorly. This paper presents PassFinder, an automated approach to effectively detecting password leakage from public repositories that involve various programming languages on a large scale. PassFinder utilizes deep neural networks to unveil the intrinsic characteristics of textual passwords and understand the semantics of the code snippets that use textual passwords for authentication, i.e., the contextual information of the passwords in the source code. Using this new technique, we performed the first large-scale and longitudinal analysis of password leakage on GitHub. We inspected newly uploaded public code files on GitHub for 75 days and found that password leakage is pervasive, affecting over sixty thousand repositories. Our work contributes to a better understanding of password leakage on GitHub, and we believe our technique could promote the security of the open-source ecosystem.",10.1145/3510003.3510150,Password;Mining Software Repositories;Deep Learning;GitHub,Codes;Semantics;Ecosystems;Neural networks;Authentication;Focusing;Passwords,application program interfaces;authorisation;computer network security;deep learning (artificial intelligence);message authentication;open systems,textual passwords;source code;password leakage;public code files;public GitHub repositories;authentication secrets;API keys;secret leakage;naive detection scheme;data security;PassFinder;deep neural networks;open-source ecosystem;time 75.0 d,3
630,Not Mentioned,Automated Handling of Anaphoric Ambiguity in Requirements: A Multi-solution Study,S. Ezzini; S. Abualhaija; C. Arora; M. Sabetzadeh,"University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; Deakin University, Australia; University of Ottawa, Canada",2022,"Ambiguity is a pervasive issue in natural-language requirements. A common source of ambiguity in requirements is when a pronoun is anaphoric. In requirements engineering, anaphoric ambiguity occurs when a pronoun can plausibly refer to different entities and thus be interpreted differently by different readers. In this paper, we develop an accurate and practical automated approach for handling anaphoric ambiguity in requirements, addressing both ambiguity detection and anaphora interpretation. In view of the multiple competing natural language processing (NLP) and machine learning (ML) technologies that one can utilize, we simultaneously pursue six alternative solutions, empirically assessing each using a col-lection of â‰ˆ1,350 industrial requirements. The alternative solution strategies that we consider are natural choices induced by the existing technologies; these choices frequently arise in other automation tasks involving natural-language requirements. A side-by-side em-pirical examination of these choices helps develop insights about the usefulness of different state-of-the-art NLP and ML technologies for addressing requirements engineering problems. For the ambigu-ity detection task, we observe that supervised ML outperforms both a large-scale language model, SpanBERT (a variant of BERT), as well as a solution assembled from off-the-shelf NLP coreference re-solvers. In contrast, for anaphora interpretation, SpanBERT yields the most accurate solution. In our evaluation, (1) the best solution for anaphoric ambiguity detection has an average precision of â‰ˆ60% and a recall of 100%, and (2) the best solution for anaphora interpretation (resolution) has an average success rate of â‰ˆ98%.",10.1145/3510003.3510157,Requirements Engineering;Natural-language Requirements;Ambiguity;Natural Language Processing (NLP);Machine Learning (ML);Language Models;BERT,Solution design;Automation;Bit error rate;Semantics;Machine learning;Natural language processing;Requirements engineering,data handling;learning (artificial intelligence);natural language processing;systems analysis,anaphora interpretation;natural-language requirements;NLP;requirements engineering problems;large-scale language model;anaphoric ambiguity detection;automated handling;multisolution study;anaphoric pronoun;natural language processing;machine learning;SpanBERT,3
631,Not Mentioned,Automated Patching for Unreproducible Builds,Z. Ren; S. Sun; J. Xuan; X. Li; Z. Zhou; H. Jiang,"Key Laboratory for Ubiquitous Network and Service Software of Liaoning Province, School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China; School of Computer Science, Wuhan University, Wuhan, China; University of Luxembourg, Luxembourg; School of Software, Dalian University of Technology, Dalian, China; School of Software, Dalian University of Technology, Dalian, China",2022,"Software reproducibility plays an essential role in establishing trust between source code and the built artifacts, by comparing compilation outputs acquired from independent users. Although the testing for unreproducible builds could be automated, fixing unreproducible build issues poses a set of challenges within the reproducible builds practice, among which we consider the localization granularity and the historical knowledge utilization as the most significant ones. To tackle these challenges, we propose a novel approach RepFix that combines tracing-based fine-grained localization with history-based patch generation mechanisms. On the one hand, to tackle the localization granularity challenge, we adopt system-level dynamic tracing to capture both the system call traces and user-space function call information. By integrating the kernel probes and user-space probes, we could determine the location of each executed build command more accurately. On the other hand, to tackle the historical knowledge utilization challenge, we design a similarity based relevant patch retrieving mechanism, and generate patches by applying the edit operations of the existing patches. With the abundant patches accumulated by the reproducible builds practice, we could generate patches to fix the unreproducible builds automatically. To evaluate the usefulness of RepFix, extensive experiments are conducted over a dataset with 116 real-world packages. Based on RepFix, we successfully fix the unreproducible build issues for 64 packages. Moreover, we apply RepFix to the Arch Linux packages, and successfully fix four packages. Two patches have been accepted by the repository, and there is one package for which the patch is pushed and accepted by its upstream repository, so that the fixing could be helpful for other downstream repositories.",10.1145/3510003.3510102,reproducible builds;dynamic tracing;automated patch generation,Location awareness;Codes;Linux;Computer bugs;Software;Reproducibility of results;Probes,configuration management;Internet;Linux;operating system kernels;program compilers;software packages,user-space function call information;user-space probes;historical knowledge utilization challenge;relevant patch retrieving mechanism;existing patches;abundant patches;reproducible;patching;unreproducible builds;software reproducibility;source code;built artifacts;independent users;fixing unreproducible build issues;approach RepFix;fine-grained localization;history-based patch generation mechanisms;localization granularity challenge;system-level dynamic;system call traces,2
632,Not Mentioned,Automated Testing of Software that Uses Machine Learning APIs,C. Wan; S. Liu; S. Xie; Y. Liu; H. Hoffmann; M. Maire; S. Lu,University of Chicago; University of Chicago; Whitney Young High School; University of Chicago; University of Chicago; University of Chicago; University of Chicago,2022,"An increasing number of software applications incorporate machine learning (ML) solutions for cognitive tasks that statistically mimic human behaviors. To test such software, tremendous human effort is needed to design image/text/audio inputs that are relevant to the software, and to judge whether the software is processing these inputs as most human beings do. Even when misbehavior is exposed, it is often unclear whether the culprit is inside the cognitive ML API or the code using the API. This paper presents Keeper, a new testing tool for software that uses cognitive ML APIs. Keeper designs a pseudo-inverse function for each ML API that reverses the corresponding cognitive task in an empirical way (e.g., an image search engine pseudo-reverses the image-classification API), and incorporates these pseudo-inverse functions into a symbolic execution engine to automatically gener-ate relevant image/text/audio inputs and judge output correctness. Once misbehavior is exposed, Keeper attempts to change how ML APIs are used in software to alleviate the misbehavior. Our evalu-ation on a variety of open-source applications shows that Keeper greatly improves the branch coverage, while identifying many pre-viously unknown bugs.",10.1145/3510003.3510068,software testing;machine learning;machine learning API,Codes;Computer bugs;Web pages;Machine learning;Search engines;Software;Test pattern generators,application program interfaces;cognition;image classification;learning (artificial intelligence);program debugging;program testing;search engines,automated testing;uses machine learning;software applications incorporate machine learning solutions;cognitive tasks;human behaviors;tremendous human effort;human beings;cognitive ML API;Keeper;testing tool;pseudoinverse function;corresponding cognitive task;image search engine;image-classification API,2
633,Not Mentioned,Automatic Detection of Performance Bugs in Database Systems using Equivalent Queries,X. Liu; Q. Zhou; J. Arulrai; A. Orso,"Georgia Institute of Technology, Atlanta, GA, USA; Meta, Seattle, WA, USA; Georgia Institute of Technology, Atlanta, GA, USA; Georgia Institute of Technology, Atlanta, GA, USA",2022,"Because modern data-intensive applications rely heavily on database systems (DBMSs), developers extensively test these systems to elim-inate bugs that negatively affect functionality. Besides functional bugs, however, there is another important class of faults that negatively affect the response time of a DBMS, known as performance bugs. Despite their potential impact on end-user experience, performance bugs have received considerably less attention than functional bugs. To fill this gap, we present Amoeba, a technique and tool for automatically detecting performance bugs in DBMSs. The core idea behind Amoeba is to construct semantically equivalent query pairs, run both queries on the DBMS under test, and compare their response time. If the queries exhibit significantly different response times, that indicates the possible presence of a performance bug in the DBMS. To construct equivalent queries, we propose to use a set of structure and expression mutation rules especially targeted at un-covering performance bugs. We also introduce feedback mechanisms for improving the effectiveness and efficiency of the approach. We evaluate Amoeba on two widely-used DBMSs, namely PostgreSQL and CockroachDB, with promising results: Amoeba has so far dis-covered 39 potential performance bugs, among which developers have already confirmed 6 bugs and fixed 5 bugs.",10.1145/3510003.3510093,Differential testing;database testing;query optimization,Computer bugs;Debugging;Database systems;Time factors;Optimization;Software engineering,database management systems;feedback;program debugging;query processing,performance bug;database systems;equivalent queries;functional bugs;DBMS;PostgreSQL;CockroachDB;feedback mechanism;expression mutation rules;Amoeba,4
634,Not Mentioned,AutoTransform: Automated Code Transformation to Support Modern Code Review Process,P. Thongtanunam; C. Pornprasit; C. Tantithamthavorn,"The University of Melbourne, Australia; Monash University, Australia; Monash University, Australia",2022,"Code review is effective, but human-intensive (e.g., developers need to manually modify source code until it is approved). Recently, prior work proposed a Neural Machine Translation (NMT) approach to automatically transform source code to the version that is reviewed and approved (i.e., the after version). Yet, its performance is still suboptimal when the after version has new identifiers or literals (e.g., renamed variables) or has many code tokens. To address these limitations, we propose AutoTransform which leverages a Byte-Pair Encoding (BPE) approach to handle new tokens and a Transformer-based NMT architecture to handle long sequences. We evaluate our approach based on 14,750 changed methods with and without new tokens for both small and medium sizes. The results show that when generating one candidate for the after version (i.e., beam width = 1), our AUTOTRANSFORM can correctly transform 1,413 changed methods, which is 567% higher than the prior work, highlighting the substantial improvement of our approach for code transformation in the context of code review. This work contributes towards automated code transformation for code reviews, which could help developers reduce their effort in modifying source code during the code review process.",10.1145/3510003.3510067,Code Reviews;Neural Machine Translation;Code Transformation,Codes;Transforms;Transformers;Encoding;Machine translation;Software engineering,autotransformers;data compression;encoding;language translation;program compilers;software maintenance;software quality;software tools;source coding,automated code transformation;modern code review process;source code;Neural Machine Translation approach;code tokens;Byte-Pair Encoding approach;Transformer-based NMT architecture;14 changed methods;750 changed methods;AUTOTRANSFORM;1 changed methods;413 changed methods;AutoTransform,2
635,Not Mentioned,BEDIVFUZZ: Integrating Behavioral Diversity into Generator-based Fuzzing,H. L. Nguyen; L. Grunske,"Humboldt-UniversitÃ¤t zu Berlin, Germany; Humboldt-UniversitÃ¤t zu Berlin, Germany",2022,"A popular metric to evaluate the performance of fuzzers is branch coverage. However, we argue that focusing solely on covering many different branches (i.e., the richness) is not sufficient since the majority of the covered branches may have been exercised only once, which does not inspire a high confidence in the reliability of the covered code. Instead, the distribution of the executed branches (i.e., the evenness) should also be considered. That is, behavioral diversity is only given if the generated inputs not only trigger many different branches, but also trigger them evenly often with diverse inputs. We introduce BEDIVFUZZ, a feedback-driven fuzzing technique for generator-based fuzzers. BEDIVFUZZ distinguishes between structure-preserving and structure-changing mutations in the space of syntactically valid inputs, and biases its mutation strategy towards validity and behavioral diversity based on the received program feedback. We have evaluated BEDIVFUZZ on Ant, Maven, Rhino, Closure, Nashorn, and Tomcat. The results show that BE-DIVFUZZ achieves better behavioral diversity than the state of the art, measured by established biodiversity metrics, namely the Hill numbers, from the field of ecology.",10.1145/3510003.3510182,Structure-aware fuzzing;behavioral diversity;random testing,Measurement;Codes;Focusing;Fuzzing;Ecology;Behavioral sciences;Reliability,ecology;fuzzy set theory;program testing,integrating behavioral diversity;branch coverage;different branches;covered branches;covered code;executed branches;generated inputs;diverse inputs;feedback-driven fuzzing technique;generator-based fuzzers;BEDIVFUZZ distinguishes;syntactically valid inputs;BE-DIVFUZZ,4
636,Not Mentioned,Big Data = Big Insights? Operationalising Brooks' Law in a Massive GitHub Data Set,C. Gote; P. Mavrodiev; F. Schweitzer; I. Scholtes,"Chair of Systems Design, ETH Zurich, Zurich, Switzerland; Chair of Systems Design, ETH Zurich, Zurich, Switzerland; Chair of Systems Design, ETH Zurich, Zurich, Switzerland; Chair of Computer Science XV - Machine Learning for Complex Networks, Julius-Maximilians-UniversitÃ¤t WÃ¼rzburg, WÃ¼rzburg, Germany",2022,"Massive data from software repositories and collaboration tools are widely used to study social aspects in software development. One question that several recent works have addressed is how a software project's size and structure influence team productivity, a question famously considered in Brooks' law. Recent studies using massive repository data suggest that developers in larger teams tend to be less productive than smaller teams. Despite using similar methods and data, other studies argue for a positive linear or even super-linear relationship between team size and productivity, thus contesting the view of software economics that software projects are diseconomies of scale. In our work, we study challenges that can explain the disagreement between recent studies of developer productivity in massive repository data. We further provide, to the best of our knowledge, the largest, curated corpus of GitHub projects tailored to investigate the influence of team size and collaboration patterns on individual and collective productivity. Our work contributes to the ongoing discussion on the choice of productivity metrics in the operationalisation of hypotheses about determinants of successful software projects. It further highlights general pitfalls in big data analysis and shows that the use of bigger data sets does not automatically lead to more reliable insights.",10.1145/3510003.3510619,,Productivity;Codes;Social sciences;Collaboration;Estimation;Big Data;Software,Big Data;data analysis;data mining;groupware;project management;public domain software;query processing;software development management;software engineering;software maintenance;team working;very large databases,big data = big insights;Brooks' law;massive GitHub data set;massive data;software repositories;collaboration tools;software development;software project;massive repository data;larger teams;smaller teams;super-linear relationship;team size;software economics;developer productivity;GitHub projects;collaboration patterns;individual productivity;collective productivity;productivity metrics;successful software projects;big data analysis;bigger data sets,
637,Not Mentioned,"Bots for Pull Requests: The Good, the Bad, and the Promising",M. Wessel; A. Abdellatif; I. Wiese; T. Conte; E. Shihab; M. A. Gerosa; I. Steinmacher,"Delft University of Technology, Netherlands; Concordia University, Canada; Universidade Tecnologica Federal do Parana, Brazil; Federal University of Amazonas, Brazil; Concordia University, Canada; Northern Arizona University, USA; Universidade Tecnologica Federal do Parana, Brazil",2022,"Software bots automate tasks within Open Source Software (OSS) projects' pull requests and save reviewing time and effort (â€œthe goodâ€). However, their interactions can be disruptive and noisy and lead to information overload (â€œthe badâ€). To identify strategies to overcome such problems, we applied Design Fiction as a participatory method with 32 practitioners. We elicited 22 design strategies for a bot mediator or the pull request user interface (â€œthe promisingâ€). Participants envisioned a separate place in the pull request interface for bot interactions and a bot mediator that can summarize and customize other bots' actions to mitigate noise. We also collected participants' perceptions about a prototype implementing the envisioned strategies. Our design strategies can guide the development of future bots and social coding platforms.",10.1145/3510003.3512765,Software Bots;GitHub Bots;Human-bot Interaction;Open Source Software;Automation;Collaborative Development;Design Fiction,Bot (Internet);Prototypes;User interfaces;Encoding;Noise measurement;Task analysis;Open source software,computer aided instruction;computer crime;groupware;Internet;project management;public domain software;user interfaces;Web sites,Design Fiction;participatory method;22 design strategies;bot mediator;pull request user interface;pull request interface;bot interactions;envisioned strategies;future bots;pull requests;bad;Software bots automate tasks;Open Source Software projects;reviewing time;information overload,6
638,Not Mentioned,Bridging Pre-trained Models and Downstream Tasks for Source Code Understanding,D. Wang; Z. Jia; S. Li; Y. Yu; Y. Xiong; W. Dong; X. Liao,"National University of Defense Technology, China; National University of Defense Technology, China; National University of Defense Technology, China; National University of Defense Technology, China; Fudan University, Shanghai, China; National University of Defense Technology, China; National University of Defense Technology, China",2022,"With the great success of pre-trained models, the pretrain-then-fine tune paradigm has been widely adopted on downstream tasks for source code understanding. However, compared to costly training a large-scale model from scratch, how to effectively adapt pre-trained models to a new task has not been fully explored. In this paper, we propose an approach to bridge pre-trained models and code-related tasks. We exploit semantic-preserving transformation to enrich downstream data diversity, and help pre-trained models learn semantic features invariant to these semantically equivalent transformations. Further, we introduce curriculum learning to or-ganize the transformed data in an easy-to-hard manner to fine-tune existing pre-trained models. We apply our approach to a range of pre-trained models, and they significantly outperform the state-of-the-art models on tasks for source code understanding, such as algorithm classification, code clone detection, and code search. Our experiments even show that without heavy pre-training on code data, natural language pre-trained model RoBERTa fine-tuned with our lightweight approach could outperform or rival existing code pre-trained models fine-tuned on the above tasks, such as CodeBERT and GraphCodeBERT. This finding suggests that there is still much room for improvement in code pre-trained models.",10.1145/3510003.3510062,fine-tuning;data augmentation;curriculum learning;test-time aug-mentation,Training;Adaptation models;Codes;Natural languages;Semantics;Cloning;Data models,learning (artificial intelligence);natural language processing;software maintenance,source code understanding;downstream tasks;existing pre-trained models;heavy pre-training;natural language pre-trained model RoBERTa fine-tuned;existing code pre-trained models;bridging pre-trained models,13
639,Not Mentioned,BugListener: Identifying and Synthesizing Bug Reports from Collaborative Live Chats,L. Shi; F. Mu; Y. Zhang; Y. Yang; J. Chen; X. Chen; H. Jiang; Z. Jiang; Q. Wang,"Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; School of Systems and Enterprises, Stevens Institute of Technology, Hoboken, NJ, USA; College of Intelligence and Computing, Tianjin University, Tianjin, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China",2022,"In community-based software development, developers frequently rely on live-chatting to discuss emergent bugs/errors they encounter in daily development tasks. However, it remains a challenging task to accurately record such knowledge due to the noisy nature of interleaved dialogs in live chat data. In this paper, we first formulate the task of identifying and synthesizing bug reports from commu-nity live chats, and propose a novel approach, named BugListener, to address the challenges. Specifically, BugListener automates three sub-tasks: 1) Disentangle the dialogs from massive chat logs by using a Feed-Forward neural network; 2) Identify the bug-report dialogs from separated dialogs by leveraging the Graph neural net-work to learn the contextual information; 3) Synthesize the bug reports by utilizing Transfer Learning techniques to classify the sentences into: observed behaviors (OB), expected behaviors (EB), and steps to reproduce the bug (SR). BugListener is evaluated on six open source projects. The results show that: for bug report identification, BugListener achieves the average Fl of 77.74%, im-proving the best baseline by 12.96%; and for bug report synthesis task, BugListener could classify the OB, EB, and SR sentences with the F1 of 84.62%, 71.46%, and 73.13%, improving the best baselines by 9.32%,12.21%,10.91%, respectively. A human evaluation study also confirms the effectiveness of Bug Listener in generating relevant and accurate bug reports. These demonstrate the significant potential of applying BugListener in community-based software development, for promoting bug discovery and quality improvement.",10.1145/3510003.3510108,Bug Report Generation;Live Chats Mining;Open Source,Computer bugs;Transfer learning;Neural networks;Collaboration;Predictive models;Software;Behavioral sciences,feedforward neural nets;graph theory;learning (artificial intelligence);neural nets;program debugging;public domain software;software engineering;statistical analysis,synthesizing bug reports;collaborative live chats;community-based software development;live-chatting;daily development tasks;interleaved dialogs;live chat data;commu-nity live chats;named BugListener;BugListener automates;massive chat logs;Feed-Forward neural network;bug-report dialogs;separated dialogs;Graph neural net-work;bug report identification;bug report synthesis task;Bug Listener;relevant bug reports;accurate bug reports;bug discovery,1
640,Not Mentioned,Buildsheriff: Change-Aware Test Failure Triage for Continuous Integration Builds,C. Zhang; B. Chen; X. Peng; W. Zhao,"School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China",2022,"Test failures are one of the most common reasons for broken builds in continuous integration. It is expensive to diagnose all test failures in a build. As test failures are usually caused by a few underlying faults, triaging test failures with respect to their underlying root causes can save test failure diagnosis cost. Existing failure triage methods are mostly developed for triaging crash or bug reports, and hence not ap-plicable in the context of test failure triage in continuous integration. In this paper, we first present a large-scale empirical study on 163,371 broken builds caused by test failures to characterize test failures in real-world Java projects. Then, motivated by our study, we propose a new change-aware approach, BuildSheriff, to triage test failures in each continuous integration build such that test failures with the same root cause are put in the same cluster. Our evaluation on 200 broken builds has demonstrated that BuildSheriff can significantly improve the state-of-the-art methods on the triaging effectiveness.",10.1145/3510003.3510132,Test Failures;Failure Triage;Continuous Integration,Java;Costs;Computer bugs;Software engineering,program debugging;program testing;software fault tolerance,continuous integration build;test failures;change-aware test failure triage;failure triage methods;test failure diagnosis;change-aware approach;BuildSheriff approach,
641,Not Mentioned,Causality in Configurable Software Systems,C. Dubslaff; K. Weis; C. Baier; S. Apel,"Centre for Tactile Internet with Human-in-the-Loop (CeTI), Technische UniversitÃ¤t Dresden, Dresden, Germany; Saarland Informatics Campus, Saarland University, SaarbrÃ¼cken, Germany; Technische UniversitÃ¤t Dresden, Dresden, Germany; Saarland Informatics Campus, Saarland University, SaarbrÃ¼cken, Germany",2022,"Detecting and understanding reasons for defects and inadvertent behavior in software is challenging due to their increasing complexity. In configurable software systems, the combinatorics that arises from the multitude of features a user might select from adds a further layer of complexity. We introduce the notion of feature causality, which is based on counterfactual reasoning and inspired by the seminal definition of actual causality by Halpern and Pearl. Feature causality operates at the level of system configurations and is capable of identifying features and their interactions that are the reason for emerging functional and non-functional properties. We present various methods to explicate these reasons, in particular well-established notions of responsibility and blame that we extend to the feature-oriented setting. Establishing a close connection of feature causality to prime implicants, we provide algorithms to effectively compute feature causes and causal explications. By means of an evaluation on a wide range of configurable software systems, including community benchmarks and real-world systems, we demonstrate the feasibility of our approach: We illustrate how our notion of causality facilitates to identify root causes, estimate the effects of features, and detect feature interactions.",10.1145/3510003.3510200,causality;configurable systems;software product lines;software analysis,Software algorithms;Benchmark testing;Software systems;Feature extraction;Cognition;Complexity theory;Software engineering,formal specification;inference mechanisms;security of data;system monitoring,configurable software systems;feature causality;counterfactual reasoning;actual causality;system configurations;feature-oriented setting;feature causes;causal explications;real-world systems;causality facilitates;feature interactions,1
642,Not Mentioned,Causality-Based Neural Network Repair,B. Sun; J. Sun; L. H. Pham; T. Shi,Singapore Management University; Singapore Management University; Singapore Management University; Huawei Singapore,2022,"Neural networks have had discernible achievements in a wide range of applications. The wide-spread adoption also raises the concern of their dependability and reliability. Similar to traditional decision-making programs, neural networks can have defects that need to be repaired. The defects may cause unsafe behaviors, raise security concerns or unjust societal impacts. In this work, we address the problem of repairing a neural network for desirable properties such as fairness and the absence of backdoor. The goal is to construct a neural network that satisfies the property by (minimally) adjusting the given neural network's parameters (i.e., weights). Specifically, we propose CARE (CAusality-based REpair), a causality-based neural network repair technique that 1) performs causality-based fault localization to identify the â€˜guiltyâ€™ neurons and 2) optimizes the parameters of the identified neurons to reduce the misbehavior. We have empirically evaluated CARE on various tasks such as backdoor removal, neural network repair for fairness and safety properties. Our experiment results show that CARE is able to repair all neural networks efficiently and effectively. For fairness repair tasks, CARE successfully improves fairness by 61.91 % on average. For backdoor removal tasks, CARE reduces the attack success rate from over 98% to less than 1 %. For safety property repair tasks, CARE reduces the property violation rate to less than 1 %. Results also show that thanks to the causality-based fault localization, CARE's repair focuses on the misbehavior and preserves the accuracy of the neural networks.",10.1145/3510003.3510080,Machine Learning with and for SE;Program Repair;Fault Localization,Location awareness;Fault diagnosis;Neurons;Maintenance engineering;Safety;Behavioral sciences;Security,maintenance engineering;neural nets;security of data,neural network;causality-based fault localization;fairness repair tasks;safety property repair tasks;causality-based neural network repair;backdoor removal tasks;security,5
643,Not Mentioned,Change Is the Only Constant: Dynamic Updates for Workflows,D. Sokolowski; P. Weisenburger; G. Salvaneschi,"University of St. Gallen, Switzerland; University of St. Gallen, Switzerland; University of St. Gallen, Switzerland",2022,"Software systems must be updated regularly to address changing requirements and urgent issues like security-related bugs. Traditionally, updates are performed by shutting down the system to replace certain components. In modern software organizations, updates are increasingly frequentup to multiple times per dayhence, shutting down the entire system is unacceptable. Safe dynamic software updating (DSU) enables component updates while the system is running by determining when the update can occur without causing errors. Safe DSU is crucial, especially for long-running or frequently executed asynchronous transactions (workflows), e.g., user-interactive sessions or order fulfillment processes. Unfortu-nately, previous research is limited to synchronous transaction models and does not address this case. In this work, we propose a unified model for safe DSU in work-flows. We discuss how state-of-the-art DSU solutions fit into this model and show that they incur significant overhead. To improve the performance, we introduce Essential Safety, a novel safe DSU approach that leverages the notion of non-essential changes, i.e., semantics preserving updates. In 106 realistic BPMN workflows, Essential Safety reduces the delay of workflow completions, on average, by 47.8 % compared to the state of the art. We show that the distinction of essential and non-essential changes plays a cru-cial role in this reduction and that, as suggested in the literature, non-essential changes are frequent: at least 60 % and often more than 90 % of systems' updates in eight monorepos we analyze.",10.1145/3510003.3510065,Software Evolution;Dynamic Software Updating;Workflows,Analytical models;Semantics;Computer bugs;Collaboration;Organizations;Software systems;Software,business data processing;configuration management;distributed processing;program debugging;software engineering;software maintenance;transaction processing;workflow management software,dynamic updates;software systems;changing requirements;security-related bugs;modern software organizations;safe dynamic software updating;component updates;user-interactive sessions;order fulfillment processes;synchronous transaction models;work-flows;state-of-the-art DSU solutions;Essential Safety;novel safe DSU approach;nonessential changes;106 realistic BPMN workflows,
644,Not Mentioned,Characterizing and Detecting Bugs in WeChat Mini-Programs,T. Wang; Q. Xu; X. Chang; W. Dou; J. Zhu; J. Xie; Y. Deng; J. Yang; J. Yang; J. Wei; T. Huang,"State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; Tencent, Inc., Guangzhou, China; Tencent, Inc., Guangzhou, China; Tencent, Inc., Guangzhou, China; Tencent, Inc., Guangzhou, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences University of Chinese Academy of Sciences, Beijing, China",2022,"Built on the WeChat social platform, WeChat Mini-Programs are widely used by more than 400 million users every day. Consequently, the reliability of Mini-Programs is particularly crucial. However, WeChat Mini-Programs suffer from various bugs related to execution environment, lifecycle management, asynchronous mechanism, etc. These bugs have seriously affected users' experience and caused serious impacts. In this paper, we conduct the first empirical study on 83 WeChat Mini-Program bugs, and perform an in-depth analysis of their root causes, impacts and fixes. From this study, we obtain many interesting findings that can open up new research directions for combating WeChat Mini-Program bugs. Based on the bug patterns found in our study, we further develop WeDetector to detect WeChat Mini-Program bugs. Our evaluation on 25 real-world Mini-Programs has found 11 previously unknown bugs, and 7 of them have been confirmed by developers.",10.1145/3510003.3510114,WeChat Mini-Programs;empirical study;bug detection,Social networking (online);Computer bugs;Message service;User experience;Reliability;Software engineering,program debugging;social networking (online),WeChat social platform;lifecycle management;asynchronous mechanism;users experience;mini-program bugs;WeChat MiniPrograms,
645,Not Mentioned,CLEAR: Contrastive Learning for API Recommendation,M. Wei; N. S. Harzevili; Y. Huang; J. Wang; S. Wang,"York University, Toronto, Canada; York University, Toronto, Canada; Institute of Software Chinese Academy of Sciences; Institute of Software Chinese Academy of Sciences; York University, Toronto, Canada",2022,"Automatic API recommendation has been studied for years. There are two orthogonal lines of approaches for this task, i.e., information-retrieval-based (IR-based) and neural-based methods. Although these approaches were reported having remarkable performance, our observation shows that existing approaches can fail due to the following two reasons: 1) most IR-based approaches treat task queries as bag-of-words and use word embedding to represent queries, which cannot capture the sequential semantic information. 2) both the IR-based and the neural-based approaches are weak at distinguishing the semantic difference among lexically similar queries. In this paper, we propose CLEAR, which leverages BERT sen-tence embedding and contrastive learning to tackle the above two is-sues. Specifically, CLEAR embeds the whole sentence of queries and Stack Overflow (SO) posts with a BERT-based model rather than the bag-of-word-based word embedding model, which can preserve the semantic-related sequential information. In addition, CLEAR uses contrastive learning to train the BERT-based embedding model for learning precise semantic representation of programming termi-nologies regardless of their lexical information. CLEAR also builds a BERT-based re-ranking model to optimize its recommendation results. Given a query, CLEAR first selects a set of candidate SO posts via the BERT sentence embedding-based similarity to reduce search space. CLEAR further leverages a BERT-based re-ranking model to rank candidate SO posts and recommends the APIs from the ranked top SO posts for the query. Our experiment results on three different test datasets confirm the effectiveness of CLEAR for both method-level and class-level API recommendation. Compared to the state-of-the-art API recom-mendation approaches, CLEAR improves the MAP by 25%-187% at method-level and 10%-100% at class-level.",10.1145/3510003.3510159,API recommendation;contrastive learning;semantic difference,Training;Codes;Linux;Semantics;Bit error rate;Programming;Task analysis,application program interfaces;computational complexity;data mining;information retrieval;learning (artificial intelligence);natural language processing;query processing;recommender systems;semantic Web;text analysis,CLEAR;contrastive learning;automatic API recommendation;information-retrieval-based;IR-based;task queries;bag-of-words;query;sequential semantic information;neural-based approaches;similar queries;BERT sen-tence embedding;BERT-based model;bag-of-word-based word embedding model;semantic-related sequential information;lexical information;re-ranking model;BERT sentence embedding-based similarity;class-level API recommendation;state-of-the-art API recom-mendation approaches,3
646,Not Mentioned,Code Search based on Context-aware Code Translation,W. Sun; C. Fang; Y. Chen; G. Tao; T. Han; Q. Zhang,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; Purdue University, West Lafayette, Indiana, USA; School of Information Management, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",2022,"Code search is a widely used technique by developers during software development. It provides semantically similar implementations from a large code corpus to developers based on their queries. Existing techniques leverage deep learning models to construct embedding representations for code snippets and queries, respectively. Features such as abstract syntactic trees, control flow graphs, etc., are commonly employed for representing the semantics of code snippets. However, the same structure of these features does not necessarily denote the same semantics of code snippets, and vice versa. In addition, these techniques utilize multiple different word mapping functions that map query words/code tokens to embedding representations. This causes diverged embeddings of the same word/token in queries and code snippets. We propose a novel context-aware code translation technique that translates code snippets into natural language descriptions (called translations). The code translation is conducted on machine instructions, where the context information is collected by simulating the execution of instructions. We further design a shared word mapping function using one single vocabulary for generating embeddings for both translations and queries. We evaluate the effectiveness of our technique, called TranCS, on the CodeSearchNet corpus with 1,000 queries. Experimental results show that TranCS significantly outperforms state-of-the-art techniques by 49.31% to 66.50% in terms of MRR (mean reciprocal rank).",10.1145/3510003.3510140,code search;deep learning;code translation,Deep learning;Vocabulary;Codes;Semantics;Natural languages;Syntactics;Software,graph theory;information retrieval;learning (artificial intelligence);natural language processing;natural languages;programming language semantics;query processing;trees (mathematics),code snippets;queries;code search;code corpus;existing techniques leverage deep learning models;embedding representations;context-aware code translation technique,
647,Not Mentioned,CodeFill: Multi-token Code Completion by Jointly learning from Structure and Naming Sequences,M. Izadi; R. Gismondi; G. Gousios,"Delft University of Technology, Delft, Netherlands; Delft University of Technology, Delft, Netherlands; Delft University of Technology, Delft, Netherlands",2022,"Code completion is an essential feature of IDEs, yet current auto-completers are restricted to either grammar-based or NLP-based single token completions. Both approaches have significant draw-backs: grammar-based autocompletion is restricted in dynamically-typed language environments, whereas NLP-based autocompleters struggle to understand the semantics of the programming language and the developer's code context. In this work, we present CodeFill, a language model for autocompletion that combines learned structure and naming information. Using a parallel Transformer architecture and multi-task learning, CodeFill consumes sequences of source code token names and their equivalent AST token types. Uniquely, CodeFill is trained both for single-token and multi-token (statement) prediction, which enables it to learn long-range dependencies among grammatical and naming elements. We train CodeFill on two datasets, consisting of 29M and 425M lines of code, respectively. To make the evaluation more realistic, we develop a method to automatically infer points in the source code at which completion matters. We compare CodeFill against four baselines and two state-of-the-art models, GPT-C and TravTrans+. CodeFill surpasses all baselines in single token prediction (MRR: 70.9% vs. 66.2% and 67.8%) and outperforms the state of the art for multi-token prediction (ROUGE-L: 63.7% vs. 52.4% and 59.2%, for $n=4$ tokens). We publicly release our source code and datasets.",10.1145/3510003.3510172,Automatic Code Completion;Transformers;Multi-Task Learning;Types;Dynamically-typed Languages,Computer languages;Codes;Semantics;Computer architecture;Transformers;Multitasking;Software engineering,grammars;learning (artificial intelligence);natural language processing,CodeFill;multitoken code completion;naming sequences;current auto-completers;NLP-based single token completions;significant draw-backs;grammar-based autocompletion;dynamically-typed language environments;NLP-based autocompleters struggle;programming language;developer;language model;naming information;multitask;source code token names;equivalent AST token types;multitoken prediction;completion matters;single token prediction,8
648,Not Mentioned,"Collaboration Challenges in Building ML-Enabled Systems: Communication, Documentation, Engineering, and Process",N. Nahar; S. Zhou; G. Lewis; C. KÃ¤stner,"Carnegie Mellon University, Pittsburgh, PA, USA; University of Toronto, Toronto, Ontario, Canada; Carnegie Mellon Software Engineering Institute, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA",2022,"The introduction of machine learning (ML) components in software projects has created the need for software engineers to collabo-rate with data scientists and other specialists. While collaboration can always be challenging, ML introduces additional challenges with its exploratory model development process, additional skills and knowledge needed, difficulties testing ML systems, need for continuous evolution and monitoring, and non-traditional quality requirements such as fairness and explainability. Through inter-views with 45 practitioners from 28 organizations, we identified key collaboration challenges that teams face when building and deploying ML systems into production. We report on common col-laboration points in the development of production ML systems for requirements, data, and integration, as well as corresponding team patterns and challenges. We find that most of these challenges center around communication, documentation, engineering, and process, and collect recommendations to address these challenges.",10.1145/3510003.3510209,Software Engineering;Machine Learning;SE4ML;SE4AI,Buildings;Collaboration;Production;Documentation;Organizations;Machine learning;Software,decision making;groupware;learning (artificial intelligence);project management;software engineering,ML-enabled systems;machine learning components;software projects;software engineers;data scientists;exploratory model development process;nontraditional quality requirements;key collaboration challenges;production ML systems,7
649,Not Mentioned,Combinatorial Testing of RESTful APIs,H. Wu; L. Xu; X. Niu; C. Nie,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",2022,"This paper presents RestCT, a systematic and fully automatic approach that adopts Combinatorial Testing (CT) to test RESTful APIs. RestCT is systematic in that it covers and tests not only the interactions of a certain number of operations in RESTful APIs, but also the interactions of particular input-parameters in every single operation. This is realised by a novel two-phase test case generation approach, which first generates a constrained sequence covering array to determine the execution orders of available operations, and then applies an adaptive strategy to generate and refine several constrained covering arrays to concretise input-parameters of each operation. RestCT is also automatic in that its application relies on only a given Swagger specification of RESTful APIs. The creation of CT test models (especially, the inferring of dependency relationships in both operations and input-parameters), and the generation and execution of test cases are performed without any human intervention. Experimental results on 11 real-world RESTful APIs demonstrate the effectiveness and efficiency of RestCT. In particular, RestCT can find eight new bugs, where only one of them can be triggered by the state-of-the-art testing tool of RESTful APIs.",10.1145/3510003.3510151,RESTful API;combinatorial testing;test case generation,Adaptation models;Systematics;Combinatorial testing;Computer bugs;Restful API;Adaptive arrays;Testing,application program interfaces;formal specification;program debugging;program testing,RestCT;RESTful API;CT test models;combinatorial testing;two-phase test case generation approach;constrained sequence covering array;adaptive strategy;Swagger specification;bugs,3
650,Not Mentioned,CONFETTI: Amplifying Concolic Guidance for Fuzzers,J. Kukucka; L. Pina; P. Ammann; J. Bell,"George Mason University, Fairfax, VA, USA; University of Illinois Chicago, Chicago, Illinois, USA; George Mason University, Fairfax, VA, USA; Northeastern University, Boston, MA, USA",2022,"Fuzz testing (fuzzing) allows developers to detect bugs and vul-nerabilities in code by automatically generating defect-revealing inputs. Most fuzzers operate by generating inputs for applications and mutating the bytes of those inputs, guiding the fuzzing pro-cess with branch coverage feedback via instrumentation. Whitebox guidance (e.g., taint tracking or concolic execution) is sometimes in-tegrated with coverage-guided fuzzing to help cover tricky-to-reach branches that are guarded by complex conditions (so-called â€œmagic valuesâ€). This integration typically takes the form of a targeted in-put mutation, e.g., placing particular byte values at a specific offset of some input in order to cover a branch. However, these dynamic analysis techniques are not perfect in practice, which can result in the loss of important relationships between input bytes and branch predicates, thus reducing the effective power of the technique. We introduce a new, surprisingly simple, but effective technique, global hinting, which allows the fuzzer to insert these interesting bytes not only at a targeted position, but in any position of any input. We implemented this idea in Java, creating Confetti, which uses both targeted and global hints for fuzzing. In an empirical com-parison with two baseline approaches, a state-of-the-art greybox Java fuzzer and a version of Confetti without global hinting, we found that Confetti covers more branches and finds 15 previously unreported bugs, including 9 that neither baseline could find. By conducting a post-mortem analysis of Confetti's execution, we determined that global hinting was at least as effective at revealing new coverage as traditional, targeted hinting.",10.1145/3510003.3510628,fuzz testing;concolic execution;taint tracking,Java;Target tracking;Codes;Instruments;Computer bugs;Fuzzing;Programming,Java;program debugging;program diagnostics;program testing,fuzz testing;defect-revealing inputs;branch coverage feedback;whitebox guidance;taint tracking;concolic execution;coverage-guided fuzzing;tricky-to-reach branches;dynamic analysis techniques;input bytes;branch predicates;concolic guidance;Confetti;greybox Java fuzzer,2
651,Not Mentioned,Conflict-aware Inference of Python Compatible Runtime Environments with Domain Knowledge Graph,W. Cheng; X. Zhu; W. Hu,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",2022,"Code sharing and reuse is a widespread use practice in software engineering. Although a vast amount of open-source Python code is accessible on many online platforms, programmers often find it difficult to restore a successful runtime environment. Previous studies validated automatic inference of Python dependencies using pre-built knowledge bases. However, these studies do not cover sufficient knowledge to accurately match the Python code and also ignore the potential conflicts between their inferred dependencies, thus resulting in a low success rate of inference. In this paper, we propose PyCRE, a new approach to automatically inferring Python compatible runtime environments with domain knowledge graph (KG). Specifically, we design a domain-specific ontology for Python third-party packages and construct KGs for over 10,000 popular packages in Python 2 and Python 3. PyCRE discovers candidate libraries by measuring the matching degree between the known libraries and the third-party resources used in target code. For the NP-complete problem of dependency solving, we propose a heuristic graph traversal algorithm to efficiently guarantee the compatibility between packages. PyCRE achieves superior performance on a real-world dataset and efficiently resolves nearly half more import errors than previous methods.",10.1145/3510003.3510078,Python;Runtime environment inference;Knowledge graph;Conflict resolution;Dependency solving;Configuration management,Knowledge engineering;Runtime environment;Codes;Software algorithms;Ontologies;Libraries;NP-complete problem,computational complexity;graph theory;inference mechanisms;ontologies (artificial intelligence);telecommunication computing,domain-specific ontology;Python third-party packages;Python 2;Python 3;target code;conflict-aware inference;Python compatible runtime environments;domain knowledge graph;open-source Python code;successful runtime environment;automatic inference;Python dependencies;pre-built knowledge bases;sufficient knowledge;potential conflicts;inferred dependencies,1
652,Not Mentioned,Control Parameters Considered Harmful: Detecting Range Specification Bugs in Drone Configuration Modules via Learning-Guided Search,R. Han; C. Yang; S. Ma; J. Ma; C. Sun; J. Li; E. Bertino,"Xidian University, Xian, China; Xidian University, Xian, China; The University of New South Wales, Canberra, Sydney, Australia; Xidian University, Xian, China; Xidian University, Xian, China; Shanghai Jiao Tong University, Shanghai, China; Purdue University, West Lafayette, USA",2022,"In order to support a variety of missions and deal with different flight environments, drone control programs typically provide configurable control parameters. However, such a flexibility introduces vulnerabilities. One such vulnerability, referred to as range specification bugs, has been recently identified. The vulnerability originates from the fact that even though each individual parameter receives a value in the recommended value range, certain combinations of parameter values may affect the drone physical stability. In this paper, we develop a novel learning-guided search system to find such combinations, that we refer to as incorrect configurations. Our system applies metaheuristic search algorithms mutating configurations to detect the configuration parameters that have values driving the drone to unstable physical states. To guide the mutations, our system leverages a machine learning based predictor as the fitness evaluator. Finally, by utilizing multi-objective optimization, our system returns the feasible ranges based on the mutation search results. Because in our system the mutations are guided by a predictor, evaluating the parameter configurations does not require realistic/simulation executions. Therefore, our system supports a comprehensive and yet efficient detection of incorrect configurations. We have carried out an experimental evaluation of our system. The evaluation results show that the system successfully reports potentially incorrect configurations, of which over 85% lead to actual unstable physical states.",10.1145/3510003.3510084,Drone security;configuration test;range specification bug;deep learning approximation,Machine learning algorithms;Computer bugs;Metaheuristics;Machine learning;Prediction algorithms;Genetics;Mobile handsets,autonomous aerial vehicles;control engineering computing;genetic algorithms;learning (artificial intelligence);search problems,control parameters;range specification bugs;drone configuration modules;flight environments;drone control programs;configurable control parameters;individual parameter;recommended value range;parameter values;drone physical stability;metaheuristic search algorithms;configuration parameters;machine learning based predictor;feasible ranges;mutation search results;parameter configurations;comprehensive detection;incorrect configurations;unstable physical states;learning-guided search system,3
653,Not Mentioned,Controlled Concurrency Testing via Periodical Scheduling,C. Wen; M. He; B. Wu; Z. Xu; S. Qin,"CSSE, Shenzhen University, Shenzhen, China; SCEDT, Teesside University, UK; CSSE, Shenzhen University, Shenzhen, China; CSSE, Shenzhen University, Shenzhen, China; Huawei Hong Kong Research Center, Hong Kong, China",2022,"Controlled concurrency testing (CCT) techniques have been shown promising for concurrency bug detection. Their key insight is to control the order in which threads get executed, and attempt to explore the space of possible interleavings of a concurrent program to detect bugs. However, various challenges remain in current CCT techniques, rendering them ineffective and ad-hoc. In this paper, we propose a novel CCT technique Period. Unlike previous works, Period models the execution of concurrent programs as periodical execution, and systematically explores the space of possible inter-leavings, where the exploration is guided by periodical scheduling and influenced by previously tested interleavings. We have evaluated Period on 10 real-world CVEs and 36 widely-used benchmark programs, and our experimental results show that Period demonstrates superiority over other CCT techniques in both effectiveness and runtime overhead. Moreover, we have discovered 5 previously unknown concurrency bugs in real-world programs.",10.1145/3510003.3510178,Concurrency Testing;Concurrency Bugs Detection;Multi-threaded Programs;Systematic Testing;Stateless Model Checking,Concurrent computing;Runtime;Computer bugs;Programming;Aerospace electronics;Model checking;Benchmark testing,concurrency control;multi-threading;program debugging;program diagnostics;program testing;scheduling,periodical scheduling;controlled concurrency testing techniques;concurrency bug detection;concurrent program;period models;periodical execution;tested interleavings;benchmark programs;concurrency bugs;real-world programs;CCT technique period;CVE,2
654,Not Mentioned,Cross-Domain Deep Code Search with Meta Learning,Y. Chai; H. Zhang; B. Shen; X. Gu,"School of Software, Shanghai Jiao Tong University, China; The University of Newcastle, Australia; School of Software, Shanghai Jiao Tong University, China; School of Software, Shanghai Jiao Tong University, China",2022,"Recently, pre-trained programming language models such as Code-BERT have demonstrated substantial gains in code search. Despite their success, they rely on the availability of large amounts of parallel data to fine-tune the semantic mappings between queries and code. This restricts their practicality in domain-specific languages with relatively scarce and expensive data. In this paper, we propose CDCS, a novel approach for domain-specific code search. CDCS employs a transfer learning framework where an initial program representation model is pre-trained on a large corpus of common programming languages (such as Java and Python), and is further adapted to domain-specific languages such as Solidity and SQL. Un-like cross-language CodeBERT, which is directly fine-tuned in the target language, CDCS adapts a few-shot meta-learning algorithm called MAML to learn the good initialization of model parameters, which can be best reused in a domain-specific language. We evaluate the proposed approach on two domain-specific languages, namely Solidity and SQL, with model transferred from two widely used languages (Python and Java). Experimental results show that CDCS significantly outperforms conventional pre-trained code models that are directly fine-tuned in domain-specific languages, and it is particularly effective for scarce data.",10.1145/3510003.3510125,Code Search;Pre-trained Code Models;Meta Learning;Few-Shot Learning;Deep Learning,Structured Query Language;Adaptation models;Solid modeling;Java;Codes;Transfer learning;Data models,Java;learning (artificial intelligence);query processing;SQL,initial program representation model;common programming languages;domain-specific language;cross-language CodeBERT;CDCS;conventional pre-trained code models;cross-domain deep Code search;pre-trained programming language models;Code-BERT;queries;domain-specific code search,1
655,Not Mentioned,Data-Driven Loop Bound Learning for Termination Analysis,R. Xu; J. Chen; F. He,"School of Software, Tsinghua University Key Laboratory for Information System Security, MoE Beijing National Research Center for Information Science and Technology, Beijing, China; School of Software, Tsinghua University Key Laboratory for Information System Security, MoE Beijing National Research Center for Information Science and Technology, Beijing, China; School of Software, Tsinghua University Key Laboratory for Information System Security, MoE Beijing National Research Center for Information Science and Technology, Beijing, China",2022,"Termination is a fundamental liveness property for program verification. A loop bound is an upper bound of the number of loop iterations for a given program. The existence of a loop bound evidences the termination of the program. This paper employs a reinforced black-box learning approach for termination proving, consisting of a loop bound learner and a validation checker. We present efficient data-driven algorithms for inferring various kinds of loop bounds, including simple loop bounds, conjunctive loop bounds, and lexicographic loop bounds. We also devise an efficient validation checker by integrating a quick bound checking algorithm and a two-way data sharing mechanism. We implemented a prototype tool called ddlTerm. Experiments on publicly accessible benchmarks show that ddlTerm outperforms state-of-the-art termination analysis tools by solving 13-48% more benchmarks and saving 40-77% solving time.",10.1145/3510003.3510220,Termination analysis;loop bound;data-driven approach,Bridges;Upper bound;Prototypes;Benchmark testing;Software engineering,iterative methods;learning (artificial intelligence);program debugging;program diagnostics;program testing;program verification,data-driven loop bound learning;fundamental liveness property;program verification;loop iterations;given program;black-box learning approach;termination proving;loop bound learner;efficient data-driven algorithms;simple loop;conjunctive loop;lexicographic loop bounds;efficient validation checker;quick bound checking algorithm;state-of-the-art termination analysis tools,1
656,Not Mentioned,DEAR: A Novel Deep Learning-based Approach for Automated Program Repair,Y. Li; S. Wang; T. N. Nguyen,"New Jersey Inst. of Technology, New Jersey, USA; New Jersey Inst. of Technology, New Jersey, USA; University of Texas, Dallas, Texas, USA",2022,"The existing deep learning (DL)-based automated program repair (APR) models are limited in fixing general software defects. We present DEAR, a DL-based approach that supports fixing for the general bugs that require dependent changes at once to one or mul-tiple consecutive statements in one or multiple hunks of code. We first design a novel fault localization (FL) technique for multi-hunk, multi-statement fixes that combines traditional spectrum-based (SB) FL with deep learning and data-flow analysis. It takes the buggy statements returned by the SBFL model, detects the buggy hunks to be fixed at once, and expands a buggy statement $s$ in a hunk to include other suspicious statements around s. We design a two-tier, tree-based LSTM model that incorporates cycle training and uses a divide-and-conquer strategy to learn proper code transformations for fixing multiple statements in the suitable fixing context consisting of surrounding subtrees. We conducted several experiments to evaluate DEAR on three datasets: Defects4J (395 bugs), BigFix (+26k bugs), and CPatMiner (+44k bugs). On Defects4J dataset, DEAR outperforms the baselines from 42%-683% in terms of the number of auto-fixed bugs with only the top-1 patches. On BigFix dataset, it fixes 31â€“145 more bugs than existing DL-based APR models with the top-1 patches. On CPatMiner dataset, among 667 fixed bugs, there are 169 (25.3%) multi-hunk/multi-statement bugs. DEAR fixes 71 and 164 more bugs, including 52 and 61 more multi-hunk/multi-statement bugs, than the state-of-the-art, DL-based APR models.",10.1145/3510003.3510177,Automated Program Repair;Deep Learning;Fault Localization,Deep learning;Training;Location awareness;Analytical models;Codes;Computer bugs;Maintenance engineering,learning (artificial intelligence);program debugging;program testing;recurrent neural nets;software fault tolerance;software maintenance;trees (mathematics),"existing deep learning-based automated program repair models;general software defects;general bugs;mul-tiple consecutive statements;novel fault localization technique;multistatement fixes;traditional spectrum-based FL;data-flow analysis;buggy statement;SBFL model;buggy hunks;hunk;suspicious statements;tree-based LSTM model;multiple statements;suitable fixing context;Defects4J, BigFix;395 bugs;Defects4J dataset;auto-fixed bugs;667 fixed bugs;164 more bugs;DL-based APR models",5
657,Not Mentioned,Decomposing Convolutional Neural Networks into Reusable and Replaceable Modules,R. Pan; H. Rajan,"Dept. of Computer Science, Iowa State University, Ames, IA, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA",2022,"Training from scratch is the most common way to build a Convolutional Neural Network (CNN) based model. What if we can build new CNN models by reusing parts from previously built CNN models? What if we can improve a CNN model by replacing (possibly faulty) parts with other parts? In both cases, instead of training, can we identify the part responsible for each output class (module) in the model(s) and reuse or replace only the desired output classes to build a model? Prior work has proposed decomposing dense-based networks into modules (one for each output class) to enable reusability and replace ability in various scenarios. However, this work is limited to the dense layers and is based on the one-to-one relationship between the nodes in consecutive layers. Due to the shared architecture in the CNN model, prior work cannot be adapted directly. In this paper, we propose to decompose a CNN model used for image classification problems into modules for each output class. These modules can further be reused or replaced to build a new model. We have evaluated our approach with CIFAR-10, CIFAR-100, and ImageNet tiny datasets with three variations of ResNet models and found that enabling decomposition comes with a small cost (1.77% and 0.85% for top-1 and top-5 accuracy, respectively). Also, building a model by reusing or replacing modules can be done with a 2.3% and 0.5% average loss of accuracy. Furthermore, reusing and replacing these modules reduces CO2e emission by ~37 times compared to training the model from scratch.",10.1145/3510003.3510051,deep learning;cnn;deep neural network;modularity;decomposition,Training;Adaptation models;Costs;Buildings;Computer architecture;Convolutional neural networks;Software engineering,convolutional neural nets;image classification,ResNet;reusable module;replaceable module;dense-based networks;CIFAR-10 dataset;CIFAR-100 dataset;ImageNet tiny dataset;convolutional neural network decomposition;CNN decomposition,3
658,Not Mentioned,Decomposing Software Verification into Off-the-Shelf Components: An Application to CEGAR,D. Beyer; J. Haltermann; T. Lemberger; H. Wehrheim,"LMU Munich, Munich, Germany; University of Oldenburg, Oldenburg, Germany; LMU Munich, Munich, Germany; University of Oldenburg, Oldenburg, Germany",2022,"Techniques for software verification are typically realized as cohesive units of software with tightly coupled components. This makes it difficult to reuse components, and the potential for workload distribution is limited. Innovations in software verification might find their way into practice faster if provided in smaller, more specialized components. In this paper, we propose to strictly decompose software verification: the verification task is split into independent subtasks, implemented by only loosely coupled components communicating via clearly defined interfaces. We apply this decomposition concept to one of the most frequently employed techniques in software verification: counterexample-guided abstraction refinement (CEGAR). CEGAR is a technique to iteratively compute an abstract model of the system. We develop a decomposition of CEGAR into independent components with clearly defined interfaces that are based on existing, standardized exchange formats. Its realization component-based CEGAR (C-CEGAR) concerns the three core tasks of CEGAR: abstract-model exploration, feasibility check, and precision refinement. We experimentally show that - despite the necessity of exchanging complex data via interfaces - the efficiency thereby only reduces by a small constant factor while the precision in solving verification tasks even increases. We furthermore illustrate the advantages of C-CEGAR by experimenting with different implementations of components, thereby further increasing the overall effectiveness and testing that substitution of components works well.",10.1145/3510003.3510064,Software engineering;Software verification;Abstraction refine-ment;CEGAR;Decomposition;Cooperative verification,Computer science;Technological innovation;Computational modeling;Software;Complexity theory;Time factors;Feeds,formal verification,software verification;tightly coupled components;verification task;loosely coupled components;realization component-based CEGAR;counter example-guided abstraction refinement;abstract-model exploration task;feasibility check task;precision refinement task,1
659,Not Mentioned,DeepAnalyze: Learning to Localize Crashes at Scale,M. Shetty; C. Bansal; S. Nath; S. Bowles; H. Wang; O. Arman; S. Ahari,"Microsoft Research, Bangalore, India; Microsoft Research, Bangalore, India; Microsoft Research, Bangalore, India; Microsoft, Redmond, USA; Microsoft, Redmond, USA; Microsoft, Redmond, USA; Microsoft, Redmond, USA",2022,"Crash localization, an important step in debugging crashes, is challenging when dealing with an extremely large number of diverse applications and platforms and underlying root causes. Large-scale error reporting systems, e.g., Windows Error Reporting (WER), commonly rely on manually developed rules and heuristics to localize blamed frames causing the crashes. As new applications and features are routinely introduced and existing applications are run under new environments, developing new rules and maintaining existing ones become extremely challenging. We propose a data-driven solution to address the problem. We start with the first large-scale empirical study of 362K crashes and their blamed methods reported to WER by tens of thousands of applications running in the field. The analysis provides valuable insights on where and how the crashes happen and what methods to blame for the crashes. These insights enable us to develop Deep-Analyze, a novel multi-task sequence labeling approach for identifying blamed frames in stack traces. We evaluate our model with over a million real-world crashes from four popular Microsoft applications and show that DeepAnalyze, trained with crashes from one set of applications, not only accurately localizes crashes of the same applications, but also bootstrap crash localization for other applications with zero to very little additional training data.",10.1145/3510003.3512759,Software Engineering;Machine Learning;Crash Localization,Location awareness;Operating systems;Training data;Debugging;Multitasking;Computer crashes;Data models,deep learning (artificial intelligence);program debugging,blamed frames;real-world crashes;DeepAnalyze;bootstrap crash localization;debugging crashes;Windows Error Reporting;Microsoft applications;WER;multitask sequence labeling,
660,Not Mentioned,DeepDiagnosis: Automatically Diagnosing Faults and Recommending Actionable Fixes in Deep Learning Programs,M. Wardat; B. D. Cruz; W. Le; H. Rajan,"Dept. of Computer Science, Iowa State University, Ames, IA, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA",2022,"Deep Neural Networks (DNNs) are used in a wide variety of applications. However, as in any software application, DNN-based apps are afflicted with bugs. Previous work observed that DNN bug fix patterns are different from traditional bug fix patterns. Furthermore, those buggy models are non-trivial to diagnose and fix due to inexplicit errors with several options to fix them. To support developers in locating and fixing bugs, we propose DeepDiagnosis, a novel debugging approach that localizes the faults, reports error symptoms and suggests fixes for DNN programs. In the first phase, our technique monitors a training model, periodically checking for eight types of error conditions. Then, in case of problems, it reports messages containing sufficient information to perform actionable repairs to the model. In the evaluation, we thoroughly examine 444 models - 53 real-world from GitHub and Stack Overflow, and 391 curated by AUTOTRAINER. DeepDiagnosis provides superior accuracy when compared to UMLUAT and DeepLocalize. Our technique is faster than AUTOTRAINER for fault localization. The results show that our approach can support additional types of models, while state-of-the-art was only able to handle classification ones. Our technique was able to report bugs that do not manifest as numerical errors during training. Also, it can provide actionable insights for fix whereas DeepLocalize can only report faults that lead to numerical errors during training. DeepDiagnosis manifests the best capabilities of fault detection, bug localization, and symptoms identification when compared to other approaches.",10.1145/3510003.3510071,deep neural networks;fault location;debugging;program analysis;deep learning bugs,Training;Location awareness;Deep learning;Computer bugs;Neural networks;Software;Numerical models,deep learning (artificial intelligence);fault diagnosis;program debugging,DeepDiagnosis;fault detection;bug localization;deep learning programs;deep neural networks;software application;DNN-based apps;DNN bug fix patterns;inexplicit errors;fixing bugs;debugging approach;actionable repairs;AUTOTRAINER;fault localization;actionable fixes recommendation;UMLUAT;GitHub;Stack Overflow;DeepLocalize;symptoms identification,1
661,Not Mentioned,DeepFD: Automated Fault Diagnosis and Localization for Deep Learning Programs,J. Cao; M. Li; X. Chen; M. Wen; Y. Tian; B. Wu; S. -C. Cheung,"The Hong Kong University of Science and Technology, and Guangzhou HKUST Fok Ying Tung Research Institute, China; The Hong Kong University of Science and Technology, China; The Hong Kong University of Science and Technology, China; The Hong Kong University of Science and Technology, China; University of Waterloo, Canada, and The Hong Kong University of Science and Technology, China; MIT-IBM Watson AI Lab, U.S.; The Hong Kong University of Science and Technology, and Guangzhou HKUST Fok Ying Tung Research Institute, China",2022,"As Deep Learning (DL) systems are widely deployed for mission-critical applications, debugging such systems becomes essential. Most existing works identify and repair suspicious neurons on the trained Deep Neural Network (DNN), which, unfortunately, might be a detour. Specifically, several existing studies have reported that many unsatisfactory behaviors are actually originated from the faults residing in DL programs. Besides, locating faulty neurons is not actionable for developers, while locating the faulty statements in DL programs can provide developers with more useful information for debugging. Though a few recent studies were proposed to pinpoint the faulty statements in DL programs or the training settings (e.g. too large learning rate), they were mainly designed based on predefined rules, leading to many false alarms or false negatives, especially when the faults are beyond their capabilities. In view of these limitations, in this paper, we proposed DeepFD, a learning-based fault diagnosis and localization framework which maps the fault localization task to a learning problem. In particu-lar, it infers the suspicious fault types via monitoring the runtime features extracted during DNN model training, and then locates the diagnosed faults in DL programs. It overcomes the limitations by identifying the root causes of faults in DL programs instead of neurons, and diagnosing the faults by a learning approach instead of a set of hard-coded rules. The evaluation exhibits the potential of DeepFD. It correctly diagnoses 52% faulty DL programs, compared with around half (27%) achieved by the best state-of-the-art works. Besides, for fault localization, DeepFD also outperforms the existing works, correctly locating 42% faulty programs, which almost doubles the best result (23%) achieved by the existing works.",10.1145/3510003.3510099,Neural Networks;Fault Diagnosis;Fault Localization;Debugging,Location awareness;Fault diagnosis;Deep learning;Training;Runtime;Neurons;Debugging,fault diagnosis;feature extraction;learning (artificial intelligence);neural nets;program debugging;software fault tolerance,DeepFD;52% faulty DL programs;42% faulty programs;automated fault diagnosis;deep learning programs;mission-critical applications;existing works identify;suspicious neurons;trained Deep Neural Network;faulty neurons;faulty statements;training settings;learning rate;learning-based fault diagnosis;localization framework;fault localization task;learning problem;suspicious fault types;DNN model training;diagnosed faults;learning approach,3
662,Not Mentioned,DeepStability: A Study of Unstable Numerical Methods and Their Solutions in Deep Learning,E. Kloberdanz; K. G. Kloberdanz; W. Le,"Department of Computer Science, Iowa State University; Cape Privacy; Department of Computer Science, Iowa State University",2022,"Deep learning (DL) has become an integral part of solutions to various important problems, which is why ensuring the quality of DL systems is essential. One of the challenges of achieving reliability and robustness of DL software is to ensure that algorithm implementations are numerically stable. DL algorithms require a large amount and a wide variety of numerical computations. A naive implementation of numerical computation can lead to errors that may result in incorrect or inaccurate learning and results. A numerical algorithm or a mathematical formula can have several implementations that are mathematically equivalent, but have different numerical stability properties. Designing numerically stable algorithm implementations is challenging, because it requires an interdisciplinary knowledge of software engineering, DL, and numerical analysis. In this paper, we study two mature DL libraries PyTorch and Tensorflow with the goal of identifying unstable numerical methods and their solutions. Specifically, we investigate which DL algorithms are numerically unstable and conduct an in-depth analysis of the root cause, manifestation, and patches to numerical instabilities. Based on these findings, we launch DeepStability, the first database of numerical stability issues and solutions in DL. Our findings and DeepStability provide future references to developers and tool builders to prevent, detect, localize and fix numerically unstable algorithm implementations. To demonstrate that, using DeepStability we have located numerical stability issues in Tensorflow, and submitted a fix which has been accepted and merged in.",10.1145/3510003.3510095,numerical stability;deep learning;numerical algorithms,Deep learning;Knowledge engineering;Databases;Software algorithms;Software;Robustness;Libraries,deep learning (artificial intelligence);numerical stability;Python;software engineering,deep learning;DL algorithms;numerical computation;naive implementation;inaccurate learning;numerical algorithm;numerical stability properties;numerical analysis;unstable numerical methods;numerical instabilities;numerical stability issues;DeepStability;numerically unstable algorithm implementations;numerically stable algorithm implementations;DL libraries;PyTorch;Tensorflow,2
663,Not Mentioned,DeepState: Selecting Test Suites to Enhance the Robustness of Recurrent Neural Networks,Z. Liu; Y. Feng; Y. Yin; Z. Chen,"State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",2022,"Deep Neural Networks (DNN) have achieved tremendous success in various software applications. However, accompanied by outstanding effectiveness, DNN-driven software systems could also exhibit incorrect behaviors and result in some critical accidents and losses. The testing and optimization of DNN-driven software systems rely on a large number of labeled data that often require many human efforts, resulting in high test costs and low efficiency. Although plenty of coverage-based criteria have been proposed to assist in the data selection of convolutional neural networks, it is difficult to apply them on Recurrent Neural Network (RNN) models due to the difference between the working nature. In this paper, we propose a test suite selection tool DeepState towards the particular neural network structures of RNN models for reducing the data labeling and computation cost. DeepState selects data based on a stateful perspective of RNN, which identifies the possibly misclassified test by capturing the state changes of neurons in RNN models. We further design a test selection method to enable testers to obtain a test suite with strong fault detection and model improvement capability from a large dataset. To evaluate DeepState, we conduct an extensive empirical study on popular datasets and prevalent RNN models containing image and text processing tasks. The experimental results demonstrate that DeepState outperforms existing coverage-based techniques in selecting tests regarding effectiveness and the inclusiveness of bug cases. Meanwhile, we observe that the selected data can improve the robustness of RNN models effectively.",10.1145/3510003.3510231,deep learning testing;deep neural networks;recurrent neural networks;test selection,Recurrent neural networks;Costs;Computational modeling;Computer bugs;Software systems;Data models;Robustness,fault diagnosis;learning (artificial intelligence);neural nets;program testing;recurrent neural nets;software maintenance;text analysis,test suite selection tool DeepState;particular neural network structures;data labeling;computation cost;possibly misclassified test;test selection method;strong fault detection;model improvement capability;prevalent RNN models;coverage-based techniques;Recurrent Neural Networks;Deep Neural Networks;tremendous success;software applications;outstanding effectiveness;DNN-driven software systems;incorrect behaviors;critical accidents;optimization;high test costs;coverage-based criteria;data selection;convolutional neural networks;Recurrent Neural Network models,3
664,Not Mentioned,DeepSTL - From English Requirements to Signal Temporal Logic,J. He; E. Bartocci; D. NiÄkoviÄ‡; H. Isakovic; R. Grosu,"Technische UniversitÃ¤t Wien, Vienna, Austria; Technische UniversitÃ¤t Wien, Vienna, Austria; AIT Austrian Institute of Technology, Vienna, Austria; Technische UniversitÃ¤t Wien, Vienna, Austria; Technische UniversitÃ¤t Wien, Vienna, Austria",2022,"Formal methods provide very powerful tools and techniques for the design and analysis of complex systems. Their practical application remains however limited, due to the widely accepted belief that formal methods require extensive expertise and a steep learning curve. Writing correct formal specifications in form of logical formulas is still considered to be a difficult and error prone task. In this paper we propose DeepSTL, a tool and technique for the translation of informal requirements, given as free English sentences, into Signal Temporal Logic (STL), a formal specification language for cyber-physical systems, used both by academia and advanced research labs in industry. A major challenge to devise such a translator is the lack of publicly available informal requirements and formal specifications. We propose a two-step workflow to address this challenge. We first design a grammar-based generation technique of synthetic data, where each output is a random STL formula and its associated set of possible English translations. In the second step, we use a state-of-the-art transformer-based neural translation technique, to train an accurate attentional translator of English to STL. The experimental results show high translation quality for patterns of English requirements that have been well trained, making this workflow promising to be extended for processing more complex translation tasks.",10.1145/3510003.3510171,Requirements Engineering;Formal Specification;Signal Temporal Logic (STL);Machine Translation,Industries;Natural languages;Computer architecture;Writing;Cyber-physical systems;Transformers;Formal specifications,control engineering computing;formal specification;formal verification;grammars;language translation;learning (artificial intelligence);specification languages;temporal logic,DeepSTL;English requirements;Signal Temporal Logic;formal methods;complex systems;widely accepted belief;extensive expertise;steep learning curve;correct formal specifications;logical formulas;difficult error prone task;free English sentences;formal specification language;cyber-physical systems;publicly available informal requirements;grammar-based generation technique;random STL formula;possible English translations;state-of-the-art transformer-based neural translation technique;accurate attentional translator;high translation quality;complex translation tasks,
665,Not Mentioned,DeepTraLog: Trace-Log Combined Microservice Anomaly Detection through Graph-based Deep Learning,C. Zhang; X. Peng; C. Sha; K. Zhang; Z. Fu; X. Wu; Q. Lin; D. Zhang,"Fudan University, China; Fudan University, China; Fudan University, China; Fudan University, China; Fudan University, China; Fudan University, China; Microsoft Research, China; Microsoft Research, China",2022,"A microservice system in industry is usually a large-scale dis-tributed system consisting of dozens to thousands of services run-ning in different machines. An anomaly of the system often can be reflected in traces and logs, which record inter-service interactions and intra-service behaviors respectively. Existing trace anomaly detection approaches treat a trace as a sequence of service invocations. They ignore the complex structure of a trace brought by its invocation hierarchy and parallel/asynchronous invocations. On the other hand, existing log anomaly detection approaches treat a log as a sequence of events and cannot handle microservice logs that are distributed in a large number of services with complex interactions. In this paper, we propose DeepTraLog, a deep learning based microservice anomaly detection approach. DeepTraLog uses a unified graph representation to describe the complex structure of a trace together with log events embedded in the structure. Based on the graph representation, DeepTraLog trains a GGNNs based deep SVDD model by combing traces and logs and detects anom-alies in new traces and the corresponding logs. Evaluation on a microservice benchmark shows that DeepTraLog achieves a high precision (0.93) and recall (0.97), outperforming state-of-the-art trace/log anomaly detection approaches with an average increase of 0.37 in F1-score. It also validates the efficiency of DeepTraLog, the contribution of the unified graph representation, and the impact of the configurations of some key parameters.",10.1145/3510003.3510180,Microservice;Anomaly Detection;Log Analysis;Tracing;Graph Neural Network;Deep Learning,Deep learning;Industries;Microservice architectures;Benchmark testing;Behavioral sciences;Time factors;Anomaly detection,deep learning (artificial intelligence);graph theory;mobile computing;program diagnostics;security of data;service-oriented architecture;support vector machines,deep learning based microservice anomaly detection approach;DeepTraLog;unified graph representation;complex structure;log events;detects anomalies;corresponding logs;microservice benchmark;trace-log;microservice system;record inter-service interactions;trace anomaly detection approaches;service invocations;microservice logs,3
666,Not Mentioned,Default: Mutual Information-based Crash Triage for Massive Crashes,X. Zhang; J. Chen; C. Feng; R. Li; W. Diao; K. Zhang; J. Lei; C. Tang,"National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; National University of Defense Technology; School of Cyber Science and Technology, Shandong University; Chinese University of Hong Kong; National University of Defense Technology; National University of Defense Technology",2022,"With the considerable success achieved by modern fuzzing in-frastructures, more crashes are produced than ever before. To dig out the root cause, rapid and faithful crash triage for large numbers of crashes has always been attractive. However, hindered by the practical difficulty of reducing analysis imprecision without compromising efficiency, this goal has not been accomplished. In this paper, we present an end-to-end crash triage solution Default, for accurately and quickly pinpointing unique root cause from large numbers of crashes. In particular, we quantify the â€œcrash relevanceâ€ of program entities based on mutual information, which serves as the criterion of unique crash bucketing and allows us to bucket massive crashes without pre-analyzing their root cause. The quantification of â€œcrash relevanceâ€ is also used in the shortening of long crashing traces. On this basis, we use the interpretability of neural networks to precisely pinpoint the root cause in the shortened traces by evaluating each basic block's impact on the crash label. Evaluated with 20 programs with 22216 crashes in total, Default demonstrates remarkable accuracy and performance, which is way beyond what the state-of-the-art techniques can achieve: crash de-duplication was achieved at a super-fast processing speed - 0.017 seconds per crashing trace, without missing any unique bugs. After that, it identifies the root cause of 43 unique crashes with no false negatives and an average false positive rate of 9.2%.",10.1145/3510003.3512760,Crash Triage;Software Security,Location awareness;Software algorithms;Neural networks;Computer bugs;Fuzzing;Computer crashes;Security,neural nets;program debugging;program diagnostics,crash de-duplication;mutual information-based crash triage;modern fuzzing in-frastructures;rapid crash triage;faithful crash triage;unique root cause;unique crash bucketing;bucket massive crashes;crash label;massive crashes;crashing traces;end-to-end crash triage solution;Default;unique bugs;neural networks,
667,Not Mentioned,Demystifying Android Non-SDK APls: Measurement and Understanding,S. Yang; R. Li; J. Chen; W. Diao; S. Guo,"School of Cyber Science and Technology, Shandong University; School of Cyber Science and Technology, Shandong University; National University of Defense Technology; School of Cyber Science and Technology, Shandong University; School of Cyber Science and Technology, Shandong University",2022,"During the Android app development, the SDK is essential, which provides rich APIs to facilitate the implementations of functional-ities. However, in the Android framework, there still exist plenty of non-SDK APIs that are not well documented. These non-SDK APIs can be invoked through unconventional ways, such as Java reflection. On the other hand, these APIs are not stable and may be changed or even removed in future Android versions, providing no guarantee for compatibility. From Android 9 (API level 28), Google began to strictly restrict the use of non-SDK APIs, and the corresponding checking mechanism has been integrated into the Android OS. In this work, we systematically study the use and design of Android non-SDK APIs. Notably, we propose four research questions covering the restriction mechanism, the present usage status, malicious usage, and the API list evolution. To answer these questions, we conducted a large-scale measurement based on over 200K apps and the source code of three recent Android versions. As a result, a series of exciting and valuable findings are obtained. For example, Google's restriction is not strong enough and can still be bypassed. Besides, app developers use only a tiny part of non-SDK APIs. Our work provides new knowledge to the research community and can help researchers improve the Android API designs.",10.1145/3510003.3510045,Android Non-SDK APIs;API Design and Evolution,Knowledge engineering;Java;Codes;Market research;Reflection;Internet;Smart phones,Android (operating system);application program interfaces;Java;mobile computing,Android app development;Android framework;future Android versions;API level;Android OS;API list evolution;Android versions;Android API designs;Android nonSDK API,
668,Not Mentioned,Demystifying the Dependency Challenge in Kernel Fuzzing,Y. Hao; H. Zhang; G. Li; X. Du; Z. Qian; A. A. Sani,"UC Riverside, Riverside, USA; Georgia Institute of Technology, Atlanta, USA; UC Riverside, Riverside, USA; UC Riverside, Riverside, USA; UC Riverside, Riverside, USA; UC Irvine, Irvine, USA",2022,"Fuzz testing operating system kernels remains a daunting task to date. One known challenge is that much of the kernel code is locked under specific kernel states and current kernel fuzzers are not ef-fective in exploring such an enormous state space. We refer to this problem as the dependency challenge. Though there are some ef-forts trying to address the dependency challenge, the prevalence and categorization of dependencies have never been studied. Most prior work simply attempted to recover dependencies opportunisti-cally whenever they are relatively easy to recognize. In this paper, we undertake a substantial measurement study to systematically understand the real challenge behind dependencies. To our surprise, we show that even for well-fuzzed kernel modules, unresolved de-pendencies still account for 59% - 88% of the uncovered branches. Furthermore, we show that the dependency challenge is only a symptom rather than the root cause of failing to achieve more cov-erage. By distilling and summarizing our findings, we believe the research provides valuable guidance to future research in kernel fuzzing. Finally, we propose a number of novel research directions directly based on the insights gained from the measurement study.",10.1145/3510003.3510126,,Codes;Linux;Manuals;Fuzzing;Gain measurement;Kernel;Task analysis,data mining;operating system kernels;program testing;security of data,kernel code;specific kernel states;current kernel fuzzers;dependency challenge;well-fuzzed kernel modules;kernel fuzzing;fuzz testing operating system kernels;known challenge,1
669,Not Mentioned,Demystifying the Vulnerability Propagation and Its Evolution via Dependency Trees in the NPM Ecosystem,C. Liu; S. Chen; L. Fan; B. Chen; Y. Liu; X. Peng,"College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Cyber Science, Nankai University, Tianjin, China; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore, Singapore; School of Computer Science and Shanghai Key Laboratory of Data Science, Fudan University, Shanghai, China",2022,"Third-party libraries with rich functionalities facilitate the fast development of JavaScript software, leading to the explosive growth of the NPM ecosystem. However, it also brings new security threats that vulnerabilities could be introduced through dependencies from third-party libraries. In particular, the threats could be excessively amplified by transitive dependencies. Existing research only considers direct dependencies or reasoning transitive dependencies based on reachability analysis, which neglects the NPM-specific dependency resolution rules as adapted during real installation, resulting in wrongly resolved dependencies. Consequently, further fine-grained analysis, such as precise vulnerability propagation and their evolution over time in dependencies, cannot be carried out precisely at a large scale, as well as deriving ecosystem-wide solutions for vulnerabilities in dependencies. To fill this gap, we propose a knowledge graph-based dependency resolution, which resolves the inner dependency relations of dependencies as trees (i.e., dependency trees), and investigates the security threats from vulnerabilities in dependency trees at a large scale. Specifically, we first construct a complete dependency-vulnerability knowledge graph (DVGraph) that captures the whole NPM ecosystem (over 10 million library versions and 60 million well-resolved dependency relations). Based on it, we propose a novel algorithm (DTResolver) to statically and precisely resolve dependency trees, as well as transitive vulnerability propagation paths, for each package by taking the official dependency resolution rules into account. Based on that, we carry out an ecosystem-wide empirical study on vulnerability propagation and its evolution in dependency trees. Our study unveils lots of useful findings, and we further discuss the lessons learned and solutions for different stakeholders to mitigate the vulnerability impact in NPM based on our findings. For example, we implement a dependency tree based vulnerability remediation method (DTReme) for NPM packages, and receive much better performance than the official tool (npm audit fix).",10.1145/3510003.3510142,,Ecosystems;Libraries;Software;Explosives;Cognition;Security;Stakeholders,ecology;graph theory;Java;knowledge based systems;reachability analysis;security of data;software engineering;trees (mathematics),NPM ecosystem;third-party libraries;security threats;direct dependencies;reasoning transitive dependencies;NPM-specific dependency resolution rules;knowledge graph-based dependency resolution;inner dependency relations;transitive vulnerability propagation paths;official dependency resolution rules;dependency tree based vulnerability remediation method;dependency-vulnerability knowledge graph;JavaScript software;reachability analysis,6
670,Not Mentioned,DescribeCtx: Context-Aware Description Synthesis for Sensitive Behaviors in Mobile Apps,S. Yang; Y. Wang; Y. Yao; H. Wang; Y. F. Ye; X. Xiao,"Case Western Reserve University; University of Illinois at Urbana-Champaign; State Key Laboratory for Novel Software Technology, Nanjing University; Beijing University of Posts and Telecommunications; University of Notre Dame; Case Western Reserve University",2022,"While mobile applications (i.e., apps) are becoming capable of handling various needs from users, their increasing access to sensitive data raises privacy concerns. To inform such sensitive behaviors to users, existing techniques propose to automatically identify explanatory sentences from app descriptions; however, many sensitive behaviors are not explained in the corresponding app descriptions. There also exist general techniques that translate code to sentences. However, these techniques lack the vocabulary to explain the uses of sensitive data and fail to consider the context (i.e., the app functionalities) of the sensitive behaviors. To address these limitations, we propose Describectx, a context-aware description synthesis approach that trains a neural machine translation model using a large set of popular apps, and generates app-specific descriptions for sensitive behaviors. Specifically, Describectx encodes three heterogeneous sources as input, i.e., vocabularies provided by privacy policies, behavior summary provided by the call graphs in code, and contextual information provided by GUI texts. Our evaluations on 1,262 Android apps show that, compared with existing baselines, Describectx produces more accurate descriptions (24.96 in BLEU) and achieves higher user ratings with respect to the reference sen-tences manually identified in the app descriptions.",10.1145/3510003.3510058,mobile apps;description synthesis;static analysis;deep learning,Training;Vocabulary;Privacy;Codes;Feature extraction;Behavioral sciences;Mobile applications,Android (operating system);data privacy;graphical user interfaces;language translation;mobile computing;natural language processing;smart phones;text analysis;ubiquitous computing,DescribeCtx;sensitive behaviors;mobile apps;sensitive data;corresponding app descriptions;app functionalities;Describectx;context-aware description synthesis approach;popular apps;app-specific descriptions;1 Android apps;262 Android apps,2
671,Not Mentioned,Detecting False Alarms from Automatic Static Analysis Tools: How Far are We?,H. J. Kang; K. L. Aw; D. Lo,"Singapore Management University Singapore, Singapore; Singapore Management University Singapore, Singapore; Singapore Management University Singapore, Singapore",2022,"Automatic static analysis tools (ASATs), such as Findbugs, have a high false alarm rate. The large number of false alarms produced poses a barrier to adoption. Researchers have proposed the use of machine learning to prune false alarms and present only actionable warnings to developers. The state-of-the-art study has identified a set of â€œGolden Featuresâ€ based on metrics computed over the characteristics and history of the file, code, and warning. Recent studies show that machine learning using these features is extremely effective and that they achieve almost perfect performance. We perform a detailed analysis to better understand the strong performance of the â€œGolden Featuresâ€. We found that several studies used an experimental procedure that results in data leakage and data duplication, which are subtle issues with significant implications. Firstly, the ground-truth labels have leaked into features that measure the proportion of actionable warnings in a given context. Secondly, many warnings in the testing dataset appear in the training dataset. Next, we demonstrate limitations in the warning oracle that determines the ground-truth labels, a heuristic comparing warnings in a given revision to a reference revision in the future. We show the choice of reference revision influences the warning distribution. Moreover, the heuristic produces labels that do not agree with human oracles. Hence, the strong performance of these techniques previously seen is overoptimistic of their true performance if adopted in practice. Our results convey several lessons and provide guidelines for evaluating false alarm detectors.",10.1145/3510003.3510214,static analysis;false alarms;data leakage;data duplication,Training;Measurement;Codes;Static analysis;Machine learning;Detectors;History,learning (artificial intelligence);program debugging;program diagnostics;security of data,Golden Features;machine learning;perfect performance;strong performance;data leakage;data duplication;ground-truth labels;actionable warnings;warning oracle;heuristic comparing warnings;reference revision;warning distribution;false alarm detectors;false alarms;automatic static analysis tools;high false alarm rate;state-of-the-art study,6
672,Not Mentioned,â€œDid You Miss My Comment or What?â€ Understanding Toxicity in Open Source Discussions,C. Miller; S. Cohen; D. Klug; B. Vasilescu; C. KÃ¤stner,Carnegie Mellon University; Wesleyan University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University,2022,"Online toxicity is ubiquitous across the internet and its negative impact on the people and that online communities that it effects has been well documented. However, toxicity manifests differently on various platforms and toxicity in open source communities, while frequently discussed, is not well understood. We take a first stride at understanding the characteristics of open source toxicity to better inform future work on designing effective intervention and detection methods. To this end, we curate a sample of 100 toxic GitHub issue discussions combining multiple search and sampling strategies. We then qualitatively analyze the sample to gain an understanding of the characteristics of open-source toxicity. We find that the pervasive forms of toxicity in open source differ from those observed on other platforms like Reddit or Wikipedia. In our sample, some of the most prevalent forms of toxicity are entitled, demanding, and arrogant comments from project users as well as insults arising from technical disagreements. In addition, not all toxicity was written by people external to the projects; project members were also common authors of toxicity. We also discuss the implications of our findings. Among others we hope that our findings will be useful for future detection work.",10.1145/3510003.3510111,open source software;online toxicity;software development,Toxicology;Social networking (online);Hate speech;Encyclopedias;Internet;Online services;Open source software,Internet;public domain software;social networking (online),Internet;toxic GitHub issue discussions;open-source toxicity;sampling strategies;multiple search;open source communities;toxicity manifests;online communities;online toxicity;open source discussions,2
673,Not Mentioned,Difuzer: Uncovering Suspicious Hidden Sensitive Operations in Android Apps,J. Samhi; L. Li; T. F. BissyandÃ©; J. Klein,"SnT, University of Luxembourg, Luxembourg,; Monash University, Australia; SnT, University of Luxembourg, Luxembourg,; SnT, University of Luxembourg, Luxembourg,",2022,"One prominent tactic used to keep malicious behavior from being detected during dynamic test campaigns is logic bombs, where malicious operations are triggered only when specific conditions are satisfied. Defusing logic bombs remains an unsolved problem in the literature. In this work, we propose to investigate Suspicious Hidden Sensitive Operations (SHSOs) as a step towards triaging logic bombs. To that end, we develop a novel hybrid approach that combines static analysis and anomaly detection techniques to un-cover SHSOs, which we predict as likely implementations of logic bombs. Concretely, Difuzer identifies SHSO entry-points using an instrumentation engine and an inter-procedural data-flow analysis. Then, it extracts trigger-specific features to characterize SHSOs and leverages One-Class SVM to implement an unsupervised learning model for detecting abnormal triggers. We evaluate our prototype and show that it yields a precision of 99.02% to detect SHSOs among which 29.7% are logic bombs. Difuzer outperforms the state-of-the-art in revealing more logic bombs while yielding less false positives in about one order of magnitude less time. All our artifacts are released to the community.",10.1145/3510003.3510135,Android Security;Logic bomb;Software Security;Malware,Support vector machines;Weapons;Instruments;Prototypes;Static analysis;Feature extraction;Internet,Android (operating system);data flow analysis;feature extraction;program testing;security of data;support vector machines;unsupervised learning,Difuzer;logic bombs;malicious behavior;SHSOs;suspicious hidden sensitive operations;Android apps;dynamic test campaigns;malicious operations;static analysis;anomaly detection;instrumentation engine;inter-procedural data-flow analysis;trigger-specific feature extraction;one-class SVM;unsupervised learning;abnormal trigger detection,4
674,Not Mentioned,Discovering Repetitive Code Changes in Python ML Systems,M. Dilhara; A. Ketkar; N. Sannidhi; D. Dig,"University of Colorado Boulder, USA; Uber Technologies Inc., USA; University of Colorado Boulder, USA; University of Colorado Boulder, USA",2022,"Over the years, researchers capitalized on the repetitiveness of software changes to automate many software evolution tasks. Despite the extraordinary rise in popularity of Python-based ML systems, they do not benefit from these advances. Without knowing what are the repetitive changes that ML developers make, researchers, tool, and library designers miss opportunities for automation, and ML developers fail to learn and use best coding practices. To fill the knowledge gap and advance the science and tooling in ML software evolution, we conducted the first and most fine-grained study on code change patterns in a diverse corpus of 1000 top-rated ML systems comprising 58 million SLOC. To conduct this study we reuse, adapt, and improve upon the state-of-the-art repetitive change mining techniques. Our novel tool, R-CPATMINER, mines over 4M commits and constructs 350K fine-grained change graphs and detects 28K change patterns. Using thematic analysis, we identified 22 pattern groups and we reveal 4 major trends of how ML developers change their code. We surveyed 650 ML developers to further shed light on these patterns and their applications, and we received a 15% response rate. We present actionable, empirically-justified implications for four audiences: (i) researchers, (ii) tool builders, (iii) ML library vendors, and (iv) developers and educators.",10.1145/3510003.3510225,Refactoring;Repetition;Code changes;Machine learning;Python,Java;Codes;Market research;Software systems;Libraries;Data mining;History,data mining;learning (artificial intelligence);public domain software;Python;software maintenance,repetitive code changes;software changes;software evolution tasks;Python-based ML systems;repetitive changes;coding practices;ML software evolution;fine-grained study;code change patterns;state-of-the-art repetitive;fine-grained change graphs;ML developers;ML library vendors;R-CPATMINER;repetitive change mining techniques;thematic analysis,6
675,Not Mentioned,Diversity-Driven Automated Formal Verification,E. First; Y. Brun,"University of Massachusetts Amherst, Amherst, MA, USA; University of Massachusetts Amherst, Amherst, MA, USA",2022,"Formally verified correctness is one of the most desirable properties of software systems. But despite great progress made via interactive theorem provers, such as Coq, writing proof scripts for verification remains one of the most effort-intensive (and often prohibitively difficult) software development activities. Recent work has created tools that automatically synthesize proofs or proof scripts. For example, CoqHammer can prove 26.6% of theorems completely automatically by reasoning using precomputed facts, while TacTok and ASTactic, which use machine learning to model proof scripts and then perform biased search through the proof-script space, can prove 12.9% and 12.3% of the theorems, respectively. Further, these three tools are highly complementary; together, they can prove 30.4% of the theorems fully automatically. Our key insight is that control over the learning process can produce a diverse set of models, and that, due to the unique nature of proof synthesis (the existence of the theorem prover, an oracle that infallibly judges a proof's correctness), this diversity can significantly improve these tools' proving power. Accordingly, we develop Diva, which uses a diverse set of models with TacTok's and ASTactic's search mech-anism to prove 21.7% of the theorems. That is, Diva proves 68% more theorems than TacTok and 77% more than ASTactic. Complementary to CoqHammer, Diva proves 781 theorems (27% added value) that CoqHammer does not, and 364 theorems no existing tool has proved automatically. Together with CoqHammer, Diva proves 33.8% of the theorems, the largest fraction to date. We explore nine dimensions for learning diverse models, and identify which dimensions lead to the most useful diversity. Further, we develop an optimization to speed up Diva's execution by 40Ã—. Our study introduces a completely new idea for using diversity in machine learning to improve the power of state-of-the-art proof-script synthesis techniques, and empirically demonstrates that the improvement is significant on a dataset of 68K theorems from 122 open-source software projects.",10.1145/3510003.3510138,Automated formal verification;language models;Coq;interactive proof assistants;proof synthesis,Process control;Machine learning;Writing;Aerospace electronics;Software systems;Cognition;Optimization,formal verification;inference mechanisms;learning (artificial intelligence);problem solving;public domain software;theorem proving,writing proof scripts;proof-script synthesis techniques;optimization;learning process;ASTactic;TacTok;precomputed facts;software systems;proof-script space;interactive theorem provers;formal verification;machine learning;diverse models;CoqHammer;Diva,3
676,Not Mentioned,Domain-Specific Analysis of Mobile App Reviews Using Keyword-Assisted Topic Models,M. Tushev; F. Ebrahimi; A. Mahmoud,"The Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, Louisiana; The Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, Louisiana; The Division of Computer Science and Engineering, Louisiana State University, Baton Rouge, Louisiana",2022,"Mobile application (app) reviews contain valuable information for app developers. A plethora of supervised and unsupervised techniques have been proposed in the literature to synthesize useful user feedback from app reviews. However, traditional supervised classification algorithms require extensive manual effort to label ground truth data, while unsupervised text mining techniques, such as topic models, often produce suboptimal results due to the sparsity of useful information in the reviews. To overcome these limitations, in this paper, we propose a fully automatic and unsupervised approach for extracting useful information from mobile app reviews. The proposed approach is based on keyATM, a keyword-assisted approach for generating topic models. keyATM overcomes the prob-lem of data sparsity by using seeding keywords extracted directly from the review corpus. These keywords are then used to generate meaningful domain-specific topics. Our approach is evaluated over two datasets of mobile app reviews sampled from the domains of Investing and Food Delivery apps. The results show that our approach produces significantly more coherent topics than traditional topic modeling techniques.",10.1145/3510003.3510201,,Text mining;Analytical models;Prototypes;Data models;Software;Mobile applications;Usability,data mining;information retrieval;mobile computing;natural language processing;pattern classification;text analysis,app developers;domain-specific analysis;fully automatic approach;keyword-assisted approach;keyword-assisted topic models;meaningful domain-specific topics;mobile app reviews;mobile application reviews;review corpus;supervised techniques;traditional supervised classification algorithms;traditional topic modeling techniques;unsupervised approach;unsupervised techniques;useful user feedback,1
677,Not Mentioned,DrAsync: Identifying and Visualizing Anti-Patterns in Asynchronous JavaScript,A. Turcotte; M. D. Shah; M. W. Aldrich; F. Tip,"Northeastern University, Boston, MA, USA; Northeastern University, Boston, MA, USA; Tufts University, Medford, MA, USA; Northeastern University, Boston, MA, USA",2022,"Promises and async/await have become popular mechanisms for implementing asynchronous computations in JavaScript, but despite their popularity, programmers have difficulty using them. This paper identifies 8 anti-patterns in promise-based JavaScript code that are prevalent across popular JavaScript repositories. We present a light-weight static analysis for automatically detecting these anti-patterns. This analysis is embedded in an interactive visualization tool that additionally relies on dynamic analysis to visualize promise lifetimes and instances of anti-patterns executed at run time. By enabling the user to navigate between promises in the visualization and the source code fragments that they originate from, problems and optimization opportunities can be identified. We implement this approach in a tool called DrAsync, and found 2.6K static instances of anti-patterns in 20 popular JavaScript repositories. Upon examination of a subset of these, we found that the majority of problematic code reported by DrAsync could be eliminated through refactoring. Further investigation revealed that, in a few cases, the elimination of anti-patterns reduced the time needed to execute the refactored code fragments. Moreover, DrAsync's visualization of promise lifetimes and relationships provides additional insight into the execution behavior of asynchronous programs and helped identify further optimization opportunities.",10.1145/3510003.3510097,JavaScript;asynchronous programming;program analysis;visualization,Visualization;Codes;Navigation;Static analysis;Performance analysis;Behavioral sciences;Optimization,data visualisation;Java;program diagnostics;software maintenance,asynchronous JavaScript;promise-based JavaScript code;lightweight static analysis;interactive visualization tool;JavaScript repositories;DrAsync tool;asynchronous program,
678,Not Mentioned,Dynamic Update for Synthesized GR(1) Controllers,G. Amram; S. Maoz; I. Segall; M. Yossef,Tel Aviv University; Tel Aviv University; Nokia Bell Labs; Tel Aviv University,2022,"Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. GR(1) is an expressive fragment of LTL that enables efficient synthesis and has been recently used in different contexts and application domains. In this paper we investigate the dynamic-update problem for GR(1): updating the behavior of an already running synthesized controller such that it would safely and dynamically, without stopping, start conforming to a modified, up-to-date specification. We formally define the dynamic-update problem and present a sound and complete solution that is based on the computation of a bridge-controller. We implemented the work in the Spectra synthesis and execution environment and evaluated it over benchmark specifications. The evaluation shows the efficiency and effectiveness of using dynamic updates. The work advances the state-of-the-art in reactive synthesis and opens the way to its use in application domains where dynamic updates are a necessary requirement.",10.1145/3510003.3510054,Reactive synthesis;GR(1);Dynamic update,Bridges;Synthesizers;Dynamics;Switches;Maintenance engineering;Benchmark testing;Behavioral sciences,formal specification;temporal logic,dynamic update;reactive synthesis;automated procedure;correct-by-construction reactive system;temporal logic specification;dynamic-update problem;up-to-date specification;bridge-controller;execution environment;benchmark specifications;Spectra synthesis;synthesized GR(1) controllers,
679,Not Mentioned,EAGLE: Creating Equivalent Graphs to Test Deep Learning Libraries,J. Wang; T. Lutellier; S. Qian; H. V. Pham; L. Tan,"Purdue University, West Lafayette, USA; University of Waterloo, Waterloo, Canada; Purdue University, West Lafayette, USA; University of Waterloo, Waterloo, Canada; Purdue University, West Lafayette, USA",2022,"Testing deep learning (DL) software is crucial and challenging. Recent approaches use differential testing to cross-check pairs of implementations of the same functionality across different libraries. Such approaches require two DL libraries implementing the same functionality, which is often unavailable. In addition, they rely on a high-level library, Keras, that implements missing functionality in all supported DL libraries, which is prohibitively expensive and thus no longer maintained. To address this issue, we propose EAGLE, a new technique that uses differential testing in a different dimension, by using equivalent graphs to test a single DL implementation (e.g., a single DL library). Equivalent graphs use different Application Programming Interfaces (APIs), data types, or optimizations to achieve the same functionality. The rationale is that two equivalent graphs executed on a single DL implementation should produce identical output given the same input. Specifically, we design 16 new DL equivalence rules and propose a technique, EAGLE, that (1) uses these equivalence rules to build concrete pairs of equivalent graphs and (2) cross-checks the output of these equivalent graphs to detect inconsistency bugs in a DL library. Our evaluation on two widely-used DL libraries, i.e., Tensor Flow and PyTorch, shows that EAGLE detects 25 bugs (18 in Tensor Flow and 7 in PyTorch), including 13 previously unknown bugs.",10.1145/3510003.3510165,software testing;deep learning;differential testing;graph equivalence,Deep learning;Tensors;Computer bugs;Libraries;Software;Optimization;Testing,application program interfaces;graph theory;learning (artificial intelligence);medical computing;neural nets;program debugging;program diagnostics;software libraries,single DL implementation;equivalence rules;EAGLE;creating equivalent graphs;test deep learning libraries;deep learning software;differential testing;different libraries;high-level library;supported DL libraries;single DL library;different Application Programming Interfaces,
680,Not Mentioned,Efficient Online Testing for DNN-Enabled Systems using Surrogate-Assisted and Many-Objective Optimization,F. U. Haq; D. Shin; L. Briand,"University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg",2022,"With the recent advances of Deep Neural Networks (DNNs) in real-world applications, such as Automated Driving Systems (ADS) for self-driving cars, ensuring the reliability and safety of such DNN-enabled Systems emerges as a fundamental topic in software testing. One of the essential testing phases of such DNN-enabled systems is online testing, where the system under test is embedded into a specific and often simulated application environment (e.g., a driving environment) and tested in a closed-loop mode in interaction with the environment. However, despite the importance of online testing for detecting safety violations, automatically generating new and diverse test data that lead to safety violations presents the following challenges: (1) there can be many safety requirements to be considered at the same time, (2) running a high-fidelity simulator is often very computationally-intensive, and (3) the space of all possible test data that may trigger safety violations is too large to be exhaustively explored. In this paper, we address the challenges by proposing a novel approach, called SAMOTA (Surrogate-Assisted Many-Objective Testing Approach), extending existing many-objective search algorithms for test suite generation to efficiently utilize surrogate models that mimic the simulator, but are much less expensive to run. Empirical evaluation results on Pylot, an advanced ADS composed of multiple DNNs, using CARLA, a high-fidelity driving simulator, show that SAMOTA is significantly more effective and efficient at detecting unknown safety requirement violations than state-of-the-art many-objective test suite generation algorithms and random search. In other words, SAMOTA appears to be a key enabler technology for online testing in practice.",10.1145/3510003.3510188,DNN testing;online testing;many-objective search;surrogate-assisted optimization;self-driving cars,Software testing;Deep learning;Software algorithms;Neural networks;Safety;Software reliability;Optimization,deep learning (artificial intelligence);driver information systems;optimisation;program testing;safety;software reliability,DNN-enabled Systems;surrogate-assisted optimization;automated driving systems;self-driving cars;reliability;software testing;testing phases;DNN-enabled systems;online testing;simulated application environment;driving environment;safety violations;diverse test data;safety requirements;high-fidelity driving simulator;unknown safety requirement violations;ADS;surrogate-assisted many-objective testing approach;many-objective test suite generation algorithms;SAMOTA;CARLA,3
681,Not Mentioned,Eflect: Porting Energy-Aware Applications to Shared Environments,T. Babakol; A. Canino; Y. D. Liu,"SUNY Binghamton, Binghamton, NY, USA; SUNY Binghamton, Binghamton, NY, USA; SUNY Binghamton, Binghamton, NY, USA",2022,"Developing energy-aware applications is a well known approach to software-based energy optimization. This promising approach is however faced with a significant hurdle when deployed to the environments shared among multiple applications, where the energy consumption effected by one application may erroneously be observed by another application. We introduce EFLECT, a novel software framework for disentangling the energy consumption of co-running applications. Our key idea, called energy virtualization, enables each energy-aware application to be only aware of the energy consumption effected by its execution. EFLECT is unique in its lightweight design: it is a purely application-level solution that requires no modification to the underlying hardware or system software. Experiments show Eflect incurs low overhead with high precision. Furthermore, it can seamlessly port existing application-level energy frameworks - one for energy-adaptive approximation and the other for energy profiling - to shared environments while retaining their intended effectiveness.",10.1145/3510003.3510145,Energy Accounting;Energy Profiling;Power Disturbance;Concur-rency,Ports (computers);Energy consumption;Hardware;System software;Virtualization;Optimization;Monitoring,approximation theory;computer networks;energy consumption;power aware computing,energy virtualization;application-level energy frameworks;EFLECT;purely application-level solution;energy-aware application;software framework;energy consumption;software-based energy optimization;energy-aware applications;shared environments;energy profiling;energy-adaptive approximation,
682,Not Mentioned,EREBA: Black-box Energy Testing of Adaptive Neural Networks,M. Haque; Y. Yadlapalli; W. Yang; C. Liu,The University of Texas at Dallas; The University of Texas at Dallas; The University of Texas at Dallas; The University of Texas at Dallas,2022,"Recently, various Deep Neural Network (DNN) models have been proposed for environments like embedded systems with stringent energy constraints. The fundamental problem of determining the ro-bustness of a DNN with respect to its energy consumption (energy robustness) is relatively unexplored compared to accuracy-based ro-bustness. This work investigates the energy robustness of Adaptive Neural Networks (AdNNs), a type of energy-saving DNNs proposed for many energy-sensitive domains and have recently gained traction. We propose EREBA, the first black-box testing method for determining the energy robustness of an AdNN. EREBA explores and infers the relationship between inputs and the energy con-sumption of AdNN s to generate energy surging samples. Extensive implementation and evaluation using three state-of-the-art AdNNs demonstrate that test inputs generated by EREBA could degrade the performance of the system substantially. The test inputs gener-ated by EREBA can increase the energy consumption of AdNN s by 2,000% compared to the original inputs. Our results also show that test inputs generated via EREBA are valuable in detecting energy surging inputs.",10.1145/3510003.3510088,Green AI;AI Energy Testing;Adversarial Machine Learning,Deep learning;Energy consumption;Adaptation models;Adaptive systems;Embedded systems;Neural networks;Green products,convolutional neural nets;deep learning (artificial intelligence);embedded systems;energy consumption;power aware computing;program testing,AdNN;energy surging inputs;black-box energy testing;Adaptive Neural Networks;Deep Neural Network models;DNN;energy constraints;energy consumption;energy robustness;energy-saving DNNs;energy-sensitive domains;black-box testing method;EREBA explores;energy surging samples,1
683,Not Mentioned,Evaluating and Improving Neural Program-Smoothing-based Fuzzing,M. Wu; L. Jiang; J. Xiang; Y. Zhang; G. Yang; H. Ma; S. Nie; S. Wu; H. Cui; L. Zhang,"Southern University of Science and Technology, Shenzhen, China; Southern University of Science and Technology, Shenzhen, China; Southern University of Science and Technology, Shenzhen, China; Southern University of Science and Technology, Shenzhen, China; The University of Queensland, Brisbane, Australia; Tencent Security Keen Lab, Shanghai, China; Tencent Security Keen Lab, Shanghai, China; Tencent Security Keen Lab, Shanghai, China; The University of Hong Kong, Hong Kong, China; University of Illinois, Urbana-Champaign, USA",2022,"Fuzzing nowadays has been commonly modeled as an optimization problem, e.g., maximizing code coverage under a given time budget via typical search-based solutions such as evolutionary algorithms. However, such solutions are widely argued to cause inefficient computing resource usage, i.e., inefficient mutations. To address this issue, two neural program-smoothing-based fuzzers, Neuzz and MTFuzz, have been recently proposed to approximate program branching behaviors via neural network models, which input byte sequences of a seed and output vectors representing program branching behaviors. Moreover, assuming that mutating the bytes with larger gradients can better explore branching behaviors, they develop strategies to mutate such bytes for generating new seeds as test cases. Meanwhile, although they have been shown to be effective in the original papers, they were only evaluated upon a limited dataset. In addition, it is still unclear how their key technical components and whether other factors can impact fuzzing performance. To further investigate neural program-smoothing-based fuzzing, we first construct a large-scale benchmark suite with a total of 28 popular open-source projects. Then, we extensively evaluate Neuzz and MTFuzz on such benchmarks. The evaluation results suggest that their edge coverage performance can be unstable. Moreover, neither neural network models nor mutation strategies can be consistently effective, and the power of their gradient-guidance mechanisms have been compromised. Inspired by such findings, we propose a simplistic technique, PreFuzz, which improves neural program-smoothing-based fuzzers with a resource-efficient edge selection mechanism to enhance their gradient guidance and a probabilistic byte selection mechanism to further boost mutation effectiveness. Our evaluation results indicate that PreFuzz can significantly increase the edge coverage of Neuzz/MTFuzz, and also reveal multiple practical guidelines to advance future research on neural program-smoothing-based fuzzing.",10.1145/3510003.3510089,,Computational modeling;Neural networks;Fuzzing;Benchmark testing;Probabilistic logic;Search problems;Behavioral sciences,evolutionary computation;neural nets;optimisation;public domain software;search problems,mutation strategies;neural program-smoothing-based fuzzing;computing resource usage;neural network models;optimization problem;PreFuzz;search-based solutions;program branching;open-source projects;Neuzz;MTFuzz;evolutionary algorithm,4
684,Not Mentioned,ExAIS: Executable AI Semantics,R. Schumi; J. Sun,"Singapore Management University, Singapore; Singapore Management University, Singapore",2022,"Neural networks can be regarded as a new programming paradigm, i.e., instead of building ever-more complex programs through (often informal) logical reasoning in the programmers' mind, complex â€˜AIâ€™ systems are built by optimising generic neural network models with big data. In this new paradigm, AI frameworks such as Ten-sorFlow and PyTorch play a key role, which is as essential as the compiler for traditional programs. It is known that the lack of a proper semantics for programming languages (such as C), i.e., a correctness specification for compilers, has contributed to many problematic program behaviours and security issues. While it is in general hard to have a correctness specification for compilers due to the high complexity of programming languages and their rapid evolution, we have a unique opportunity to do it right this time for neural networks (which have a limited set of functions, and most of them have stable semantics). In this work, we report our effort on providing a correctness specification of neural net-work frameworks such as TensorFlow. We specify the semantics of almost all TensorFlow layers in the logical programming language Prolog. We demonstrate the usefulness of the semantics through two applications. One is a fuzzing engine for TensorFlow, which features a strong oracle and a systematic way of generating valid neural networks. The other is a model validation approach which enables consistent bug reporting for TensorFlow models.",10.1145/3510003.3510112,AI frameworks;AI libraries;deep learning models;semantics;specification;test case generation;model validation;AI model generation,Deep learning;Computer languages;Program processors;Systematics;Semantics;Computer bugs;Neural networks,inference mechanisms;logic programming languages;neural nets;program debugging;program diagnostics;PROLOG;software fault tolerance,correctness specification;compiler;programming languages;stable semantics;net-work frameworks;logical programming language Prolog;valid neural networks;TensorFlow models;programming paradigm;complex programs;logical reasoning;programmers;complex AI systems;generic neural network models;big data;AI frameworks;Ten-sorFlow;traditional programs;proper semantics;problematic program behaviours;security issues,
685,Not Mentioned,Explanation-Guided Fairness Testing through Genetic Algorithm,M. Fan; W. Wei; W. Jin; Z. Yang; T. Liu,"Xi'an Jiaotong University, China; Xi'an Jiaotong University, China; Xi'an Jiaotong University, China; Xi'an Jiaotong University, China; Xi'an Jiaotong University, China",2022,"The fairness characteristic is a critical attribute of trusted AI systems. A plethora of research has proposed diverse methods for individual fairness testing. However, they are suffering from three major limitations, i.e., low efficiency, low effectiveness, and model-specificity. This work proposes ExpGA, an explanation-guided fairness testing approach through a genetic algorithm (GA). ExpGA employs the explanation results generated by interpretable methods to collect high-quality initial seeds, which are prone to derive discriminatory samples by slightly modifying feature values. ExpGA then adopts GA to search discriminatory sample candidates by optimizing a fitness value. Benefiting from this combination of explanation results and GA, ExpGA is both efficient and effective to detect discriminatory individuals. Moreover, ExpGA only requires prediction probabilities of the tested model, resulting in a better generalization capability to various models. Experiments on multiple real-world benchmarks, including tabular and text datasets, show that ExpGA presents higher efficiency and effectiveness than four state-of-the-art approaches.",10.1145/3510003.3510137,Explanation result;fairness testing;genetic algorithm,Software algorithms;Predictive models;Benchmark testing;Software;Artificial intelligence;Genetic algorithms;Software engineering,genetic algorithms;probability;program testing;search problems;trusted computing,discriminatory sample candidates;high-quality initial seeds;interpretable methods;GA;genetic algorithm;explanation-guided fairness testing approach;ExpGA;model-specificity;trusted AI systems,1
686,Not Mentioned,Exploiting Input Sanitization for Regex Denial of Service,E. Barlas; X. Du; J. C. Davis,"Purdue University, West Lafayette, Indiana, USA; Purdue University, West Lafayette, Indiana, USA; Purdue University, West Lafayette, Indiana, USA",2022,"Web services use server-side input sanitization to guard against harmful input. Some web services publish their sanitization logic to make their client interface more usable, e.g., allowing clients to debug invalid requests locally. However, this usability practice poses a security risk. Specifically, services may share the regexes they use to sanitize input strings - and regex-based denial of service (ReDoS) is an emerging threat. Although prominent service outages caused by ReDoS have spurred interest in this topic, we know little about the degree to which live web services are vulnerable to ReDoS. In this paper, we conduct the first black-box study measuring the extent of ReDoS vulnerabilities in live web services. We apply the Consistent Sanitization Assumption: that client-side sanitization logic, including regexes, is consistent with the sanitization logic on the server-side. We identify a service's regex-based input sanitization in its HTML forms or its API, find vulnerable regexes among these regexes, craft ReDoS probes, and pinpoint vulnerabilities. We analyzed the HTML forms of 1,000 services and the APIs of 475 services. Of these, 355 services publish regexes; 17 services publish unsafe regexes; and 6 services are vulnerable to ReDoS through their APIs (6 domains; 15 subdomains). Both Microsoft and Amazon Web Services patched their web services as a result of our disclosure. Since these vulnerabilities were from API specifications, not HTML forms, we proposed a ReDoS defense for a popular API validation library, and our patch has been merged. To summarize: in client-visible sanitization logic, some web services advertise Re-DoS vulnerabilities in plain sight. Our results motivate short-term patches and long-term fundamental solutions. â€œMake measurable what cannot be measured.â€ -Galileo Galilei",10.1145/3510003.3510047,Empirical software engineering;regular expressions;ReDoS;web security;denial of service;algorithmic complexity attacks,Web services;Software;Libraries;Servers;Security;Usability;Probes,application program interfaces;formal logic;hypermedia markup languages;security of data;Web services,server-side input sanitization;Web services;ReDoS vulnerabilities;regex-based input sanitization;HTML forms;client-visible sanitization logic;regex denial of service;consistent sanitization assumption;API;security risk,1
687,Not Mentioned,FADATest: Fast and Adaptive Performance Regression Testing of Dynamic Binary Translation Systems,J. Wu; J. Dong; R. Fang; W. Zhang; W. Wang; D. Zuo,"Harbin Institute of Technology, China; Harbin Institute of Technology, China; University of Georgia, USA; University of Georgia, USA; University of Georgia, USA; Harbin Institute of Technology, China",2022,"Dynamic binary translation (DBT) is the cornerstone of many im-portant applications. In practice, however, it is quite difficult to maintain the performance efficiency of a DBT system due to its inherent complexity. Although performance regression testing is an effective approach to detect potential performance regression issues, it is not easy to apply performance regression testing to DBT sys-tems, because of the natural differences between DBT systems and common software systems and the limited availability of effective test programs. In this paper, we present FADATest, which devises several novel techniques to address these challenges. Specifically, FADATest automatically generates adaptable test programs from existing real benchmark programs of DBT systems according to the runtime characteristics of the benchmarks. The test programs can then be used to achieve highly efficient and adaptive performance regression testing of DBT systems. We have implemented a proto-type of FADATest. Experimental results show that FADATest can successfully uncover the same performance regression issues across the evaluated versions of two popular DBT systems, QEMU and Valgrind, as the original benchmark programs. Moreover, the testing efficiency is improved significantly on two different hardware platforms powered by x86-64 and AArch64, respectively.",10.1145/3510003.3510169,Performance regression testing;DBT;Test program generation,Adaptive systems;Runtime;Codes;Prototypes;Benchmark testing;Software systems;Hardware,multiprocessing systems;program interpreters;program testing;regression analysis;storage management;virtual machines,FADATest;adaptive performance regression testing;dynamic binary translation systems;performance efficiency;DBT system;potential performance regression issues;DBT sys-tems;common software systems;effective test programs;adaptable test programs;popular DBT systems;testing efficiency,
688,Not Mentioned,Fairness-aware Configuration of Machine Learning Libraries,S. Tizpaz-Niari; A. Kumar; G. Tan; A. Trivedi,University of Texas at El Paso; Pennsylvania State University; Pennsylvania State University; University of Colorado Boulder,2022,"This paper investigates the parameter space of machine learning (ML) algorithms in aggravating or mitigating fairness bugs. Data-driven software is increasingly applied in social-critical applications where ensuring fairness is of paramount importance. The existing approaches focus on addressing fairness bugs by either modifying the input dataset or modifying the learning algorithms. On the other hand, the selection of hyperparameters, which provide finer controls of ML algorithms, may enable a less intrusive approach to influence the fairness. Can hyperparameters amplify or suppress discrimination present in the input dataset? How can we help programmers in detecting, understanding, and exploiting the role of hyperparameters to improve the fairness? We design three search-based software testing algorithms to un-cover the precision-fairness frontier of the hyperparameter space. We complement these algorithms with statistical debugging to explain the role of these parameters in improving fairness. We implement the proposed approaches in the tool Parfait-ML (PARameter FAIrness Testing for ML Libraries) and show its effectiveness and utility over five mature ML algorithms as used in six social-critical applications. In these applications, our approach successfully iden-tified hyperparameters that significantly improve (vis-a-vis the state-of-the-art techniques) the fairness without sacrificing precision. Surprisingly, for some algorithms (e.g., random forest), our approach showed that certain configuration of hyperparameters (e.g., restricting the search space of attributes) can amplify biases across applications. Upon further investigation, we found intuitive explanations of these phenomena, and the results corroborate simi-lar observations from the literature.",10.1145/3510003.3510202,,Software testing;Machine learning algorithms;Software algorithms;Computer bugs;Libraries;Software;Space exploration,learning (artificial intelligence);program debugging;program testing;statistical analysis,hyperparameter space;improving fairness;tool Parfait-ML;PARameter FAIrness;ML Libraries;mature ML algorithms;social-critical applications;iden-tified hyperparameters;fairness-aware configuration;parameter space;machine learning algorithms;aggravating fairness bugs;mitigating fairness bugs;data-driven software;ensuring fairness;input dataset;intrusive approach;search-based software testing algorithms;precision-fairness frontier,3
689,Not Mentioned,Fairneuron: Improving Deep Neural Network Fairness with Adversary Games on Selective Neurons,X. Gao; J. Zhai; S. Ma; C. Shen; Y. Chen; Q. Wang,"Xi'an Jiaotong University, Xi'an, China; Rutgers University, United States; Rutgers University, United States; Xi'an Jiaotong University, Xi'an, China; Xi'an Jiaotong University, Xi'an, China; Wuhan University, Wuhan, China",2022,"With Deep Neural Network (DNN) being integrated into a growing number of critical systems with far-reaching impacts on society, there are increasing concerns on their ethical performance, such as fairness. Unfortunately, model fairness and accuracy in many cases are contradictory goals to optimize during model training. To solve this issue, there has been a number of works trying to improve model fairness by formalizing an adversarial game in the model level. This approach introduces an adversary that evaluates the fairness of a model besides its prediction accuracy on the main task, and performs joint-optimization to achieve a balanced result. In this paper, we noticed that when performing backward prop-agation based training, such contradictory phenomenon are also observable on individual neuron level. Based on this observation, we propose Fairneuron, a Dnn model automatic repairing tool, to mitigate fairness concerns and balance the accuracy-fairness trade-off without introducing another model. It works on detecting neurons with contradictory optimization directions from accuracy and fairness training goals, and achieving a trade-off by selective dropout. Comparing with state-of-the-art methods, our approach is lightweight, scaling to large models and more efficient. Our eval-uation on three datasets shows that Fairneuron can effectively improve all models' fairness while maintaining a stable utility.",10.1145/3510003.3510087,fairness;path analysis;neural networks,Training;Deep learning;Neurons;Neural networks;Games;Predictive models;Search problems,deep learning (artificial intelligence);game theory;optimisation,joint-optimization;neuron level;Fairneuron;accuracy-fairness trade-off;contradictory optimization directions;deep neural network fairness;selective neurons;model fairness;model training;adversarial game;DNN;ethical performance;backward propagation based training;automatic repairing tool;fairness training goals,2
690,Not Mentioned,Fast and Precise Application Code Analysis using a Partial Library,A. Utture; J. Palsberg,"University of California, Los Angeles, U.S.A.; University of California, Los Angeles, U.S.A.",2022,"Long analysis times are a key bottleneck for the widespread adoption of whole-program static analysis tools. Fortunately, however, a user is often only interested in finding errors in the application code, which constitutes a small fraction of the whole program. Current application-focused analysis tools overapproximate the effect of the library and hence reduce the precision of the analysis results. However, empirical studies have shown that users have high expectations on precision and will ignore tool results that don't meet these expectations. In this paper, we introduce the first tool QueryMax that significantly speeds up an application code analysis without dropping any precision. QueryMax acts as a pre-processor to an existing analysis tool to select a partial library that is most relevant to the analysis queries in the application code. The selected partial library plus the application is given as input to the existing static analysis tool, with the remaining library pointers treated as the bottom element in the abstract domain. This achieves a significant speedup over a whole-program analysis, at the cost of a few lost errors, and with no loss in precision. We instantiate and run experiments on QueryMax for a cast-check analysis and a null-pointer analysis. For a particular configuration, QueryMax enables these two analyses to achieve, relative to a whole-program analysis, an average recall of 87%, a precision of 100% and a geometric mean speedup of 10x.",10.1145/3510003.3510046,Static Analysis,Codes;Costs;Static analysis;Libraries;Software engineering,program diagnostics;software libraries,whole-program static analysis tools;application code analysis;static analysis tool;cast-check analysis;null-pointer analysis;library pointers;partial library;QueryMax;application-focused analysis tools,
691,Not Mentioned,Fast Changeset-based Bug Localization with BERT,A. Ciborowska; K. Damevski,"Department of Computer Science, Virginia Commonwealth University, Richmond, VA, USA; Department of Computer Science, Virginia Commonwealth University, Richmond, VA, USA",2022,"Automatically localizing software bugs to the changesets that induced them has the potential to improve software developer efficiency and to positively affect software quality. To facilitate this automation, a bug report has to be effectively matched with source code changes, even when a significant lexical gap exists between natural language used to describe the bug and identifier naming practices used by developers. To bridge this gap, we need techniques that are able to capture software engineering-specific and project-specific semantics in order to detect relatedness between the two types of documents that goes beyond exact term matching. Popular transformer-based deep learning architectures, such as BERT, excel at leveraging contextual information, hence appear to be a suitable candidate for the task. However, BERT-like models are computationally expensive, which precludes them from being used in an environment where response time is important. In this paper, we describe how BERT can be made fast enough to be applicable to changeset-based bug localization. We also explore several design decisions in using BERT for this purpose, including how best to encode changesets and how to match bug reports to individual changes for improved accuracy. We compare the accuracy and performance of our model to a non-contextual baseline (i.e., vector space model) and BERT-based architectures previously used in software engineering. Our evaluation results demonstrate advantages in using the proposed BERT model compared to the baselines, especially for bug reports that lack any hints about related code elements.",10.1145/3510003.3510042,bug localization;changesets;information retrieval;BERT,Location awareness;Codes;Computer bugs;Bit error rate;Semantics;Computer architecture;Transformers,information retrieval;learning (artificial intelligence);natural language processing;program debugging;software engineering;software maintenance;software quality;source code (software),bug report;source code changes;significant lexical gap;identifier naming practices;software engineering-specific;project-specific semantics;exact term matching;popular transformer-based deep learning architectures;changesets;BERT model;fast changeset-based bug localization;automatically localizing software bugs;software developer efficiency;software quality,4
692,Not Mentioned,Fault Localization via Efficient Probabilistic Modeling of Program Semantics,M. Zeng; Y. Wu; Z. Ye; Y. Xiong; X. Zhang; L. Zhang,"Key Laboratory of High Confidence Software Technologies, School of Computer Science, Ministry of Education (Peking University), Peking University, Beijing, PR, China; Key Laboratory of High Confidence Software Technologies, School of Computer Science, Ministry of Education (Peking University), Peking University, Beijing, PR, China; Key Laboratory of High Confidence Software Technologies, School of Computer Science, Ministry of Education (Peking University), Peking University, Beijing, PR, China; Key Laboratory of High Confidence Software Technologies, School of Computer Science, Ministry of Education (Peking University), Peking University, Beijing, PR, China; Key Laboratory of High Confidence Software Technologies, School of Computer Science, Ministry of Education (Peking University), Peking University, Beijing, PR, China; Key Laboratory of High Confidence Software Technologies, School of Computer Science, Ministry of Education (Peking University), Peking University, Beijing, PR, China",2022,"Testing-based fault localization has been a significant topic in software engineering in the past decades. It localizes a faulty program element based on a set of passing and failing test executions. Since whether a fault could be triggered and detected by a test is related to program semantics, it is crucial to model program semantics in fault localization approaches. Existing approaches either consider the full semantics of the program (e.g., mutation-based fault localization and angelic debugging), leading to scalability issues, or ignore the semantics of the program (e.g., spectrum-based fault localization), leading to imprecise localization results. Our key idea is: by modeling only the correctness of program values but not their full semantics, a balance could be reached between effectiveness and scalability. To realize this idea, we introduce a probabilistic approach to model program semantics and utilize information from static analysis and dynamic execution traces in our modeling. Our approach, SmartFL (SeMantics bAsed pRobabilisTic Fault Localization), is evaluated on a real-world dataset, Defects4J. The top-1 statement-level accuracy of our approach is 21 %, which is the best among state-of-the-art methods. The average time cost is 210 seconds per fault while existing methods that capture full semantics are often 10x or more slower.",10.1145/3510003.3510073,fault localization;semantics;probabilistic modeling,Location awareness;Analytical models;Scalability;Semantics;Static analysis;Debugging;Probabilistic logic,probability;program debugging;program diagnostics;program testing;program verification;programming language semantics;software fault tolerance,program values;program semantics;probabilistic modeling;faulty program element;mutation-based fault localization;spectrum-based fault localization;software engineering;angelic debugging;scalability issues;static analysis;dynamic execution traces;SmartFL;SeMantics bAsed pRobabilisTic Fault Localization;Defects4J,
693,Not Mentioned,FIRA: Fine-Grained Graph-Based Code Change Representation for Automated Commit Message Generation,J. Dong; Y. Lou; Q. Zhu; Z. Sun; Z. Li; W. Zhang; D. Hao,"Key Laboratory of High Confidence Software Technologies (Peking University), MoE School of Computer Science, Peking University, Beijing, China; Department of Computer Science, Purdue University West Lafayette, IN, USA; Key Laboratory of High Confidence Software Technologies (Peking University), MoE School of Computer Science, Peking University, Beijing, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE School of Computer Science, Peking University, Beijing, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE School of Computer Science, Peking University, Beijing, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE School of Computer Science, Peking University, Beijing, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE School of Computer Science, Peking University, Beijing, China",2022,"Commit messages summarize code changes of each commit in nat-ural language, which help developers understand code changes without digging into detailed implementations and play an essen-tial role in comprehending software evolution. To alleviate human efforts in writing commit messages, researchers have proposed var-ious automated techniques to generate commit messages, including template-based, information retrieval-based, and learning-based techniques. Although promising, previous techniques have limited effectiveness due to their coarse-grained code change representations. This work proposes a novel commit message generation technique, FIRA, which first represents code changes via fine-grained graphs and then learns to generate commit messages automati-cally. Different from previous techniques, FIRA represents the code changes with fine-grained graphs, which explicitly describe the code edit operations between the old version and the new version, and code tokens at different granularities (i.e., sub-tokens and integral tokens). Based on the graph-based representation, FIRA generates commit messages by a generation model, which includes a graph-neural-network-based encoder and a transformer-based decoder. To make both sub-tokens and integral tokens as available ingredients for commit message generation, the decoder is further incorporated with a novel dual copy mechanism. We further per-form an extensive study to evaluate the effectiveness of FIRA. Our quantitative results show that FIRA outperforms state-of-the-art techniques in terms of BLEU, ROUGE-L, and METEOR; and our ablation analysis further shows that major components in our technique both positively contribute to the effectiveness of FIRA. In addition, we further perform a human study to evaluate the quality of generated commit messages from the perspective of developers, and the results consistently show the effectiveness of FIRA over the compared techniques.",10.1145/3510003.3510069,Commit Message Generation;Graph Neural Network;Code Change Representation,Vocabulary;Codes;Writing;Benchmark testing;Transformers;Software;Graph neural networks,configuration management;graph theory;information retrieval;learning (artificial intelligence);neural nets;public domain software;software maintenance,code changes;fine-grained graphs;code edit operations;code tokens;graph-based representation;FIRA;graph-neural-network-based encoder;generated commit messages;automated commit message generation;information retrieval-based;learning-based techniques;coarse-grained code change representations;novel commit message generation technique,
694,Not Mentioned,FlakiMe: Laboratory-Controlled Test Flakiness Impact Assessment,M. Cordy; R. Rwemalika; A. Franci; M. Papadakis; M. Harman,"SnT, University of Luxembourg, Luxembourg; SnT, University of Luxembourg, Luxembourg; SnT, University of Luxembourg, Luxembourg; SnT, University of Luxembourg, Luxembourg; Meta Platforms inc., and University College, London, United Kingdom",2022,"Much research on software testing makes an implicit assumption that test failures are deterministic such that they always witness the presence of the same defects. However, this assumption is not always true because some test failures are due to so-called flaky tests, i.e., tests with non-deterministic outcomes. To help testing researchers better investigate flakiness, we introduce a test flakiness assessment and experimentation platform, called FlakiMe. FlakiMe supports the seeding of a (controllable) degree of flakiness into the behaviour of a given test suite. Thereby, FlakiMe equips researchers with ways to investigate the impact of test flakiness on their techniques under laboratory-controlled conditions. To demonstrate the application of FlakiMe, we use it to assess the impact of flakiness on mutation testing and program repair (the PRAPR and ARJA methods). These results indicate that a 10% flakiness is sufficient to affect the mutation score, but the effect size is modest (2% â€“ 5%), while it reduces the number of patches produced for repair by 20% up to 100% of repair problems; a devastating impact on this application of testing. Our experiments with FlakiMe demonstrate that flakiness affects different testing applications in very different ways, thereby motivating the need for a laboratory-controllable flakiness impact assessment platform and approach such as FlakiMe.",10.1145/3510003.3510194,,Software testing;Location awareness;Sensitivity;Maintenance engineering;Aerospace electronics;Software engineering;Resilience,program testing,software testing;implicit assumption;test failures;flaky tests;nondeterministic outcomes;test flakiness assessment;experimentation platform;test suite;FlakiMe;mutation testing;program repair;laboratory-controlled test flakiness impact assessment;ARJA methods;PRAPR,2
695,Not Mentioned,Free Lunch for Testing: Fuzzing Deep-Learning Libraries from Open Source,A. Wei; Y. Deng; C. Yang; L. Zhang,Stanford University; University of Illinois at Urbana-Champaign; Nanjing University; University of Illinois at Urbana-Champaign,2022,"Deep learning (DL) systems can make our life much easier, and thus are gaining more and more attention from both academia and industry. Meanwhile, bugs in DL systems can be disastrous, and can even threaten human lives in safety-critical applications. To date, a huge body of research efforts have been dedicated to testing DL models. However, interestingly, there is still limited work for testing the underlying DL libraries, which are the foundation for building, optimizing, and running DL models. One potential reason is that test generation for the underlying DL libraries can be rather challenging since their public APIs are mainly exposed in Python, making it even hard to automatically determine the API input parameter types due to dynamic typing. In this paper, we propose FreeFuzz, the first approach to fuzzing DL libraries via mining from open source. More specifically, FreeFuzz obtains code/models from three different sources: 1) code snippets from the library documentation, 2) library developer tests, and 3) DL models in the wild. Then, FreeFuzz automatically runs all the collected code/models with instrumentation to trace the dynamic information for each covered API, including the types and values of each parameter during invocation, and shapes of input/output tensors. Lastly, FreeFuzz will leverage the traced dynamic information to perform fuzz testing for each covered API. The extensive study of FreeFuzz on PyTorch and TensorFlow, two of the most popular DL libraries, shows that FreeFuzz is able to automatically trace valid dynamic information for fuzzing 1158 popular APIs, 9X more than state-of-the-art LEMON with 3.5X lower overhead than LEMON. To date, FreeFuzz has detected 49 bugs for PyTorch and TensorFlow (with 38 already confirmed by developers as previously unknown).",10.1145/3510003.3510041,,Tensors;Shape;Instruments;Computer bugs;Documentation;Fuzzing;Libraries,application program interfaces;data mining;fuzzy set theory;learning (artificial intelligence);program debugging;program testing;security of data;Web services,dynamic typing;FreeFuzz;fuzzing DL libraries;open source;library documentation;covered API;traced dynamic information;fuzz testing;popular DL libraries;valid dynamic information;1158 popular APIs;free lunch;deep-learning libraries;deep learning systems;DL systems;human lives;safety-critical applications;huge body;underlying DL libraries;running DL models;test generation;public APIs;API input parameter types,7
696,Not Mentioned,Fuzzing Class Specifications,F. Molina; M. D'Amorim; N. Aguirre,"University of Rio Cuarto and CONICET, Argentina; Federal University of Pernambuco, Brazil; University of Rio Cuarto and CONICET, Argentina",2022,"Expressing class specifications via executable constraints is important for various software engineering tasks such as test generation, bug finding and automated debugging, but developers rarely write them. Techniques that infer specifications from code exist to fill this gap, but they are designed to support specific kinds of assertions and are difficult to adapt to support different assertion languages, e.g., to add support for quantification, or additional comparison operators, such as membership or containment. To address the above issue, we present SPECFUZZER, a novel technique that combines grammar-based fuzzing, dynamic invariant detection, and mutation analysis, to automatically produce class specifications. SPECFUZZER uses: (i) a fuzzer as a generator of candidate assertions derived from a grammar that is automatically obtained from the class definition; (ii) a dynamic invariant detector -Daikon- to filter out assertions invalidated by a test suite; and (iii) a mutation-based mechanism to cluster and rank assertions, so that similar constraints are grouped and then the stronger prioritized. Grammar-based fuzzing enables SPECFUZZER to be straightforwardly adapted to support different specification languages, by manipulating the fuzzing grammar, e.g., to include additional operators. We evaluate our technique on a benchmark of 43 Java methods employed in the evaluation of the state-of-the-art techniques GAssert and EvoSpex. Our results show that SPECFUZZER can easily support a more expressive assertion language, over which is more effective than GAssert and EvoSpex in inferring specifications, according to standard performance metrics.",10.1145/3510003.3510120,Oracle problem;specification inference;grammar-based fuzzing,Measurement;Java;Fuzzing;Generators;Grammar;Specification languages;Test pattern generators,formal specification;grammars;Java;program debugging;program testing;program verification;security of data;software engineering;specification languages,class specifications;executable constraints;software engineering tasks;test generation;infer specifications;support specific kinds;support different assertion languages;additional comparison operators;SPECFUZZER;grammar-based fuzzing;dynamic invariant detection;candidate assertions;class definition;dynamic invariant detector -Daikon;mutation-based mechanism;cluster;rank assertions;support different specification languages;expressive assertion language;inferring specifications,3
697,Not Mentioned,Garbage Collection Makes Rust Easier to Use: A Randomized Controlled Trial of the Bronze Garbage Collector,M. Coblenz; M. L. Mazurek; M. Hicks,"University of Maryland College Park, Maryland, USA; University of Maryland College Park, Maryland, USA; University of Maryland College Park, Maryland, USA",2022,"Rust is a general-purpose programming language that is both type-and memory-safe. Rust does not use a garbage collector, but rather achieves these properties through a sophisticated, but complex, type system. Doing so makes Rust very efficient, but makes Rust relatively hard to learn and use. We designed Bronze, an optional, library-based garbage collector for Rust. To see whether Bronze could make Rust more usable, we conducted a randomized con-trolled trial with volunteers from a 633-person class, collecting data from 428 students in total. We found that for a task that required managing complex aliasing, Bronze users were more likely to complete the task in the time available, and those who did so required only about a third as much time (4 hours vs. 12 hours). We found no significant difference in total time, even though Bronze users re-did the task without Bronze afterward. Surveys indicated that ownership, borrowing, and lifetimes were primary causes of the challenges that users faced when using Rust.",10.1145/3510003.3510107,Rust;garbage collection;usability of programming languages;em-pirical study of programming languages;programming education,Computer languages;Education;Task analysis;Programming profession;Software engineering,program compilers;programming languages;storage management,randomized controlled trial;Bronze garbage collector;garbage collection;optional-based garbage collector;library-based garbage collector;general-purpose programming language;Rust;memory-safe;type-safe;data collection;aliasing,2
698,Not Mentioned,Generating and Visualizing Trace Link Explanations,Y. Liu; J. Lin; O. Anuyah; R. Metoyer; J. Cleland-Huang,"University of Notre Dame, Notre Dame, IN; University of Notre Dame, Notre Dame, IN; University of Notre Dame, Notre Dame, IN; University of Notre Dame, Notre Dame, IN; University of Notre Dame, Notre Dame, IN",2022,"Recent breakthroughs in deep-learning (DL) approaches have resulted in the dynamic generation of trace links that are far more accurate than was previously possible. However, DL-generated links lack clear explanations, and therefore non-experts in the domain can find it difficult to understand the underlying semantics of the link, making it hard for them to evaluate the link's correctness or suitability for a specific software engineering task. In this paper we present a novel NLP pipeline for generating and visualizing trace link explanations. Our approach identifies domain-specific concepts, retrieves a corpus of concept-related sentences, mines concept definitions and usage examples, and identifies relations between cross-artifact concepts in order to explain the links. It applies a post-processing step to prioritize the most likely acronyms and definitions and to eliminate non-relevant ones. We evaluate our approach using project artifacts from three different domains of interstellar telescopes, positive train control, and electronic health-care systems, and then report coverage, correctness, and potential utility of the generated definitions. We design and utilize an explanation interface which leverages concept definitions and relations to visualize and explain trace link rationales, and we report results from a user study that was conducted to evaluate the effectiveness of the explanation interface. Results show that the explanations presented in the interface helped non-experts to understand the underlying semantics of a trace link and improved their ability to vet the correctness of the link.",10.1145/3510003.3510129,Software traceability;explanation interface;concept mining,Terminology;Statistical analysis;Semantics;Pipelines;Telescopes;Software;Positive train control,data mining;data visualisation;deep learning (artificial intelligence);information retrieval;natural language processing;software engineering;user interfaces,software engineering task;domain-specific concepts;concept-related sentences;cross-artifact concepts;explanation interface;trace link rationales;deep-learning approaches;dynamic generation;DL-generated links;trace link explanations visualization;interstellar telescopes;positive train control;electronic health-care systems;NLP pipeline,
699,Not Mentioned,GIFdroid: Automated Replay of Visual Bug Reports for Android Apps,S. Feng; C. Chen,"Monash University, Melbourne, Australia; Monash University, Melbourne, Australia",2022,"Bug reports are vital for software maintenance that allow users to inform developers of the problems encountered while using software. However, it is difficult for non-technical users to write clear descriptions about the bug occurrence. Therefore, more and more users begin to record the screen for reporting bugs as it is easy to be created and contains detailed procedures triggering the bug. But it is still tedious and time-consuming for developers to reproduce the bug due to the length and unclear actions within the recording. To overcome these issues, we propose GIFdroid, a light-weight approach to automatically replay the execution trace from visual bug reports. GIFdroid adopts image processing techniques to extract the keyframes from the recording, map them to states in GUI Transitions Graph, and generate the execution trace of those states to trigger the bug. Our automated experiments and user study demonstrate its accuracy, efficiency, and usefulness of the approach.",10.1145/3510003.3510048,bug replay;visual recording;android testing,Visualization;Software maintenance;Image processing;Computer bugs;Software algorithms;Life estimation;Recording,data visualisation;graphical user interfaces;image processing;program debugging;software maintenance,visual bug reports;software maintenance;nontechnical users;bug occurrence;execution trace;GIFdroid adopts image;automated experiments;automated replay,5
700,Not Mentioned,GitHub Sponsors: Exploring a New Way to Contribute to Open Source,N. Shimada; T. Xiao; H. Hata; C. Treude; K. Matsumoto,"Nara Institute of Science and Technology, Japan; Nara Institute of Science and Technology, Japan; Shinshu University, Japan; University of Melbourne, Australia; Nara Institute of Science and Technology, Japan",2022,"GitHub Sponsors, launched in 2019, enables donations to individual open source software (OSS) developers. Financial support for OSS maintainers and developers is a major issue in terms of sustaining OSS projects, and the ability to donate to individuals is expected to support the sustainability of developers, projects, and community. In this work, we conducted a mixed-methods study of GitHub Sponsors, including quantitative and qualitative analyses, to understand the characteristics of developers who are likely to receive donations and what developers think about donations to individuals. We found that: (1) sponsored developers are more active than non-sponsored developers, (2) the possibility to receive donations is related to whether there is someone in their community who is donating, and (3) developers are sponsoring as a new way to contribute to OSS. Our findings are the first step towards data-informed guidance for using GitHub Sponsors, opening up avenues for future work on this new way of financially sustaining the OSS community.",10.1145/3510003.3510116,GitHub Sponsors;Open Source;Sponsorship,Focusing;Sustainable development;Open source software;Software development management;Software engineering,public domain software;software development management;software maintenance,GitHub Sponsors;open source software developers;OSS maintainers;OSS projects;OSS community;quantitative analysis;qualitative analysis,
701,Not Mentioned,GraphFuzz: Library API Fuzzing with Lifetime-aware Dataflow Graphs,H. Green; T. Avgerinos,"ForAllSecure, U.S.A.; ForAllSecure, U.S.A.",2022,"We present the design and implementation of GraphFuzz, a new structure-, coverage- and object lifetime-aware fuzzer capable of automatically testing low-level Library APIs. Unlike other fuzzers, GraphFuzz models sequences of executed functions as a dataflow graph, thus enabling it to perform graph-based mutations both at the data and at the execution trace level. GraphFuzz comes with an automated specification generator to minimize the developer integration effort. We use GraphFuzz to analyze Skia-the rigorously tested Google Chrome graphics library-and benchmark GraphFuzz-generated fuzzing harnesses against hand-optimized, painstakingly written libFuzzer harnesses. We find that GraphFuzz generates test cases that achieve 2-3x more code coverage on average with minimal development effort, and also uncovered previous unknown defects in the process. We demonstrate GraphFuzz's applicability on low-level APIs by analyzing four additional open-source libraries and finding dozens of previously unknown defects. All security relevant findings have already been reported and fixed by the developers. Last, we open-source GraphFuzz under a permissive license and provide code to reproduce all results in this paper.",10.1145/3510003.3510228,fuzzing;graph;security;structure aware,Graphics;Codes;Fuzzing;Benchmark testing;Generators;Security,application program interfaces;data flow graphs;graph theory;program testing;security of data,Library API fuzzing;lifetime-aware dataflow graphs;lifetime-aware fuzzer capable;low-level Library APIs;GraphFuzz models sequences;dataflow graph;graph-based mutations;execution trace level;automated specification generator;minimal development effort;GraphFuzz's applicability;low-level APIs;additional open-source libraries;finding dozens;open-source GraphFuzz,1
702,Not Mentioned,Green AI: Do Deep Learning Frameworks Have Different Costs?,S. Georgiou; M. Kechagia; T. Sharma; F. Sarro; Y. Zou,Queen's University; University College London; Dalhousie Uninversity; University College London; Queen's University,2022,"The use of Artificial Intelligence (AI), and more specifically of Deep Learning (DL), in modern software systems, is nowadays widespread and continues to grow. At the same time, its usage is energy de-manding and contributes to the increased CO2 emissions, and has a great financial cost as well. Even though there are many studies that examine the capabilities of DL, only a few focus on its green aspects, such as energy consumption. This paper aims at raising awareness of the costs incurred when using different DL frameworks. To this end, we perform a thorough empirical study to measure and compare the energy consumption and run-time performance of six different DL models written in the two most popular DL frameworks, namely PYTORCH and TENSORFLOW. We use a well-known benchmark of DL models, Deep LEARNINGEXAMPLES, created by NVIDIA, to compare both the training and inference costs of DL. Finally, we manually investigate the functions of these frameworks that took most of the time to execute in our experiments. The results of our empirical study reveal that there is a statistically significant difference between the cost incurred by the two DL frameworks in 94% of the cases studied. While Tensorflow achieves significantly better energy and run-time performance than PYTORCH, and with large effect sizes in 100% of the cases for the training phase, PYTORCH instead exhibits significantly better energy and run-time performance than TENSORFLOW in the inference phase for 66% of the cases, always, with large effect sizes. Such a large difference in performance costs does not, however, seem to affect the accuracy of the models produced, as both frameworks achieve comparable scores under the same configurations. Our manual analysis, of the documentation and source code of the functions examined, reveals that such a difference in performance costs is under-documented, in these frameworks. This suggests that developers need to improve the documentation of their DL frameworks, the source code of the functions used in these frameworks, as well as to enhance existing DL algorithms.",10.1145/3510003.3510221,Energy consumption;run-time performance;deep learning;APIS,Training;Deep learning;Energy consumption;Costs;Codes;Documentation;Manuals,deep learning (artificial intelligence);statistical analysis,PyTorch;DL frameworks;green AI;deep learning;modern software systems;emissions;financial cost;energy consumption;inference costs;Tensorflow;artificial intelligence,6
703,Not Mentioned,Guidelines for Assessing the Accuracy of Log Message Template Identification Techniques,Z. A. Khan; D. Shin; D. Bianculli; L. Briand,"University of Luxembourg Luxembourg, Luxembourg; University of Luxembourg Luxembourg, Luxembourg; University of Luxembourg Luxembourg, Luxembourg; University of Luxembourg Luxembourg, Luxembourg University of Ottawa, Ottawa, Canada",2022,"Log message template identification aims to convert raw logs containing free-formed log messages into structured logs to be processed by automated log-based analysis, such as anomaly detection and model inference. While many techniques have been proposed in the literature, only two recent studies provide a comprehensive evaluation and comparison of the techniques using an established benchmark composed of real-world logs. Nevertheless, we argue that both studies have the following issues: (1) they used different accuracy metrics without comparison between them, (2) some ground-truth (oracle) templates are incorrect, and (3) the accuracy evaluation results do not provide any information regarding incorrectly identified templates. In this paper, we address the above issues by providing three guidelines for assessing the accuracy of log template identification techniques: (1) use appropriate accuracy metrics, (2) perform oracle template correction, and (3) perform analysis of incorrect templates. We then assess the application of such guidelines through a comprehensive evaluation of 14 existing template identification techniques on the established benchmark logs. Results show very different insights than existing studies and in particular a much less optimistic outlook on existing techniques.",10.1145/3510003.3510101,logs;template identification;metrics,Measurement;Analytical models;Benchmark testing;Anomaly detection;Guidelines;Software engineering,data mining;security of data;system monitoring,ground-truth templates;log template identification techniques;accuracy metrics;log message template identification techniques;automated log-based analysis;anomaly detection,3
704,Not Mentioned,"Hashing It Out: A Survey of Programmers' Cannabis Usage, Perception, and Motivation",M. Endres; K. Boehnke; W. Weimer,"University of Michigan, Ann Arbor, Michigan, USA; University of Michigan, Ann Arbor, Michigan, USA; University of Michigan, Ann Arbor, Michigan, USA",2022,"Cannabis is one of the most common mind-altering substances. It is used both medicinally and recreationally and is enmeshed in a complex and changing legal landscape. Anecdotal evidence suggests that some software developers may use cannabis to aid some programming tasks. At the same time, anti-drug policies and tests remain common in many software engineering environments, sometimes leading to hiring shortages for certain jobs. Despite these connections, little is actually known about the prevalence of, and motivation for, cannabis use while programming. In this paper, we report the results of the first large-scale survey of cannabis use by programmers. We report findings about 803 developers' (in-cluding 450 full-time programmers') cannabis usage prevalence, perceptions, and motivations. For example, we find that some programmers do regularly use cannabis while programming: 35% of our sample has tried programming while using cannabis, and 18% currently do so at least once a month. Furthermore, this cannabis usage is primarily motivated by a perceived enhancement to cer-tain software development skills (such as brainstorming or getting into a programming zone) rather than medicinal reasons (such as pain relief). Finally, we find that cannabis use while programming occurs at similar rates for programming employees, managers, and students despite differences in cannabis perceptions and visibility. Our results have implications for programming job drug policies and motivate future research into cannabis use while programming.",10.1145/3510003.3510156,Software Development Process;Cannabis;Corporate Drug Policy,Drugs;Social networking (online);Pain;Law;Employment;Software;Task analysis,computer aided instruction;computer science education;DP industry;drugs;human factors;personnel;software engineering,full-time programmers;cannabis usage prevalence;cer-tain software development skills;programming zone;cannabis use;programming employees;cannabis perceptions;programming job drug policies;common mind-altering substances;complex landscape;changing legal landscape;software developers;programming tasks;software engineering environments,2
705,Not Mentioned,Hiding Critical Program Components via Ambiguous Translation,C. Jung; D. Kim; A. Chen; W. Wang; Y. Zheng; K. H. Lee; Y. Kwon,"University of Virginia, Charlottesville, VA; University of Tennessee, Knoxville, Knoxville, TN; University of Georgia, Athens, GA; University at Buffalo, SUNY, Buffalo, NY; IBM Research, Yorktown Heights, NY; University of Georgia, Athens, GA; University of Virginia, Charlottesville, VA",2022,"Software systems may contain critical program components such as patented program logic or sensitive data. When those components are reverse-engineered by adversaries, it can cause significantly damage (e.g., financial loss or operational failures). While protecting critical program components (e.g., code or data) in software systems is of utmost importance, existing approaches, unfortunately, have two major weaknesses: (1) they can be reverse-engineered via various program analysis techniques and (2) when an adversary obtains a legitimate-looking critical program component, he or she can be sure that it is genuine. In this paper, we propose Ambitr, a novel technique that hides critical program components. The core of Ambitr is Ambiguous Translator that can generate the critical program components when the input is a correct secret key. The translator is ambiguous as it can accept any inputs and produces a number of legitimate-looking outputs, making it difficult to know whether an input is correct secret key or not. The executions of the translator when it processes the correct secret key and other inputs are also indistinguishable, making the analysis inconclusive. Our evaluation results show that static, dynamic and symbolic analysis techniques fail to identify the hidden information in Ambitr. We also demonstrate that manual analysis of Ambitr is extremely challenging.",10.1145/3510003.3510139,program translation;software protection;reverse engineering,Codes;Manuals;Software systems;Performance analysis;Software engineering,patents;private key cryptography;program diagnostics;reverse engineering,ambiguous translation;software systems;critical program component;patented program logic;program analysis techniques;critical program components;correct secret key;ambiguous translator,
706,Not Mentioned,History-Driven Test Program Synthesis for JVM Testing,Y. Zhao; Z. Wang; J. Chen; M. Liu; M. Wu; Y. Zhang; L. Zhang,"College of Intelligence and Computing, Tianjin University, China; College of Intelligence and Computing, Tianjin University, China; College of Intelligence and Computing, Tianjin University, China; College of Intelligence and Computing, Tianjin University, China; Southern University of Science and Technology, China; Southern University of Science and Technology, China; University of Illinois Urbana-Champaign, United States",2022,"Java Virtual Machine (JVM) provides the runtime environment for Java programs, which allows Java to be â€œwrite once, run anywhereâ€. JVM plays a decisive role in the correctness of all Java programs running on it. Therefore, ensuring the correctness and robustness of JVM implementations is essential for Java programs. To date, various techniques have been proposed to expose JVM bugs via generating potential bug-revealing test programs. However, the diversity and effectiveness of test programs generated by existing research are far from enough since they mainly focus on minor syntactic/semantic mutations. In this paper, we propose JavaTailor, the first history-driven test program synthesis technique, which synthesizes diverse test programs by weaving the ingredients extracted from JVM historical bug-revealing test programs into seed programs for covering more JVM behaviors/paths. More specifically, JavaTailor first extracts five types of code ingredients from the historical bug-revealing test programs. Then, to synthesize diverse test programs, it iteratively inserts the extracted ingredients into the seed programs and strengthens their interactions via introducing extra data dependencies between them. Finally, JavaTailor employs these synthesized test programs to differentially test JVMs. Our experimental results on popular JVM implementations (i.e., HotSpot and OpenJ9) show that JavaTailor outperforms the state-of-the-art technique in generating more diverse and effective test programs, e.g., test programs generated by JavaTailor can achieve higher JVM code coverage and detect many more unique inconsistencies than the state-of-the-art technique. Furthermore, JavaTailor has detected 10 previously unknown bugs, 6 of which have been confirmed/fixed by developers.",10.1145/3510003.3510059,Java Virtual Machine;Program Synthesis;JVM Testing;Compiler Testing,Java;Runtime environment;Codes;Computer bugs;Semantics;Syntactics;Virtual machining,Java;program debugging;program testing;virtual machines,JVM testing;Java virtual machine;Java programs;JVM bugs;JavaTailor;history-driven test program synthesis technique;JVM historical bug-revealing test programs;seed programs;synthesized test programs;JVM code coverage,12
707,Not Mentioned,"If a Human Can See It, So Should Your System: Reliability Requirements for Machine Vision Components",B. C. Hu; L. Marsso; K. Czarnecki; R. Salay; H. Shen; M. Chechik,"University of Toronto, Toronto, Ontario, Canada; University of Toronto, Toronto, Ontario, Canada; University of Waterloo, Waterloo, Ontario, Canada; University of Waterloo, Waterloo, Ontario, Canada; University of Toronto, Toronto, Ontario, Canada; University of Toronto, Toronto, Ontario, Canada",2022,"Machine Vision Components (MVC) are becoming safety-critical. Assuring their quality, including safety, is essential for their successful deployment. Assurance relies on the availability of precisely specified and, ideally, machine-verifiable requirements. MVCs with state-of-the-art performance rely on machine learning (ML) and training data, but largely lack such requirements. In this paper, we address the need for defining machine-verifiable reliability requirements for MVCs against transformations that simulate the full range of realistic and safety-critical changes in the environment. Using human performance as a baseline, we define reliability requirements as: â€˜if the changes in an image do not affect a human's decision, neither should they affect the MVC's.â€™ To this end, we provide: (1) a class of safety-related image transformations; (2) reliability requirement classes to specify correctness-preservation and prediction-preservation for MVCs; (3) a method to instantiate machine-verifiable requirements from these requirements classes using human performance experiment data; (4) human performance experiment data for image recognition involving eight commonly used transformations, from about 2000 human participants; and (5) a method for automatically checking whether an MVC satisfies our requirements. Further, we show that our reliability requirements are feasible and reusable by evaluating our methods on 13 state-of-the-art pre-trained image classification models. Finally, we demonstrate that our approach detects reliability gaps in MVCs that other existing methods are unable to detect.",10.1145/3510003.3510109,Software Engineering for Artificial Intelligence;Requirements Engineering;Software Analysis,Machine vision;Training data;Object detection;Reliability engineering;Software;Software reliability;Safety,computer vision;image recognition;learning (artificial intelligence);object detection;reliability,image recognition;machine learning;machine-verifiable reliability;pretrained image classification models;safety-related image transformations;machine-verifiable requirements;MVC;machine vision components,4
708,Not Mentioned,Imperative versus Declarative Collection Processing: An RCT on the Understandability of Traditional Loops versus the Stream API in Java,N. Mehlhorn; S. Hanenberg,"Independent Consultant, Essen; Paluno - The Ruhr Institute for Software Technology, University of Duisburg- Essen, Germany",2022,"Java introduced in version 8 with the Stream API means to operate on collections using lambda expressions. Since then, this API is an alternative way to handle collections in a more declarative manner instead of the traditional, imperative style using loops. However, whether the Stream API is beneficial in comparison to loops in terms of usability is unclear. The present paper introduces a randomized control trial (RCT) on the understandability of collection operations performed on 20 participants with the dependent variables response time and correctness. As tasks, subjects had to determine the results for collection operations (either defined with the Stream API or with loops). The results indicate that the Stream API has a significant $(\mathrm{p} <. 001)$ and large $(\eta_{p}^{2}=.695;\frac{M_{loop}}{M_{stream}}\ \sim 178\%)$ positive effect on the response times. Furthermore, the usage of the Stream API caused significantly less errors. And finally, the participants perceived their speed with the Stream API higher compared to the loop-based code and the participants considered the code based on the Stream API as more readable. Hence, while existing studies found a negative effect of declarative constructs (in terms of lambda expressions) on the usability of a main stream programming language, the present study found the opposite: the present study gives evidence that declarative code on collections using the Stream API based on lambda expressions has a large, positive effect in comparison to traditional loops.",10.1145/3510003.3519016,Programming Languages;Lambda Expressions;Declarative;Imperative;Java;Streams,Java;Computer languages;Codes;Programming;Time factors;Usability;Task analysis,application program interfaces;Java;message passing;program control structures;security of data,traditional loops;collection operations;Stream API higher compared;imperative versus declarative collection processing,
709,Not Mentioned,Improving Fault Localization and Program Repair with Deep Semantic Features and Transferred Knowledge,X. Meng; X. Wang; H. Zhang; H. Sun; X. Liu,"SKLSDE Lab, Beihang University, Beijing, China; SKLSDE Lab, Beihang University, Beijing, China; The University of Newcastle, NSW, Australia; SKLSDE Lab, Beihang University, Beijing, China; SKLSDE Lab, Beihang University, Beijing, China",2022,"Automatic software debugging mainly includes two tasks of fault lo-calization and automated program repair. Compared with the traditional spectrum-based and mutation-based methods, deep learning-based methods are proposed to achieve better performance for fault localization. However, the existing methods ignore the deep seman-tic features or only consider simple code representations. They do not leverage the existing bug-related knowledge from large-scale open-source projects either. In addition, existing template-based program repair techniques can incorporate project specific information better than deep-learning approaches. However, they are weak in selecting the fix templates for efficient program repair. In this work, we propose a novel approach called TRANSFER, which lever-ages the deep semantic features and transferred knowledge from open-source data to improve fault localization and program repair. First, we build two large-scale open-source bug datasets and design 11 BiLSTM-based binary classifiers and a BiLSTM-based multi-classifier to learn deep semantic features of statements for fault localization and program repair, respectively. Second, we combine semantic-based, spectrum-based and mutation-based features and use an MLP-based model for fault localization. Third, the semantic-based features are leveraged to rank the fix templates for program repair. Our extensive experiments on widely-used benchmark De-fects4J show that TRANSFER outperforms all baselines in fault localization, and is better than existing deep-learning methods in automated program repair. Compared with the typical template-based work TBar, TRANSFER can correctly repair 6 more bugs (47 in total) on Defects4J.",10.1145/3510003.3510147,Fault localization;program repair;transfer learning;neural networks;software debugging,Location awareness;Codes;Semantics;Computer bugs;Maintenance engineering;Feature extraction;Software debugging,learning (artificial intelligence);multilayer perceptrons;pattern classification;program debugging;software maintenance,transferred knowledge;fault lo-calization;automated program repair;traditional spectrum-based;mutation-based methods;deep learning-based methods;fault localization;deep seman-tic features;existing bug-related knowledge;large-scale open-source projects;template-based program repair techniques;deep-learning approaches;fix templates;efficient program repair;deep semantic features;large-scale open-source bug datasets;BiLSTM-based multiclassifier;mutation-based features;MLP-based model;semantic-based features;deep-learning methods;typical template-based work TBar,2
710,Not Mentioned,Improving Machine Translation Systems via Isotopic Replacement,Z. Sun; J. M. Zhang; Y. Xiong; M. Harman; M. Papadakis; L. Zhang,"Key Laboratory of High Confidence Software Technologies, MoE, School of Computer Science, Peking University; University College London; Key Laboratory of High Confidence Software Technologies, MoE, School of Computer Science, Peking University; Meta platforms, University College London; University of Luxembourg; Key Laboratory of High Confidence Software Technologies, MoE, School of Computer Science, Peking University",2022,"Machine translation plays an essential role in people's daily international communication. However, machine translation systems are far from perfect. To tackle this problem, researchers have proposed several approaches to testing machine translation. A promising trend among these approaches is to use word replacement, where only one word in the original sentence is replaced with another word to form a sentence pair. However, precise control of the impact of word replacement remains an outstanding issue in these approaches. To address this issue, we propose CAT, a novel word-replacement-based approach, whose basic idea is to identify word replacement with controlled impact (referred to as isotopic replacement). To achieve this purpose, we use a neural-based language model to encode the sentence context, and design a neural-network-based algorithm to evaluate context-aware semantic similarity between two words. Furthermore, similar to TransRepair, a state-of-the-art word-replacement-based approach, CAT also provides automatic fixing of revealed bugs without model retraining. Our evaluation on Google Translate and Transformer indicates that CAT achieves significant improvements over TransRepair. In particular, 1) CAT detects seven more types of bugs than TransRe-pair; 2) CAT detects 129% more translation bugs than TransRepair; 3) CAT repairs twice more bugs than TransRepair, many of which may bring serious consequences if left unfixed; and 4) CAT has better efficiency than TransRepair in input generation (0.01s v.s. 0.41s) and comparable efficiency with TransRepair in bug repair (1.92s v.s. 1.34s).",10.1145/3510003.3510206,machine translation;testing and repair;machine learning testing;neural networks,Computer bugs;Semantics;Maintenance engineering;Transformers;Market research;Internet;Machine translation,language translation;natural language processing;neural nets,CAT;word replacement;isotopic replacement;neural-based language model;neural network;machine translation systems,3
711,Not Mentioned,Inference and Test Generation Using Program Invariants in Chemical Reaction Networks,M. C. Gerten; A. L. Marsh; J. I. Lathrop; M. B. Cohen; A. S. Miner; T. H. Klinge,"Iowa State University, Ames, Iowa, USA; Iowa State University, Ames, Iowa, USA; Iowa State University, Ames, Iowa, USA; Iowa State University, Ames, Iowa, USA; Iowa State University, Ames, Iowa, USA; Iowa State University, Ames, Iowa, USA",2022,"Chemical reaction networks (CRNs) are an emerging distributed computational paradigm where programs are encoded as a set of abstract chemical reactions. CRNs can be compiled into DNA strands which perform the computations in vitro, creating a foundation for intelligent nanodevices. Recent research proposed a software testing framework for stochastic CRN programs in simulation, however, it relies on existing program specifications. In practice, specifications are often lacking and when they do exist, transforming them into test cases is time-intensive and can be error prone. In this work, we propose an inference technique called ChemFlow which extracts 3 types of invariants from an existing CRN model. The extracted invariants can then be used for test generation or model validation against program implementations. We applied ChemFlow to 13 CRN programs ranging from toy examples to real biological models with hundreds of reactions. We find that the invariants provide strong fault detection and often exhibit less flakiness than specification derived tests. In the biological models we showed invariants to developers and they confirmed that some of these point to parts of the model that are biologically incorrect or incomplete suggesting we may be able to use ChemFlow to improve model quality.",10.1145/3510003.3510176,chemical reaction networks;test generation;invariants;Petri nets,Software testing;Biological system modeling;Toy manufacturing industry;Stochastic processes;Chemical reactions;Nanoscale devices;Test pattern generators,biology computing;chemical reactions;DNA;fault diagnosis;inference mechanisms;molecular biophysics;program testing,test generation;program implementations;ChemFlow;biological models;program invariants;chemical reaction networks;CRNs;emerging distributed computational paradigm;abstract chemical reactions;DNA strands;software testing framework;stochastic CRN programs;program specifications;inference technique;extracted invariants;time-intensive,1
712,Not Mentioned,Inferring and Applying Type Changes,A. Ketkar; O. Smirnov; N. Tsantalis; D. Dig; T. Bryksin,"Uber Technologies Inc., USA; JetBrains Research, St Petersburg University, Russia; Concordia University, Canada; University of Colorado Boulder, USA; JetBrains Research HSE University, Russia",2022,"Developers frequently change the type of a program element and update all its references to increase performance, security, or maintainability. Manually performing type changes is tedious, error-prone, and it overwhelms developers. Researchers and tool builders have proposed advanced techniques to assist developers when performing type changes. A major obstacle in using these techniques is that the developer has to manually encode rules for defining the type changes. Handcrafting such rules is difficult and often involves multiple trial-error iterations. Given that open-source repositories contain many examples of type-changes, if we could infer the adaptations, we would eliminate the burden on developers. We introduce TC-Infer, a novel technique that infers rewrite rules that capture the required adaptations from the version histories of open source projects. We then use these rules (expressed in the Comby language) as input to existing type change tools. To evaluate the effectiveness of TC-Infer, we use it to infer 4,931 rules for 605 popular type changes in a corpus of 400K commits. Our results show that TC-Infer deduced rewrite rules for 93% of the most popular type change patterns. Our results also show that the rewrite rules produced by TC-Infer are highly effective at applying type changes (99.2% precision and 93.4% recall). To advance the existing tooling we released IntelliTC, an interactive and configurable refactoring plugin for IntelliJ IDEA to perform type changes.",10.1145/3510003.3510115,Refactoring;source code mining;type change;type migration,Java;Codes;Automation;History;Security;DSL;Open source software,iterative methods;public domain software;rewriting systems;software maintenance;software performance evaluation;type theory,type change patterns;type change tools;trial-error iterations;TC-Infer;rewrite rules;IntelliJ IDEA;Comby language,3
713,Not Mentioned,Jigsaw: Large Language Models meet Program Synthesis,N. Jain; S. Vaidyanath; A. Iyer; N. Natarajan; S. Parthasarathy; S. Rajamani; R. Sharma,"Microsoft Research, Bangalore, India; Stanford University, Stanford, USA; Microsoft Research, Bangalore, India; Microsoft Research, Bangalore, India; Microsoft Research, Bangalore, India; Microsoft Research, Bangalore, India; Microsoft Research, Bangalore, India",2022,"Large pre-trained language models such as GPT-3 [10], Codex [11], and Coogle's language model [7] are now capable of generating code from natural language specifications of programmer intent. We view these developments with a mixture of optimism and caution. On the optimistic side, such large language models have the potential to improve productivity by providing an automated AI pair programmer for every programmer in the world. On the cautionary side, since these large language models do not understand program semantics, they offer no guarantees about quality of the suggested code. In this paper, we present an approach to augment these large language models with post-processing steps based on program analysis and synthesis techniques, that understand the syntax and semantics of programs. Further, we show that such techniques can make use of user feedback and improve with usage. We present our experiences from building and evaluating such a tool Jigsaw, targeted at synthesizing code for using Python Pandas API using multi-modal inputs. Our experience suggests that as these large language models evolve for synthesizing code from intent, Jigsaw has an important role to play in improving the accuracy of the systems.",10.1145/3510003.3510203,Program Synthesis;Machine Learning,Productivity;Analytical models;Codes;Semantics;Natural languages;Buildings;Syntactics,application program interfaces;computational linguistics;formal specification;natural language processing;natural languages;program compilers;program diagnostics;program verification;programming language semantics;video signal processing,pre-trained language models;Coogle's language model;natural language specifications,6
714,Not Mentioned,JuCify: A Step Towards Android Code Unification for Enhanced Static Analysis,J. Samhi; J. Gao; N. Daoudi; P. Graux; H. Hoyez; X. Sun; K. Allix; T. F. BissyandÃ©; J. Klein,"SnT, University of Luxembourg, Luxembourg; SnT, University of Luxembourg, Luxembourg; SnT, University of Luxembourg, Luxembourg; SnT, University of Luxembourg, Luxembourg; Tcchnische Universitat Kaiserslautern, Germany; Monash University, Australia; SnT, University of Luxembourg, Luxembourg; SnT, University of Luxembourg, Luxembourg; SnT, University of Luxembourg, Luxembourg",2022,"Native code is now commonplace within Android app packages where it co-exists and interacts with Dex bytecode through the Java Native Interface to deliver rich app functionalities. Yet, state-of-the-art static analysis approaches have mostly overlooked the presence of such native code, which, however, may implement some key sensitive, or even malicious, parts of the app behavior. This limitation of the state of the art is a severe threat to validity in a large range of static analyses that do not have a complete view of the executable code in apps. To address this issue, we propose a new advance in the ambitious research direction of building a unified model of all code in Android apps. The JUCIFY approach presented in this paper is a significant step towards such a model, where we extract and merge call graphs of native code and bytecode to make the final model readily-usable by a common Android analysis framework: in our implementation, JUCIFY builds on the Soot internal intermediate representation. We performed empirical investigations to highlight how, without the unified model, a significant amount of Java methods called from the native code are â€œunreachableâ€ in apps' callgraphs, both in goodware and malware. Using JUCIFY, we were able to enable static analyzers to reveal cases where malware relied on native code to hide invocation of payment library code or of other sensitive code in the Android framework. Additionally, JUCIFY'S model enables state-of-the-art tools to achieve better precision and recall in detecting data leaks through native code. Finally, we show that by using JUCIFY we can find sensitive data leaks that pass through native code.",10.1145/3510003.3512766,Android;Native code;Security;static analysis;Software Engineering,Java;Analytical models;Codes;Buildings;Static analysis;Malware;Libraries,Android (operating system);invasive software;Java;mobile computing;program diagnostics;security of data;smart phones;system monitoring,Android code unification;native code;Android app packages;state-of-the-art static analysis approaches;Android apps;common Android analysis framework;JUCIFY;JuCify,6
715,Not Mentioned,Knowledge-Based Environment Dependency Inference for Python Programs,H. Ye; W. Chen; W. Dou; G. Wu; J. Wei,"State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Lab of Computer Sciences, Institute of Software, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China",2022,"Besides third-party packages, the Python interpreter and system libraries are also critical dependencies of a Python program. In our empirical study, 34% programs are only compatible with specific Python interpreter versions, and 24% programs require specific system libraries. However, existing techniques mainly focus on inferring third-party package dependencies. Therefore, they can lack other necessary dependencies and violate version constraints, thus resulting in program build failures and runtime errors. This paper proposes a knowledge-based technique named PyEGo, which can automatically infer dependencies of third-party packages, the Python interpreter, and system libraries at compatible versions for Python programs. We first construct the dependency knowl-edge graph PyKG, which can portray the relations and constraints among third-party packages, the Python interpreter, and system libraries. Then, by querying PyKG with extracted program features, PyEGo constructs a program-related sub-graph with dependency candidates of the three types. It finally outputs the latest compatible dependency versions by solving constraints in the sub-graph. We evaluate PyEGo on 2,891 single-file Python gists, 100 open-source Python projects and 4,836 jupyter notebooks. The experimental re-sults show that PyEGo achieves better accuracy, 0.2x to 3.5x higher than the state-of-the-art approaches.",10.1145/3510003.3510127,Python;environment dependency inference;version constraint;knowledge graph,Runtime;Knowledge based systems;Feature extraction;Libraries;Data mining;Open source software;Python,graph theory;inference mechanisms;object-oriented languages;program debugging;software engineering,third-party packages;Python program;single-file Python gists;open-source Python projects;environment dependency inference;third-party package dependencies;version constraints;knowledge-based technique;PyEGo;Python interpreter versions,1
716,Not Mentioned,Large-scale Security Measurements on the Android Firmware Ecosystem,Q. Hou; W. Diao; Y. Wang; X. Liu; S. Liu; L. Ying; S. Guo; Y. Li; M. Nie; H. Duan,"School of Cyber Science and Technology, Shandong University; School of Cyber Science and Technology, Shandong University; QI-ANXIN Technology Research Institute; School of Cyber Science and Technology, Shandong University; QI-ANXIN Technology Research Institute; QI-ANXIN Technology Research Institute; School of Cyber Science and Technology, Shandong University; QI-ANXIN Technology Research Institute; QI-ANXIN Technology Research Institute; Institute for Network Science and Cyberspace, Tsinghua University",2022,"Android is the most popular smartphone platform with over 85% market share. Its success is built on openness, and phone vendors can utilize the Android source code to make products with unique software/hardware features. On the other hand, the fragmentation and customization of Android also bring many security risks that have attracted the attention of researchers. Many efforts were put in to investigate the security of customized Android firmware. However, most of the previous work focuses on designing efficient analysis tools or analyzing particular aspects of the firmware. There still lacks a panoramic view of Android firmware ecosystem secu-rity and the corresponding understandings based on large-scale firmware datasets. In this work, we made a large-scale compre-hensive measurement of the Android firmware ecosystem security. Our study is based on 6,261 firmware images from 153 vendors and 602 Android-related CVEs, which is the largest Android firmware dataset ever used for security measurements. In particular, our study followed a series of research questions, covering vulnerabili-ties, patches, security updates, and pre-installed apps. To automate the analysis process, we designed a framework, ANDSCANNER, to complete ROM crawling, ROM parsing, patch analysis, and app analysis. Through massive data analysis and case explorations, sev-eral interesting findings are obtained. For example, the patch delay and missing issues are widespread in Android images, say 24.2% and 6.1 % of all images, respectively. The latest images of several phones still contain vulnerable pre-installed apps, and even the corresponding vulnerabilities have been publicly disclosed. In ad-dition to data measurements, we also explore the causes behind these security threats through case studies and demonstrate that the discovered security threats can be converted into exploitable vulnerabilities via 38 newfound vulnerabilities by our framework, 32 of which have been assigned CVE/CNVD numbers. This study provides much new knowledge of the Android firmware ecosystem with deep understanding of software engineering security practices.",10.1145/3510003.3510072,Android Firmware Ecosystem;Security Measurements;Security Patches;Pre-installed Apps,Knowledge engineering;Atmospheric measurements;Ecosystems;Particle measurements;Security;Software measurement;Smart phones,Android (operating system);data analysis;data privacy;firmware;groupware;mobile computing;smart phones;software engineering,Android source code;security risks;Android firmware ecosystem security;firmware images;security updates;patch analysis;app analysis;data analysis;Android images;software engineering security practices;Android firmware dataset;Android-related CVEs;CNVD numbers;ANDSCANNER;ROM parsing;ROM crawling;fragmentation;hardware features;software features,3
717,Not Mentioned,Learning and Programming Challenges of Rust: A Mixed-Methods Study,S. Zhu; Z. Zhang; B. Qin; A. Xiong; L. Song,"Pennsylvania State University, USA; University of Wisconsin-Madison, USA; China Telecom Cloud Computing, China; Pennsylvania State University, USA; Pennsylvania State University, USA",2022,"Rust is a young systems programming language designed to provide both the safety guarantees of high-level languages and the execution performance of low-level languages. To achieve this design goal, Rust provides a suite of safety rules and checks against those rules at the compile time to eliminate many memory-safety and thread-safety issues. Due to its safety and performance, Rust's popularity has increased significantly in recent years, and it has already been adopted to build many safety-critical software systems. It is critical to understand the learning and programming challenges imposed by Rust's safety rules. For this purpose, we first conducted an empirical study through close, manual inspection of 100 Rust-related Stack Overflow questions. We sought to understand (1) what safety rules are challenging to learn and program with, (2) under which contexts a safety rule becomes more difficult to apply, and (3) whether the Rust compiler is sufficiently helpful in debugging safety-rule violations. We then performed an online survey with 101 Rust programmers to validate the findings of the empirical study. We invited participants to evaluate program variants that differ from each other, either in terms of violated safety rules or the code constructs involved in the violation, and compared the participants' performance on the variants. Our mixed-methods investigation revealed a range of consistent findings that can benefit Rust learners, practitioners, and language designers.",10.1145/3510003.3510164,Rust;Programming Challenges;Empirical Study;Online Survey,Context;Instruction sets;Manuals;Debugging;Programming;Inspection;Software,program compilers;program debugging;programming languages;safety-critical software,mixed-methods study;high-level languages;low-level languages;memory-safety issues;thread-safety issues;Rust's popularity;safety-critical software systems;programming challenges;Rust's safety rules;Rust compiler;learning challenges;safety-rule violations debugging;Rust-related Stack Overflow,2
718,Not Mentioned,Learning Probabilistic Models for Static Analysis Alarms,H. Kim; M. Raghothaman; K. Heo,"KAIST, Korea; University of Southern California, USA; KAIST, Korea",2022,"We present BayeSmith, a general framework for automatically learning probabilistic models of static analysis alarms. Several prob-abilistic reasoning techniques have recently been proposed which incorporate external feedback on semantic facts and thereby reduce the user's alarm inspection burden. However, these approaches are fundamentally limited to models with pre-defined structure, and are therefore unable to learn or transfer knowledge regarding an analysis from one program to another. Furthermore, these probabilistic models often aggressively generalize from external feedback and falsely suppress real bugs. To address these problems, we propose BayeSmith that learns the structure and weights of the probabilistic model. Starting from an initial model and a set of training programs with bug labels, BayeSmith refines the model to effectively prioritize real bugs based on feedback. We evaluate the approach with two static analyses on a suite of C programs. We demonstrate that the learned models significantly improve the performance of three state-of-the-art probabilistic reasoning systems.",10.1145/3510003.3510098,,Training;Analytical models;Computer bugs;Semantics;Static analysis;Inspection;Probabilistic logic,inference mechanisms;inspection;learning (artificial intelligence);probability;program debugging;program diagnostics;uncertainty handling,BayeSmith;probabilistic model;static analysis alarms;probabilistic reasoning techniques;external feedback;state-of-the-art probabilistic reasoning systems;bug labels;C programs;learned models;training programs,
719,Not Mentioned,Learning to Recommend Method Names with Global Context,F. Liu; G. Li; Z. Fu; S. Lu; Y. Hao; Z. Jin,"Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China; Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China; Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China; Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China; Silicon Heart Tech Co., Ltd, Beijing, China; Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China",2022,"In programming, the names for the program entities, especially for the methods, are the intuitive characteristic for understanding the functionality of the code. To ensure the readability and maintainability of the programs, method names should be named properly. Specifically, the names should be meaningful and consistent with other names used in related contexts in their codebase. In recent years, many automated approaches are proposed to suggest consistent names for methods, among which neural machine translation (NMT) based models are widely used and have achieved state-of-the-art results. However, these NMT-based models mainly focus on extracting the code-specific features from the method body or the surrounding methods, the project-specific context and documentation of the target method are ignored. We conduct a statistical analysis to explore the relationship between the method names and their contexts. Based on the statistical results, we propose GTNM, a Global Transformer-based Neural Model for method name suggestion, which considers the local context, the project-specific context, and the documentation of the method simultaneously. Experimental results on java methods show that our model can outperform the state-of-the-art results by a large margin on method name suggestion, demonstrating the effectiveness of our proposed model.",10.1145/3510003.3510154,method name recommendation;global context;deep learning,Java;Codes;Statistical analysis;Documentation;Programming;Transformers;Feature extraction,Java;learning (artificial intelligence);neural nets;statistical analysis,project-specific context;target method;method names;method name suggestion;Java methods;program entities;neural machine translation based models;NMT-based models;global transformer-based neural model;statistical analysis,4
720,Not Mentioned,Learning to Reduce False Positives in Analytic Bug Detectors,A. Kharkar; R. Z. Moghaddam; M. Jin; X. Liu; X. Shi; C. Clement; N. Sundaresan,"Microsoft, Redmond, Washington, USA; Microsoft, Redmond, Washington, USA; Microsoft, Redmond, Washington, USA; Microsoft, Redmond, Washington, USA; Microsoft, Redmond, Washington, USA; Microsoft, Redmond, Washington, USA; Microsoft, Redmond, Washington, USA",2022,"Due to increasingly complex software design and rapid iterative development, code defects and security vulnerabilities are prevalent in modern software. In response, programmers rely on static analysis tools to regularly scan their codebases and find potential bugs. In order to maximize coverage, however, these tools generally tend to report a significant number of false positives, requiring developers to manually verify each warning. To address this problem, we propose a Transformer-based learning approach to identify false positive bug warnings. We demonstrate that our models can improve the precision of static analysis by 17.5%. In addition, we validated the generalizability of this approach across two major bug types: null dereference and resource leak.",10.1145/3510003.3510153,datasets;neural networks;gaze detection;text tagging,Training;Analytical models;Codes;Computer bugs;Static analysis;Detectors;Tagging,learning (artificial intelligence);program debugging;program diagnostics;program verification,false positive bug warnings;bug types;reduce false positives;analytic bug detectors;increasingly complex software design;rapid iterative development;code defects;security vulnerabilities;modern software;static analysis tools;codebases;potential bugs;warning;Transformer-based,4
721,Not Mentioned,Less is More: Supporting Developers in Vulnerability Detection during Code Review,L. Braz; C. Aeberhard; G. Ã‡alikli; A. Bacchelli,University of Zurich; University of Zurich; University of Glasgow; University of Zurich,2022,"Reviewing source code from a security perspective has proven to be a difficult task. Indeed, previous research has shown that developers often miss even popular and easy-to-detect vulnerabilities during code review. Initial evidence suggests that a significant cause may lie in the reviewers' mental attitude and common practices. In this study, we investigate whether and how explicitly asking developers to focus on security during a code review affects the detection of vulnerabilities. Furthermore, we evaluate the effect of providing a security checklist to guide the security review. To this aim, we conduct an online experiment with 150 participants, of which 71% report to have three or more years of professional development experience. Our results show that simply asking reviewers to focus on security during the code review increases eight times the probability of vulnerability detection. The presence of a security checklist does not significantly improve the outcome further, even when the checklist is tailored to the change under review and the existing vulnerabilities in the change. These results provide evidence supporting the mental attitude hypothesis and call for further work on security checklists' effectiveness and design. Preprint: https://arxiv.org/abs/2202.04586 Data and materials: https://doi.org/10.5281/zenodo.6026291",10.1145/3510003.3511560,code review;security vulnerability;checklist;mental attitude,Codes;Software algorithms;Software;Security;Cryptography;Task analysis;Standards,data handling;program diagnostics;research and development;security of data,vulnerability detection;code review;security perspective;security checklist;security review;security checklists,4
722,Not Mentioned,Lessons from Eight Years of Operational Data from a Continuous Integration Service: An Exploratory Case Study of CircleCI,K. Gallaba; M. Lamothe; S. McIntosh,"Centre for Software Excellence, Huawei Canada, Kingston, Canada; Polytechnique MontrÃ©al, MontrÃ©al, Canada; University of Waterloo, Waterloo, Canada",2022,"Continuous Integration (CI) is a popular practice that enables the rapid pace of modern software development. Cloud-based CI services have made CI ubiquitous by relieving software teams of the hassle of maintaining a CI infrastructure. To improve these CI services, prior research has focused on analyzing historical CI data to help service consumers. However, finding areas of improvement for CI service providers could also improve the experience for service consumers. To search for these opportunities, we conduct an empirical study of 22.2 million builds spanning 7,795 open-source projects that used CircleCI from 2012 to 2020. First, we quantitatively analyze the builds (i.e., invocations of the CI service) with passing or failing outcomes. We observe that the heavy and typical service consumer groups spend significantly different proportions of time on seven of the nine build actions (e.g., dependency retrieval). On the other hand, the compilation and testing actions consistently consume a large proportion of build time across consumer groups (median 33%). Second, we study builds that terminate prior to generating a pass or fail signal. Through a systematic manual analysis, we find that availability issues, configuration errors, user cancellation, and exceeding time limits are key reasons that lead to premature build termination. Our observations suggest that (1) heavy service consumers would benefit most from build acceleration approaches that tackle long build durations (e.g., skipping build steps) or high throughput rates (e.g., optimizing CI service job queues), (2) efficiency in CI pipelines can be improved for most CI consumers by focusing on the compilation and testing stages, and (3) avoiding misconfigurations and tackling service availability issues present the largest opportunities for improving the robustness of CI services.",10.1145/3510003.3510211,Automated Builds;Build Systems;Continuous Integration,Systematics;Pipelines;Manuals;Life estimation;Throughput;Robustness;Queueing analysis,cloud computing;information retrieval;pipelines;project management;public domain software;software engineering,continuous Integration service;cloud-based CI services;CI ubiquitous;CI infrastructure;premature build termination;CI service job queues;CI consumers;software development;open-source;service consumers,1
723,Not Mentioned,Linear-time Temporal Logic guided Greybox Fuzzing,R. Meng; Z. Dong; J. Li; I. Beschastnikh; A. Roychoudhury,"National University of Singapore, Singapore; Fudan University China; National University of Singapore, Singapore; University of British Columbia, Canada; National University of Singapore, Singapore",2022,"Software model checking as well as runtime verification are verification techniques which are widely used for checking temporal properties of software systems. Even though they are property verification techniques, their common usage in practice is in â€œbug findingâ€, that is, finding violations of temporal properties. Motivated by this observation and leveraging the recent progress in fuzzing, we build a greybox fuzzing framework to find violations of Linear-time Temporal Logic (LTL) properties. Our framework takes as input a sequential program written in C/C++, and an LTL property. It finds violations, or counterexample traces, of the LTL property in stateful software systems; however, it does not achieve verification. Our work substantially extends directed greybox fuzzing to witness arbitrarily complex event or-derings. We note that existing directed greybox fuzzing approaches are limited to witnessing reaching a location or witnessing simple event orderings like use-after-free. At the same time, compared to model checkers, our approach finds the counterexamples faster, thereby finding more counterexamples within a given time budget. Our LTL-FUZZER tool, built on top of the AFL fuzzer, is shown to be effective in detecting bugs in well-known protocol implementations, such as OpenSSL and Telnet. We use LTL-FUZZER to reproduce known vulnerabilities (CVEs), to find 15 zero-day bugs by checking properties extracted from RFCs (for which 12 CVEs have been assigned), and to find violations of both safety as well as liveness properties in real-world protocol implementations. Our work represents a practical advance over software model checkers - while simultaneously representing a conceptual advance over existing greybox fuzzers. Our work thus provides a starting point for understanding the unexplored synergies among software model checking, runtime verification and greybox fuzzing.",10.1145/3510003.3510082,,Runtime;Protocols;Computer bugs;Fuzzing;Model checking;Software systems;Safety,program debugging;program verification;temporal logic,software model checking;runtime verification;bug finding;linear-time temporal logic;LTL property;greybox fuzzing;LTL-FUZZER tool;property verification;software systems;arbitrarily complex event orderings,2
724,Not Mentioned,Log-based Anomaly Detection with Deep Learning: How Far Are We?,V. -H. Le; H. Zhang,"The University of Newcastle, NSW, Australia; The University of Newcastle, NSW, Australia",2022,"Software-intensive systems produce logs for troubleshooting pur-poses. Recently, many deep learning models have been proposed to automatically detect system anomalies based on log data. These models typically claim very high detection accuracy. For example, most models report an F-measure greater than 0.9 on the commonly-used HDFS dataset. To achieve a profound understanding of how far we are from solving the problem of log-based anomaly detection, in this paper, we conduct an in-depth analysis of five state-of-the-art deep learning-based models for detecting system anomalies on four public log datasets. Our experiments focus on several aspects of model evaluation, including training data selection, data grouping, class distribution, data noise, and early detection ability. Our re-sults point out that all these aspects have significant impact on the evaluation, and that all the studied models do not always work well. The problem of log-based anomaly detection has not been solved yet. Based on our findings, we also suggest possible future work.",10.1145/3510003.3510155,Anomaly Detection;Log Analysis;Log Parsing;Deep Learning,Deep learning;Analytical models;Codes;Training data;Data models;Anomaly detection;Software engineering,deep learning (artificial intelligence);security of data,system anomalies;public log datasets;log-based anomaly detection;software-intensive systems;deep learning models;HDFS dataset;F-measure,8
725,Not Mentioned,Manas: Mining Software Repositories to Assist AutoML,G. Nguyen; M. J. Islam; R. Pan; H. Rajan,"Dept. of Computer Science, Iowa State University, Ames, IA, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA",2022,"Today deep learning is widely used for building software. A software engineering problem with deep learning is that finding an appropriate convolutional neural network (CNN) model for the task can be a challenge for developers. Recent work on AutoML, more precisely neural architecture search (NAS), embodied by tools like Auto-Keras aims to solve this problem by essentially viewing it as a search problem where the starting point is a default CNN model, and mutation of this CNN model allows exploration of the space of CNN models to find a CNN model that will work best for the problem. These works have had significant success in producing high-accuracy CNN models. There are two problems, however. First, NAS can be very costly, often taking several hours to complete. Second, CNN models produced by NAS can be very complex that makes it harder to understand them and costlier to train them. We propose a novel approach for NAS, where instead of starting from a default CNN model, the initial model is selected from a repository of models extracted from GitHub. The intuition being that developers solving a similar problem may have developed a better starting point compared to the default model. We also analyze common layer patterns of CNN models in the wild to understand changes that the developers make to improve their models. Our approach uses commonly occurring changes as mutation operators in NAS. We have extended Auto-Keras to implement our approach. Our evaluation using 8 top voted problems from Kaggle for tasks including image classification and image regression shows that given the same search time, without loss of accuracy, Manas produces models with 42.9% to 99.6% fewer number of parameters than Auto-Keras' models. Benchmarked on GPU, Manas' models train 30.3% to 641.6% faster than Auto-Keras' models.",10.1145/3510003.3510052,Deep Learning;AutoML;Mining Software Repositories;MSR,Deep learning;Training;Analytical models;Search problems;Software;Space exploration;Convolutional neural networks,convolutional neural nets;data mining;deep learning (artificial intelligence);image classification;medical computing;search problems;software engineering,deep learning;software engineering problem;NAS;high-accuracy CNN models;neural architecture search;Manas models;Auto-Keras models;image regression;image classification;convolutional neural network model,5
726,Not Mentioned,Modeling Review History for Reviewer Recommendation: A Hypergraph Approach,G. Rong; Y. Zhang; L. Yang; F. Zhang; H. Kuang; H. Zhang,"State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, Nanjing, Jiangsu, China; State Key Laboratory of Novel Software Technology, Software Institute, Nanjing University, Nanjing, Jiangsu, China",2022,"Modern code review is a critical and indispensable practice in a pull-request development paradigm that prevails in Open Source Software (OSS) development. Finding a suitable reviewer in projects with massive participants thus becomes an increasingly challenging task. Many reviewer recommendation approaches (recommenders) have been developed to support this task which apply a similar strategy, i.e. modeling the review history first then followed by predicting/recommending a reviewer based on the model. Apparently, the better the model reflects the reality in review history, the higher recommender's performance we may expect. However, one typical scenario in a pull-request development paradigm, i.e. one Pull-Request (PR) (such as a revision or addition submitted by a contributor) may have multiple reviewers and they may impact each other through publicly posted comments, has not been modeled well in existing recommenders. We adopted the hypergraph technique to model this high-order relationship (i.e. one PR with multiple reviewers herein) and developed a new recommender, namely HGRec, which is evaluated by 12 OSS projects with more than 87K PRs, 680K comments in terms of accuracy and recommen-dation distribution. The results indicate that HGRec outperforms the state-of-the-art recommenders on recommendation accuracy. Besides, among the top three accurate recommenders, HGRec is more likely to recommend a diversity of reviewers, which can help to relieve the core reviewers' workload congestion issue. Moreover, since HGRec is based on hypergraph, which is a natural and interpretable representation to model review history, it is easy to accommodate more types of entities and realistic relationships in modern code review scenarios. As the first attempt, this study reveals the potentials of hypergraph on advancing the pragmatic solutions for code reviewer recommendation.",10.1145/3510003.3510213,Modern code review;reviewer recommendation;hypergraph,Codes;Heuristic algorithms;Software algorithms;Computer architecture;Predictive models;History;Task analysis,graph theory;program diagnostics;public domain software;recommender systems,open source software development;reviewer recommendation approaches;pull-request development paradigm;HGRec;model review history;modern code review scenarios;code reviewer recommendation;hypergraph approach,2
727,Not Mentioned,ModX: Binary Level Partially Imported Third-Party Library Detection via Program Modularization and Semantic Matching,C. Yang; Z. Xu; H. Chen; Y. Liu; X. Gong; B. Liu,"School of Cyber Security, UCAS; School of Computer Science and Engineering, NTU; Huawei Technologies Co., Ltd.; School of Computer Science and Engineering, NTU; School of Cyber Security, UCAS; School of Cyber Security, UCAS",2022,"With the rapid growth of software, using third-party libraries (TPLs) has become increasingly popular. The prosperity of the library us-age has provided the software engineers with a handful of methods to facilitate and boost the program development. Unfortunately, it also poses great challenges as it becomes much more difficult to manage the large volume of libraries. Researches and studies have been proposed to detect and understand the TPLs in the soft-ware. However, most existing approaches rely on syntactic features, which are not robust when these features are changed or deliber-ately hidden by the adversarial parties. Moreover, these approaches typically model each of the imported libraries as a whole, there-fore, cannot be applied to scenarios where the host software only partially uses the library code segments. To detect both fully and partially imported TPLs at the semantic level, we propose Modx, a framework that leverages novel program modularization techniques to decompose the program into fine-grained functionality-based modules. By extracting both syntactic and semantic features, it measures the distance between modules to detect similar library module reuse in the program. Experimental results show that Modx outperforms other modularization tools by distinguishing more coherent program modules with 353% higher module quality scores and beats other TPL detection tools with on average 17% better in precision and 8% better in recall.",10.1145/3510003.3510627,Third-Party Library Detection;Program Modularization;Semantic Matcing,Codes;Semantics;Software algorithms;Reverse engineering;Syntactics;Feature extraction;Libraries,program diagnostics;security of data;software engineering;software libraries,third-party libraries;TPLs;library usage;software engineers;program development;syntactic features;adversarial parties;host software;library code segments;semantic level;program modularization techniques;fine-grained functionality-based modules;modularization tools;coherent program modules;TPL detection tools;ModX;binary level partially imported third-party library detection;library module reuse;Modx framework;semantic features,1
728,Not Mentioned,Morest: Model-based RESTful API Testing with Execution Feedback,Y. Liu; Y. Li; G. Deng; Y. Liu; R. Wan; R. Wu; D. Ji; S. Xu; M. Bao,"Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Huawei Cloud Computing Technologies Co., Ltd, China; Huawei Cloud Computing Technologies Co., Ltd, China; Huawei Technologies Co., Ltd, China; Huawei Cloud Computing Technologies Co., Ltd, China; Huawei Cloud Computing Technologies Co., Ltd, China",2022,"RESTful APIs are arguably the most popular endpoints for accessing Web services. Blackbox testing is one of the emerging techniques for ensuring the reliability of RESTful APIs. The major challenge in testing RESTful APIs is the need for correct sequences of API operation calls for in-depth testing. To build meaningful operation call sequences, researchers have proposed techniques to learn and utilize the API dependencies based on OpenAPI specifications. However, these techniques either lack the overall awareness of how all the APIs are connected or the flexibility of adaptively fixing the learned knowledge. In this paper, we propose Morest, a model-based RESTful API testing technique that builds and maintains a dynamically updating RESTful-service Property Graph (RPG) to model the behaviors of RESTful-services and guide the call sequence generation. We empirically evaluated Morest and the results demonstrate that Morest can successfully request an average of 152.66%-232.45% more API operations, cover 26.16%-103.24% more lines of code, and detect 40.64%-215.94% more bugs than state-of-the-art techniques. In total, we applied Morest to 6 real-world projects and found 44 bugs (13 of them cannot be detected by existing approaches). Specifically, 2 of the confirmed bugs are from Bitbucket, a famous code management service with more than 6 million users.",10.1145/3510003.3510133,RESTful service;model-based testing,Adaptation models;Codes;Web services;Computer bugs;Restful API;Behavioral sciences;Reliability,application program interfaces;program testing;Web services,Morest;RESTful APIs;blackbox testing;emerging techniques;API operation;in-depth testing;meaningful operation call sequences;API dependencies;model-based RESTful API testing technique;RESTful-service Property Graph;RESTful-services,1
729,Not Mentioned,Muffin: Testing Deep Learning Libraries via Neural Architecture Fuzzing,J. Gu; X. Luo; Y. Zhou; X. Wang,"Shanghai Key Lab. of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China; Shanghai Key Lab. of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China; Shanghai Key Lab. of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China; Shanghai Key Lab. of Intelligent Information Processing, School of Computer Science, Fudan University, Shanghai, China",2022,"Deep learning (DL) techniques are proven effective in many chal-lenging tasks, and become widely-adopted in practice. However, previous work has shown that DL libraries, the basis of building and executing DL models, contain bugs and can cause severe con-sequences. Unfortunately, existing testing approaches still cannot comprehensively exercise DL libraries. They utilize existing trained models and only detect bugs in model inference phase. In this work we propose Muffin to address these issues. To this end, Muffin applies a specifically-designed model fuzzing approach, which al-lows it to generate diverse DL models to explore the target library, instead of relying only on existing trained models. Muffin makes differential testing feasible in the model training phase by tailoring a set of metrics to measure the inconsistencies between different DL libraries. In this way, Muffin can best exercise the library code to detect more bugs. To evaluate the effectiveness of Muffin, we conduct experiments on three widely-used DL libraries. The results demonstrate that Muffin can detect 39 new bugs in the latest release versions of popular DL libraries, including Tensorflow, CNTK, and Theano.",10.1145/3510003.3510092,Deep Learning Testing;Library Testing;Model Generation;Fuzzing,Training;Deep learning;Phase measurement;Codes;Computer bugs;Fuzzing;Libraries,deep learning (artificial intelligence);fuzzy systems;neural net architecture;program debugging;program testing;software libraries,Muffin;neural architecture fuzzing;deep learning techniques;DL libraries;bugs;model inference phase;model fuzzing approach;target library;differential testing;library code;deep learning libraries testing,2
730,Not Mentioned,Multi-Intention-Aware Configuration Selection for Performance Tuning,H. He; Z. Jia; S. Li; Y. Yu; C. Zhou; Q. Liao; J. Wang; X. Liao,"National University of Defense Technology, China; National University of Defense Technology, China; National University of Defense Technology, China; National University of Defense Technology, China; National University of Defense Technology, China; Harbin Institute of Technology, Shenzhen, China; National University of Defense Technology, China; National University of Defense Technology, China",2022,"Automatic configuration tuning helps users who intend to improve software performance. However, the auto-tuners are limited by the huge configuration search space. More importantly, they fo-cus only on performance improvement while being unaware of other important user intentions (e.g., reliability, security). To re-duce the search space, researchers mainly focus on pre-selecting performance-related parameters which requires a heavy stage of dynamically running under different configurations to build per-formance models. Given that other important user intentions are not paid attention to, we focus on guiding users in pre-selecting performance-related parameters in general while warning about side-effects on non-performance intentions. We find that the con-figuration document often, if it does not always, contains rich in-formation about the parameters' relationship with diverse user intentions, but documents might also be long and domain-specific. In this paper, we first conduct a comprehensive study on 13 representative software containing 7,349 configuration parame-ters, and derive six types of ways in which configuration parame-ters may affect non-performance intentions. Guided by this study, we design SAFETUNE, a multi-intention-aware method that pre-selects important performance-related parameters and warns about their side-effects on non-performance intentions. Evaluation on target software shows that SAFETUNE correctly identifies 22â€“26 performance-related parameters that are missed by state-of-the-art tools but have significant performance impact (up to 14.7x). Furthermore, we illustrate eight representative cases to show that SAFETUNE can effectively prevent real-world and critical side-effects on other user intentions.",10.1145/3510003.3510094,Performance tuning;user intention;non-performance property,Software performance;Software systems;Software reliability;Security;Tuning;Software engineering,software performance evaluation,multiintention-aware configuration selection;automatic configuration tuning;software performance;performance improvement;SAFETUNE,
731,Not Mentioned,Multilingual training for Software Engineering,T. Ahmed; P. Devanbu,"University of California, Davis Davis, California, USA; University of California, Davis Davis, California, USA",2022,"Well-trained machine-learning models, which leverage large amounts of open-source software data, have now become an interesting approach to automating many software engineering tasks. Several SE tasks have all been subject to this approach, with performance gradually improving over the past several years with better models and training methods. More, and more diverse, clean, labeled data is better for training; but constructing good-quality datasets is time-consuming and challenging. Ways of augmenting the volume and diversity of clean, labeled data generally have wide applicability. For some languages (e.g., Ruby) labeled data is less abundant; in others (e.g., JavaScript) the available data maybe more focused on some application domains, and thus less diverse. As a way around such data bottlenecks, we present evidence suggesting that human-written code in different languages (which performs the same function), is rather similar, and particularly preserving of identifier naming patterns; we further present evidence suggesting that identifiers are a very important element of training data for software engineering tasks. We leverage this rather fortuitous phenomenon to find evidence that available multilingual training data (across different languages) can be used to amplify performance. We study this for 3 different tasks: code summarization, code retrieval, and function naming. We note that this data-augmenting approach is broadly compatible with different tasks, languages, and machine-learning models.",10.1145/3510003.3510049,code summarization;code search;method name prediction;deep learning,Training;Computer languages;Codes;Natural languages;Training data;Machine learning;Syntactics,learning (artificial intelligence);natural language processing;public domain software;software engineering,data bottlenecks;human-written code;software engineering tasks;leverage this rather fortuitous phenomenon;available multilingual training data;3 different tasks;data-augmenting approach;machine-learning models;leverage large amounts;open-source software data;SE tasks;training methods;good-quality datasets;application domains,5
732,Not Mentioned,MVD: Memory-Related Vulnerability Detection Based on Flow-Sensitive Graph Neural Networks,S. Cao; X. Sun; L. Bo; R. Wu; B. Li; C. Tao,"Yangzhou University, Yangzhou, China; Yangzhou University, Yangzhou, China; Yangzhou University, Yangzhou, China; Xiamen University, Xiamen, China; Yangzhou University, Yangzhou, China; Nanjing University of Aeronautics and Astronautics, Nanjing, China",2022,"Memory-related vulnerabilities constitute severe threats to the security of modern software. Despite the success of deep learning-based approaches to generic vulnerability detection, they are still limited by the underutilization of flow information when applied for detecting memory-related vulnerabilities, leading to high false positives. In this paper, we propose MVD, a statement-level Memory-related Vulnerability Detection approach based on flow-sensitive graph neural networks (FS-GNN). FS-GNN is employed to jointly embed both unstructured information (i.e., source code) and structured information (i.e., control- and data-flow) to capture implicit memory-related vulnerability patterns. We evaluate MVD on the dataset which contains 4,353 real-world memory-related vulnerabilities, and compare our approach with three state-of-the-art deep learning-based approaches as well as five popular static analysis-based memory detectors. The experiment results show that MVD achieves better detection accuracy, outperforming both state-of-the-art DL-based and static analysis-based approaches. Furthermore, MVD makes a great trade-off between accuracy and efficiency.",10.1145/3510003.3510219,Memory-Related Vulnerability;Vulnerability Detection;Graph Neural Networks;Flow Analysis,Analytical models;Codes;Semantics;Detectors;Graph neural networks;Software;Security,deep learning (artificial intelligence);graph theory;program diagnostics;security of data,FS-GNN;unstructured information;structured information;implicit memory-related vulnerability patterns;MVD;deep learning;static analysis;flow-sensitive graph neural networks;generic vulnerability detection;flow information;memory detectors,9
733,Not Mentioned,Nalin: learning from Runtime Behavior to Find Name-Value Inconsistencies in Jupyter Notebooks,J. Patra; M. Pradel,"University of Stuttgart, Germany; University of Stuttgart, Germany",2022,"Variable names are important to understand and maintain code. If a variable name and the value stored in the variable do not match, then the program suffers from a name-value inconsistency, which is due to one of two situations that developers may want to fix: Either a correct value is referred to through a misleading name, which negatively affects code understandability and maintainability, or the correct name is bound to a wrong value, which may cause unexpected runtime behavior. Finding name-value inconsistencies is hard because it requires an understanding of the meaning of names and knowledge about the values assigned to a variable at runtime. This paper presents Nalin, a technique to automatically detect name-value inconsistencies. The approach combines a dynamic analysis that tracks assignments of values to names with a neural machine learning model that predicts whether a name and a value fit together. To the best of our knowledge, this is the first work to formulate the problem of finding coding issues as a classification problem over names and runtime values. We apply Nalin to 106,652 real-world Python programs, where meaningful names are particularly important due to the absence of statically declared types. Our results show that the classifier detects name-value inconsistencies with high accuracy, that the warnings reported by Nalin have a precision of 80% and a recall of 76% w.r.t. a ground truth created in a user study, and that our approach complements existing techniques for finding coding issues.",10.1145/3510003.3510144,Neural software analysis;identifier names;learning-based bug detection,Analytical models;Runtime;Codes;Computer bugs;Machine learning;Predictive models;Encoding,learning (artificial intelligence);program diagnostics;Python,name-value inconsistency;runtime behavior;Jupyter notebooks;neural machine learning model;Python programs,1
734,Not Mentioned,Natural Attack for Pre-trained Models of Code,Z. Yang; J. Shi; J. He; D. Lo,"School of Computing and Information Systems, Singapore Management University; School of Computing and Information Systems, Singapore Management University; School of Computing and Information Systems, Singapore Management University; School of Computing and Information Systems, Singapore Management University",2022,"Pre-trained models of code have achieved success in many important software engineering tasks. However, these powerful models are vulnerable to adversarial attacks that slightly perturb model inputs to make a victim model produce wrong outputs. Current works mainly attack models of code with examples that preserve operational program semantics but ignore a fundamental requirement for adversarial example generation: perturbations should be natural to human judges, which we refer to as naturalness requirement. In this paper, we propose ALERT (Naturalness Aware Attack), a black-box attack that adversarially transforms inputs to make victim models produce wrong outputs. Different from prior works, this paper considers the natural semantic of generated examples at the same time as preserving the operational semantic of original inputs. Our user study demonstrates that human developers consistently consider that adversarial examples generated by ALERT are more natural than those generated by the state-of-the-art work by Zhang et al. that ignores the naturalness requirement. On attacking CodeBERT, our approach can achieve attack success rates of 53.62%, 27.79%, and 35.78% across three downstream tasks: vulnerability prediction, clone detection and code authorship attribution. On GraphCodeBERT, our approach can achieve average success rates of 76.95%, 7.96% and 61.47% on the three tasks. The above outperforms the baseline by 14.07% and 18.56% on the two pretrained models on average. Finally, we investigated the value of the generated adversarial examples to harden victim models through an adversarial fine-tuning procedure and demonstrated the accuracy of CodeBERT and GraphCodeBERT against ALERT-generated adversarial examples increased by 87.59% and 92.32%, respectively.",10.1145/3510003.3510146,Genetic Algorithm;Adversarial Attack;Pre-Trained Models,Codes;Perturbation methods;Semantics;Cloning;Transforms;Task analysis;Software engineering,cryptography;natural language processing;neural nets;software engineering,victim model produce wrong outputs;operational program semantics;adversarial example generation;naturalness requirement;black-box attack;adversarial fine-tuning procedure;ALERT-generated adversarial examples;software engineering tasks;adversarial attacks;natural attack;perturb model inputs;naturalness aware attack;CodeBERT;GraphCodeBERT;code authorship attribution;clone detection;vulnerability prediction;vulnerability prediction,10
735,Not Mentioned,Nessie: Automatically Testing JavaScript APIs with Asynchronous Callbacks,E. Arteca; S. Harner; M. Pradel; F. Tip,"Northeastern University, USA; University of Stuttgart, Germany; University of Stuttgart, Germany; Northeastern University, USA",2022,"Previous algorithms for feedback-directed unit test generation iteratively create sequences of API calls by executing partial tests and by adding new API calls at the end of the test. These algorithms are challenged by a popular class of APIs: higher-order functions that receive callback arguments, which often are invoked asyn-chronously. Existing test generators cannot effectively test such APIs because they only sequence API calls, but do not nest one call into the callback function of another. This paper presents Nessie, the first feedback-directed unit test generator that supports nesting of API calls and that tests asynchronous callbacks. Nesting API calls enables a test to use values produced by an API that are available only once a callback has been invoked, and is often necessary to ensure that methods are invoked in a specific order. The core contributions of our approach are a tree-based representation of unit tests with callbacks and a novel algorithm to iteratively generate such tests in a feedback-directed manner. We evaluate our approach on ten popular JavaScript libraries with both asynchronous and synchronous callbacks. The results show that, in a comparison with LambdaTester, a state of the art test generation technique that only considers sequencing of method calls, Nessie finds more behavioral differences and achieves slightly higher coverage. Notably, Nessie needs to generate significantly fewer tests to achieve and exceed the coverage achieved by the state of the art.",10.1145/3510003.3510106,asynchronous programming;test generation;JavaScript;testing,Sequential analysis;Generators;Libraries;Behavioral sciences;Test pattern generators;Testing;Software engineering,application program interfaces;Java;program testing,Nessie;fewer tests;JavaScript APIs;feedback-directed unit test generation;partial tests;API calls;higher-order functions;callback arguments;test generators;sequence API;nest one call;callback function;feedback-directed unit test generator;tests asynchronous callbacks;nesting API;unit tests;feedback-directed manner;art test generation technique;method calls,2
736,Not Mentioned,Neural Program Repair with Execution-based Backpropagation,H. Ye; M. Martinez; M. Monperrus,"KTH Royal Institute of Technology, Sweden; UniversitÃ© Polytechnique, France; KTH Royal Institute of Technology, Sweden",2022,"Neural machine translation (NMT) architectures have achieved promising results for automatic program repair. Yet, they have the limitation of generating low-quality patches (e.g., not compilable patches). This is because the existing works only optimize a purely syntactic loss function based on characters and tokens without incorporating program-specific information during neural network weight optimization. In this paper, we propose a novel program repair model called RewardRepair. The core novelty of RewardRepair is to improve NMT-based program repair with a loss function based on program compilation and test execution information, rewarding the network to produce patches that compile and that do not overfit. We conduct several experiments to evaluate RewardRepair showing that it is feasible and effective to use compilation and test execution results to optimize the underlying neural repair model. RewardRepair correctly repairs 207 bugs over four benchmarks. we report on repair success for 121 bugs that are fixed for the first time in the literature. Also, RewardRepair produces up to 45.3% of compilable patches, an improvement over the 39% by the state-of-the-art.",10.1145/3510003.3510222,automated program repair,Backpropagation;Training;Computer bugs;Semantics;Neural networks;Maintenance engineering;Syntactics,backpropagation;maintenance engineering;neural nets;program compilers;program debugging;program testing;software maintenance,neural network weight optimization;program repair model;NMT-based program repair;program compilation;test execution information;compile;underlying neural repair model;RewardRepair correctly repairs;repair success;compilable patches;neural program repair;execution-based backpropagation;neural machine translation architectures;automatic program repair;low-quality patches;purely syntactic loss function;program-specific information,14
737,Not Mentioned,NeuronFair: Interpretable White-Box Fairness Testing through Biased Neuron Identification,H. Zheng; Z. Chen; T. Du; X. Zhang; Y. Cheng; S. Ti; J. Wang; Y. Yu; J. Chen,Zhejiang University of Technology; Zhejiang University; Zhejiang University; Zhejiang University; Huawei International; Zhejiang University; Zhejiang University; National University of Defense Technology; Zhejiang University of Technology,2022,"Deep neural networks (DNNs) have demonstrated their outper-formance in various domains. However, it raises a social concern whether DNNs can produce reliable and fair decisions especially when they are applied to sensitive domains involving valuable re-source allocation, such as education, loan, and employment. It is crucial to conduct fairness testing before DNNs are reliably de-ployed to such sensitive domains, i.e., generating as many instances as possible to uncover fairness violations. However, the existing testing methods are still limited from three aspects: interpretabil-ity, performance, and generalizability. To overcome the challenges, we propose NeuronFair, a new DNN fairness testing framework that differs from previous work in several key aspects: (1) inter-pretable - it quantitatively interprets DNNs' fairness violations for the biased decision; (2) effective - it uses the interpretation results to guide the generation of more diverse instances in less time; (3) generic - it can handle both structured and unstructured data. Extensive evaluations across 7 datasets and the corresponding DNNs demonstrate NeuronFair's superior performance. For instance, on structured datasets, it generates much more instances (~ Ã—5.84) and saves more time (with an average speedup of 534.56%) compared with the state-of-the-art methods. Besides, the instances of NeuronFair can also be leveraged to improve the fairness of the biased DNNs, which helps build more fair and trustworthy deep learning systems. The code of NeuronFair is open-sourced at https:/github.com/haibinzheng/NeuronFair.",10.1145/3510003.3510123,Interpretability;fairness testing;discriminatory instance;deep learning;biased neuron,Deep learning;Neurons;Neural networks;Employment;Education;Reliability;Resource management,data mining;feature extraction;learning (artificial intelligence);medical computing;natural language processing;neural nets;regression analysis,biased DNNs;fair learning systems;trustworthy deep learning systems;interpretable white-box fairness testing;biased neuron identification;deep neural networks;outper-formance;social concern;reliable decisions;fair decisions;sensitive domains;valuable re-source allocation;loan;existing testing methods;interpretabil-ity;DNN fairness;DNNs' fairness violations;biased decision;interpretation results;diverse instances;structured data;unstructured data;corresponding DNNs;NeuronFair's superior performance;structured datasets,2
738,Not Mentioned,NPEX: Repairing Java Null Pointer Exceptions without Tests,J. Lee; S. Hong; H. Oh,"Korea University, Republic of Korea; Korea University, Republic of Korea; Korea University, Republic of Korea",2022,"We present NPEX, a new technique for repairing Java null pointer exceptions (NPEs) without tests. State-of-the-art NPE repair techniques rely on test suites written by developers for patch validation. Unfortunately, however, those are typically future test cases that are unavailable at the time bugs are reported or insufficient to identify correct patches. Unlike existing techniques, NPEX does not require test cases; instead, NPEX automatically infers the repair specification of the buggy program and uses the inferred specification to validate patches. The key idea is to learn a statistical model that predicts how developers would handle NPEs by mining null-handling patterns from existing codebases, and to use a variant of symbolic execution that can infer the repair specification from the buggy program using the model. We evaluated NPEX on real-world NPEs collected from diverse open-source projects. The results show that NPEX significantly outperforms the current state-of-the-art.",10.1145/3510003.3510186,,Java;Computer bugs;Maintenance engineering;Predictive models;Behavioral sciences;Open source software;Software engineering,data mining;Java;program debugging;program diagnostics;program testing;program verification;software fault tolerance,repairing Java null pointer exceptions;state-of-the-art NPE repair techniques;test suites;patch validation;typically future test cases;correct patches;NPEX;repair specification;buggy program;inferred specification,
739,Not Mentioned,Nufix: Escape From NuGet Dependency Maze,Z. Li; Y. Wang; Z. Lin; S. -C. Cheung; J. -G. Lou,"Software College, Northeastern University, Shenyang, China; Software College, Northeastern University, Shenyang, China; Microsoft Research Asia, Beijing, China; The Hong Kong University of Science and Technology and Guangzhou HKUST Fok Ying Tung Research Institute, Hong Kong, China; Microsoft Research Asia, Beijing, China",2022,"Developers usually suffer from dependency maze (DM) issues, i.e., package dependency constraints are violated when a project's platform or dependencies are changed. This problem is especially serious in. NET ecosystem due to its fragmented platforms (e.g.,. NET Framework,. NET Core, and. NET Standard). Fixing DM issues is challenging due to the complexity of dependency constraints: multiple DM issues often occur in one project; solving one DM issue usually causes another DM issue cropping up; the exponential search space of possible dependency combinations is also a barrier. In this paper, we aim to help. NET developers tackle the DM issues. First, we empirically studied a set of real DM issues, learning their common fixing strategies and developers' preferences in adopting these strategies. Based on these findings, we propose NuFIX, an automated technique to repair DM issues. NUFIX formulates the repair task as a binary integer linear optimization problem to effectively derive an optimal fix in line with the learnt developers' preferences. The experiment results and expert validation show that NUFIX can generate high-quality fixes for all the DM issues with 262 popular. NET projects. Encouragingly, 20 projects (including affected projects such as Dropbox) have approved and merged our generated fixes, and shown great interests in our technique.",10.1145/3510003.3510118,.NET;NuGet;dependencies;empirical study,Ecosystems;Maintenance engineering;Complexity theory;Task analysis;Standards;Optimization;Software engineering,integer programming;linear programming;optimisation;program diagnostics,nufix;NuGet dependency maze;dependency maze issues;package dependency constraints;DM issue;multiple DM issues;NUFIX,
740,Not Mentioned,OJXPERF: Featherlight Object Replica Detection for Java Programs,B. Li; H. Xu; Q. Zhao; P. Su; M. Chabbi; S. Jiao; X. Liu,"North Carolina State University, Raleigh, North Carolina, USA; College of William and Mary, Williamsburg, Virginia, USA; North Carolina State University, Raleigh, North Carolina, USA; University of California, Merced, Merced, California, USA; Scalable Machines Research; North Carolina State University, Raleigh, North Carolina, USA; North Carolina State University, Raleigh, North Carolina, USA",2022,"Memory bloat is an important source of inefficiency in complex production software, especially in software written in managed languages such as Java. Prior approaches to this problem have focused on identifying objects that outlive their life span. Few studies have, however, looked into whether and to what extent myriad objects of the same type are identical. A quantitative assessment of identical objects with code-level attribution can assist developers in refactoring code to eliminate object bloat, and favor reuse of existing object(s). The result is reduced memory pressure, reduced allocation and garbage collection, enhanced data locality, and reduced re-computation, all of which result in superior performance. We develop OJXPerf, a lightweight sampling-based profiler, which probabilistically identifies identical objects. OJXPerf employs hardware performance monitoring units (PMU) in conjunction with hardware debug registers to sample and compare field values of different objects of the same type allocated at the same calling context but potentially accessed at different program points. The result is a lightweight measurement â€“ a combination of object allocation contexts and usage contexts ordered by duplication frequency. This class of duplicated objects is relatively easier to optimize. OJXPerf incurs 9% runtime and 6% memory overheads on average. We empirically show the benefit of OJXPerf by using its profiles to instruct us to optimize a number of Java programs, including well-known benchmarks and real-world applications. The results show a noticeable reduction in memory usage (up to 11%) and a significant speedup (up to 25%).",10.1145/3510003.3510083,,Java;Runtime;Codes;Software;Hardware;Registers;Frequency measurement,Java;program debugging;software maintenance;storage management,OJXPERF;featherlight object replica detection;Java programs;memory bloat;complex production software;managed languages;outlive their life span;extent myriad objects;identical objects;code-level attribution;object bloat;memory pressure;reduced allocation;garbage collection;enhanced data locality;OJXPerf;lightweight sampling-based profiler;hardware performance monitoring units;hardware debug registers;different program points;object allocation contexts;duplicated objects;6% memory overheads;memory usage,
741,Not Mentioned,On Debugging the Performance of Configurable Software Systems: Developer Needs and Tailored Tool Support,M. Velez; P. Jamshidi; N. Siegmund; S. Apel; C. KÃ¤stner,Carnegie Mellon University; University of South Carolina; Leipzig University; Saarland Informatics Campus - Saarland University; Carnegie Mellon University,2022,"Determining whether a configurable software system has a performance bug or it was misconfigured is often challenging. While there are numerous debugging techniques that can support developers in this task, there is limited empirical evidence of how useful the techniques are to address the actual needs that developers have when debugging the performance of configurable software systems; most techniques are often evaluated in terms of technical accuracy instead of their usability. In this paper, we take a human-centered approach to identify, design, implement, and evaluate a solution to support developers in the process of debugging the performance of configurable software systems. We first conduct an exploratory study with 19 developers to identify the information needs that developers have during this process. Subsequently, we design and implement a tailored tool, adapting techniques from prior work, to support those needs. Two user studies, with a total of 20 developers, validate and confirm that the information that we provide helps developers debug the performance of configurable software systems.",10.1145/3510003.3510043,,Adaptation models;Computer bugs;Debugging;Software systems;Usability;Task analysis;Software engineering,program debugging;software performance evaluation;software tools,configurable software system;tailored tool support;debugging techniques;performance bug;human-centered approach;information needs,2
742,Not Mentioned,On the Benefits and Limits of Incremental Build of Software Configurations: An Exploratory Study,G. A. Randrianaina; X. TÃ«rnava; D. E. Khelladi; M. Acher,"Univ Rennes, CNRS, Inria, IRISA - UMR 6074, Rennes, France; Univ Rennes, CNRS, Inria, IRISA - UMR 6074, Rennes, France; Univ Rennes, CNRS, Inria, IRISA - UMR 6074, Rennes, France; Univ Rennes, CNRS, Inria, IRISA - UMR 6074 Institut Universitaire de France (IUF), Rennes, France",2022,"Software projects use build systems to automate the compilation, testing, and continuous deployment of their software products. As software becomes increasingly configurable, the build of multiple configurations is a pressing need, but expensive and challenging to implement. The current state of practice is to build independently (a.k.a., clean build) a software for a subset of configurations. While incremental build has been studied for software evolution and relatively small changes of the source code, it has surprisingly not been considered for software configurations. In this exploratory study, we examine the benefits and limits of building software configurations incrementally, rather than always building them cleanly. By using five real-life configurable systems as subjects, we explore whether incremental build works, outperforms a sequence of clean builds, is correct w.r.t. clean build, and can be used to find an optimal ordering for building configurations. Our results show that incremental build is feasible in 100% of the times in four subjects and in 78% of the times in one subject. In average, 88.5% of the configurations could be built faster with incremental build while also finding several alternatives faster incremental builds. However, only 60% of faster incremental builds are correct. Still, when considering those correct incremental builds with clean builds, we could always find an optimal order that is faster than just a collection of clean builds with a gain up to 11.76%.",10.1145/3510457.3513035,Configurable software systems;build systems;configuration build,Knowledge engineering;Costs;Codes;Buildings;Pressing;Software systems;Software,project management;software development management;software product lines,software projects;build systems;software products;multiple configurations;clean build;software evolution;software configurations;real-life configurable systems;source code,1
743,Not Mentioned,On the Evaluation of Neural Code Summarization,E. Shi; Y. Wang; L. Du; J. Chen; S. Han; H. Zhang; D. Zhang; H. Sun,Xi'an Jiaotong University; Microsoft Research; Microsoft Research; Tianjin University; Microsoft Research; The University of Newcastle; Microsoft Research; Xi'an Jiaotong University,2022,"Source code summaries are important for program comprehension and maintenance. However, there are plenty of programs with missing, outdated, or mismatched summaries. Recently, deep learning techniques have been exploited to automatically generate summaries for given code snippets. To achieve a profound understanding of how far we are from solving this problem and provide suggestions to future research, in this paper, we conduct a systematic and in-depth analysis of 5 state-of-the-art neural code summarization models on 6 widely used BLEU variants, 4 pre-processing operations and their combinations, and 3 widely used datasets. The evaluation results show that some important factors have a great influence on the model evaluation, especially on the performance of models and the ranking among the models. However, these factors might be easily overlooked. Specifically, (1) the BLEU metric widely used in existing work of evaluating code summarization models has many variants. Ignoring the differences among these variants could greatly affect the validity of the claimed results. Besides, we discover and resolve an important and previously unknown bug in BLEU calculation in a commonly-used software package. Furthermore, we conduct human evaluations and find that the metric BLEU-DC is most correlated to human perception; (2) code pre-processing choices can have a large (from âˆ’18% to +25%) impact on the summarization performance and should not be neglected. We also explore the aggregation of pre-processing combinations and boost the performance of models; (3) some important char-acteristics of datasets (corpus sizes, data splitting methods, and duplication ratios) have a significant impact on model evaluation. Based on the experimental results, we give actionable suggestions for evaluating code summarization and choosing the best method in different scenarios. We also build a shared code summarization toolbox to facilitate future research.",10.1145/3510003.3510060,Code summarization;Empirical study;Deep learning;Evaluation,Measurement;Analytical models;Codes;Systematics;Smoothing methods;Software packages;Data models,language translation;learning (artificial intelligence);natural language processing;software maintenance;text analysis,"mismatched summaries;deep learning techniques;given code snippets;profound understanding;systematic depth analysis;in-depth analysis;5 state-of-the-art neural code summarization models;4 pre-processing operations;claimed results;important bug;previously unknown bug;BLEU calculation;human evaluations;metric BLEU-DC;summarization performance;pre-processing combinations;important char-acteristics;shared code summarization toolbox;source code summaries;program comprehension;maintenance;missing summaries;outdated, summaries",11
744,Not Mentioned,On the Importance of Building High-quality Training Datasets for Neural Code Search,Z. Sun; L. Li; Y. Liu; X. Du; L. Li,"Monash University, Melbourne, Victoria, Australia; Tongji University, Shanghai, China; Tongji University, Shanghai, China; Monash University, Melbourne, Victoria, Australia; Monash University, Melbourne, Victoria, Australia",2022,"The performance of neural code search is significantly influenced by the quality of the training data from which the neural models are derived. A large corpus of high-quality query and code pairs is demanded to establish a precise mapping from the natural language to the programming language. Due to the limited availability, most widely-used code search datasets are established with compromise, such as using code comments as a replacement of queries. Our empirical study on a famous code search dataset reveals that over one-third of its queries contain noises that make them deviate from natural user queries. Models trained through noisy data are faced with severe performance degradation when applied in real-world scenarios. To improve the dataset quality and make the queries of its samples semantically identical to real user queries is critical for the practical usability of neural code search. In this paper, we propose a data cleaning framework consisting of two subsequent filters: a rule-based syntactic filter and a model-based semantic filter. This is the first framework that applies semantic query cleaning to code search datasets. Experimentally, we evaluated the effectiveness of our framework on two widely-used code search models and three manually-annotated code retrieval benchmarks. Training the popular DeepCS model with the filtered dataset from our framework improves its performance by 19.2% MRR and 21.3% Answer@l, on average with the three validation benchmarks.",10.1145/3510003.3510160,Code search;dataset;data cleaning;deep learning,Training;Codes;Computational modeling;Semantics;Training data;Benchmark testing;Data models,data handling;learning (artificial intelligence);natural language processing;query processing,building high-quality training datasets;neural code search;training data;neural models;high-quality query;code pairs;code search datasets;code comments;famous code search dataset;natural user queries;dataset quality;model-based semantic filter;semantic query cleaning;code search models;manually-annotated code retrieval benchmarks;filtered dataset,7
745,Not Mentioned,On the Reliability of Coverage-Based Fuzzer Benchmarking,M. BÃ¶hme; L. Szekeres; J. Metzman,"MPI-SP, Germany Monash University, Australia; Google, USA; Google, USA",2022,"Given a program where none of our fuzzers finds any bugs, how do we know which fuzzer is better? In practice, we often look to code coverage as a proxy measure of fuzzer effectiveness and consider the fuzzer which achieves more coverage as the better one. Indeed, evaluating 10 fuzzers for 23 hours on 24 programs, we find that a fuzzer that covers more code also finds more bugs. There is a very strong correlation between the coverage achieved and the number of bugs found by a fuzzer. Hence, it might seem reasonable to compare fuzzers in terms of coverage achieved, and from that derive empirical claims about a fuzzer's superiority at finding bugs. Curiously enough, however, we find no strong agreement on which fuzzer is superior if we compared multiple fuzzers in terms of coverage achieved instead of the number of bugs found. The fuzzer best at achieving coverage, may not be best at finding bugs.",10.1145/3510003.3510230,fuzzing;benchmarking;test suite effectiveness;code coverage,Codes;Correlation;Computer bugs;Benchmark testing;Reliability engineering;Software reliability;Software engineering,program debugging;program testing;software reliability,program bugs;coverage-based fuzzer;fuzzer effectiveness;code coverage;coverage-based fuzzer benchmarking reliability,7
746,Not Mentioned,One Fuzzing Strategy to Rule Them All,M. Wu; L. Jiang; J. Xiang; Y. Huang; H. Cui; L. Zhang; Y. Zhang,"Southern University of Science and Technology, Shenzhen, China; Southern University of Science and Technology, Shenzhen, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; The University of Hong Kong, Hong Kong, China; University of Illinois, Urbana-Champaign, USA; Southern University of Science and Technology, Shenzhen, China",2022,"Coverage-guided fuzzing has become mainstream in fuzzing to automatically expose program vulnerabilities. Recently, a group of fuzzers are proposed to adopt a random search mechanism namely Havoc, explicitly or implicitly, to augment their edge exploration. However, they only tend to adopt the default setup of Havoc as an implementation option while none of them attempts to explore its power under diverse setups or inspect its rationale for potential improvement. In this paper, to address such issues, we conduct the first empirical study on Havoc to enhance the understanding of its characteristics. Specifically, we first find that applying the default setup of Havoc to fuzzers can significantly improve their edge coverage performance. Interestingly, we further observe that even simply executing Havoc itself without appending it to any fuzzer can lead to strong edge coverage performance and outper-form most of our studied fuzzers. Moreover, we also extend the execution time of Havoc and find that most fuzzers can not only achieve significantly higher edge coverage, but also tend to perform similarly (i.e., their performance gaps get largely bridged). Inspired by the findings, we further propose HavocMAB, which models the Havoc mutation strategy as a multi-armed bandit problem to be solved by dynamically adjusting the mutation strategy. The evaluation result presents that HavocMAB can significantly increase the edge coverage by 11.1% on average for all the benchmark projects compared with Havoc and even slightly outperform state-of-the-art QSYM which augments its computing resource by adopting three parallel threads. We further execute HavocMAB with three parallel threads and result in 9% higher average edge coverage over QSYM upon all the benchmark projects.",10.1145/3510003.3510174,,Instruction sets;Image edge detection;Fuzzing;Benchmark testing;Software engineering,benchmark testing;evolutionary computation;learning (artificial intelligence);program testing;search problems;security of data;stochastic processes,coverage-guided fuzzing;random search mechanism;edge exploration;default setup;diverse setups;strong edge coverage performance;studied fuzzers;higher edge coverage;Havoc mutation strategy;higher average edge coverage;fuzzing strategy,5
747,Not Mentioned,Online Summarizing Alerts through Semantic and Behavior Information,J. Chen; P. Wang; W. Wang,"Fudan University, Shanghai, China; Fudan University, Shanghai, China; Fudan University, Shanghai, China",2022,"Alerts, which record details about system failures, are crucial data for monitoring a online service system. Due to the complex correlation between system components, a system failure usually triggers a large number of alerts, making the traditional manual handling of alerts insufficient. Thus, automatically summarizing alerts is a problem demanding prompt solution. This paper tackles this challenge through a novel approach based on supervised learning. The proposed approach, OAS (Online Alert Summarizing), first learns two types of information from alerts, semantic information and behavior information, respectively. Then, OAS adopts a specific deep learning model to aggregate semantic and behavior repre-sentations of alerts and thus determines the correlation between alerts. OAS is able to summarize the newly reported alert online. Extensive experiments, which are conducted on real alert datasets from two large commercial banks, demonstrate the efficiency and the effectiveness of OAS.",10.1145/3510003.3510055,alert summary;online service systems;system maintenance,Deep learning;Correlation;Aggregates;Semantics;Supervised learning;Manuals;Maintenance engineering,data mining;deep learning (artificial intelligence);Internet;security of data;supervised learning;system monitoring,system failure;system components;OAS;semantic information;behavior information;alert datasets;online summarizing alerts;online alert summarizing;online service system monitoring;supervised learning;deep learning;behavior representation,1
748,Not Mentioned,Path Transitions Tell More: Optimizing Fuzzing Schedules via Runtime Program States,K. Zhang; X. Xiao; X. Zhu; R. Sun; M. Xue; S. Wen,"Tsinghua Shenzhen International Graduate School, Tsinghua University; Tsinghua Shenzhen International Graduate School, Tsinghua University; Swinburne University of Technology; The University of Adelaide; The University of Adelaide; Swinburne University of Technology",2022,"Coverage-guided Greybox Fuzzing (CGF) is one of the most successful and widely-used techniques for bug hunting. Two major approaches are adopted to optimize CGF: (i) to reduce search space of inputs by inferring relationships between input bytes and path constraints; (ii) to formulate fuzzing processes (e.g., path transitions) and build up probability distributions to optimize power schedules, i.e., the number of inputs generated per seed. However, the former is subjective to the inference results which may include extra bytes for a path constraint, thereby limiting the efficiency of path constraints resolution, code coverage discovery, and bugs exposure; the latter formalization, concentrating on power schedules for seeds alone, is inattentive to the schedule for bytes in a seed. In this paper, we propose a lightweight fuzzing approach, Truzz, to optimize existing Coverage-guided Greybox Fuzzers (CGFs). To address two aforementioned challenges, Truzz identifies the bytes related to the validation checks (i.e., the checks guarding error-handling code), and protects those bytes from being frequently mutated, making most generated inputs examine the functionalities of programs, in lieu of being rejected by validation checks. The byte-wise relationship determination mitigates the problem of loading extra bytes when fuzzers infer the byte-constraint relation. Furthermore, the proposed path transition within Truzz can efficiently prioritize the seed as the new path, harvesting many new edges, and the new path likely belongs to a code region with many undiscovered code lines. To evaluate our approach, we implemented 6 state-of-the-art fuzzers, AFL, AFLFast, NEUZZ, MOPT, FuzzFactory and GreyOne, in Truzz. The experimental results show that on average, Truzz can generate 16.14% more inputs flowing into functional code, in addition to 24.75% more new edges than the vanilla fuzzers. Finally, our approach exposes 13 bugs in 8 target programs, and 6 of them have not been identified by the vanilla fuzzers.",10.1145/3510003.3510063,Fuzzing;Software Security;Path Transition;Mutation,Schedules;Codes;Runtime;Limiting;Computer bugs;Loading;Fuzzing,program debugging;program testing;security of data,path transition;CGF;bug hunting;path constraint;fuzzing processes;path constraints resolution;code coverage discovery;bugs exposure;lightweight fuzzing approach;coverage-guided greybox fuzzers;error-handling code;byte-wise relationship determination;byte-constraint relation;vanilla fuzzers;Truzz approach;AFL fuzzer;AFLFast fuzzer;NEUZZ fuzzer;MOPT fuzzer;FuzzFactory fuzzer;GreyOne fuzzer,1
749,Not Mentioned,PerfSig: Extracting Performance Bug Signatures via Multi-modality Causal Analysis,J. He; Y. Lin; X. Gu; C. -C. M. Yeh; Z. Zhuang,"ShanghaiTech University, Shanghai, China; North Carolina State University, Raleigh, NC, USA; North Carolina State University, Raleigh, NC, USA; Visa Research, Palo Alto, CA, USA; Visa Research, Palo Alto, CA, USA",2022,"Diagnosing a performance bug triggered in production cloud environments is notoriously challenging. Extracting performance bug signatures can help cloud operators quickly pinpoint the problem and avoid repeating manual efforts for diagnosing similar performance bugs. In this paper, we present PerfSig, a multi-modality performance bug signature extraction tool which can identify principal anomaly patterns and root cause functions for performance bugs. PerfSig performs fine-grained anomaly detection over various machine data such as system metrics, system logs, and function call traces. We then conduct causal analysis across different machine data using information theory method to pinpoint the root cause function of a performance bug. PerfSig generates bug signatures as the combination of the identified anomaly patterns and root cause functions. We have implemented a prototype of PerfSig and conducted evaluation using 20 real world performance bugs in six commonly used cloud systems. Our experimental results show that PerfSig captures various kinds of fine-grained anomaly patterns from different machine data and successfully identifies the root cause functions through multi-modality causal analysis for 19 out of 20 tested performance bugs.",10.1145/3510003.3510110,Debugging;Bug signatures;Software reliability;Performance,Measurement;Computer bugs;Prototypes;Production;Reliability engineering;Software reliability;System analysis and design,cloud computing;data analysis;program debugging;security of data;system monitoring,20 tested performance bugs;performance bug signatures;multimodality causal analysis;production cloud environments;diagnosing similar performance bugs;multimodality performance bug signature extraction tool;root cause function;PerfSig performs fine-grained anomaly detection;different machine data;identified anomaly patterns;20 real world performance bugs;fine-grained anomaly patterns,
750,Not Mentioned,Practical Automated Detection of Malicious npm Packages,A. Sejfia; M. SchÃ¤fer,"University of Southern California, Los Angeles, USA; GitHub, Oxford, UK",2022,"The npm registry is one of the pillars of the JavaScript and Type-Script ecosystems, hosting over 1.7 million packages ranging from simple utility libraries to complex frameworks and entire applications. Each day, developers publish tens of thousands of updates as well as hundreds of new packages. Due to the overwhelming popularity of npm, it has become a prime target for malicious actors, who publish new packages or compromise existing packages to introduce malware that tampers with or exfiltrates sensitive data from users who install either these packages or any package that (transitively) depends on them. Defending against such attacks is essential to maintaining the integrity of the software supply chain, but the sheer volume of package updates makes comprehensive manual review infeasible. We present Amalfi, a machine-learning based approach for automatically detecting potentially malicious packages comprised of three complementary techniques. We start with classifiers trained on known examples of malicious and benign packages. If a package is flagged as malicious by a classifier, we then check whether it includes metadata about its source repository, and if so whether the package can be reproduced from its source code. Packages that are reproducible from source are not usually malicious, so this step allows us to weed out false positives. Finally, we also employ a simple textual clone-detection technique to identify copies of malicious packages that may have been missed by the classifiers, reducing the number of false negatives. Amalfi improves on the state of the art in that it is lightweight, requiring only a few seconds per package to extract features and run the classifiers, and gives good results in practice: running it on 96287 package versions published over the course of one week, we were able to identify 95 previously unknown malware samples, with a manageable number of false positives.",10.1145/3510003.3510104,supply chain security;malware detection,Training;Supply chains;Ecosystems;Manuals;Static analysis;Syntactics;Feature extraction,computer crime;computer network security;feature extraction;invasive software;learning (artificial intelligence);meta data;security of data;software maintenance;software packages,JavaScript ecosystems;Type-Script ecosystems;million packages;malicious actors;existing packages;package updates;potentially malicious packages;classifier;benign packages;simple textual clone-detection technique;96287 package versions;practical automated detection;malicious npm packages;npm registry,5
751,Not Mentioned,Practitioners' Expectations on Automated Code Comment Generation,X. Hu; X. Xia; D. Lo; Z. Wan; Q. Chen; T. Zimmermann,"School of Software Technology, Zhejiang University, Ningbo, China; Zhejiang University, Hangzhou, China; Singapore Management University, Singapore; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Microsoft Research, Seattle, USA",2022,"Good comments are invaluable assets to software projects, as they help developers understand and maintain projects. However, due to some poor commenting practices, comments are often missing or inconsistent with the source code. Software engineering practitioners often spend a significant amount of time and effort reading and understanding programs without or with poor comments. To counter this, researchers have proposed various techniques to au-tomatically generate code comments in recent years, which can not only save developers time writing comments but also help them better understand existing software projects. However, it is unclear whether these techniques can alleviate comment issues and whether practitioners appreciate this line of research. To fill this gap, we performed an empirical study by interviewing and surveying practitioners about their expectations of research in code comment generation. We then compared what practitioners need and the current state-of-the-art research by performing a literature review of papers on code comment generation techniques pub-lished in the premier publication venues from 2010 to 2020. From this comparison, we highlighted the directions where researchers need to put effort to develop comment generation techniques that matter to practitioners.",10.1145/3510003.3510152,Code Comment Generation;Empirical Study;Practitioners' Expectations,Codes;Bibliographies;Software;Software engineering,software maintenance;software management,code comment generation;automated code comment;commenting practices;source code;software engineering practitioners;software projects,2
752,Not Mentioned,PREACH: A Heuristic for Probabilistic Reachability to Identify Hard to Reach Statements,S. Saha; M. Downing; T. Brennan; T. Bultan,"University of California, Santa Barbara, Santa Barbara, CA, USA; University of California, Santa Barbara, Santa Barbara, CA, USA; University of California, Santa Barbara, Santa Barbara, CA, USA; University of California, Santa Barbara, Santa Barbara, CA, USA",2022,"We present a heuristic for approximating the likelihood of reaching a given program statement using 1) branch selectivity (representing the percentage of values that satisfy a branch condition), which we compute using model counting, 2) dependency analysis, which we use to identify input-dependent branch conditions that influence statement reachability, 3) abstract interpretation, which we use to identify the set of values that reach a branch condition, and 4) a discrete-time Markov chain model, which we construct to capture the control flow structure of the program together with the selectivity of each branch. Our experiments indicate that our heuristic-based probabilistic reachability analysis tool PReach can identify hard to reach statements with high precision and accuracy in benchmarks from software verification and testing competitions, Apache Commons Lang, and the DARPA STAC program. We provide a detailed comparison with probabilistic symbolic execution and statistical symbolic execution for the purpose of identifying hard to reach statements. PREACH achieves comparable precision and accuracy to both probabilistic and statistical symbolic execution for bounded execution depth and better precision and accuracy when execution depth is unbounded and the number of program paths grows exponentially. Moreover, PReach is more scalable than both probabilistic and statistical symbolic execution.",10.1145/3510003.3510227,,Analytical models;Computational modeling;Government;Benchmark testing;Probability;Model checking;Probabilistic logic,Markov processes;probability;program diagnostics;program verification;reachability analysis,influence statement reachability;branch condition;discrete-time Markov chain model;control flow structure;probabilistic reachability analysis tool PReach;statements;software verification;testing competitions;DARPA STAC program;probabilistic symbolic execution;statistical symbolic execution;bounded execution depth;program paths;heuristic;identify hard;given program statement;model counting;input-dependent branch conditions,
753,Not Mentioned,Precise Divide-By-Zero Detection with Affirmative Evidence,Y. Guo; J. Zhou; P. Yao; Q. Shi; C. Zhang,"The Hong Kong University of Science and Technology, Hong Kong, China; Ant Group, China; The Hong Kong University of Science and Technology, Hong Kong, China; Ant Group, China; The Hong Kong University of Science and Technology, Hong Kong, China",2022,"The static detection of divide-by-zero, a common programming error, is particularly prone to false positives because conventional static analysis reports a divide-by-zero bug whenever it cannot prove the safety property â€“ the divisor variable is not zero in all executions. When reasoning the program semantics over a large number of under-constrained variables, conventional static analyses significantly loose the bounds of divisor variables, which easily fails the safety proof and leads to a massive number of false positives. We propose a static analysis to detect divide-by-zero bugs taking additional evidence for under-constrained variables into consideration. Based on an extensive empirical study of known divide-by-zero bugs, we no longer arbitrarily report a bug once the safety verification fails. Instead, we actively look for affirmative evidences, namely source evidence and bound evidence, that imply a high possibility of the bug to be triggerable at runtime. When applying our tool Wit to the real-world software such as the Linux kernel, we have found 72 new divide-by-zero bugs with a low false positive rate of 22%.",10.1145/3510003.3510066,Static program analysis;bug detection;divide-by-zero,Runtime;Linux;Computer bugs;Semantics;Static analysis;Programming;Software,Linux;program debugging;program diagnostics;program verification,static detection;Precise Divide-By-Zero Detection;affirmative evidence;known divide-by-zero bugs;conventional static analyses;under-constrained variables;divisor variable;divide-by-zero bug;conventional static analysis;false positives;common programming error,
754,Not Mentioned,Preempting Flaky Tests via Non-Idempotent-Outcome Tests,A. Wei; P. Yi; Z. Li; T. Xie; D. Marinov; W. Lam,Stanford University; Peking University; University of Illinois; Peking University; University of Illinois; George Mason University,2022,"Regression testing can greatly help in software development, but it can be seriously undermined by flaky tests, which can both pass and fail, seemingly nondeterministically, on the same code commit. Flaky tests are an emerging topic in both research and industry. Prior work has identified multiple categories of flaky tests, developed techniques for detecting these flaky tests, and analyzed some detected flaky tests. To proactively detect, i.e., preempt, flaky tests, we propose to detect non-idempotent-outcome (NIO) tests, a novel category related to flaky tests. In particular, we run each test twice in the same test execution environment, e.g., run each Java test twice in the same Java Virtual Machine. A test is NIO if it passes in the first run but fails in the second. Each NIO test has side effects and â€œself-pollutesâ€ the state shared among test runs. We perform experiments on both Java and Python open-source projects, detecting 223 NIO Java tests and 138 NIO Python tests. We have inspected all 361 detected tests and opened pull requests that fix 268 tests, with 192 already accepted, only 6 rejected, and the remaining 70 pending.",10.1145/3510003.3510170,,Industries;Java;Codes;Virtual machining;Open source software;Python;Testing,Java;program testing;public domain software;regression analysis;virtual machines,Java test;NIO test;test runs;223 NIO Java tests;138 NIO Python tests;361 detected tests;nonidempotent-outcome tests;regression testing;detected flaky tests;test execution environment,5
755,Not Mentioned,Prioritizing Mutants to Guide Mutation Testing,S. J. Kaufman; R. Featherman; J. Alvin; B. Kurtz; P. Ammann; R. Just,"University of Washington, Seattle, Washington, USA; University of Washington, Seattle, Washington, USA; University of Massachusetts, Amherst, Massachusetts, USA; George Mason University, Fairfax, Virginia, USA; University of Washington, Seattle, Washington, USA; George Mason University, Fairfax, Virginia, USA",2022,"Mutation testing offers concrete test goals (mutants) and a rigorous test efficacy criterion, but it is expensive due to vast numbers of mutants, many of which are neither useful nor actionable. Prior work has focused on selecting representative and sufficient mutant subsets, measuring whether a test set that is mutation-adequate for the subset is equally adequate for the entire set. However, no known industrial application of mutation testing uses or even computes mutation adequacy, instead focusing on iteratively presenting very few mutants as concrete test goals for developers to write tests. This paper (1) articulates important differences between mutation analysis, where measuring mutation adequacy is of interest, and mutation testing, where mutants are of interest insofar as they serve as concrete test goals to elict effective tests; (2) introduces a new measure of mutant usefulness, called test completeness advancement probability (TCAP); (3) introduces an approach to prioritizing mutants by incrementally selecting mutants based on their predicted TCAP; and (4) presents simulations showing that TCAP-based prioritization of mutants advances test completeness more rapidly than prioritization with the previous state-of-the-art.",10.1145/3510003.3510187,mutation testing;mutant selection;mutant utility;test completeness advancement probability;TCAP;machine learning,Analytical models;Codes;Current measurement;Semantics;Focusing;Predictive models;Syntactics,program testing;set theory,concrete test goals;mutants advances test completeness;test set;mutation analysis;test completeness advancement probability;TCAP;mutation testing;test efficacy criterion;mutant subsets,6
756,Not Mentioned,PROMAL: Precise Window Transition Graphs for Android via Synergy of Program Analysis and Machine Learning,C. Liu; H. Wang; T. Liu; D. Gu; Y. Ma; H. Wang; X. Xiao,Case Western Reserve University; Case Western Reserve University; Monash University; Peking University; Peking University; Beijing University of Posts and Telecommunications; Case Western Reserve University,2022,"Mobile apps have been an integral part in our daily life. As these apps become more complex, it is critical to provide automated analysis techniques to ensure the correctness, security, and performance of these apps. A key component for these automated analysis techniques is to create a graphical user interface (GUI) model of an app, i.e., a window transition graph (WTG), that models windows and transitions among the windows. While existing work has provided both static and dynamic analysis to build the WTG for an app, the constructed WTG misses many transitions or contains many infeasible transitions due to the coverage issues of dynamic analysis and over-approximation of the static analysis. We propose ProMal, a â€œtribridâ€ analysis that synergistically combines static analysis, dynamic analysis, and machine learning to construct a precise WTG. Specifically, ProMal first applies static analysis to build a static WTG, and then applies dynamic analysis to verify the transitions in the static WTG. For the unverified transitions, ProMal further provides machine learning techniques that leverage runtime information (i.e., screenshots, UI layouts, and text information) to predict whether they are feasible transitions. Our evaluations on 40 real-world apps demonstrate the superiority of ProMal in building WTGs over static analysis, dynamic analysis, and machine learning techniques when they are applied separately.",10.1145/3510003.3510037,mobile apps;window transition graph;static analysis;deep learning,Analytical models;Runtime;Buildings;Static analysis;Machine learning;Windows;Performance analysis,Android (operating system);graphical user interfaces;learning (artificial intelligence);mobile computing;program diagnostics;system monitoring;user interfaces,PROMAL;precise window transition graphs;program analysis;mobile apps;automated analysis techniques;graphical user interface model;window transition graph;models windows;dynamic analysis;infeasible transitions;static analysis;ProMal;tribrid analysis;precise WTG;static WTG;unverified transitions;machine learning techniques;feasible transitions;real-world apps,1
757,Not Mentioned,PROPR: Property-Based Automatic Program Repair,M. P. Gissurarson; L. Applis; A. Panichella; A. van Deursen; D. Sands,"Chalmers University of Technology, Gothenburg, Sweden; TU Delft, Delft, Netherlands; TU Delft, Delft, Netherlands; TU Delft, Delft, Netherlands; Chalmers University of Technology, Gothenburg, Sweden",2022,"Automatic program repair (APR) regularly faces the challenge of overfitting patches - patches that pass the test suite, but do not actually address the problems when evaluated manually. Currently, overfit detection requires manual inspection or an oracle making quality control of APR an expensive task. With this work, we want to introduce properties in addition to unit tests for APR to address the problem of overfitting. To that end, we design and implement PROPR, a program repair tool for Haskell that leverages both property-based testing (via QuickCheck) and the rich type sys-tem and synthesis offered by the Haskell compiler. We compare the repair-ratio, time-to-first-patch and overfitting-ratio when using unit tests, property-based tests, and their combination. Our results show that properties lead to quicker results and have a lower overfit ratio than unit tests. The created overfit patches provide valuable insight into the underlying problems of the program to repair (e.g., in terms of fault localization or test quality). We consider this step towards fitter, or at least insightful, patches a critical contribution to bring APR into developer workflows.",10.1145/3510003.3510620,automatic program repair;search based software engineering;synthesis;property-based testing;typed holes,Location awareness;Program processors;Redundancy;Manuals;Maintenance engineering;Inspection;Software,automatic programming;functional languages;program testing;program verification;quality control;software maintenance;software quality,PROPR;property-based automatic program repair;test suite;overfit detection;manual inspection;oracle making quality control;program repair tool;property-based testing;rich type system;Haskell compiler;repair-ratio;time-to-first-patch;fault localization;overfitting-ratio;unit tests;QuickCheck;overfit patches,3
758,Not Mentioned,PUS: A Fast and Highly Efficient Solver for Inclusion-based Pointer Analysis,P. Liu; Y. Li; B. Swain; J. Huang,"Texas A&M University, College Station, USA; Texas A&M University, College Station, USA; Texas A&M University, College Station, USA; Texas A&M University, College Station, USA",2022,"A crucial performance bottleneck in most interprocedural static analyses is solving pointer analysis constraints. We present Pus, a highly efficient solver for inclusion-based pointer analysis. At the heart of Pus is a new constraint solving algorithm that signifi-cantly advances the state-of-the-art. Unlike the existing algorithms (i.e., wave and deep propagation) which construct a holistic constraint graph, at each stage Pus only considers partial constraints that causally affect the final fixed-point computation. In each iteration Pus extracts a small causality subgraph and it guarantees that only processing the causality subgraph is sufficient to reach the same global fixed point. Our extensive evaluation of Pus on a wide range of real-world large complex programs yields highly promising results. Pus is able to analyze millions of lines of code such as PostgreSQL in 10 minutes on a commodity laptop. On average, Pus is more than 7x faster in solving context-sensitive constraints, and more than 2x faster in solving context-insensitive constraints compared to the state of the art wave and deep propagation algorithms. Moreover, Pus has been used to find tens of previous unknown bugs in high-profile codebases including Linux, Redis, and Memcached.",10.1145/3510003.3510075,Static Analysis;Pointer Analysis;Causality Subgraph,Heart;Portable computers;Codes;Linux;Software algorithms;Computer bugs;Static analysis,computational complexity;constraint handling;graph theory;optimisation,inclusion-based pointer analysis;pointer analysis constraints;constraint solving algorithm;constraint graph;stage Pus;partial constraints;iteration Pus;causality subgraph;context-sensitive constraints;context-insensitive constraints;codebases;Linux;Redis;Memcached;PostgreSQL;fixed-point computation;deep propagation;wave propagation;static analyses,
759,Not Mentioned,Push-Button Synthesis of Watch Companions for Android Apps,C. Li; Y. Jiang; C. Xu,"Department of Computer Science and Technology, State Key Lab for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Lab for Novel Software Technology, Nanjing University, Nanjing, China; Department of Computer Science and Technology, State Key Lab for Novel Software Technology, Nanjing University, Nanjing, China",2022,"Most Android apps lack their counterparts on convenient smart-watch devices, possibly due to non-trivial engineering efforts required in the new app design and code development. Inspired by the observation that widgets on a smartphone can be mirrored to a smartwatch, this paper presents the Jigsaw framework to greatly alleviate such engineering efforts. Particularly, Jigsaw enables a push-button development of smartphone's companion watch apps by leveraging the programming by example paradigm, version space algebra, and constraint solving. Our experiments on 16 popular open-source apps validated the effectiveness of our synthesis algorithm, as well as their practical usefulness in synthesizing usable watch companions.",10.1145/3510003.3510056,Android apps;WearOS apps;program synthesis,Codes;Automation;Annotations;Algebra;Programming;Task analysis;Open source software,automatic programming;mobile computing;smart phones;wearable computers,Jigsaw framework;open-source apps;push-button synthesis;Android apps;smart-watch devices;app design;code development;widgets;smartphone companion watch apps;programming by example paradigm;version space algebra;constraint solving,1
760,Not Mentioned,Quantifying Permissiveness of Access Control Policies,W. Eiers; G. Sankaran; A. Li; E. O'Mahony; B. Prince; T. Bultan,"University of California Santa Barbara, Santa Barbara, CA, USA; University of California Santa Barbara, Santa Barbara, CA, USA; University of California Santa Barbara, Santa Barbara, CA, USA; University of California Santa Barbara, Santa Barbara, CA, USA; University of California Santa Barbara, Santa Barbara, CA, USA; University of California Santa Barbara, Santa Barbara, CA, USA",2022,"Due to ubiquitous use of software services, protecting the confidentiality of private information stored in compute clouds is becoming an increasingly critical problem. Although access control specification languages and libraries provide mechanisms for protecting confidentiality of information, without verification and validation techniques that can assist developers in writing policies, complex policy specifications are likely to have errors that can lead to unintended and unauthorized access to data, possibly with disastrous consequences. In this paper, we present a quantitative and differential policy analysis framework that not only identifies if one policy is more permissive than another policy, but also quantifies the relative permissiveness of access control policies. We quantify permissiveness of policies using a model counting constraint solver. We present a heuristic that transforms constraints extracted from access control policies and significantly improves the model counting performance. We demonstrate the effectiveness of our approach by applying it to policies written in Amazon's AWS Identity and Access Management (IAM) policy language and Microsoft's Azure policy language.",10.1145/3510003.3510233,Formal Methods;Access Control;Validation and Verification;Privacy,Access control;Statistical analysis;Soft sensors;Transforms;Writing;Maintenance engineering;Libraries,authorisation;specification languages,permissiveness;confidentiality;access control specification languages;libraries;writing policies;complex policy specifications;unintended access;unauthorized access;quantitative policy analysis framework;differential policy analysis framework;Microsoft Azure policy language;access management policy language,
761,Not Mentioned,R2Z2: Detecting Rendering Regressions in Web Browsers through Differential Fuzz Testing,S. Song; J. Hur; S. Kim; P. Rogers; B. Lee,"Seoul National University, Seoul, South Korea; Seoul National University, Seoul, South Korea; Seoul National University, Seoul, South Korea; Google, Mountain View, CA, United States; Seoul National University, Seoul, South Korea",2022,"A rendering regression is a bug introduced by a web browser where a web page no longer functions as users expect. Such rendering bugs critically harm the usability of web browsers as well as web applications. The unique aspect of rendering bugs is that they affect the presented visual appearance of web pages, but those web pages have no pre-defined correct appearance. Therefore, it is challenging to automatically detect errors in their appearance. In practice, web browser vendors rely on non-trivial and time-prohibitive manual analysis to detect and handle rendering regressions. This paper proposes R2Z2, an automated tool to find rendering regressions. R2Z2 uses the differential fuzz testing approach, which repeatedly compares the rendering results of two different versions of a browser while providing the same HTML as input. If the rendering results are different, R2Z2 further performs cross browser compatibility testing to check if the rendering difference is indeed a rendering regression. After identifying a rendering regression, R2Z2 will perform an in-depth analysis to aid in fixing the regression. Specifically, R2Z2 performs a delta-debugging-like analysis to pinpoint the exact browser source code commit causing the regression, as well as inspecting the rendering pipeline stages to pinpoint which pipeline stage is responsible. We implemented a prototype of R2Z2 particularly targeting the Chrome browser. So far, R2Z2 found 11 previously undiscovered rendering regressions in Chrome, all of which were confirmed by the Chrome developers. Importantly, in each case, R2Z2 correctly reported the culprit commit. Moreover, R2Z2 correctly pin-pointed the culprit rendering pipeline stage in all but one case.",10.1145/3510003.3510044,rendering regression;web-browser;differential testing,Visualization;Computer bugs;Pipelines;Web pages;Prototypes;Fuzzing;Rendering (computer graphics),human computer interaction;Internet;online front-ends;program debugging;program testing;rendering (computer graphics);security of data,R2Z2;rendering regression;web browser;web page;rendering bugs;rendering results,2
762,Not Mentioned,Recommending Good First Issues in GitHub OSS Projects,W. Xiao; H. He; W. Xu; X. Tan; J. Dong; M. Zhou,"Key Laboratory of High Confidence Software Technologies, Ministry of Education, School of Computer Science, Peking University, Beijing, China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, School of Computer Science, Peking University, Beijing, China; School of Computer Science and Technology, Soochow University, Suzhou, China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, School of Computer Science, Peking University, Beijing, China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, School of Computer Science, Peking University, Beijing, China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, School of Computer Science, Peking University, Beijing, China",2022,"Attracting and retaining newcomers is vital for the sustainability of an open-source software project. However, it is difficult for new-comers to locate suitable development tasks, while existing â€œGood First Issuesâ€ (GFI) in GitHub are often insufficient and inappropriate. In this paper, we propose RECGFI, an effective practical approach for the recommendation of good first issues to newcomers, which can be used to relieve maintainers' burden and help newcomers onboard. RECGFI models an issue with features from multiple dimensions (content, background, and dynamics) and uses an XGBoost classifier to generate its probability of being a GFI. To evaluate RECGFI, we collect 53,510 resolved issues among 100 GitHub projects and care-fully restore their historical states to build ground truth datasets. Our evaluation shows that RECGFI can achieve up to 0.853 AUC in the ground truth dataset and outperforms alternative models. Our interpretable analysis of the trained model further reveals in-teresting observations about GFI characteristics. Finally, we report latest issues (without GFI-signaling labels but recommended as GFI by our approach) to project maintainers among which 16 are confirmed as real GFIs and five have been resolved by a newcomer.",10.1145/3510003.3510196,open-source software;onboarding;good first issues,Bot (Internet);Analytical models;Machine learning;Task analysis;Sustainable development;Open source software;Software development management,feature extraction;learning (artificial intelligence);medical image processing;project management;public domain software;social aspects of automation;software development management;software engineering,GitHub OSS projects;attracting;retaining newcomers;sustainability;open-source software project;new-comers;suitable development tasks;effective practical approach;newcomer;maintainers;RECGFI models;multiple dimensions;XGBoost classifier;100 GitHub projects;historical states;ground truth dataset;trained model;GFI characteristics;latest issues;GFI-signaling labels,4
763,Not Mentioned,REFTY: Refinement Types for Valid Deep Learning Models,Y. Gao; Z. Li; H. Lin; H. Zhang; M. Wu; M. Yang,"Microsoft Research, China; Microsoft Research, China; Microsoft Research, China; The University of Newcastle, Australia; Shanghai Tree-Graph Blockchain Research Institute, China; Microsoft Research, China",2022,"Deep learning has been increasingly adopted in many application areas. To construct valid deep learning models, developers must conform to certain computational constraints by carefully selecting appropriate neural architectures and hyperparameter values. For example, the kernel size hyperparameter of the 2D convolution operator cannot be overlarge to ensure that the height and width of the output tensor remain positive. Because model construction is largely manual and lacks necessary tooling support, it is possible to violate those constraints and raise type errors of deep learning models, causing either runtime exceptions or wrong output results. In this paper, we propose Refty, a refinement type-based tool for statically checking the validity of deep learning models ahead of job execution. Refty refines each type of deep learning operator with framework-independent logical formulae that describe the computational constraints on both tensors and hyperparameters. Given the neural architecture and hyperparameter domains of a model, Refty visits every operator, generates a set of constraints that the model should satisfy, and utilizes an SMT solver for solving the constraints. We have evaluated Refty on both individual operators and representative real-world models with various hyperparameter values under PyTorch and TensorFlow. We also compare it with an existing shape-checking tool. The experimental results show that Refty finds all the type errors and achieves 100% Precision and Recall, demonstrating its effectiveness.",10.1145/3510003.3510077,deep learning;validity checking;type error;refinement type,Deep learning;Tensors;Runtime;Convolution;Computational modeling;Computer architecture;Manuals,deep learning (artificial intelligence);formal logic;neural net architecture,hyperparameter domains;deep learning;kernel size hyperparameter;2D convolution operator;refinement type-based tool;neural architecture;Refty;framework-independent logical formulae;PyTorch;TensorFlow;SMT solver,
764,Not Mentioned,ReMoS: Reducing Defect Inheritance in Transfer Learning via Relevant Model Slicing,Z. Zhang; Y. Li; J. Wang; B. Liu; D. Li; Y. Guo; X. Chen; Y. Liu,"Key Laboratory of High-Confidence Software Technologies (MOE), School of Computer Science, Peking University, Beijing, China; Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China; Microsoft Research, Beijing, China; Key Laboratory of High-Confidence Software Technologies (MOE), School of Computer Science, Peking University, Beijing, China; Key Laboratory of High-Confidence Software Technologies (MOE), School of Computer Science, Peking University, Beijing, China; Key Laboratory of High-Confidence Software Technologies (MOE), School of Computer Science, Peking University, Beijing, China; Key Laboratory of High-Confidence Software Technologies (MOE), School of Computer Science, Peking University, Beijing, China; Key Laboratory of High-Confidence Software Technologies (MOE), School of Computer Science, Peking University, Beijing, China",2022,"Transfer learning is a popular software reuse technique in the deep learning community that enables developers to build custom mod-els (students) based on sophisticated pretrained models (teachers). However, like vulnerability inheritance in traditional software reuse, some defects in the teacher model may also be inherited by students, such as well-known adversarial vulnerabilities and backdoors. Re-ducing such defects is challenging since the student is unaware of how the teacher is trained and/or attacked. In this paper, we propose ReMoS, a relevant model slicing technique to reduce defect inheri-tance during transfer learning while retaining useful knowledge from the teacher model. Specifically, ReMoS computes a model slice (a subset of model weights) that is relevant to the student task based on the neuron coverage information obtained by profiling the teacher model on the student task. Only the relevant slice is used to fine-tune the student model, while the irrelevant weights are retrained from scratch to minimize the risk of inheriting defects. Our experi-ments on seven DNN defects, four DNN models, and eight datasets demonstrate that ReMoS can reduce inherited defects effectively (by 63% to 86% for CV tasks and by 40% to 61 % for NLP tasks) and efficiently with minimal sacrifice of accuracy (3% on average).",10.1145/3510003.3510191,Program slicing;deep neural networks;relevant slicing,Deep learning;Computational modeling;Transfer learning;Neurons;Task analysis;Software reusability;Biological neural networks,data mining;learning (artificial intelligence);natural language processing;neural nets;program slicing;software reusability,inheriting defects;seven DNN defects;DNN models;ReMoS;defect inheritance;transfer learning;popular software reuse technique;deep learning community;custom models;sophisticated pretrained models;vulnerability inheritance;traditional software reuse;teacher model;well-known adversarial vulnerabilities;backdoors;relevant model;model slice;model weights;student task;relevant slice;student model,
765,Not Mentioned,Repairing Brain-Computer Interfaces with Fault-Based Data Acquisition,C. Winston; C. Winston; C. N. Winston; C. Winston; C. Winston; R. P. N. Rao; R. Just,"University of Washington, Seattle, Washington, USA; University of Washington, Seattle, Washington, USA; University of Washington, Seattle, Washington, USA; University of Washington, Seattle, Washington, USA; University of Washington, Seattle, Washington, USA; University of Washington, Seattle, Washington, USA; University of Washington, Seattle, Washington, USA",2022,"Brain-computer interfaces (BCls) decode recorded neural signals from the brain and/or stimulate the brain with encoded neural sig-nals. BCls span both hardware and software and have a wide range of applications in restorative medicine, from restoring movement through prostheses and robotic limbs to restoring sensation and communication through spellers. BCls also have applications in di-agnostic medicine, e.g., providing clinicians with data for detecting seizures, sleep patterns, or emotions. Despite their promise, BCls have not yet been adopted for long-term, day-to-day use because of challenges related to reliability and robustness, which are needed for safe operation in all scenarios. Ensuring safe operation currently requires hours of manual data collection and recalibration, involving both patients and clinicians. However, data collection is not targeted at eliminating specific faults in a BCI. This paper presents a new methodology for char-acterizing, detecting, and localizing faults in BCls. Specifically, it proposes partial test oracles as a method for detecting faults and slice functions as a method for localizing faults to characteristic patterns in the input data or relevant tasks performed by the user. Through targeted data acquisition and retraining, the proposed methodology improves the correctness of BCls. We evaluated the proposed methodology on five BCl applications. The results show that the proposed methodology (1) precisely localizes faults and (2) can significantly reduce the frequency of faults through retraining based on targeted, fault-based data acquisition. These results sug-gest that the proposed methodology is a promising step towards repairing faulty BCls.",10.1145/3510003.3512764,Brain-computer interface;neural decoding;partial test oracles;fault localization,Software testing;Location awareness;Data acquisition;Data collection;Robot sensing systems;Brain-computer interfaces;Software,bioelectric potentials;brain-computer interfaces;data acquisition;electroencephalography;medical disorders;medical signal detection;medical signal processing;neurophysiology,brain-computer interfaces;neural signals;restorative medicine;faulty BCl;fault-based data acquisition;seizures;sleep patterns;emotions,
766,Not Mentioned,Repairing Order-Dependent Flaky Tests via Test Generation,C. Li; C. Zhu; W. Wang; A. Shi,"The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA; The University of Texas at Austin, Austin, TX, USA",2022,"Flaky tests are tests that pass or fail nondeterministically on the same version of code. These tests can mislead developers concerning the quality of their code changes during regression testing. A common kind of flaky tests are order-dependent tests, whose pass/ fail outcomes depend on the test order in which they are run. Such tests have different outcomes because other tests running before them pollute shared state. Prior work has proposed repairing order-dependent tests by searching for existing tests, known as â€œcleanersâ€, that reset the shared state, allowing the order-dependent test to pass when run after a polluted shared state. The code within a cleaner represents a patch to repair the order-dependent test. However, this technique requires cleaners to already exist in the test suite. We propose ODRepair, an automated technique to repair order-dependent tests even without existing cleaners. The idea is to first determine the exact polluted shared state that results in the order-dependent test to fail and then generate code that can modify and reset the shared state so that the order-dependent test can pass. We focus on shared state through internal heap memory, in particular shared state reachable from static fields. Once we know which static field leads to the pollution, we search for reset-methods in the code-base that can potentially access and modify state reachable from that static field. We then apply an automatic test-generation tool to generate method-call sequences, targeting these reset-methods. Our evaluation on 327 order-dependent tests from a publicly available dataset shows that ODRepair automatically identifies the polluted static field for 181 tests, and it can generate patches for 141 of these tests. Compared against state-of-the-art iFixFlakies, ODRepair can generate patches for 24 tests that iFixFlakies cannot.",10.1145/3510003.3510173,flaky test;order-dependent test;test generation;automated repair,Codes;Pollution;Maintenance engineering;Test pattern generators;Testing;Software engineering,automatic test pattern generation;automatic test software;program testing;regression analysis;statistical testing,order-dependent flaky tests;shared state;code changes;automatic test generation;regression testing;ODRepair;internal heap memory;method-call sequences;reset-methods;iFixFlakies,4
767,Not Mentioned,Retrieving Data Constraint Implementations Using Fine-Grained Code Patterns,J. M. Florez; J. Perry; S. Wei; A. Marcus,"The University of Texas at Dallas, Richardson, Texas, USA; The University of Texas at Dallas, Richardson, Texas, USA; The University of Texas at Dallas, Richardson, Texas, USA; The University of Texas at Dallas, Richardson, Texas, USA",2022,"Business rules are an important part of the requirements of software systems that are meant to support an organization. These rules describe the operations, definitions, and constraints that apply to the organization. Within the software system, business rules are often translated into constraints on the values that are required or allowed for data, called data constraints. Business rules are subject to frequent changes, which in turn require changes to the corre-sponding data constraints in the software. The ability to efficiently and precisely identify where data constraints are implemented in the source code is essential for performing such necessary changes. In this paper, we introduce Lasso, the first technique that automatically retrieves the method and line of code where a given data constraint is enforced. Lasso is based on traceability link recovery approaches and leverages results from recent research that identified line-of-code level implementation patterns for data constraints. We implement three versions of Lasso that can retrieve data constraint implementations when they are implemented with any one of 13 frequently occurring patterns. We evaluate the three versions on a set of 299 data constraints from 15 real-world Java systems, and find that they improve method-level link recovery by 30%,70%, and 163%, in terms of true positives within the first 10 results, compared to their text-retrieval-based baseline. More importantly, the Lasso variants correctly identify the line of code implementing the constraint inside the methods for 68% of the 299 constraints.",10.1145/3510003.3510167,business rule;data constraint;traceability link recovery;empirical study;fine-grained traceability;code pattern,Java;Codes;Organizations;Software systems;Pattern matching;Software engineering,data mining;information retrieval;Java;software maintenance;text analysis,Lasso;data constraint implementations;299 data constraints;fine-grained code patterns;business rules;software system;called data constraints;corre-sponding data constraints;given data constraint;identified line-of-code level implementation patterns,1
768,Not Mentioned,RoPGen: Towards Robust Code Authorship Attribution via Automatic Coding Style Transformation,Z. Li; G. Q. Chen; C. Chen; Y. Zou; S. Xu,"University of Texas, San Antonio, USA; University of Texas, San Antonio, USA; Center for Research in Computer Vision, University of Central Florida, USA; Northeastern University, China; University of Colorado Colorado Springs, USA",2022,"Source code authorship attribution is an important problem often encountered in applications such as software forensics, bug fixing, and software quality analysis. Recent studies show that current source code authorship attribution methods can be compromised by attackers exploiting adversarial examples and coding style ma-nipulation. This calls for robust solutions to the problem of code authorship attribution. In this paper, we initiate the study on making Deep Learning (DL)-based code authorship attribution robust. We propose an innovative framework called Robust coding style Patterns Generation (RoPGen), which essentially learns authors' unique coding style patterns that are hard for attackers to manip-ulate or imitate. The key idea is to combine data augmentation and gradient augmentation at the adversarial training phase. This effectively increases the diversity of training examples, generates meaningful perturbations to gradients of deep neural networks, and learns diversified representations of coding styles. We evaluate the effectiveness of RoPGen using four datasets of programs written in C, C++, and Java. Experimental results show that RoPGen can significantly improve the robustness of DL-based code authorship attribution, by respectively reducing 22.8% and 41.0% of the success rate of targeted and untargeted attacks on average.",10.1145/3510003.3510181,Authorship attribution;source code;coding style;robustness;deep learning,Training;Deep learning;Java;Codes;Perturbation methods;Neural networks;Software quality,Java;learning (artificial intelligence);neural nets;software quality;source code (software),adversarial examples;coding style ma-nipulation;robust solutions;Deep Learning-based code authorship;Robust coding style Patterns Generation;RoPGen;authors;data augmentation;gradient augmentation;adversarial training phase;coding styles;DL-based code authorship attribution;towards Robust code authorship attribution;automatic coding style transformation;source code authorship attribution;software forensics;software quality analysis;current source code authorship,1
769,Not Mentioned,Rotten Apples Spoil the Bunch: An Anatomy of Google Play Malware,M. Cao; K. Ahmed; J. Rubin,"Univ. of British Columbia, Canada; Univ. of British Columbia, Canada; Univ. of British Columbia, Canada",2022,"This paper provides an in-depth analysis of Android malware that bypassed the strictest defenses of the Google Play application store and penetrated the official Android market between January 2016 and July 2021. We systematically identified 1,238 such malicious applications, grouped them into 134 families, and manually analyzed one application from 105 distinct families. During our manual analysis, we identified malicious payloads the applications execute, conditions guarding execution of the payloads, hiding techniques applications employ to evade detection by the user, and other implementation-level properties relevant for automated malware detection. As most applications in our dataset contain multiple payloads, each triggered via its own complex activation logic, we also contribute a graph-based representation showing activation paths for all application payloads in form of a control- and data-flow graph. Furthermore, we discuss the capabilities of existing malware detection tools, put them in context of the properties observed in the analyzed malware, and identify gaps and future research directions. We believe that our detailed analysis of the recent, evasive malware will be of interest to researchers and practitioners and will help further improve malware detection tools.",10.1145/3510003.3510161,Android;malware;dataset;malware detection;manual analysis,Codes;Manuals;Malware;Internet;Behavioral sciences;Payloads;Software engineering,Android (operating system);graph theory;invasive software,rotten apples;google play malware;Android malware;official Android market;malicious payloads;automated malware detection;graph-based representation;data-flow graph,1
770,Not Mentioned,SAPIENTML: Synthesizing Machine Learning Pipelines by Learning from Human-Written Solutions,R. K. Saha; A. Ura; S. Mahajan; C. Zhu; L. Li; Y. Hu; H. Yoshida; S. Khurshid; M. R. Prasad,"Fujitsu Research of America, Inc.; Fujitsu Ltd.; Fujitsu Research of America, Inc.; The University of Texas at Austin; University of Illinois at Urbana-Champaign; The University of Texas at Austin; Fujitsu Research of America, Inc.; The University of Texas at Austin; Fujitsu Research of America, Inc.",2022,"Automatic machine learning, or AutoML, holds the promise of truly democratizing the use of machine learning (ML), by substantially automating the work of data scientists. However, the huge combinatorial search space of candidate pipelines means that current AutoML techniques, generate sub-optimal pipelines, or none at all, especially on large, complex datasets. In this work we propose an AutoML technique SapientML, that can learn from a corpus of existing datasets and their human-written pipelines, and efficiently generate a high-quality pipeline for a predictive task on a new dataset. To combat the search space explosion of AutoML, SapientML employs a novel divide-and-conquer strategy realized as a three-stage program synthesis approach, that reasons on successively smaller search spaces. The first stage uses meta-learning to predict a set of plausible ML components to constitute a pipeline. In the second stage, this is then refined into a small pool of viable concrete pipelines using a pipeline dataflow model derived from the corpus. Dynamically evaluating these few pipelines, in the third stage, provides the best solution. We instantiate SapientML as part of a fully automated tool-chain that creates a cleaned, labeled learning corpus by mining Kaggle, learns from it, and uses the learned models to then synthesize pipelines for new predictive tasks. We have created a training corpus of 1,094 pipelines spanning 170 datasets, and evaluated SapientML on a set of 41 benchmark datasets, including 10 new, large, real-world datasets from Kaggle, and against 3 state-of-the-art AutoML tools and 4 baselines. Our evaluation shows that SapientML produces the best or comparable accuracy on 27 of the benchmarks while the second best tool fails to even produce a pipeline on 9 of the instances. This difference is amplified on the 10 most challenging benchmarks, where SapientML wins on 9 instances with the other tools failing to produce pipelines on 4 or more benchmarks.",10.1145/3510003.3510226,Program Synthesis;Machine Learning;AutoML;Program Analysis,Training;Pipelines;Machine learning;Benchmark testing;Predictive models;Explosions;Data mining,data mining;divide and conquer methods;learning (artificial intelligence);pipeline processing;search problems,human-written solutions;automatic machine learning;huge combinatorial search space;AutoML techniques;sub-optimal pipelines;complex datasets;human-written pipelines;high-quality pipeline;search space explosion;three-stage program synthesis approach;meta-learning;viable concrete pipelines;pipeline dataflow model;AutoML tools;machine learning pipeline synthesis;divide-and-conquer strategy;Kaggle mining;automated tool-chain;SapientML,1
771,Not Mentioned,Search-based Diverse Sampling from Real-world Software Product Lines,Y. Xiang; H. Huang; Y. Zhou; S. Li; C. Luo; Q. Lin; M. Li; X. Yang,"South China University of Technology, Guangzhou, China; South China University of Technology, Guangzhou, China; School of Computer Science and Engineering, Sun Yat-Sen University, Guangzhou, China; South China University of Technology, Guangzhou, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; University of Birmingham, Birmingham, UK; South China University of Technology, Guangzhou, China",2022,"Real-world software product lines (SPLs) often encompass enormous valid configurations that are impossible to enumerate. To understand properties of the space formed by all valid configurations, a feasible way is to select a small and valid sample set. Even though a number of sampling strategies have been proposed, they either fail to produce diverse samples with respect to the number of selected features (an important property to characterize behaviors of configurations), or achieve diverse sampling but with limited scalability (the handleable configuration space size is limited to 1013). To resolve this dilemma, we propose a scalable diverse sampling strategy, which uses a distance metric in combination with the novelty search algorithm to produce diverse samples in an incremental way. The distance metric is carefully designed to measure similarities between configurations, and further diversity of a sample set. The novelty search incrementally improves diversity of samples through the search for novel configurations. We evaluate our sampling algorithm on 39 real-world SPLs. It is able to generate the required number of samples for all the SPLs, including those which cannot be counted by sharpSAT, a state-of-the-art model counting solver. Moreover, it performs better than or at least competitively to state-of-the-art samplers regarding diversity of the sample set. Experimental results suggest that only the proposed sampler (among all the tested ones) achieves scalable diverse sampling.",10.1145/3510003.3510053,Software product lines;diverse sampling;novelty search;distance metric,Measurement;Scalability;Software algorithms;Search problems;Prediction algorithms;Software;Behavioral sciences,sampling methods;search problems;software product lines,real-world software product lines;small sample set;valid sample set;handleable configuration space size;scalable diverse sampling strategy;search-based diverse sampling;distance metric;sharpSAT,
772,Not Mentioned,Semantic Image Fuzzing of AI Perception Systems,T. Woodlief; S. Elbaum; K. Sullivan,"University of Virginia, Charlottesville, Virginia, USA; University of Virginia, Charlottesville, Virginia, USA; University of Virginia, Charlottesville, Virginia, USA",2022,"Perception systems enable autonomous systems to interpret raw sensor readings of the physical world. Testing of perception systems aims to reveal misinterpretations that could cause system failures. Current testing methods, however, are inadequate. The cost of human interpretation and annotation of real-world input data is high, so manual test suites tend to be small. The simulation-reality gap reduces the validity of test results based on simulated worlds. And methods for synthesizing test inputs do not provide corresponding expected interpretations. To address these limitations, we developed semSensFuzz, a new approach to fuzz testing of perception systems based on semantic mutation of test cases that pair realworld sensor readings with their ground-truth interpretations. We implemented our approach to assess its feasibility and potential to improve software testing for perception systems. We used it to generate 150,000 semantically mutated image inputs for five state-of-the-art perception systems. We found that it synthesized tests with novel and subjectively realistic image inputs, and that it discovered inputs that revealed significant inconsistencies between the specified and computed interpretations. We also found that it produced such test cases at a cost that was very low compared to that of manual semantic annotation of real-world images.",10.1145/3510003.3510212,semantic fuzzing;autonomous systems;perception,Costs;Annotations;Autonomous systems;Semantics;Manuals;Fuzzing;Artificial intelligence,artificial intelligence;image processing;program testing,software testing;semantic image fuzzing;AI perception systems;system failures;testing methods;human interpretation;fuzz testing;semSensFuzz approach,2
773,Not Mentioned,ShellFusion: Answer Generation for Shell Programming Tasks via Knowledge Fusion,N. Zhang; C. Liu; X. Xia; C. Treude; Y. Zou; D. Lo; Z. Zheng,"School of Software Engineering, Sun Yat-sen University, China; School of Big Data and Software Engineering, Chongqing University, China; Software Engineering Application Technology Lab, Huawei, China; School of Computing and Information Systems, University of Melbourne, Australia; Department of Electrical and Computer Engineering, Queen's University, Canada; School of Information Systems, Singapore Management University, Singapore; School of Software Engineering, Sun Yat-sen University, China",2022,"Shell commands are widely used for accomplishing tasks, such as network management and file manipulation, in Unix and Linux platforms. There are a large number of shell commands available. For example, 50,000+ commands are documented in the Ubuntu Manual Pages (MPs). Quite often, programmers feel frustrated when searching and orchestrating appropriate shell commands to accomplish specific tasks. To address the challenge, the shell programming community calls for easy-to-use tutorials for shell commands. However, existing tutorials (e.g., TLDR) only cover a limited number of frequently used commands for shell beginners and provide limited support for users to search for commands by a task. We propose an approach, i.e., ShellFusion, to automatically generate comprehensive answers (including relevant shell commands, scripts, and explanations) for shell programming tasks. Our approach integrates knowledge mined from Q&A posts in Stack Exchange, Ubuntu MPs, and TLDR tutorials. For a query that describes a shell programming task, ShellFusion recommends a list of relevant shell commands. Specifically, ShellFusion retrieves the top-n Q&A posts with questions similar to the query and detects shell commands with options (e.g., ls -t) from the accepted answers of the retrieved posts. Next, ShellFusion filters out irrelevant commands with descriptions in MP and TLDR that share little semantics with the query, and further ranks the candidate commands based on their similarities with the query and the retrieved posts. To help users understand how to achieve the task using a recommended command, ShellFusion generates a comprehensive answer for each command by synthesizing knowledge from Q&A posts, MPs, and TLDR. Our evaluation of 434 shell programming tasks shows that ShellFusion significantly outperforms Magnum (the state-of-the-art natural language-to-Bash command approach) by at least 179.6% in terms of MRR@K and MAP@K. A user study conducted with 20 shell programmers further shows that ShellFusion can help users address programming tasks more efficiently and accurately, compared with Magnum and DeepAns (a recent answer recommendation baseline).",10.1145/3510003.3510131,Shell Programming;Answer Generation;Knowledge Fusion,Knowledge engineering;Linux;Semantics;Tutorials;Manuals;Programming;Task analysis,data mining;information retrieval;Linux;object-oriented programming;query processing;Web sites,ShellFusion;Q&A posts;shell commands;TLDR;language-to-Bash command approach;shell programming task;Linux platform;Unix platform;Stack Exchange,1
774,Not Mentioned,SnR: Constraint-Based Type Inference for Incomplete Java Code Snippets,Y. Dong; T. Gu; Y. Tian; C. Sun,"David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada; Alibaba Group, China; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada; David R. Cheriton School of Computer Science, University of Waterloo, Waterloo, Ontario, Canada",2022,"Code snippets are prevalent on websites such as Stack Overflow and are effective in demonstrating API usages concisely. However they are usually difficult to be used directly because most code snippets not only are syntactically incomplete but also lack dependency information, and thus do not compile. For example, Java snippets usually do not have import statements or required library names; only 6.88% of Java snippets on Stack Overflow include import statements necessary for compilation. This paper proposes SnR, a precise, efficient, constraint-based technique to automatically infer the exact types used in code snippets and the libraries containing the inferred types, to compile and therefore reuse the code snippets. Initially, SnR builds a knowledge base of APIs, i.e., various facts about the available APIs, from a corpus of Java libraries. Given a code snippet with missing import statements, SnR automatically extracts typing constraints from the snippet, solves the constraints against the knowledge base, and returns a set of APIs that satisfies the constraints to be imported into the snippet. We have evaluated SnR on a benchmark of 267 code snippets from Stack Overflow. SnR significantly outperforms the state-of-the-art tool Coster. SnR correctly infers 91.0% of the import statements, which makes 73.8% of the snippets compile, compared to 36.0% of the import statements and 9.0% of the snippets by Coster.",10.1145/3510003.3510061,type inference;constraint satisfaction;automated repair;datalog,Productivity;Java;Codes;Knowledge based systems;Transforms;Benchmark testing;Maintenance engineering,application program interfaces;Java;software libraries;type theory,SnR;incomplete Java code snippets;code snippet;Stack Overflow;Java snippets;import statements;precise based technique;efficient based technique;constraint-based technique;267 code snippets;snippets compile,
775,Not Mentioned,Social Science Theories in Software Engineering Research,T. Lorey; P. Ralph; M. Felderer,"Department of Computer Science, University of Innsbruck, Innsbruck, Austria; Faculty of Computer Science, Dalhousie University, Halifax, Canada; Department of Computer Science, University of Innsbruck, Innsbruck, Austria",2022,"As software engineering research becomes more concerned with the psychological, sociological and managerial aspects of software development, relevant theories from reference disciplines are in-creasingly important for understanding the field's core phenomena of interest. However, the degree to which software engineering research draws on relevant social sciences remains unclear. This study therefore investigates the use of social science theories in five influential software engineering journals over 13 years. It analyzes not only the extent of theory use but also what, how and where these theories are used. While 87 different theories are used, less than two percent of papers use a social science theory, most theories are used in only one paper, most social sciences are ignored, and the theories are rarely tested for applicability to software engineering contexts. Ignoring relevant social science theories may (1) under-mine the community's ability to generate, elaborate and maintain a cumulative body of knowledge; and (2) lead to oversimplified mod-els of software engineering phenomena. More attention to theory is needed for software engineering to mature as a scientific discipline.",10.1145/3510003.3510076,software engineering;theory;social science,Knowledge engineering;Analytical models;Software design;Social sciences;Psychology;Predictive models;Software,research and development;social aspects of automation;software engineering,software engineering research;software development;influential software engineering journals;software engineering contexts;social science theories;software engineering phenomena,1
776,Not Mentioned,SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source Code Representations,C. Niu; C. Li; V. Ng; J. Ge; L. Huang; B. Luo,"State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Human Language Technology Research Institute University of Texas at Dallas, Richardson, Texas, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Dept. of Computer Science, Southern Methodist University, Dallas, Texas, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",2022,"Recent years have seen the successful application of large pretrained models to code representation learning, resulting in substantial improvements on many code-related downstream tasks. But there are issues surrounding their application to SE tasks. First, the majority of the pre-trained models focus on pre-training only the encoder of the Transformer. For generation tasks that are addressed using models with the encoder-decoder architecture, however, there is no reason why the decoder should be left out during pre-training. Second, many existing pre-trained models, including state-of-the-art models such as T5-learning, simply reuse the pretraining tasks designed for natural languages. Moreover, to learn the natural language description of source code needed eventually for code-related tasks such as code summarization, existing pretraining tasks require a bilingual corpus composed of source code and the associated natural language description, which severely limits the amount of data for pre-training. To this end, we propose SPT-Code, a sequence-to-sequence pre-trained model for source code. In order to pre-train SPT-Code in a sequence-to-sequence manner and address the aforementioned weaknesses associated with existing pre-training tasks, we introduce three pre-training tasks that are specifically designed to enable SPT-Code to learn knowledge of source code, the corresponding code structure, as well as a natural language description of the code without relying on any bilingual corpus, and eventually exploit these three sources of information when it is applied to downstream tasks. Experimental results demonstrate that SPT-Code achieves state-of-the-art performance on five code-related downstream tasks after fine-tuning.",10.1145/3510003.3510096,pre-training;code representation learning;sequence-to-sequence,Representation learning;Codes;Natural languages;Computer architecture;Transformers;Decoding;Task analysis,decoding;image representation;learning (artificial intelligence);natural language processing;natural languages;text analysis,SE tasks;generation tasks;existing pre-trained models;pretraining tasks;code-related tasks;code summarization;associated natural language description;SPT-Code;sequence-to-sequence pre-trained model;sequence-to-sequence manner;existing pre-training tasks;corresponding code structure;code-related downstream tasks;sequence-to-sequence pre-training;source Code representations;pretrained models;code representation learning,11
777,Not Mentioned,Static Inference Meets Deep learning: A Hybrid Type Inference Approach for Python,Y. Peng; C. Gao; Z. Li; B. Gao; D. Lo; Q. Zhang; M. Lyu,"The Chinese University of Hong Kong, Hong Kong, China; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Harbin Institute of Technology, Shenzhen, China; Singapore Management University, Singapore; Georgia Institute of Technology, United States; The Chinese University of Hong Kong, Hong Kong, China",2022,"Type inference for dynamic programming languages such as Python is an important yet challenging task. Static type inference techniques can precisely infer variables with enough static constraints but are unable to handle variables with dynamic features. Deep learning (DL) based approaches are feature-agnostic, but they can-not guarantee the correctness of the predicted types. Their per-formance significantly depends on the quality of the training data (i.e., DL models perform poorly on some common types that rarely appear in the training dataset). It is interesting to note that the static and DL-based approaches offer complementary benefits. Un-fortunately, to our knowledge, precise type inference based on both static inference and neural predictions has not been exploited and remains an open challenge. In particular, it is hard to integrate DL models into the framework of rule-based static approaches. This paper fills the gap and proposes a hybrid type inference approach named Hityper based on both static inference and deep learning. Specifically, our key insight is to record type dependen-cies among variables in each function and encode the dependency information in type dependency graphs (TDGs). Based on TDGs, we can easily integrate type inference rules in the nodes to conduct static inference and type rejection rules to inspect the correctness of neural predictions. Hityper iteratively conducts static inference and DL-based prediction until the TDG is fully inferred. Experi-ments on two benchmark datasets show that Hityper outperforms state-of-the-art DL models by exactly matching 10% more human annotations. Hityper also achieves an increase of more than 30% on inferring rare types. Considering only the static part of Hityper, it infers 2Ã— ~3Ã— more types than existing static type inference tools. Moreover, Hityper successfully corrected seven wrong human an-notations in six GitHub projects, and two of them have already been approved by the repository owners.",10.1145/3510003.3510038,Type Inference;AI For SE;Static Analysis,Deep learning;Training;Training data;Static analysis;Predictive models;Dynamic programming;Task analysis,dynamic programming;graph theory;inference mechanisms;learning (artificial intelligence);reasoning about programs;type theory,Hityper;static inference meets deep learning;hybrid type inference approach;static type inference techniques;static constraints;deep learning based approaches;predicted types;common types;static DL-based approaches;precise type inference;rule-based static approaches;type dependen-cies;type dependency graphs;type inference rules;type rejection rules;rare types;static part;static type inference tools,1
778,Not Mentioned,Static Stack-Preserving Intra-Procedural Slicing of WebAssembly Binaries,Q. StiÃ©venart; D. W. Binkley; C. De Roover,"Vrije Universiteit Brussel, Brussels, Belgium; Loyola University Maryland, Baltimore, MD, USA; Vrije Universiteit Brussel, Brussels, Belgium",2022,"The recently introduced WebAssembly standard aims to be a portable compilation target, enabling the cross-platform distribution of pro-grams written in a variety of languages. We propose an approach to slice WebAssembly programs in order to enable applications in reverse engineering, code comprehension, and security among others. Given a program and a location in that program, program slicing produces a minimal version of the program that preserves the behavior at the given location. Specifically, our approach is a static, intra-procedural, backward slicing approach that takes into account WebAssembly-specific dependences to identify the instructions of the slice. To do so it must correctly overcome the considerable challenges of performing dependence analysis at the bi-nary level. Furthermore, for the slice to be executable, the approach needs to ensure that the stack behavior of its output complies with WebAssembly's validation requirements. We implemented and eval-uated our approach on a suite of 8 386 real-world WebAssembly binaries, finding that the average size of the 495 204 868 slices computed is 53% of the original code, an improvement over the 60% attained by related work slicing ARM binaries. To gain a more qual-itative understanding of the slices produced by our approach, we compared them to 1 956 source-level slices of benchmark C pro-grams. This inspection helps to illustrate the slicer's strengths and to uncover potential future improvements.",10.1145/3510003.3510070,Static program slicing;WebAssembly;Binary analysis,Codes;Reverse engineering;Inspection;Benchmark testing;Behavioral sciences;Security;Standards,computerised tomography;program compilers;program debugging;program slicing;reverse engineering,account WebAssembly-specific dependences;dependence analysis;bi-nary level;stack behavior;WebAssembly's validation requirements;real-world WebAssembly binaries;related work slicing ARM binaries;1 956 source-level slices;pro-grams;static stack-preserving intra-procedural slicing;recently introduced WebAssembly standard;portable compilation target;cross-platform distribution;slice WebAssembly programs;reverse engineering;code comprehension;program slicing;intra-procedural slicing approach;backward slicing approach,2
779,Not Mentioned,Striking a Balance: Pruning False-Positives from Static Call Graphs,A. Utture; S. Liu; C. G. Kalhauge; J. Palsberg,"University of California, Los Angeles, U.S.A.; University of California, Los Angeles, U.S.A.; DTU, Denmark; University of California, Los Angeles, U.S.A.",2022,"Researchers have reported that static analysis tools rarely achieve a false-positive rate that would make them attractive to developers. We overcome this problem by a technique that leads to reporting fewer bugs but also much fewer false positives. Our technique prunes the static call graph that sits at the core of many static analyses. Specifically, static call-graph construction proceeds as usual, after which a call-graph pruner removes many false-positive edges but few true edges. The challenge is to strike a balance between being aggressive in removing false-positive edges but not so aggressive that no true edges remain. We achieve this goal by automatically producing a call-graph pruner through an automatic, ahead-of-time learning process. We added such a call-graph pruner to a software tool for null-pointer analysis and found that the false-positive rate decreased from 73% to 23%. This improvement makes the tool more useful to developers.",10.1145/3510003.3510166,Static Analysis;Machine learning classification;Call graphs,Computer bugs;Static analysis;Software tools;Software engineering,graph theory;learning (artificial intelligence);program diagnostics;software tools,pruning false-positives;static call graph;static analysis tools;false-positive rate;fewer bugs;fewer false positives;static analyses;static call-graph construction proceeds;call-graph pruner;false-positive edges,1
780,Not Mentioned,SugarC: Scalable Desugaring of Real-World Preprocessor Usage into Pure C,Z. Patterson; Z. Zhang; B. Pappas; S. Wei; P. Gazzillo,"The University of Texas at Dallas, USA; The University of Texas at Dallas, USA; University of Central Florida, USA; The University of Texas at Dallas, USA; University of Central Florida, USA",2022,"Variability-aware analysis is critical for ensuring the quality of con-figurable C software. An important step toward the development of variability-aware analysis at scale is to transform real-world C soft-ware that uses both C and preprocessor into pure C code, by replacing the preprocessor's compile-time variability with C's runtime-variability. In this work, we design and implement a desugaring tool, SugarC, that transforms away real-world preprocessor usage. SugarC augments C's formal grammar specification with translation rules, performs simultaneous type checking during de sugaring, and introduces numerous optimizations to address challenges that appear in real-world preprocessor usage. The experiments on DesugarBench, a benchmark consisting of 108 manually-created programs, show that SugarC supports many more language features than two existing desugaring tools. When applied on three real-world configurable C software, SugarC desugared 774 out of 813 files in the three programs, taking at most ten minutes in the worst case and less than two minutes for 95% of the C files.",10.1145/3510003.3512763,C preprocessor;syntax-directed translation;desugaring,Codes;Computer bugs;C languages;Transforms;Manuals;Syntactics;Software,C language;formal specification;grammars;optimisation;program compilers;program diagnostics;program processors;software maintenance;software performance evaluation,SugarC;scalable desugaring;real-world preprocessor usage;variability-aware analysis;runtime-variability;real-world configurable C software;desugaring tools;formal grammar specification;DesugarBench;simultaneous type checking;C code,1
781,Not Mentioned,SYMTUNER: Maximizing the Power of Symbolic Execution by Adaptively Tuning External Parameters,S. Cha; M. Lee; S. Lee; H. Oh,"Sungkyunkwan University, Republic of Korea; Korea University, Republic of Korea; Korea University, Republic of Korea; Korea University, Republic of Korea",2022,"We present SYMTUNER, a novel technique to automatically tune external parameters of symbolic execution. Practical symbolic execution tools have important external parameters (e.g., symbolic arguments, seed input) that critically affect their performance. Due to the huge parameter space, however, manually customizing those parameters is notoriously difficult even for experts. As a consequence, symbolic execution tools have typically been used in a suboptimal manner that, for example, simply relies on the default parameter settings of the tools and loses the opportunity for better performance. In this paper, we aim to change this situation by automatically configuring symbolic execution parameters. With Symtuner that takes parameter spaces to be tuned, symbolic executors are run without manual parameter configurations; instead, appropriate parameter values are learned and adjusted during symbolic execution. To achieve this, we present a learning algorithm that observes the behavior of symbolic execution and accordingly updates the sampling probability of each parameter space. We evaluated Symtuner with KLEE on 12 open-source C programs. The results show that Symtuner increases branch coverage of KLEE by 56% on average and finds 8 more bugs than KLEE with its default parameters over the latest releases of the programs.",10.1145/3510003.3510185,Symbolic Execution;Software Testing,Computer bugs;Manuals;Behavioral sciences;Tuning;Open source software;Software engineering,learning (artificial intelligence);probability;program debugging;program diagnostics;program testing;symbol manipulation,parameter configurations;Symtuner;SYMTUNER;symbolic execution tools;symbolic arguments;symbolic execution parameters;KLEE;open-source C programs;sampling probability;learning algorithm,
782,Not Mentioned,Testing Time Limits in Screener Questions for Online Surveys with Programmers,A. Danilova; S. Horstmann; M. Smith; A. Naiakshina,"University of Bonn; University of Bonn; Fraunhofer FKIE, University of Bonn; University of Bonn",2022,"Recruiting study participants with programming skill is essential for researchers. As programming is not a common skill, recruiting programmers as participants in large numbers is challenging. Plat-forms like Amazon MTurk or Qualtrics offer to recruit participants with programming knowledge. As this is self-reported, participants without programming experience could still take part, either due to a misunderstanding or to obtain the study compensation. If these participants are not detected, the data quality will suffer. To tackle this, Danilova et al. [11] developed and tested screening tasks to detect non-programmers. Unfortunately, the most reliable screen-ers were also those that took the most time. Since screeners should take as little time as possible, we examine whether the introduction of time limits allows us to create more efficient (i.e., quicker but still reliable) screeners. Our results show that this is possible and we extend the pool of screeners and make recommendations on how to improve the process.",10.1145/3510003.3510223,Developer Study;Methodology Developer Studies,Data integrity;Programming;Reliability engineering;Software reliability;Time factors;Task analysis;Testing,human factors;Internet;programming;recruitment,programming experience;study compensation;nonprogrammers;screener questions;online surveys;programming skill;programming knowledge;tested screening tasks;Amazon MTurk,
783,Not Mentioned,"The Art and Practice of Data Science Pipelines: A Comprehensive Study of Data Science Pipelines In Theory, In-The-Small, and In-The-Large",S. Biswas; M. Wardat; H. Rajan,"Iowa State University, Ames, IA, USA; Iowa State University, Ames, IA, USA; Iowa State University, Ames, IA, USA",2022,"Increasingly larger number of software systems today are including data science components for descriptive, predictive, and prescriptive analytics. The collection of data science stages from acquisition, to cleaning/curation, to modeling, and so on are referred to as data science pipelines. To facilitate research and practice on data science pipelines, it is essential to understand their nature. What are the typical stages of a data science pipeline? How are they connected? Do the pipelines differ in the theoretical representations and that in the practice? Today we do not fully understand these architectural characteristics of data science pipelines. In this work, we present a three-pronged comprehensive study to answer this for the state-of-the-art, data science in-the-small, and data science in-the-large, Our study analyzes three datasets: a collection of 71 proposals for data science pipelines and related concepts in theory, a collection of over 105 implementations of curated data science pipelines from Kaggle competitions to understand data science in-the-small, and a collection of 21 mature data science projects from GitHub to understand data science in-the-large. Our study has led to three representations of data science pipelines that capture the essence of our subjects in theory, in-the-small, and in-the-large.",10.1145/3510003.3510057,data science pipelines;data science processes;descriptive;predictive,Feedback loop;Art;Pipelines;Data science;Software systems;Data models;Proposals,data analysis;data mining;software engineering,data science pipeline;data science stage collection;software systems;data science components;prescriptive analytics;descriptive analytics;predictive analytics;data science in-the-large;data science in-the-small;data science in-theory,5
784,Not Mentioned,The Extent of Orphan Vulnerabilities from Code Reuse in Open Source Software,D. Reid; M. Jahanshahi; A. Mockus,"University of Tennessee, Knoxville, TN, USA; University of Tennessee, Knoxville, TN, USA; University of Tennessee, Knoxville, TN, USA",2022,"Motivation: A key premise of open source software is the ability to copy code to other open source projects (white-box reuse). Such copying accelerates development of new projects, but the code flaws in the original projects, such as vulnerabilities, may also spread even if fixed in the projects from where the code was appropriated. The extent of the spread of vulnerabilities through code reuse, the potential impact of such spread, or avenues for mitigating risk of these secondary vulnerabilities has not been studied in the context of a nearly complete collection of open source code. Aim: We aim to find ways to detect the white-box reuse induced vulnerabilities, determine how prevalent they are, and explore how they may be addressed. Method: We rely on World of Code infrastructure that provides a curated and cross-referenced collection of nearly all open source software to conduct a case study of a few known vulnerabilities. To conduct our case study we develop a tool, VDiOS, to help identify and fix white-box-reuse-induced vulnerabilities that have been already patched in the original projects (orphan vulnerabilities). Results: We find numerous instances of orphan vulnerabilities even in currently active and in highly popular projects (over 1K stars). Even apparently inactive projects are still publicly available for others to use and spread the vulnerability further. The often long delay in fixing orphan vulnerabilities even in highly popular projects increases the chances of it spreading to new projects. We provided patches to a number of project maintainers and found that only a small percentage accepted and applied the patch. We hope that VDiOS will lead to further study and mitigation of risks from orphan vulnerabilities and other orphan code flaws.",10.1145/3510003.3510216,code reuse;CVE;security vulnerabilities;git,Codes;Costs;Computer bugs;Stars;Delays;Security;Open source software,project management;public domain software;security of data;software development management;software reusability,orphan vulnerabilities;code reuse;open source software;open source projects;white-box reuse;original projects;open source code;code infrastructure;known vulnerabilities;white-box-reuse-induced vulnerabilities;apparently inactive projects;project maintainers;orphan code flaws;VDiOS;world of code infrastructure,
785,Not Mentioned,â€œThis Is Damn Slick!â€ Estimating the Impact of Tweets on Open Source Project Popularity and New Contributors,H. Fang; H. Lamba; J. Herbsleb; B. Vasilescu,"Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA",2022,"Twitter is widely used by software developers. But how effective are tweets at promoting open source projects? How could one use Twitter to increase a project's popularity or attract new contributors? In this paper we report on a mixed-methods empirical study of 44,544 tweets containing links to 2,370 open-source GitHub repositories, looking for evidence of causal effects of these tweets on the projects attracting new GitHub stars and contributors, as well as characterizing the high-impact tweets, the people likely being attracted by them, and how they differ from contributors attracted otherwise. Among others, we find that tweets have a statistically significant and practically sizable effect on obtaining new stars and a small average effect on attracting new contributors. The popularity, content of the tweet, as well as the identity of tweet authors all affect the scale of the attraction effect. In addition, our qualitative analysis suggests that forming an active Twitter community for an open source project plays an important role in attracting new committers via tweets. We also report that developers who are new to GitHub or have a long history of Twitter usage but few tweets posted are most likely to be attracted as contributors to the repositories mentioned by tweets. Our work contributes to the literature on open source sustainability.",10.1145/3510003.3510121,Open source software;GitHub;Twitter;Promotion,Social networking (online);Blogs;Stars;History;Sustainable development;Open source software;Software development management,Internet;public domain software;social networking (online),open-source GitHub repositories;high-impact tweets;tweet authors;attraction effect;open source sustainability;open source project popularity;qualitative analysis;Twitter usage,
786,Not Mentioned,TOGA: A Neural Method for Test Oracle Generation,E. Dinella; G. Ryan; T. Mytkowicz; S. K. Lahiri,University of Pennsylvania; Columbia University; Microsoft Research; Microsoft Research,2022,"Testing is widely recognized as an important stage of the software development lifecycle. Effective software testing can provide benefits such as bug finding, preventing regressions, and documentation. In terms of documentation, unit tests express a unit's intended functionality, as conceived by the developer. A test oracle, typically expressed as an condition, documents the intended behavior of a unit under a given test prefix. Synthesizing a functional test oracle is a challenging problem, as it must capture the intended functionality rather than the implemented functionality. In this paper, we propose TOGA (a neural method for Test Oracle GenerAtion), a unified transformer-based neural approach to infer both exceptional and assertion test oracles based on the context of the focal method. Our approach can handle units with ambiguous or missing documentation, and even units with a missing implementation. We evaluate our approach on both oracle inference accuracy and functional bug-finding. Our technique improves accuracy by 33% over existing oracle inference approaches, achieving 96% over-all accuracy on a held out test dataset. Furthermore, we show that when integrated with a automated test generation tool (EvoSuite), our approach finds 57 real world bugs in large-scale Java programs, including 30 bugs that are not found by any other automated testing method in our evaluation.",10.1145/3510003.3510141,Testing;transformers;machine learning;language models;test oracles;software testing,Software testing;Java;Computer bugs;Documentation;Computer architecture;Transformers;Software,Java;neural nets;program debugging;program testing,unified transformer-based neural approach;test oracle generation;exceptional assertion test oracles;oracle inference accuracy;functional bug;automated test generation tool;automated testing method;TOGA;software development lifecycle;software testing;functional test oracle;large-scale Java program;oracle inference approaches,5
787,Not Mentioned,Towards Automatically Repairing Compatibility Issues in Published Android Apps,Y. Zhao; L. Li; K. Liu; J. Grundy,"Monash University, Melbourne, Australia; Monash University, Melbourne, Australia; Huawei, Hangzhou, China; Monash University, Melbourne, Australia",2022,"The heavy fragmentation of the Android ecosystem has led to se-vere compatibility issues with apps, including those that crash at runtime or cannot be installed on certain devices but work well on other devices. To address this problem, various approaches have been proposed to detect and fix compatibility issues automatically. However, these all come with various limitations on fixing the com-patibility issues, e.g., can only fix one specific type of issues, cannot deal with multi-invocation issues in a single line and issues in re-leased apps. To overcome these limitations, we propose a generic approach that aims at fixing more types of compatibility issues in released Android apps. To this end, our prototype tool, Repair-Droid, provides a generic app patch description language for users to create fix templates for compatibility issues. The created tem-plates will then be leveraged by RepairDroid to automatically fix the corresponding issue at the bytecode level (e.g., right before users install the app). RepairDroid can support template creations for OS-induced, device-specific and inter-callback compatibility issues detected by three state-of-the-art approaches. Our experimental re-sults show that RepairDroid can fix 7,660 out of 8,976 compatibility issues in 1,000 randomly selected Google Play apps. RepairDroid is generic to configure new compatibility issues and outperforms the state-of-the-art on effectively repairing compatibility issues in released Android apps.",10.1145/3510003.3510128,Android;Compatibility Issue;Automated Program Repair,Runtime;Operating systems;Ecosystems;Prototypes;Maintenance engineering;Computer crashes;Internet,Android (operating system);mobile computing,multiinvocation issues;inter-callback compatibility issues;device-specific compatibility issues;OS-induced compatibility issues;published Android apps;RepairDroid;Google Play apps,3
788,Not Mentioned,Towards Bidirectional Live Programming for Incomplete Programs,X. Zhang; Z. Hu,"Key Laboratory of High Confidence Software Technologies, MoE School of Computer Science, Peking University; Key Laboratory of High Confidence Software Technologies, MoE School of Computer Science, Peking University",2022,"Bidirectional live programming not only allows software developers to see continuous feedback on the output as they write the program, but also allows them to modify the program by directly manipulating the output, so that the modified program can get the output that was directly manipulated. Despite the appealing of existing bidirectional live programming systems, there is a big limitation: they cannot deal with incomplete programs where code blanks exist in the source programs. In this paper, we propose a framework to support bidirectional live programming for incomplete programs, by extending the output value structure, introducing hole binding, and formally defining bidirectional evaluators that are well-behaved. To illustrate the usefulness of the framework, we realize the core bidirectional evaluations of incomplete programs in a tool called Bidirectional Preview. Our experimental results show that our extended back-ward evaluation for incomplete programs is as efficient as that for complete programs in that it is only $21 ms$ slower on a program with 10 holes than that on its full program, and our extended forward evaluation makes no difference. Furthermore, we use quick sort and student grades, two nontrivial examples of incomplete programs, to demonstrate its usefulness in algorithm teaching and program debugging.",10.1145/3510003.3510195,live programming;bidirectional evaluation;direct manipulation;hole bindings;hole closures,Codes;Education;Software algorithms;Debugging;Programming;Software;Software engineering,computer science education;program debugging;program verification;software engineering;sorting;teaching,source programs;incomplete programs;program debugging;modified program;bidirectional live programming systems;value structure;bidirectional preview;software developers;quick sort,
789,Not Mentioned,Towards Boosting Patch Execution On-the-Fly,S. Benton; Y. Xie; L. Lu; M. Zhang; X. Li; L. Zhang,"University of Texas at Dallas; Tsinghua University; Southern University of Science and Technology; Meta Platforms, Inc.; Kennesaw State University; University of Illinois at Urbana-Champaign",2022,"Program repair is an integral part of every software system's life-cycle but can be extremely challenging. To date, various automated program repair (APR) techniques have been proposed to reduce manual debugging efforts. However, given a real-world buggy program, a typical APR technique can generate a large number of patches, each of which needs to be validated against the original test suite, incurring extremely high computation costs. Although existing APR techniques have already leveraged various static and/or dynamic information to find the desired patches faster, they are still rather costly. In this work, we propose SeAPR (Self-Boosted Automated Program Repair), the first general-purpose technique to leverage the earlier patch execution information during APR to directly boost existing APR techniques themselves on-the-fly. Our basic intuition is that patches similar to earlier high-quality/low-quality patches should be promoted/degraded to speed up the detection of the desired patches. The experimental study on 13 state-of-the-art APR tools demonstrates that, overall, SeAPR can sub-stantially reduce the number of patch executions with negligible overhead. Our study also investigates the impact of various configurations on SeAPR. Lastly, our study demonstrates that SeAPR can even leverage the historical patch execution information from other APR tools for the same buggy program to further boost the current APR tool.",10.1145/3510003.3510117,,Costs;Manuals;Debugging;Maintenance engineering;Benchmark testing;Boosting;Software,program debugging;program testing,APR technique;test suite;static information;dynamic information;SeAPR;historical patch execution information;APR tool;software system;manual debugging efforts;real-world buggy program;patch execution on-the-fly boosting;self-boosted automated program repair,
790,Not Mentioned,Towards language-independent Brown Build Detection,D. Olewicki; M. Nayrolles; B. Adams,"Polytechnique MontrÃ©al, MontrÃ©al, Canada; Ubisoft MontrÃ©al, MontrÃ©al, Canada; Queen's University, Kingston, Canada",2022,"In principle, continuous integration (CI) practices allow modern software organizations to build and test their products after each code change to detect quality issues as soon as possible. In reality, issues with the build scripts (e.g., missing dependencies) and/or the presence of â€œflaky testsâ€ lead to build failures that essentially are false positives, not indicative of actual quality problems of the source code. For our industrial partner, which is active in the video game industry, such â€œbrown buildsâ€ not only require multidisci-plinary teams to spend more effort interpreting or even re-running the build, leading to substantial redundant build activity, but also slows down the integration pipeline. Hence, this paper aims to prototype and evaluate approaches for early detection of brown build results based on textual similarity to build logs of prior brown builds. The approach is tested on 7 projects (6 closed-source from our industrial collaborators and 1 open-source, Graphviz). We find that our model manages to detect brown builds with a mean F1-score of 53% on the studied projects, which is three times more than the best baseline considered, and at least as good as human experts (but with less effort). Furthermore, we found that cross-project prediction can be used for a project's onboarding phase, that a training set of 30-weeks works best, and that our retraining heuristics keep the F1-score higher than the baseline, while retraining only every 4â€“5 weeks.",10.1145/3510003.3510122,Brown Build;Build automation;Continuous integration;Classification;Concept drift,Training;Solid modeling;Time-frequency analysis;Codes;Switches;Predictive models;Solids,computer games;DP industry;natural language processing;project management;public domain software;software engineering;text analysis,industrial partner;video game industry;multidisci-plinary teams;substantial redundant build activity;integration pipeline;build logs;industrial collaborators;modern software organizations;code change;build scripts;flaky tests;build failures;false positives;source code;language-independent brown build detection,
791,Not Mentioned,Towards Practical Robustness Analysis for DNNs based on PAC-Model Learning,R. Li; P. Yang; C. -C. Huang; Y. Sun; B. Xue; L. Zhang,"SKLCS, Institute of Software, CAS, University of Chinese Academy of Sciences, China; SKLCS, Institute of Software, CAS, China; Pazhou Lab, Nanjing Institute of Software Technology, ISCAS, China; Queen's University Belfast, United Kingdom; SKLCS, Institute of Software, CAS, University of Chinese Academy of Sciences, China; SKLCS, Institute of Software, CAS, University of Chinese Academy of Sciences, China",2022,"To analyse local robustness properties of deep neural networks (DNNs), we present a practical framework from a model learning perspective. Based on black-box model learning with scenario optimisation, we abstract the local behaviour of a DNN via an affine model with the probably approximately correct (PAC) guarantee. From the learned model, we can infer the corresponding PAC-model robustness property. The innovation of our work is the integration of model learning into PAC robustness analysis: that is, we construct a PAC guarantee on the model level instead of sample distribution, which induces a more faithful and accurate robustness evaluation. This is in contrast to existing statistical methods without model learning. We implement our method in a prototypical tool named DeepPAC. As a black-box method, DeepPAC is scalable and efficient, especially when DNNs have complex structures or high-dimensional inputs. We extensively evaluate DeepPAC, with 4 baselines (using formal verification, statistical methods, testing and adversarial attack) and 20 DNN models across 3 datasets, including MNIST, CIFAR-10, and ImageNet. It is shown that DeepPAC outperforms the state-of-the-art statistical method PROVERO, and it achieves more practical robustness analysis than the formal verification tool ERAN. Also, its results are consistent with existing DNN testing work like DeepGini.",10.1145/3510003.3510143,neural networks;PAC-model robustness;model learning;scenario optimization,Deep learning;Analytical models;Technological innovation;Statistical analysis;Neural networks;Robustness;Picture archiving and communication systems,deep learning (artificial intelligence);formal verification;statistical analysis,affine model;PAC robustness analysis;accurate robustness evaluation;statistical methods;DeepPAC;black-box method;adversarial attack;DNN models;PAC-model learning;analyse local robustness properties;deep neural networks;model learning perspective;black-box model learning;local behaviour;probably approximately correct;statistical method PROVERO,
792,Not Mentioned,Towards Training Reproducible Deep Learning Models,B. Chen; M. Wen; Y. Shi; D. Lin; G. K. Rajbahadur; Z. M. Jiang,"Centre for Software Excellence, Huawei Canada, Kingston, Canada; Huawei Technologies, Shenzhen, China; Huawei Technologies, Shenzhen, China; Centre for Software Excellence, Huawei Canada, Kingston, Canada; Centre for Software Excellence, Huawei Canada, Kingston, Canada; York University, Toronto, Canada",2022,"Reproducibility is an increasing concern in Artificial Intelligence (AI), particularly in the area of Deep Learning (DL). Being able to reproduce DL models is crucial for AI-based systems, as it is closely tied to various tasks like training, testing, debugging, and auditing. However, DL models are challenging to be reproduced due to issues like randomness in the software (e.g., DL algorithms) and non-determinism in the hardware (e.g., GPU). There are various practices to mitigate some of the aforementioned issues. However, many of them are either too intrusive or can only work for a specific usage context. In this paper, we propose a systematic approach to training reproducible DL models. Our approach includes three main parts: (1) a set of general criteria to thoroughly evaluate the reproducibility of DL models for two different domains, (2) a unified framework which leverages a record-and-replay technique to mitigate software-related randomness and a profile-and-patch technique to control hardware-related non-determinism, and (3) a reproducibility guideline which explains the rationales and the mitigation strategies on conducting a reproducible training process for DL models. Case study results show our approach can successfully reproduce six open source and one commercial DL models.",10.1145/3510003.3510163,Artificial Intelligence;Deep Learning;Software Engineering;Reproducibility,Training;Deep learning;Systematics;Reproducibility of results;Software;Hardware;Artificial intelligence,artificial intelligence;knowledge acquisition;learning (artificial intelligence);program debugging,systematic approach;training reproducible DL models;software-related randomness;hardware-related nondeterminism;reproducibility guideline;reproducible training process;commercial DL models;towards training reproducible Deep Learning models;Artificial Intelligence;AI-based systems;aforementioned issues;specific usage context,1
793,Not Mentioned,Training Data Debugging for the Fairness of Machine Learning Software,Y. Li; L. Meng; L. Chen; L. Yu; D. Wu; Y. Zhou; B. Xu,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; Momenta, Suzhou, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",2022,"With the widespread application of machine learning (ML) software, especially in high-risk tasks, the concern about their unfairness has been raised towards both developers and users of ML software. The unfairness of ML software indicates the software behavior affected by the sensitive features (e.g., sex), which leads to biased and illegal decisions and has become a worthy problem for the whole software engineering community. According to the â€œdata-drivenâ€ programming paradigm of ML software, we consider the root cause of the unfairness as biased features in training data. Inspired by software debugging, we propose a novel method, Linear-regression based Training Data Debugging (LTDD), to debug feature values in training data, i.e., (a) identify which features and which parts of them are biased, and (b) exclude the biased parts of such features to recover as much valuable and unbiased information as possible to build fair ML software. We conduct an extensive study on nine data sets and three classifiers to evaluate the effect of our method LTDD compared with four baseline methods. Experimental results show that (a) LTDD can better improve the fairness of ML software with less or comparable damage to the performance, and (b) LTDD is more actionable for fairness improvement in realistic scenarios.",10.1145/3510003.3510091,Debugging;Fairness;ML Software;Training Data,Training;Linear regression;Training data;Machine learning;Debugging;Programming;Software,learning (artificial intelligence);program debugging;regression analysis;software engineering,Training Data Debugging;machine learning software;unfairness;software behavior;software engineering community;data-driven programming paradigm;biased features;software debugging;fair ML software,2
794,Not Mentioned,Trust Enhancement Issues in Program Repair,Y. Noller; R. Shariffdeen; X. Gao; A. Roychoudhury,"National University of Singapore, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore; National University of Singapore, Singapore",2022,"Automated program repair is an emerging technology that seeks to automatically rectify bugs and vulnerabilities using learning, search, and semantic analysis. Trust in automatically generated patches is necessary for achieving greater adoption of program repair. Towards this goal, we survey more than 100 software practitioners to understand the artifacts and setups needed to enhance trust in automatically generated patches. Based on the feedback from the survey on developer preferences, we quantitatively evaluate existing test-suite based program repair tools. We find that they cannot produce high-quality patches within a top-10 ranking and an acceptable time period of 1 hour. The developer feedback from our qualitative study and the observations from our quantitative examination of existing repair tools point to actionable insights to drive program repair research. Specifically, we note that producing repairs within an acceptable time-bound is very much dependent on leveraging an abstract search space representation of a rich enough search space. Moreover, while additional developer inputs are valuable for generating or ranking patches, developers do not seem to be interested in a significant human-in-the-loop interaction.",10.1145/3510003.3510040,program repair,Semantics;Computer bugs;Maintenance engineering;Software;Human in the loop;Software engineering,program debugging;program testing;software maintenance;software quality,semantic analysis;automatically generated patches;software practitioners;developer preferences;high-quality patches;acceptable time period;developer feedback;repair tools point;program repair research;acceptable time-bound;abstract search space representation;trust enhancement issues;automated program repair;bugs;test-suite based program repair tools,6
795,Not Mentioned,Type4Py: Practical Deep Similarity Learning-Based Type Inference for Python,A. M. Mir; E. LatoÅ¡kinas; S. Proksch; G. Gousios,"Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands; Delft University of Technology, Delft, The Netherlands; Meta, Menlo Park, USA",2022,"Dynamic languages, such as Python and Javascript, trade static typing for developer flexibility and productivity. Lack of static typing can cause run-time exceptions and is a major factor for weak IDE support. To alleviate these issues, PEP 484 introduced optional type annotations for Python. As retrofitting types to existing code-bases is error-prone and laborious, machine learning (ML)-based approaches have been proposed to enable automatic type infer-ence based on existing, partially annotated codebases. However, previous ML-based approaches are trained and evaluated on human-provided type annotations, which might not always be sound, and hence this may limit the practicality for real-world usage. In this paper, we present TYPE4Py, a deep similarity learning-based hier-archical neural network model. It learns to discriminate between similar and dissimilar types in a high-dimensional space, which results in clusters of types. Likely types for arguments, variables, and return values can then be inferred through the nearest neigh-bor search. Unlike previous work, we trained and evaluated our model on a type-checked dataset and used mean reciprocal rank (MRR) to reflect the performance perceived by users. The obtained results show that TYPE4Py achieves an MRR of 77.1 %, which is a substantial improvement of 8.1% and 16.7% over the state-of-the-art approaches Typilus and Typewriter, respectively. Finally, to aid developers with retrofitting types, we released a Visual Stu-dio Code extension, which uses TYPE4Py to provide ML-based type auto-completion for Python.",10.1145/3510003.3510124,Type Inference;Similarity Learning;Machine Learning;Mean Reciprocal Rank;Python,Productivity;Visualization;Codes;Annotations;Neural networks;Machine learning;Python,Java;learning (artificial intelligence);neural nets;program diagnostics;Python,type-checked dataset;TYPE4Py;ML-based type auto-completion;Python;type4Py;practical deep similarity learning-based type inference;static typing;machine learning-based approach;ML-based approach;human-provided type annotations;deep similarity learning;automatic type inference,8
796,Not Mentioned,Unleashing the Power of Compiler Intermediate Representation to Enhance Neural Program Embeddings,Z. Li; P. Ma; H. Wang; S. Wang; Q. Tang; S. Nie; S. Wu,"The Hong Kong University of Science and Technology, Hong Kong SAR; The Hong Kong University of Science and Technology, Hong Kong SAR; The Hong Kong University of Science and Technology, Hong Kong SAR; The Hong Kong University of Science and Technology, Hong Kong SAR; Tencent Security Keen Lab, China; Tencent Security Keen Lab, China; Tencent Security Keen Lab, China",2022,"Neural program embeddings have demonstrated considerable promise in a range of program analysis tasks, including clone identification, program repair, code completion, and program synthesis. However, most existing methods generate neural program embeddings di-rectly from the program source codes, by learning from features such as tokens, abstract syntax trees, and control flow graphs. This paper takes a fresh look at how to improve program embed-dings by leveraging compiler intermediate representation (IR). We first demonstrate simple yet highly effective methods for enhancing embedding quality by training embedding models alongside source code and LLVM IR generated by default optimization levels (e.g., -02). We then introduce IRGEN, a framework based on genetic algorithms (GA), to identify (near-)optimal sequences of optimization flags that can significantly improve embedding quality. We use IRGEN to find optimal sequences of LLVM optimization flags by performing GA on source code datasets. We then extend a popular code embedding model, CodeCMR, by adding a new objective based on triplet loss to enable a joint learning over source code and LLVM IR. We benchmark the quality of embedding using a rep-resentative downstream application, code clone detection. When CodeCMR was trained with source code and LLVM IRs optimized by findings of IRGEN, the embedding quality was significantly im-proved, outperforming the state-of-the-art model, CodeBERT, which was trained only with source code. Our augmented CodeCMR also outperformed CodeCMR trained over source code and IR optimized with default optimization levels. We investigate the properties of optimization flags that increase embedding quality, demonstrate IRGEN's generalization in boosting other embedding models, and establish IRGEN's use in settings with extremely limited training data. Our research and findings demonstrate that a straightforward addition to modern neural code embedding models can provide a highly effective enhancement.",10.1145/3510003.3510217,program embedding;deep learing;compiler technique,Training;Codes;Program processors;Cloning;Training data;Syntactics;Task analysis,embedded systems;genetic algorithms;learning (artificial intelligence);neural nets;program compilers;program diagnostics;program verification,compiler intermediate representation;neural program embeddings;program analysis tasks;program repair;code completion;program synthesis;program source codes;program embed-dings;embedding quality;training embedding models;LLVM IR;default optimization levels;IRGEN;optimal sequences;LLVM optimization flags;source code datasets;popular code embedding model;code clone detection;modern neural code embedding models,2
797,Not Mentioned,Use of Test Doubles in Android Testing: An In-Depth Investigation,M. Fazzini; C. Choi; J. M. Copia; G. Lee; Y. Kakehi; A. Gorla; A. Orso,"University of Minnesota, Minneapolis, MN, USA; University of Minnesota, Minneapolis, MN, USA; IMDEA Software Institute, Madrid, Spain; University of Minnesota, Minneapolis, MN, USA; Georgia Institute of Technology, Atlanta, GA, USA; IMDEA Software Institute, Madrid, Spain; Georgia Institute of Technology, Atlanta, GA, USA",2022,"Android apps interact with their environment extensively, which can result in flaky, slow, or hard-to-debug tests. Developers often address these problems using test doublesâ€”developer-defined objects that replace app or library classes during test execution. Although test doubles are widely used, there is limited understanding of how they are used in practice. To bridge this gap, we present an in-depth empirical study that aims to shed light on how developers create and use test doubles in Android apps. In our study, we first analyze a dataset of 1,006 apps with publicly available test suites to identify which frameworks and approaches developers most commonly use to create test doubles. We then investigate several research questions by studying how test doubles defined using these popular frameworks are created and used in the ten apps in the dataset that define the highest number of test doubles using these frameworks. Our results, based on the analysis of 2,365 test doubles that replace a total of 784 classes, provide insight into the types of test doubles used within Android apps and how they are utilized. Our results also show that test doubles used in Android apps and traditional Java test doubles differ in at least some respect. Finally, our results show that test doubles can introduce test smells and even mistakes in the test code. In the paper, we also discuss some implications of our findings that can help researchers and practitioners working in this area and guide future research.",10.1145/3510003.3510175,Test mocking;mobile apps;software environment,Couplings;Bridges;Waste materials;Java;Codes;Libraries;Testing,Java;mobile computing;program testing,test doubles-developer-defined objects;Android apps;2 test doubles;365 test doubles;traditional Java test doubles,2
798,Not Mentioned,Using Deep Learning to Generate Complete Log Statements,A. Mastropaolo; L. Pascarella; G. Bavota,"SEART @ Software Institute, UniversitÃ della Svizzera italiana, Switzerland; SEART @ Software Institute, UniversitÃ della Svizzera italiana, Switzerland; SEART @ Software Institute, UniversitÃ della Svizzera italiana, Switzerland",2022,"Logging is a practice widely adopted in several phases of the software lifecycle. For example, during software development log statements allow engineers to verify and debug the system by exposing fine-grained information of the running software. While the benefits of logging are undisputed, taking proper decisions about where to inject log statements, what information to log, and at which log level (e.g., error, warning) is crucial for the logging effectiveness. In this paper, we present LANCE (Log stAtemeNt reCommEnder), the first approach supporting developers in all these decisions. LANCE features a Text-To-Text-Transfer-Transformer (T5) model that has been trained on 6,894,456 Java methods. LANCE takes as input a Java method and injects in it a full log statement, including a human-comprehensible logging message and properly choosing the needed log level and the statement location. Our results show that LANCE is able to (i) properly identify the location in the code where to inject the statement in 65.9% of Java methods requiring it; (ii) selecting the proper log level in 66.2% of cases; and (iii) generate a completely correct log statement including a meaningful logging message in 15.2% of cases.",10.1145/3510003.3511561,Logging;Empirical Study;Machine Learning on Code,Deep learning;Java;Codes;Software;Software engineering,deep learning (artificial intelligence);information retrieval;Java;software engineering;system monitoring,software lifecycle;software development;fine-grained information;logging effectiveness;Java methods;injects;human-comprehensible logging message;statement location;proper log level;completely correct log statement;meaningful logging message;text-to-text-transfer-transformer model;log statement recommender;complete log statements;LANCE;T5,5
799,Not Mentioned,Using Pre-Trained Models to Boost Code Review Automation,R. Tufano; S. Masiero; A. Mastropaolo; L. Pascarella; D. Poshyvanyk; G. Bavota,"SEART @ Software Institute, UniversitÃ della Svizzera italiana, Switzerland; SEART @ Software Institute, UniversitÃ della Svizzera italiana, Switzerland; SEART @ Software Institute, UniversitÃ della Svizzera italiana, Switzerland; SEART @ Software Institute, UniversitÃ della Svizzera italiana, Switzerland; SEMERU @ Computer Science Department, William and Mary, USA; SEART @ Software Institute, UniversitÃ della Svizzera italiana, Switzerland",2022,"Code review is a practice widely adopted in open source and industrial projects. Given the non-negligible cost of such a process, researchers started investigating the possibility of automating specific code review tasks. We recently proposed Deep Learning (DL) models targeting the automation of two tasks: the first model takes as input a code submitted for review and implements in it changes likely to be recommended by a reviewer; the second takes as input the submitted code and a reviewer comment posted in natural language and automatically implements the change required by the reviewer. While the preliminary results we achieved are encouraging, both models had been tested in rather simple code review scenarios, substantially simplifying the targeted problem. This was also due to the choices we made when designing both the technique and the experiments. In this paper, we build on top of that work by demonstrating that a pre-trained Text-To-Text Transfer Transformer (T5) model can outperform previous DL models for automating code review tasks. Also, we conducted our experiments on a larger and more realistic (and challenging) dataset of code review activities.",10.1145/3510003.3510621,Code Review;Empirical Study;Machine Learning on Code,Deep learning;Codes;Automation;Costs;Natural languages;Transformers;Task analysis,deep learning (artificial intelligence);natural language processing;program diagnostics;software engineering;source code (software);text analysis,industrial projects;reviewer comment;pre-trained text-to-text transfer transformer model;code review automation;open source projects;deep learning model;natural language;code review tasks,11
800,Not Mentioned,Using Reinforcement Learning for Load Testing of Video Games,R. Tufano; S. Scalabrino; L. Pascarella; E. Aghajani; R. Oliveto; G. Bavota,"SEART @ Software Institute, UniversitÃ della Svizzera italiana, Switzerland; STAKE Lab, University of Molise, Italy; SEART @ Software Institute, UniversitÃ della Svizzera italiana, Switzerland; SEART @ Software Institute, UniversitÃ della Svizzera italiana, Switzerland; STAKE Lab, University of Molise, Italy; SEART @ Software Institute, UniversitÃ della Svizzera italiana, Switzerland",2022,"Different from what happens for most types of software systems, testing video games has largely remained a manual activity per-formed by human testers. This is mostly due to the continuous and intelligent user interaction video games require. Recently, rein-forcement learning (RL) has been exploited to partially automate functional testing. RL enables training smart agents that can even achieve super-human performance in playing games, thus being suitable to explore them looking for bugs. We investigate the pos-sibility of using RL for load testing video games. Indeed, the goal of game testing is not only to identify functional bugs, but also to examine the game's performance, such as its ability to avoid lags and keep a minimum number of frames per second (FPS) when high-demanding 3D scenes are shown on screen. We define a method-ology employing RL to train an agent able to play the game as a human while also trying to identify areas of the game resulting in a drop of FPS. We demonstrate the feasibility of our approach on three games. Two of them are used as proof-of-concept, by injecting artificial performance bugs. The third one is an open-source 3D game that we load test using the trained agent showing its potential to identify areas of the game resulting in lower FPS.",10.1145/3510003.3510625,Reinforcement Learning;Load Testing,Training;Three-dimensional displays;Computer bugs;Games;Reinforcement learning;Manuals;Software systems,computer games;interactive video;program debugging;reinforcement learning,continuous user interaction video games;intelligent user interaction video games;reinforcement learning;partially automate functional testing;game testing;RL;load testing;frames per second;artificial performance bugs;open-source 3D game,
801,Not Mentioned,Utilizing Parallelism in Smart Contracts on Decentralized Blockchains by Taming Application-Inherent Conflicts,P. Garamvolgyi; Y. Liu; D. Zhou; F. Long; M. Wu,"Shanghai Tree-Graph Blockchain Research Institute, Shanghai, China; Duke University, Durham, North Carolina, USA; Tsinghua University, Beijing, China; University of Toronto Toronto, Canada Shanghai Tree-Graph Blockchain Research Institute, Shanghai, China; Shanghai Tree-Graph Blockchain Research Institute, Shanghai, China",2022,"Traditional public blockchain systems typically had very limited transaction throughput because of the bottleneck of the consensus protocol itself. With recent advances in consensus technology, the performance limit has been greatly lifted, typically to thousands of transactions per second. With this, transaction execution has become a new performance bottleneck. Exploiting parallelism in transaction execution is a clear and direct way to address this and to further increase transaction throughput. Although some recent literature introduced concurrency control mechanisms to execute smart contract transactions in parallel, the reported speedup that they can achieve is far from ideal. The main reason is that the proposed parallel execution mechanisms cannot effectively deal with the conflicts inherent in many blockchain applications. In this work, we thoroughly study the historical transaction exe-cution traces in Ethereum. We observe that application-inherent conflicts are the major factors that limit the exploitable parallelism during execution. We propose to use partitioned counters and spe-cial commutative instructions to break up the application conflict chains in order to maximize the potential speedup. When we eval-uated the maximum parallel speedup achievable, these techniques doubled this limit to an 18x overall speedup compared to serial execution, thus approaching the optimum. We also propose OCC-DA, an optimistic concurrency control scheduler with deterministic aborts, which makes it possible to use OCC scheduling in public blockchain settings.",10.1145/3510003.3510086,blockchain;distributed ledgers;smart contracts;parallel execution;optimistic concurrency;deterministic concurrency,Concurrent computing;Smart contracts;Parallel processing;Programming;Throughput;Concurrency control;Consensus protocol,blockchains;concurrency control;protocols;telecommunication scheduling;transaction processing,smart contracts;decentralized blockchains;application-inherent conflicts;public blockchain systems;consensus protocol;consensus technology;performance limit;transaction execution;increase transaction throughput;concurrency control mechanisms;smart contract transactions;reported speedup;parallel execution mechanisms;blockchain applications;historical transaction exe-cution traces;exploitable parallelism;application conflict chains;maximum parallel speedup achievable;serial execution;public blockchain settings,4
802,Not Mentioned,VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning,Q. Chen; J. Lacomis; E. J. Schwartz; G. Neubig; B. Vasilescu; C. L. Goues,Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University Software Engineering Institute; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University,2022,"Variable names are critical for conveying intended program behavior. Machine learning-based program analysis methods use variable name representations for a wide range of tasks, such as suggesting new variable names and bug detection. Ideally, such methods could capture semantic relationships between names beyond syntactic similarity, e.g., the fact that the names average and mean are similar. Unfortunately, previous work has found that even the best of previous representation approaches primarily capture â€œrelatednessâ€ (whether two variables are linked at all), rather than â€œsimilarityâ€ (whether they actually have the same meaning). We propose Varclr, a new approach for learning semantic representations of variable names that effectively captures variable similarity in this stricter sense. We observe that this problem is an excellent fit for contrastive learning, which aims to minimize the distance between explicitly similar inputs, while maximizing the distance between dissimilar inputs. This requires labeled training data, and thus we construct a novel, weakly-supervised variable renaming dataset mined from GitHub edits. We show that Varclr enables the effective application of sophisticated, general-purpose language models like BERT, to variable name representation and thus also to related downstream tasks like variable name similarity search or spelling correction. Varclr produces models that significantly outperform the state-of-the-art on IDBENCH, an existing benchmark that explicitly captures variable similarity (as distinct from relatedness). Finally, we contribute a release of all data, code, and pre-trained models, aiming to provide a drop-in replacement for variable representations used in either existing or future program analyses that rely on variable names.",10.1145/3510003.3510162,,Codes;Semantics;Computer bugs;Bit error rate;Training data;Syntactics;Data models,data mining;learning (artificial intelligence);program debugging;program diagnostics,VarCLR;variable semantic representation pre-training;contrastive learning;variable names;machine learning-based program analysis methods;variable name representation;names average;previous representation;Varclr;semantic representations;variable similarity;explicitly similar inputs;weakly-supervised variable renaming dataset;variable name similarity search;variable representations,3
803,Not Mentioned,Verification of ORM-based Controllers by Summary Inference,G. Chawla; N. Aman; R. Komondoor; A. Bokil; N. Kharat,"Indian Institute of Science, Bangalore, India; Indian Institute of Science, Bangalore, India; Indian Institute of Science, Bangalore, India; Indian Institute of Science, Bangalore, India; Indian Institute of Science, Bangalore, India",2022,"In this work we describe a novel approach for modeling, analysis and verification of database-accessing applications that use the ORM (Object Relational Mapping) paradigm. Rather than directly analyze ORM code to check specific properties, our approach infers a general-purpose relational algebra summary of each controller in the application. This summary can then be fed into any off-the-shelf relational algebra solver to check for properties or specifications given by a developer. The summaries can also aid program understanding, and may have other applications. We have implemented our approach as a prototype tool that works for â€˜Springâ€™ based MVC applications. A preliminary evaluation reveals that the approach is efficient, and gives good results while checking a set of properties given by human subjects.",10.1145/3510003.3510148,program analysis;database applications;relational algebra,Analytical models;Codes;Algebra;Prototypes;Software engineering,Internet;relational algebra;relational databases;reverse engineering,modeling analysis;database-accessing applications;ORM paradigm;Object Relational Mapping;ORM code;general-purpose relational algebra summary;off-the-shelf relational algebra solver;summaries;program understanding;Spring based MVC applications;ORM-based controllers;summary inference,
804,Not Mentioned,V-SZZ: Automatic Identification of Version Ranges Affected by CVE Vulnerabilities,L. Bao; X. Xia; A. E. Hassan; X. Yang,"Zhejiang University, China; Huawei, China; Queen's University, Canada; Zhejiang University, China",2022,"Vulnerabilities publicly disclosed in the National Vulnerability Data-base (NVD) are assigned with CVE (Common Vulnerabilities and Exposures) IDs and associated with specific software versions. Many organizations, including IT companies and government, heavily rely on the disclosed vulnerabilities in NVD to mitigate their security risks. Once a software is claimed as vulnerable by NVD, these organizations would examine the presence of the vulnerable versions of the software and assess the impact on themselves. However, the version information about vulnerable software in NVD is not always reliable. Nguyen et al. find that the version information of many CVE vulnerabilities is spurious and propose an approach based on the original SZZ algorithm (i.e., an approach to identify bug-introducing commits) to assess the software versions affected by CVE vulnerabilities. However, SZZ algorithms are designed for common bugs, while vulnerabilities and bugs are different. Many bugs are introduced by a recent bug-fixing commit, but vulnerabilities are usually introduced in their initial versions. Thus, the current SZZ algorithms often fail to identify the inducing commits for vulnerabilities. Therefore, in this study, we propose an approach based on an improved SZZ algorithm to refine software versions affected by CVE vulnerabilities. Our proposed SZZ algorithm leverages the line mapping algorithms to identify the earliest commit that modified the vulnerable lines, and then considers these commits to be the vulnerability-inducing commits, as opposed to the previous SZZ algorithms that assume the commits that last modified the buggy lines as the inducing commits. To evaluate our proposed approach, we manually annotate the true inducing commits and verify the vulnerable versions for 172 CVE vulnerabilities with fixing commits from two publicly available datasets with five C/C++ and 41 Java projects, respectively. We find that 99 out of 172 vulnerabilities whose version information is spurious. The experiment results show that our proposed approach can identify more vulnerabilities with the true inducing commits and correct vulnerable versions than the previous SZZ algorithms. Our approach outperforms the previous SZZ algorithms in terms of F1-score for identifying vulnerability-inducing commits on both C/C++ and Java projects (0.736 and 0.630, respectively). For refining vulnerable versions, our approach also achieves the best performance on the two datasets in terms of F1-score (0.928 and 0.952).",10.1145/3510003.3510113,SZZ;Vulnerability;CVE,Java;Software algorithms;Computer bugs;Refining;Government;Companies;Software,C++ language;computer crime;Java;program debugging;program diagnostics;public domain software,NVD;version information;vulnerable software;bug-introducing commits;vulnerable lines;vulnerability-inducing commits;CVE vulnerabilities;vulnerable versions;national vulnerability database;SZZ algorithms;bug-fixing commit;C/C++;software versions,3
805,Not Mentioned,VulCNN: An Image-inspired Scalable Vulnerability Detection System,Y. Wu; D. Zou; S. Dou; W. Yang; D. Xu; H. Jin,"Huazhong University of Science and Technology, China; Huazhong University of Science and Technology, China; Fudan University, China; University of Texas at Dallas, United States; Huazhong University of Science and Technology, China; Huazhong University of Science and Technology, China",2022,"Since deep learning (DL) can automatically learn features from source code, it has been widely used to detect source code vulnerability. To achieve scalable vulnerability scanning, some prior studies intend to process the source code directly by treating them as text. To achieve accurate vulnerability detection, other approaches consider distilling the program semantics into graph representations and using them to detect vulnerability. In practice, text-based techniques are scalable but not accurate due to the lack of program semantics. Graph-based methods are accurate but not scalable since graph analysis is typically time-consuming. In this paper, we aim to achieve both scalability and accuracy on scanning large-scale source code vulnerabilities. Inspired by existing DL-based image classification which has the ability to analyze millions of images accurately, we prefer to use these techniques to accomplish our purpose. Specifically, we propose a novel idea that can efficiently convert the source code of a function into an image while preserving the program details. We implement Vul-CNN and evaluate it on a dataset of 13,687 vulnerable functions and 26,970 non-vulnerable functions. Experimental results report that VulCNN can achieve better accuracy than eight state-of-the-art vul-nerability detectors (i.e., Checkmarx, FlawFinder, RATS, TokenCNN, VulDeePecker, SySeVR, VulDeeLocator, and Devign). As for scalability, VulCNN is about four times faster than VulDeePecker and SySeVR, about 15 times faster than VulDeeLocator, and about six times faster than Devign. Furthermore, we conduct a case study on more than 25 million lines of code and the result indicates that VulCNN can detect large-scale vulnerability. Through the scanning reports, we finally discover 73 vulnerabilities that are not reported in NVD.",10.1145/3510003.3510229,Vulnerability Detection;CNN;Large Scale;Image,Deep learning;Codes;Scalability;Semantics;Detectors;Transforms;Rats,graph theory;image classification;learning (artificial intelligence);program diagnostics;public domain software;security of data;software performance evaluation;software tools;text analysis,large-scale vulnerability;VulCNN;image-inspired scalable vulnerability detection system;deep learning;source code vulnerability;scalable vulnerability scanning;accurate vulnerability detection;program semantics;graph representations;text-based techniques;graph-based methods;graph analysis;large-scale source code vulnerabilities;existing DL-based image classification;program details;Vul-CNN;times faster,6
806,Not Mentioned,What Do They Capture? - A Structural Analysis of Pre-Trained Language Models for Source Code,Y. Wan; W. Zhao; H. Zhang; Y. Sui; G. Xu; H. Jin,"School of Computer Science and Technology, Huazhong University of Science and Technology, China; School of Computer Science and Technology, Huazhong University of Science and Technology, China; University of Newcastle, Australia; School of Computer Science, University of Technology Sydney, Australia; School of Computer Science, University of Technology Sydney, Australia; School of Computer Science and Technology, Huazhong University of Science and Technology, China",2022,"Recently, many pre-trained language models for source code have been proposed to model the context of code and serve as a basis for downstream code intelligence tasks such as code completion, code search, and code summarization. These models leverage masked pre-training and Transformer and have achieved promising results. However, currently there is still little progress regarding interpretability of existing pre-trained code models. It is not clear why these models work and what feature correlations they can capture. In this paper, we conduct a thorough structural analysis aiming to provide an interpretation of pre-trained language models for source code (e.g., CodeBERT, and GraphCodeBERT) from three distinctive perspectives: (1) attention analysis, (2) probing on the word embedding, and (3) syntax tree induction. Through comprehensive analysis, this paper reveals several insightful findings that may inspire future studies: (1) Attention aligns strongly with the syntax structure of code. (2) Pre-training language models of code can preserve the syntax structure of code in the intermediate representations of each Transformer layer. (3) The pre-trained models of code have the ability of inducing syntax trees of code. Theses findings suggest that it may be helpful to incorporate the syntax structure of code into the process of pre-training for better code representations.",10.1145/3510003.3510050,Code representation;deep learning;pre-trained language model;probing;attention analysis;syntax tree induction,Training;Representation learning;Analytical models;Codes;Correlation;Syntactics;Transformers,computational linguistics;learning (artificial intelligence);natural language processing;source code (software);trees (mathematics),structural analysis;pretrained language models;source code;downstream code intelligence tasks;code completion;code search;code summarization;pretrained code models;code representations;attention analysis;word embedding;syntax tree induction;code syntax structure;transformer layer,2
807,Not Mentioned,What Makes a Good Commit Message?,Y. Tian; Y. Zhang; K. -J. Stol; L. Jiang; H. Liu,"Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China; University College Cork and Lero, School of Computer Science and IT, Cork, Ireland; Beijing Institute of Technology, Beijing, China; Beijing Institute of Technology, Beijing, China",2022,"A key issue in collaborative software development is communication among developers. One modality of communication is a commit message, in which developers describe the changes they make in a repository. As such, commit messages serve as an â€œaudit trailâ€ by which developers can understand how the source code of a project has changed-and why. Hence, the quality of commit messages affects the effectiveness of communication among developers. Commit messages are often of poor quality as developers lack time and motivation to craft a good message. Several automatic approaches have been proposed to generate commit messages. However, these are based on uncurated datasets including considerable proportions of poorly phrased commit messages. In this multi-method study, we first define what constitutes a â€œgoodâ€ commit message, and then establish what proportion of commit messages lack information using a sample of almost 1,600 messages from five highly active open source projects. We find that an average of circa 44% of messages could be improved, suggesting the use of uncurated datasets may be a major threat when commit message generators are trained with such data. We also observe that prior work has not considered semantics of commit messages, and there is surprisingly little guidance available for writing good commit messages. To that end, we develop a taxonomy based on recurring patterns in commit messages' expressions. Finally, we investigate whether â€œgoodâ€ commit messages can be automatically identified; such automation could prompt developers to write better commit messages.",10.1145/3510003.3510205,Commit-based software development;open collaboration;commit message quality,Codes;Collaborative software;Taxonomy;Semantics;Writing;Maintenance engineering;Software,groupware;human factors;project management;public domain software;software engineering,commit message generators;collaborative software development;audit trail;source code;open source project,7
808,Not Mentioned,What Makes Effective Leadership in Agile Software Development Teams?,L. Gren; P. Ralph,"Volvo Cars and Chalmers | University of Gothenburg, Gothenburg, Sweden; Dalhousie University, Halifax, Canada",2022,"Effective leadership is one of the key drivers of business and project success, and one of the most active areas of management research. But how does leadership work in agile software development, which emphasizes self-management and self-organization and marginalizes traditional leadership roles? To find out, this study examines agile leadership from the perspective of thirteen professionals who identify as agile leaders, in different roles, at ten different software development companies of varying sizes. Data from semi-structured interviews reveals that leadership: (1) is dynamically shared among team members; (2) engenders a sense of belonging to the team; and (3) involves balancing competing organizational cultures (e.g. balancing the new agile culture with the old milestone-driven culture). In other words, agile leadership is a property of a team, not a role, and effectiveness depends on agile team members' identifying with the team, accepting responsibility, and being sensitive to cultural conflict.",10.1145/3510003.3510100,agile methods;leadership;management;culture,Leadership;Companies;Software;Cultural differences;Interviews;Software engineering;Business,human resource management;organisational aspects;software engineering,agile software development team;active areas;management research;leadership work;self-management;self-organization;agile leadership;agile leaders;software development companies;agile culture;agile team members;organizational culture;cultural conflict,1
809,Not Mentioned,What the Fork? Finding Hidden Code Clones in npm,E. Wyss; L. De Carli; D. Davidson,"University of Kansas, Lawrence, KS, USA; Worcester Polytechnic Institute, Worcester, MA, USA; University of Kansas, Lawrence, KS, USA",2022,"This work presents findings and mitigations on an under-studied issue, which we term shrinkwrapped clones, that is endemic to the npm software package ecosystem. A shrink-wrapped clone is a package which duplicates, or near-duplicates, the code of another package without any indication or refer-ence to the original package. This phenomenon represents a challenge to the hygiene of package ecosystems, as a clone package may siphon interest from the package being cloned, or create hidden duplicates of vulnerable, insecure code which can fly under the radar of audit processes. Motivated by these considerations, we propose UNWRAP-PER, a mechanism to programmatically detect shrinkwrapped clones and match them to their source package. UNWRAP-PER uses a package difference metric based on directory tree similarity, augmented with a prefilter which quickly weeds out packages unlikely to be clones of a target. Overall, our prototype can compare a given package within the entire npm ecosystem (1,716,061 packages with 20,190,452 differ-ent versions) in 72.85 seconds, and it is thus practical for live deployment. Using our tool, we performed an analysis of a subset of npm packages, which resulted in finding up to 6,292 previously unknown shrinkwrapped clones, of which up to 207 carried vulnerabilities from the original package that had already been fixed in the original package. None of such vulnerabilities were discoverable via the standard npm audit process.",10.1145/3510003.3510168,code clone;clone;npm,Codes;Software packages;Ecosystems;Cloning;Prototypes;Real-time systems;Security,image colour analysis;invasive software;public domain software;security of data;software packages;trees (mathematics);Web sites,hidden code clones;mitigations;npm software package ecosystem;shrink-wrapped clone;original package;package ecosystems;clone package;hidden duplicates;vulnerable code;insecure code;source package;package difference metric;given package;entire npm ecosystem;npm packages;6 clones;292 previously unknown shrinkwrapped clones;standard npm audit process,
810,Not Mentioned,Where is Your App Frustrating Users?,Y. Wang; J. Wang; H. Zhang; X. Ming; L. Shi; Q. Wang,"Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; The University of Newcastle, Callaghan, Australia; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China; Laboratory for Internet Software Technologies, Institute of Software Chinese Academy of Sciences, Beijing, China",2022,"User reviews of mobile apps provide a communication channel for developers to perceive user satisfaction. Many app features that users have problems with are usually expressed by key phrases such as â€œupload picturesâ€, which could be buried in the review texts. The lack of fine-grained view about problematic features could obscure the developers' understanding of where the app is frustrating users, and postpone the improvement of the apps. Existing pattern-based approaches to extract target phrases suffer from low accuracy due to insufficient semantic understanding of the reviews, thus can only summarize the high-level topics/aspects of the reviews. This paper proposes a semantic-aware, fine-grained app review analysis approach (SIRA) to extract, cluster, and visualize the problematic features of apps. The main component of SIRA is a novel BERT+Attr-CRF model for fine-grained problematic feature extraction, which combines textual descriptions and review attributes to better model the semantics of reviews and boost the performance of the traditional BERT-CRF model. SIRA also clusters the extracted phrases based on their semantic relations and presents a visualization of the summaries. Our evaluation on 3,426 reviews from six apps confirms the effectiveness of SIRA in problematic feature extraction and clustering. We further conduct an empirical study with SIRA on 318,534 reviews of 18 popular apps to explore its potential application and examine its usefulness in real-world practice.",,App Review;Information Extraction;Deep Learning,Codes;Clustering methods;Semantics;Communication channels;Feature extraction;Mobile applications;Data mining,feature extraction;formal specification;information filtering;Internet;mobile computing;natural language processing;text analysis,app frustrating users;user reviews;mobile apps;user satisfaction;app features;key phrases;review texts;fine-grained view;problematic features;developers;existing pattern-based approaches;target phrases;insufficient semantic understanding;semantic-aware review analysis approach;fine-grained app review analysis approach;SIRA;novel BERT+Attr-CRF model;fine-grained problematic feature extraction;traditional BERT-CRF model;extracted phrases;clustering;18 popular apps,
811,Not Mentioned,Windranger: A Directed Greybox Fuzzer driven by Deviation Basic Blocks,Z. Du; Y. Li; Y. Liu; B. Mao,"State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",2022,"Directed grey-box fuzzing (DGF) is a security testing technique that aims to steer the fuzzer towards predefined target sites in the program. To gain directedness, DGF prioritizes the seeds whose execution traces are closer to the target sites. Therefore, evaluating the distance between the execution trace of a seed and the target sites (aka, the seed distance) is important for DGF. The first directed grey-box fuzzer, AFLGo, uses an approach of calculating the basic block level distances during static analysis and accumulating the distances of the executed basic blocks to compute the seed distance. Following AFLGo, most of the existing state-of-the-art DGF techniques use all the basic blocks on the execution trace and only the control flow information for seed distance calculation. However, not every basic block is equally important and there are certain basic blocks where the execution trace starts to deviate from the target sites (aka, deviation basic blocks). In this paper, we propose a technique called Windranger which leverages deviation basic blocks to facilitate DGF. To identify the deviation basic blocks, Windranger applies both static reachability analysis and dynamic filtering. To conduct directed fuzzing, Windranger uses the deviation basic blocks and their related data flow information for seed distance calculation, mutation, seed prioritization as well as explore-exploit scheduling. We evaluated Windranger on 3 datasets consisting of 29 programs. The experiment results show that Windranger outperforms AFLGo, AFL, and FAIRFuzz by reaching the target sites 21%, 34%, and 37% faster and detecting the target crashes 44%, 66%, and 77% faster respectively. Moreover, we found a 0-day vulnerability with a CVE ID assigned in ffmpeg (a popular multimedia library extensively fuzzed by OSS-fuzz) with Windranger by supplying manually identified suspect locations as the target sites.",10.1145/3510003.3510197,Fuzz Testing;Directed Testing,Filtering;Static analysis;Fuzzing;Media;Libraries;Computer crashes;Security,program diagnostics;program testing;reachability analysis;security of data,Windranger;deviation basic blocks;predefined target sites;execution traces;execution trace;basic block level distances;executed basic blocks;seed distance calculation;state-of-the-art DGF techniques;directed greybox fuzzer;AFLGo;dynamic filtering;FAIRFuzz;CVE ID;ffmpeg;OSS-fuzz;multimedia library,3
812,Keynotes,Future Software for Life in Trusted Futures,S. Pink,"Monash University, Australia",2023,"How will people, other species, software and hardware live together in as yet unknown futures? How can we work towards trusted and safe futures where human values and the environment are supported by emerging technologies? Research demonstrates that human values and everyday life priorities, ethics, routines and activities will shape our possible futures. I will draw on ethnographic research to outline how people anticipate and imagine everyday life futures with emerging technologies in their homes and neighbourhoods, and how technology workers envisage futures in their professional lives. If, as social science research shows, technologies cannot solve human and societal problems, what roles should they play in future life? What are the implications for future software? What values should underpin its design? Where should it be developed? By and in collaboration with whom? What role can software play in generating the circumstances for trusted futures?",10.1109/ICSE48619.2023.00010,Software Engineering and Society,Software;Software engineering;Social sciences;Smart homes;Scholarships;Industries;Indexes,social aspects of automation,future software;trusted futures,
813,Keynotes,The Road Toward Dependable AI Based Systems,P. Tonella,"Software Institute, UniversitÃ della Svizzera italiana (USI), Lugano, Switzerland",2023,"With the advent of deep learning, AI components have achieved unprecedented performance on complex, human competitive tasks, such as image, video, text and audio processing. Hence, they are increasingly integrated into sophisticated software systems, some of which (e.g., autonomous vehicles) are required to deliver certified dependability warranties. In this talk, I will consider the unique features of AI based systems and of the faults possibly affecting them, in order to revise the testing fundamentals and redefine the overall goal of testing, taking a statistical view on the dependability warranties that can be actually delivered. Then, I will consider the key elements of a revised testing process for AI based systems, including the test oracle and the test input generation problems. I will also introduce the notion of runtime supervision, to deal with unexpected error conditions that may occur in the field. Finally, I will identify the future steps that are essential to close the loop from testing to operation, proposing an empirical framework that reconnects the output of testing to its original goals.",10.1109/ICSE48619.2023.00011,Software Testing;Deep Learning;Reliability and Dependability,Testing;Warranties;Indexes;Deep learning;Task analysis;Software testing;Software systems,deep learning (artificial intelligence);program testing,AI components;audio processing;certified dependability warranties;deep learning;dependable AI based systems;human competitive tasks;revised testing process;sophisticated software systems;test input generation problems;test oracle,
814,Keynotes,Software Engineering as the Linchpin of Responsible AI,L. Zhu,"CSIRO's Data61, Sydney, Australia",2023,"From humanity's existential risks to safety risks in critical systems to ethical risks, responsible AI, as the saviour, has become a major research challenge with significant real-world consequences. However, achieving responsible AI remains elusive despite the plethora of high-level ethical principles, risk frameworks and progress in algorithmic assurance. In the meantime, software engineering (SE) is being upended by AI, grappling with building system-level quality and alignment from inscrutable machine learning models and code generated from natural language prompts. The upending poses new challenges and opportunities for engineering AI systems responsibly. This talk will share our experiences in helping the industry achieve responsible AI systems by inventing new SE approaches. It will dive into industry challenges (such as risk silos and principle-algorithm gaps) and research challenges (such as lack of requirements, emerging properties and inscrutable systems) and make the point that SE is the linchpin of responsible AI. But SE also requires some fundamental rethinking - shifting from building functions into AI systems to discovering and managing emerging functions from AI systems. Only by doing so can SE take on critical new roles, from understanding human intelligence to building a thriving human-AI symbiosis.",10.1109/ICSE48619.2023.00012,Responsible AI;Ethical AI;Trustworthy AI;AI Engineering;SE4AI,Industries;Symbiosis;Ethics;Machine learning algorithms;Buildings;Software algorithms;Natural languages,ethical aspects;learning (artificial intelligence);software engineering,critical systems;engineering AI systems;ethical risks;high-level ethical principles;human intelligence;human-AI symbiosis;humanity;inscrutable systems;machine learning models;natural language prompts;principle-algorithm gaps;responsible AI systems;risk silos;safety risks;SE;software engineering;system-level quality,
815,AI Models for SE,One Adapter for All Programming Languages? Adapter Tuning for Code Search and Summarization,D. Wang; B. Chen; S. Li; W. Luo; S. Peng; W. Dong; X. Liao,"National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; Hunan University, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China",2023,"As pre-trained models automate many code intel-ligence tasks, a widely used paradigm is to fine-tune a model on the task dataset for each programming language. A recent study reported that multilingual fine-tuning benefits a range of tasks and models. However, we find that multilingual fine-tuning leads to performance degradation on recent models UniXcoder and CodeT5. To alleviate the potentially catastrophic forgetting issue in multilingual models, we fix all pre-trained model parameters, insert the parameter-efficient structure adapter, and fine-tune it. Updating only 0.6% of the overall parameters compared to full-model fine-tuning for each programming language, adapter tuning yields consistent improvements on code search and sum-marization tasks, achieving state-of-the-art results. In addition, we experimentally show its effectiveness in cross-lingual and low-resource scenarios. Multilingual fine-tuning with 200 samples per programming language approaches the results fine-tuned with the entire dataset on code summarization. Our experiments on three probing tasks show that adapter tuning significantly outperforms full-model fine-tuning and effectively overcomes catastrophic forgetting.",10.1109/ICSE48619.2023.00013,transfer learning;adapter;multilingual task,Degradation;Training;Adaptation models;Computer languages;Codes;Source coding;Transfer learning,deep learning (artificial intelligence);learning (artificial intelligence);natural language processing,adapter tuning yields consistent improvements;code search;code summarization;fine-tuning benefits;full-model fine-tuning;multilingual fine-tuning;multilingual models;parameter-efficient structure adapter;potentially catastrophic forgetting issue;pre-trained model parameters;pre-trained models automate many code intel-ligence tasks;probing tasks;programming language;sum-marization tasks;task dataset,
816,AI Models for SE,CCRep: Learning Code Change Representations via Pre-Trained Code Model and Query Back,Z. Liu; Z. Tang; X. Xia; X. Yang,"Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China; Zhejiang University, Hangzhou, China",2023,"Representing code changes as numeric feature vectors, i.e., code change representations, is usually an essential step to automate many software engineering tasks related to code changes, e.g., commit message generation and just-in-time defect prediction. Intuitively, the quality of code change representations is crucial for the effectiveness of automated approaches. Prior work on code changes usually designs and evaluates code change representation approaches for a specific task, and little work has investigated code change encoders that can be used and jointly trained on various tasks. To fill this gap, this work proposes a novel Code Change Representation learning approach named CCRep, which can learn to encode code changes as feature vectors for diverse downstream tasks. Specifically, CCRep regards a code change as the combination of its before-change and after-change code, leverages a pre-trained code model to obtain high-quality contextual embeddings of code, and uses a novel mechanism named query back to extract and encode the changed code fragments and make them explicitly interact with the whole code change. To evaluate CCRep and demonstrate its applicability to diverse code-change-related tasks, we apply it to three tasks: commit message generation, patch correctness assessment, and just-in-time defect prediction. Experimental results show that CCRep outperforms the state-of-the-art techniques on each task.",10.1109/ICSE48619.2023.00014,code change;representation learning;commit message generation;patch correctness assessment;just-in-time defect prediction,Representation learning;Adaptation models;Codes;Feature extraction;Task analysis;Software engineering;Context modeling,feature extraction;just-in-time;learning (artificial intelligence);software engineering,after-change code;changed code fragments;code change representations;diverse code-change-related tasks;novel Code Change Representation;pre-trained code model,
817,AI Models for SE,Keeping Pace with Ever-Increasing Data: Towards Continual Learning of Code Intelligence Models,S. Gao; H. Zhang; C. Gao; C. Wang,"School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Big Data and Software Engineering, Chongqing University, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China",2023,"Previous research on code intelligence usually trains a deep learning model on a fixed dataset in an offline manner. However, in real-world scenarios, new code repositories emerge incessantly, and the carried new knowledge is beneficial for providing up-to-date code intelligence services to developers. In this paper, we aim at the following problem: How to enable code intelligence models to continually learn from ever-increasing data? One major challenge here is catastrophic forgetting, meaning that the model can easily forget knowledge learned from previous datasets when learning from the new dataset. To tackle this challenge, we propose REPEAT, a novel method for continual learning of code intelligence models. Specifically, REPEAT addresses the catastrophic forgetting problem with representative exemplars replay and adaptive parameter regularization. The representative exemplars replay component selects informative and diverse exemplars in each dataset and uses them to re-train model periodically. The adaptive parameter regularization component recognizes important parameters in the model and adaptively penalizes their changes to preserve the knowledge learned before. We evaluate the proposed approach on three code intelligence tasks including code summarization, software vulnerability detection, and code clone detection. Extensive experiments demonstrate that REPEAT consistently outperforms baseline methods on all tasks. For example, REPEAT improves the conventional fine-tuning method by 1.22, 5.61, and 1.72 on code summarization, vulnerability detection and clone detection, respectively.",10.1109/ICSE48619.2023.00015,,Deep learning;Adaptation models;Codes;Cloning;Data models;Software;Task analysis,deep learning (artificial intelligence);program diagnostics;software engineering,catastrophic forgetting problem;code clone detection;code intelligence models;code repositories;code summarization;continual learning;deep learning model;re-train model;REPEAT;software vulnerability detection,
818,Fuzzing: Applications,Detecting JVM JIT Compiler Bugs via Exploring Two-Dimensional Input Spaces,H. Jia; M. Wen; Z. Xie; X. Guo; R. Wu; M. Sun; K. Chen; H. Jin,"School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Informatics, Xiamen University, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Computer Science and Technology, Huazhong University of Science and Technology, China",2023,"Java Virtual Machine (JVM) is the fundamental software system that supports the interpretation and execution of Java bytecode. To support the surging performance demands for the increasingly complex and large-scale Java programs, Just-In-Time (JIT) compiler was proposed to perform sophisticated runtime optimization. However, this inevitably induces various bugs, which are becoming more pervasive over the decades and can often cause significant consequences. To facilitate the design of effective and efficient testing techniques to detect JIT compiler bugs. This study first performs a preliminary study aiming to understand the characteristics of JIT compiler bugs and the corresponding triggering test cases. Inspired by the empirical findings, we propose JOpFuzzer, a new JVM testing approach with a specific focus on JIT compiler bugs. The main novelty of JOpFuzzer is embodied in three aspects. First, besides generating new seeds, JOpFuzzer also searches for diverse configurations along the new dimension of optimization options. Second, JOpFuzzer learns the correlations between various code features and different optimization options to guide the process of seed mutation and option exploration. Third, it leverages the profile data, which can reveal the program execution information, to guide the fuzzing process. Such nov-elties enable JOpFuzzer to effectively and efficiently explore the two-dimensional input spaces. Extensive evaluation shows that JOpFuzzer outperforms the state-of-the-art approaches in terms of the achieved code coverages. More importantly, it has detected 41 bugs in OpenJDK, and 25 of them have already been confirmed or fixed by the corresponding developers.",10.1109/ICSE48619.2023.00016,JVM;JIT Compiler;JVM Testing,Java;Codes;Runtime;Computer bugs;Fuzzing;Software systems;Virtual machining,Java;just-in-time;program compilers;program debugging;program diagnostics;program testing;virtual machines,corresponding triggering test cases;effective testing techniques;efficient testing techniques;Java Virtual Machine;JOpFuzzer;Just-In-Time compiler;JVM JIT compiler bugs;JVM testing approach;large-scale Java programs;option exploration;seed mutation;surging performance demands;two-dimensional input spaces,
819,Fuzzing: Applications,JITfuzz: Coverage-guided Fuzzing for JVM Just-in-Time Compilers,M. Wu; M. Lu; H. Cui; J. Chen; Y. Zhang; L. Zhang,"Southern University of Science and Technology, Shenzhen, China; Southern University of Science and Technology, Shenzhen, China; The University of Hong Kong, Hong Kong, China; Tianjin University, Tianjin, China; Southern University of Science and Technology, Shenzhen, China; University of Illinois Urbana-Champaign, Champaign, USA",2023,"As a widely-used platform to support various Java-bytecode-based applications, Java Virtual Machine (JVM) incurs severe performance loss caused by its real-time program interpretation mechanism. To tackle this issue, the Just-in- Time compiler (JIT) has been widely adopted to strengthen the efficacy of JVM. Therefore, how to effectively and efficiently detect JIT bugs becomes critical to ensure the correctness of JVM. In this paper, we propose a coverage-guided fuzzing framework, namely JITfuzz, to automatically detect JIT bugs. In particular, JITfuzz adopts a set of optimization-activating mutators to trigger the usage of typical JIT optimizations, e.g., function inlining and simplification. Meanwhile, given JIT optimizations are closely coupled with program control flows, JITfuzz also adopts mutators to enrich the control flows of target programs. Moreover, JITfuzz also proposes a mutator scheduler which iteratively schedules mutators according to the coverage updates to maximize the code coverage of JIT. To evaluate the effectiveness of JITfuzz, we conduct a set of experiments based on a benchmark suite with 16 popular JVM-based projects from GitHub. The experimental results suggest that JITfuzz outperforms the state-of-the-art mutation-based and generation-based JVM fuzzers by 27.9 % and 18.6 % respectively in terms of edge coverage on average. Furthermore, JITfuzz also successfully detects 36 previously unknown bugs (including 23 JIT bugs) and 27 bugs (including 18 JIT bugs) have been confirmed by the developers.",10.1109/ICSE48619.2023.00017,,Java;Schedules;Image edge detection;Computer bugs;Fuzzing;Benchmark testing;Virtual machining,Java;just-in-time;program compilers;program debugging;program diagnostics;program interpreters;program testing;virtual machines,16 popular JVM-based projects;coverage-guided fuzzing framework;generation-based JVM fuzzers;given JIT optimizations;including 18 JIT bugs;including 23 JIT bugs;Java Virtual Machine;Java-bytecode-based applications;JITfuzz;Just-in- Time compiler;Just-in-Time compilers;optimization-activating mutators;real-time program interpretation mechanism;state-of-the-art mutation-based,
820,Fuzzing: Applications,Validating SMT Solvers via Skeleton Enumeration Empowered by Historical Bug-Triggering Inputs,M. Sun; Y. Yang; M. Wen; Y. Wang; Y. Zhou; H. Jin,"School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; School of Computer Science and Technology, Huazhong University of Science and Technology, China",2023,"SMT solvers check the satisfiability of logic formulas over first-order theories, which have been utilized in a rich number of critical applications, such as software verification, test case generation, and program synthesis. Bugs hidden in SMT solvers would severely mislead those applications and further cause severe consequences. Therefore, ensuring the reliability and robustness of SMT solvers is of critical importance. Although many approaches have been proposed to test SMT solvers, it is still a challenge to discover bugs effectively. To tackle such a challenge, we conduct an empirical study on the historical bug-triggering formulas in SMT solvers' bug tracking systems. We observe that the historical bug-triggering formulas contain valuable skeletons (i.e., core structures of formulas) as well as associated atomic formulas which can cast significant impacts on formulas' ability in triggering bugs. Therefore, we propose a novel approach that utilizes the skeletons extracted from the historical bug-triggering formulas and enumerates atomic formulas under the guidance of association rules derived from historical formulas. In this study, we realized our approach as a practical fuzzing tool HistFuzz and conducted extensive testing on the well-known SMT solvers Z3 and cvc5. To date, HistFuzz has found 111 confirmed new bugs for Z3 and cvc5, of which 108 have been fixed by the developers. More notably, out of the confirmed bugs, 23 are soundness bugs and invalid model bugs found in the solvers' default mode, which are essential for SMT solvers. In addition, our experiments also demonstrate that HistFuzz outperforms the state-of-the-art SMT solver fuzzers in terms of achieved code coverage and effectiveness.",10.1109/ICSE48619.2023.00018,SMT solver;fuzzing;skeleton enumeration;association rules;bug detection,Codes;Computer bugs;Fuzzing;Reliability theory;Skeleton;Software;Robustness,computability;data mining;formal verification;inference mechanisms;program debugging;program diagnostics;program testing;program verification;source code (software),associated atomic formulas;enumerates atomic formulas;historical bug-triggering formulas;historical bug-triggering inputs;historical formulas;triggering bugs;well-known SMT solvers,
821,Fuzzing: Applications,Regression Fuzzing for Deep Learning Systems,H. You; Z. Wang; J. Chen; S. Liu; S. Li,"College of Intelligence and Computing Tianjin University, Tianjin, China; College of Intelligence and Computing Tianjin University, Tianjin, China; College of Intelligence and Computing Tianjin University, Tianjin, China; College of Intelligence and Computing Tianjin University, Tianjin, China; College of Intelligence and Computing Tianjin University, Tianjin, China",2023,"Deep learning (DL) Systems have been widely used in various domains. Similar to traditional software, DL system evolution may also incur regression faults. To find the regression faults between versions of a DL system, we propose a novel regression fuzzing technique called DRFuzz, which facilitates generating inputs that trigger diverse regression faults and have high fidelity. To enhance the diversity of the found regression faults, DRFuzz proposes a diversity-oriented test criterion to explore as many faulty behaviors as possible. Then, DRFuzz incorporates the GAN model to guarantee the fidelity of generated test inputs. We conduct an extensive study on four subjects in four regression scenarios of DL systems. The experimental results demonstrate the superiority of DRFuzz over the two compared state-of-the-art approaches, with an average improvement of 1,177% and 539% in terms of the number of detected regression faults.",10.1109/ICSE48619.2023.00019,Regression;Fuzzing;Deep Learning,Deep learning;Fuzzing;Software;Behavioral sciences;Regression tree analysis;Software engineering,deep learning (artificial intelligence);fault tolerant computing;program testing;regression analysis,deep learning systems;diversity-oriented test criterion;DL systems;DRFuzz;GAN model;regression faults;regression fuzzing,2
822,Fuzzing: Applications,Operand-Variation-Oriented Differential Analysis for Fuzzing Binding Calls in PDF Readers,S. Guo; X. Wan; W. You; B. Liang; W. Shi; Y. Zhang; J. Huang; J. Zhang,"School of Information, Renmin University of China, Beijing, China; School of Information, Renmin University of China, Beijing, China; School of Information, Renmin University of China, Beijing, China; School of Information, Renmin University of China, Beijing, China; School of Information, Renmin University of China, Beijing, China; School of Information, Renmin University of China, Beijing, China; School of Information, Renmin University of China, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China",2023,"Binding calls of embedded scripting engines introduce a serious attack surface in PDF readers. To effectively test binding calls, the knowledge of parameter types is necessary. Unfortunately, due to the absence or incompleteness of documentation and the lack of sufficient samples, automatic type reasoning for binding call parameters is a big challenge. In this paper, we propose a novel operand-variation-oriented differential analysis approach, which automatically extracts features from execution traces as oracles for inferring parameter types. In particular, the parameter types of a binding call are inferred by executing the binding call with different values of different types and investigating which types cause an expected effect on the instruction operands. The inferred type information is used to guide the test generation in fuzzing. Through the evaluation on two popular PDF readers (Adobe Reader and Foxit Reader), we demonstrated the accuracy of our type reasoning method and the effectiveness of the inferred type information for improving fuzzing in both code coverage and vulnerability discovery. We found 38 previously unknown security vulnerabilities, 26 of which were certified with CVE numbers.",10.1109/ICSE48619.2023.00020,binding call;PDF reader;type reasoning;fuzzing,Codes;Documentation;Fuzzing;Portable document format;Feature extraction;Cognition;Test pattern generators,computer network security;feature extraction;inference mechanisms;program diagnostics;program testing;security of data,Adobe Reader;automatic type reasoning;binding call parameters;binding calls;embedded scripting engines;Foxit Reader;inferred type information;instruction operands;novel operand-variation-oriented differential analysis approach;parameter types;popular PDF readers;serious attack surface;type reasoning method,
823,Mining software Repositories,The untold story of code refactoring customizations in practice,D. Oliveira; W. K. G. AssunÃ§Ã£o; A. Garcia; A. C. Bibiano; M. Ribeiro; R. Gheyi; B. Fonseca,"Informatics Department, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Rio de Janeiro, Brazil; Informatics Department, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Rio de Janeiro, Brazil; Informatics Department, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Rio de Janeiro, Brazil; Informatics Department, Pontifical Catholic University of Rio de Janeiro (PUC-Rio), Rio de Janeiro, Brazil; Computing Institute - Federal University of Alagoas (UFAL), MaceiÃ³, Brazil; Department of Computing and Systems, Federal University of Campina Grande (UFCG), Campina Grande, Brazil; Computing Institute - Federal University of Alagoas (UFAL), MaceiÃ³, Brazil",2023,"Refactoring is a common software maintenance practice. The literature defines standard code modifications for each refactoring type and popular IDEs provide refactoring tools aiming to support these standard modifications. However, previous studies indicated that developers either frequently avoid using these tools or end up modifying and even reversing the code automatically refactored by IDEs. Thus, developers are forced to manually apply refactorings, which is cumbersome and error-prone. This means that refactoring support may not be entirely aligned with practical needs. The improvement of tooling support for refactoring in practice requires understanding in what ways developers tailor refactoring modifications. To address this issue, we conduct an analysis of 1,162 refactorings composed of more than 100k program modifications from 13 software projects. The results reveal that developers recurrently apply patterns of additional modifications along with the standard ones, from here on called patterns of customized refactorings. For instance, we found customized refactorings in 80.77% of the Move Method instances observed in the software projects. We also investigated the features of refactoring tools in popular IDEs and observed that most of the customization patterns are not fully supported by them. Additionally, to understand the relevance of these customizations, we conducted a survey with 40 developers about the most frequent customization patterns we found. Developers confirm the relevance of customization patterns and agree that improvements in IDE's refactoring support are needed. These observations highlight that refactoring guidelines must be updated to reflect typical refactoring customizations. Also, IDE builders can use our results as a basis to enable a more flexible application of automated refactorings. For example, developers should be able to choose which method must handle exceptions when extracting an exception code into a new method.",10.1109/ICSE48619.2023.00021,Refactoring;Custom Refactoring;Refactoring Tooling Support,Surveys;Software maintenance;Java;Codes;Standards;Software engineering;Guidelines,Java;object-oriented programming;program diagnostics;software maintenance,100k program modifications;13 software projects;additional modifications;automated refactorings;common software maintenance practice;customized refactorings;exception code;frequent customization patterns;IDE's refactoring support;popular IDEs;refactoring guidelines;refactoring tools;refactoring type;standard code modifications;standard modifications;standard ones;tooling support;typical refactoring customizations;ways developers tailor refactoring modifications,
824,Mining software Repositories,Data Quality for Software Vulnerability Datasets,R. Croft; M. A. Babar; M. M. Kholoosi,"School of Computer Science, CREST, University of Adelaide, Australia; School of Computer Science, CREST, University of Adelaide, Australia; School of Computer Science, CREST, University of Adelaide, Australia",2023,"The use of learning-based techniques to achieve automated software vulnerability detection has been of longstanding interest within the software security domain. These data-driven solutions are enabled by large software vulnerability datasets used for training and benchmarking. However, we observe that the quality of the data powering these solutions is currently ill-considered, hindering the reliability and value of produced outcomes. Whilst awareness of software vulnerability data preparation challenges is growing, there has been little investigation into the potential negative impacts of software vulnerability data quality. For instance, we lack confirmation that vulnerability labels are correct or consistent. Our study seeks to address such shortcomings by inspecting five inherent data quality attributes for four state-of-the-art software vulnerability datasets and the subsequent impacts that issues can have on software vulnerability prediction models. Surprisingly, we found that all the analyzed datasets exhibit some data quality problems. In particular, we found 20â€“71% of vulnerability labels to be inaccurate in real-world datasets, and 17-99% of data points were duplicated. We observed that these issues could cause significant impacts on downstream models, either preventing effective model training or inflating benchmark performance. We advocate for the need to overcome such challenges. Our findings will enable better consideration and assessment of software vulnerability data quality in the future.",10.1109/ICSE48619.2023.00022,software vulnerability;data quality;machine learning,Training;Data integrity;Benchmark testing;Predictive models;Software;Data models;Software reliability,learning (artificial intelligence);security of data;software quality,automated software vulnerability detection;data points;data powering;data quality problems;data-driven solutions;software security domain;software vulnerability data preparation challenges;software vulnerability data quality;software vulnerability datasets;software vulnerability prediction models;vulnerability labels,3
825,Mining software Repositories,Do code refactorings influence the merge effort?,A. Oliveira; V. Neves; A. Plastino; A. C. Bibiano; A. Garcia; L. Murta,"Instituto de ComputaÃ§Ã£o (IC), Universidade Federal Fluminense (UFF), NiterÃ³i, Brazil; Instituto de ComputaÃ§Ã£o (IC), Universidade Federal Fluminense (UFF), NiterÃ³i, Brazil; Instituto de ComputaÃ§Ã£o (IC), Universidade Federal Fluminense (UFF), NiterÃ³i, Brazil; Informatics Department, PUC-Rio, Rio de Janeiro, Brazil; Informatics Department, PUC-Rio, Rio de Janeiro, Brazil; Instituto de ComputaÃ§Ã£o (IC), Universidade Federal Fluminense (UFF), NiterÃ³i, Brazil",2023,"In collaborative software development, multiple contributors frequently change the source code in parallel to implement new features, fix bugs, refactor existing code, and make other changes. These simultaneous changes need to be merged into the same version of the source code. However, the merge operation can fail, and developer intervention is required to resolve the conflicts. Studies in the literature show that 10 to 20 percent of all merge attempts result in conflicts, which require the manual developer's intervention to complete the process. In this paper, we concern about a specific type of change that affects the structure of the source code and has the potential to increase the merge effort: code refactorings. We analyze the relationship between the occurrence of refactorings and the merge effort. To do so, we applied a data mining technique called association rule extraction to find patterns of behavior that allow us to analyze the influence of refactorings on the merge effort. Our experiments extracted association rules from 40,248 merge commits that occurred in 28 popular open-source projects. The results indicate that: (i) the occurrence of refactorings increases the chances of having merge effort; (ii) the more refactorings, the greater the chances of effort; (iii) the more refactorings, the greater the effort; and (iv) parallel refactorings increase even more the chances of having effort, as well as the intensity of it. The results obtained may suggest behavioral changes in the way refactorings are implemented by developer teams. In addition, they can indicate possible ways to improve tools that support code merging and those that recommend refactorings, considering the number of refactorings and merge effort attributes.",10.1109/ICSE48619.2023.00023,Software Merge;Merge Effort;Refactoring;Association Rules;Data Mining,Computer languages;Codes;Source coding;Merging;Semantics;Software;Behavioral sciences,data mining;software maintenance;source code (software),association rule extraction;behavioral changes;code merging;code refactorings;collaborative software development;data mining technique;open-source projects;parallel refactorings;source code,
826,Mining software Repositories,A Comprehensive Study of Real-World Bugs in Machine Learning Model Optimization,H. Guan; Y. Xiao; J. Li; Y. Liu; G. Bai,"The University of Queensland, Brisbane, Australia; Southern University of Science and Technology, Shenzhen, China; Microsoft Software Technology Center Asia, Beijing, China; Southern University of Science and Technology, Shenzhen, China; The University of Queensland, Brisbane, Australia",2023,"Due to the great advance in machine learning (ML) techniques, numerous ML models are expanding their application domains in recent years. To adapt for resource-constrained platforms such as mobile and Internet of Things (IoT) devices, pre-trained models are often processed to enhance their efficiency and compactness, using optimization techniques such as pruning and quantization. Similar to the optimization process in other complex systems, e.g., program compilers and databases, optimizations for ML models can contain bugs, leading to severe consequences such as system crashes and financial loss. While bugs in training, compiling and deployment stages have been extensively studied, there is still a lack of systematic understanding and characterization of model optimization bugs (MOBs). In this work, we conduct the first empirical study to identify and characterize MOBs. We collect a comprehensive dataset containing 371 MOBs from TensorFlow and PyTorch, the most extensively used open-source ML frameworks, covering the entire development time span of their optimizers (May 2019 to August 2022). We then investigate the collected bugs from various perspectives, including their symptoms, root causes, life cycles, detection and fixes. Our work unveils the status quo of MOBs in the wild, and reveals their features on which future detection techniques can be based. Our findings also serve as a warning to the developers and the users of ML frameworks, and an appeal to our research community to enact dedicated countermeasures.",10.1109/ICSE48619.2023.00024,Machine Learning;Model Optimization;Bugs,Training;Adaptation models;Analytical models;Systematics;Computer bugs;Machine learning;Internet of Things,learning (artificial intelligence);optimisation;program compilers;program debugging,databases;financial loss;Internet of Things devices;machine learning model optimization;ML models;MOB;mobile devices;model optimization bugs;open-source ML frameworks;program compilers;pruning;PyTorch;quantization;resource-constrained platforms;system crashes;TensorFlow,
827,Fault Localization,Evaluating the Impact of Experimental Assumptions in Automated Fault Localization,E. Soremekun; L. Kirschner; M. BÃ¶hme; M. Papadakis,"Royal Holloway, University of London, UK; Saarland University, Germany; MPI-SP, Germany; SnT, Luxembourg",2023,"Much research on automated program debugging often assumes that bug fix location(s) indicate the faults' root causes and that root causes of faults lie within single code elements (statements). It is also often assumed that the number of statements a developer would need to inspect before finding the first faulty statement reflects debugging effort. Although intuitive, these three assumptions are typically used (55% of experiments in surveyed publications make at least one of these three assumptions) without any consideration of their effects on the debugger's effectiveness and potential impact on developers in practice. To deal with this issue, we perform controlled experimentation, split testing in particular, using 352 bugs from 46 open-source C programs, 19 Automated Fault Localization (AFL) techniques (18 statistical debugging formulas and dynamic slicing), two (2) state-of-the-art automated program repair (APR) techniques (GenProg and Angelix) and 76 professional developers. Our results show that these assumptions conceal the difficulty of debugging. They make AFL techniques appear to be (up to 38%) more effective, and make APR tools appear to be (2X) less effective. We also find that most developers (83%) consider these assumptions to be unsuitable for debuggers and, perhaps worse, that they may inhibit development productivity. The majority (66%) of developers prefer debugging diagnoses without these assumptions twice as much as with the assumptions. Our findings motivate the need to assess debuggers conservatively, i.e., without these assumptions.",10.1109/ICSE48619.2023.00025,fault localization;program repair;user study,Location awareness;Productivity;Codes;Computer bugs;Debugging;Maintenance engineering;Fault location,C language;fault diagnosis;program debugging;program testing;public domain software;software maintenance;statistical analysis,AFL techniques;APR;automated fault localization techniques;automated program debugging;automated program repair techniques;experimental assumptions;open-source C programs;single code elements;split testing;statistical debugging formulas,
828,Fault Localization,Locating Framework-specific Crashing Faults with Compact and Explainable Candidate Set,J. Yan; M. Wang; Y. Liu; J. Yan; L. Zhang,"Technology Center of Software Engineering, Institute of Software, Chinese Academy of Sciences, Beijing, China; Technology Center of Software Engineering, Institute of Software, Chinese Academy of Sciences, Beijing, China; Department of Computer Science and Engineering, Southern University of Science and Technology, Shenzhen, China; University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, Beijing, China",2023,"Nowadays, many applications do not exist independently but rely on various frameworks or libraries. The frequent evolution and the complex implementation of framework APIs induce lots of unexpected post-release crashes. Starting from the crash stack traces, existing approaches either perform application-level call graph (CG) tracing or construct datasets with similar crash-fixing records to locate buggy methods. However, these approaches are limited by the completeness of CG or dependent on historical fixing records, and some of them only focus on specific manually modeled exception types. To achieve effective debugging on complex framework-specific crashes, we propose a code-separation-based locating approach that weakly relies on CG tracing and does not require any prior knowledge. Our key insight is that one crash trace with the description message can be mapped to a definite exception-thrown point in the framework, the semantics analysis of which can help to figure out the root causes of the crash-triggering procedure. Thus, we can pre-construct reusable summaries for all the framework-specific exceptions to support fault localization in application code. Based on that idea, we design the exception-thrown summary (ETS) that describes both the key variables and key APIs related to the exception triggering. Then, we perform static analysis to automatically compute such summaries and make a data-tracking of key variables and APIs in the application code to get the ranked buggy candidates. In the scenario of locating Android framework-specific crashing faults, our tool CrashTracker exhibited an overall MRR value of 0.91 and outperforms the state-of-the-art tool Anchor with higher precision. It only provides a compact candidate set and gives user-friendly reports with explainable reasons for each candidate.",10.1109/ICSE48619.2023.00026,Fault Localization;Framework-specific Exception;Crash Stack Trace;Android Application,Location awareness;Codes;Semantics;Debugging;Static analysis;Computer crashes;Libraries,Android (operating system);application program interfaces;program debugging;program diagnostics;software fault tolerance,application code;application-level call graph;buggy methods;CG tracing;code-separation-based locating approach;compact candidate set;complex framework-specific crashes;complex implementation;crash stack;crash trace;crash-triggering procedure;definite exception-thrown point;exception-thrown summary;explainable candidate set;fault localization;framework APIs;framework-specific crashing faults;framework-specific exceptions;frequent evolution;historical fixing records;key APIs;key variables;libraries;locating Android framework-specific;ranked buggy candidates;similar crash-fixing records;specific manually modeled exception types;unexpected post-release crashes,
829,Fault Localization,PExReport: Automatic Creation of Pruned Executable Cross-Project Failure Reports,S. Huang; X. Wang,"Department of Computer Science, University of Texas at San Antonio, San Antonio, USA; Department of Computer Science, University of Texas at San Antonio, San Antonio, USA",2023,"Modern software development extensively depends on existing libraries written by other developer teams from the same or a different organization. When a developer executes the software, the execution trace may go across the boundaries of multiple software products and create cross-project failures (CPFs). Existing studies show that a stand-alone executable failure report may enable the most effective communication, but creating such a report is often challenging due to the complicated files and dependencies interactions in the software ecosystems. In this paper, to solve the CPF report trilemma, we developed PExReport, which automatically creates stand-alone executable CPF reports. PExReport leverages build tools to prune source code and dependencies, and further analyzes the build process to create a pruned build environment for reproducing the CPF. We performed an evaluation on 74 software project issues with 198 CPFs, and the evaluation results show that PExReport can create executable CPF reports for 184 out of 198 test failures in our dataset, with an average reduction of 72.97% on source classes and the classes in internal JARs.",10.1109/ICSE48619.2023.00027,cross-project failure;executable failure report;failure reproduction;build tool;build environment;debloating,Codes;Source coding;Ecosystems;Organizations;Software;Libraries;Hybrid power systems,program testing;software maintenance,automatic creation;CPF report trilemma;cross-project failures;dependencies interactions;executable CPF reports;modern software development;multiple software products;PExReport leverages;prune source code;pruned build environment;pruned executable cross-project failure reports;software ecosystems;software project issues;stand-alone executable failure report;test failures,
830,Fault Localization,RAT: A Refactoring-Aware Traceability Model for Bug Localization,F. Niu; W. K. G. AssunÃ§Ã£o; L. Huang; C. Mayr-Dorn; J. Ge; B. Luo; A. Egyed,"State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Institute for Software Systems Engineering, Johannes Kepler University, Linz, Austria; Department of Computer Science and Engineering, Southern Methodist University, Dallas, Texas, USA; Institute for Software Systems Engineering, Johannes Kepler University, Linz, Austria; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Institute for Software Systems Engineering, Johannes Kepler University, Linz, Austria",2023,"A large number of bug reports are created during the evolution of a software system. Locating the source code files that need to be changed in order to fix these bugs is a challenging task. Information retrieval-based bug localization techniques do so by correlating bug reports with historical information about the source code (e.g., previously resolved bug reports, commit logs). These techniques have shown to be efficient and easy to use. However, one flaw that is nearly omnipresent in all these techniques is that they ignore code refactorings. Code refactorings are common during software system evolution, but from the perspective of typical version control systems, they break the code history. For example, a class when renamed then appears as two separate classes with separate histories. Obviously, this is a problem that affects any technique that leverages code history. This paper proposes a refactoring-aware traceability model to keep track of the code evolution history. With this model, we reconstruct the code history by analyzing the impact of code refactorings to correctly stitch together what would otherwise be a fragmented history. To demonstrate that a refactoring aware history is indeed beneficial, we investigated three widely adopted bug localization techniques that make use of code history, which are important components in existing approaches. Our evaluation on 11 open source projects shows that taking code refactorings into account significantly improves the results of these bug localization techniques without significant changes to the techniques themselves. The more refactorings are used in a project, the stronger the benefit we observed. Based on our findings, we believe that much of the state of the art leveraging code history should benefit from our work.",10.1109/ICSE48619.2023.00028,bug localization;bug report similarity;code refactoring;traceability;commit history;information retrieval,Location awareness;Analytical models;Codes;Source coding;Computer bugs;Software systems;Control systems,configuration management;information retrieval;program debugging;software maintenance;source code (software),art leveraging code history;bug reports;bugs;code evolution history;code refactorings;information retrieval-based bug localization techniques;refactoring aware history;refactoring-aware traceability model;separate histories;software system evolution;source code files;typical version control systems;widely adopted bug localization techniques,
831,Formal Verification,How Do We Read Formal Claims? Eye-Tracking and the Cognition of Proofs about Algorithms,H. Ahmad; Z. Karas; K. Diaz; A. Kamil; J. -B. Jeannin; W. Weimer,"University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor; University of Michigan, Ann Arbor",2023,"Formal methods are used successfully in high-assurance software, but they require rigorous mathematical and logical training that practitioners often lack. As such, integrating formal methods into software has been associated with numerous challenges. While educators have placed emphasis on formalisms in undergraduate theory courses, such courses often struggle with poor student outcomes and satisfaction. In this paper, we present a controlled eye-tracking human study (n = 34) investigating the problem-solving strategies employed by students with different levels of incoming preparation (as assessed by theory coursework taken and pre-screening performance on a proof comprehension task), and how educators can better prepare low-outcome students for the rigorous logical reasoning that is a core part of formal methods in software engineering. Surprisingly, we find that incoming preparation is not a good predictor of student outcomes for formalism comprehension tasks, and that student self-reports are not accurate at identifying factors associated with high outcomes for such tasks. Instead, and importantly, we find that differences in outcomes can be attributed to performance for proofs by induction and recursive algorithms, and that better-performing students exhibit significantly more attention switching behaviors, a result that has several implications for pedagogy in terms of the design of teaching materials. Our results suggest the need for a substantial pedagogical intervention in core theory courses to better align student outcomes with the objectives of mastery and retaining the material, and thus bettering preparing students for high-assurance software engineering.",10.1109/ICSE48619.2023.00029,formalism comprehension;student cognition;eye-tracking;facial behavior analysis;human study,Training;Software algorithms;Gaze tracking;Switches;Prediction algorithms;Software;Cognition,computer science education;educational courses;software engineering;teaching,controlled eye-tracking human study;formal claims;formal methods;formalism comprehension tasks;high-assurance software engineering;logical training;low-outcome students;rigorous logical reasoning;rigorous mathematical training;undergraduate theory courses,
832,Formal Verification,Which of My Assumptions are Unnecessary for Realizability and Why Should I Care?,R. Shalom; S. Maoz,"Tel Aviv University, Tel Aviv, Israel; Tel Aviv University, Tel Aviv, Israel",2023,"Specifications for reactive systems synthesis consist of assumptions and guarantees. However, some specifications may include unnecessary assumptions, i.e., assumptions that are not necessary for realizability. While the controllers that are synthesized from such specifications are correct, they are also inflexible and fragile; their executions will satisfy the specification's guarantees in only very specific environments. In this work we show how to detect unnecessary assumptions, and to transform any realizable specification into a corresponding realizable core specification, one that includes the same guarantees but no unnecessary assumptions. We do this by computing an assumptions core, a locally minimal subset of assumptions that suffices for realizability. Controllers that are synthesized from a core specification are not only correct but, importantly, more general; their executions will satisfy the specification's guarantees in more environments. We implemented our ideas in the Spectra synthesis environment, and evaluated their impact over different benchmarks from the literature. The evaluation provides evidence for the motivation and significance of our work, by showing (1) that unnecessary assumptions are highly prevalent, (2) that in almost all cases the fully-automated removal of unnecessary assumptions pays off in total synthesis time, and (3) that core specifications induce more general controllers whose reachable state space is larger but whose representation more memory efficient.",10.1109/ICSE48619.2023.00030,Reactive synthesis;Formal specifications,Memory management;Transforms;Benchmark testing;Aerospace electronics;Software engineering,formal verification;reachability analysis,realizable core specification;specific environments;Spectra synthesis environment;total synthesis time,
833,APIs and Libraries,UPCY: Safely Updating Outdated Dependencies,A. Dann; B. Hermann; E. Bodden,"CodeShield GmbH, Paderborn, Germany; Technical University Dortmund, Dortmund, Germany; Heinz Nixdorf Institute & Fraunhofer IEM, Paderborn, Germany",2023,"Recent research has shown that developers hesitate to update dependencies and mistrust automated approaches such as Dependabot, since they are afraid of introducing incompatibilities that break their project. In fact, such approaches only suggest naÃ¯ve updates for a single outdated library but do not ensure compatibility with other dependent libraries in the project. To alleviate this situation and support developers in finding updates with minimal incompatibilities, we present UPCY. UPCY applies the min-(s,t)-cut algorithm and leverages a graph database of Maven Central to identify a list of valid update steps to update a dependency to a target version while minimizing incompatibilities with other libraries. By executing 29,698 updates in 380 projects, we compare the effectiveness of UPCY with the naÃ¯ve updates applied by state-of-the-art tools. We find that in 41.1% of the cases where the naÃ¯ve approach fails UPCY generates updates with fewer incompatibilities, and even 70.1% of the generated updates have zero incompatibilities.",10.1109/ICSE48619.2023.00031,Semantic versioning;Library updates;Package management;Dependency management;Software maintenance,Measurement;Java;Databases;Semantics;Static analysis;Maintenance engineering;Libraries,graph theory;software libraries,dependent libraries;generated updates;graph database;Maven Central;minimal incompatibilities;naive updates;single outdated library;support developers;UPCY;valid update steps;zero incompatibilities,
834,APIs and Libraries,APICAD: Augmenting API Misuse Detection through Specifications from Code and Documents,X. Wang; L. Zhao,"Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China; Key Laboratory of Aerospace Information Security and Trusted Computing, Ministry of Education, School of Cyber Science and Engineering, Wuhan University, Wuhan, China",2023,"Using API should follow its specifications. Otherwise, it can bring security impacts while the functionality is damaged. To detect API misuse, we need to know what its specifications are. In addition to being provided manually, current tools usually mine the majority usage in the existing codebase as specifications, or capture specifications from its relevant texts in human language. However, the former depends on the quality of the codebase itself, while the latter is limited to the irregularity of the text. In this work, we observe that the information carried by code and documents can complement each other. To mitigate the demand for a high-quality codebase and reduce the pressure to capture valid information from texts, we present APICAD to detect API misuse bugs of C/C++ by combining the specifications mined from code and documents. On the one hand, we effectively build the contexts for API invocations and mine specifications from them through a frequency-based method. On the other hand, we acquire the specifications from documents by using lightweight keyword-based and NLP-assisted techniques. Finally, the combined specifications are generated for bug detection. Experiments show that APICAD can handle diverse API usage semantics to deal with different types of API misuse bugs. With the help of APICAD, we report 153 new bugs in Curl, Httpd, OpenSSL and Linux kernel, 145 of which have been confirmed and 126 have applied our patches.",10.1109/ICSE48619.2023.00032,,Codes;Text analysis;Linux;Computer bugs;Semantics;Prototypes;Benchmark testing,application program interfaces;data mining;Linux;natural language processing;program debugging;security of data,API invocations;API misuse bugs;API misuse detection;APICAD;bug detection;capture specifications;combined specifications;diverse API usage semantics;high-quality codebase;mine specifications,
835,APIs and Libraries,Compatibility Issue Detection for Android Apps Based on Path-Sensitive Semantic Analysis,S. Yang; S. Chen; L. Fan; S. Xu; Z. Hui; S. Huang,"Command and Control Engineering College, Army Engineering, University of PLA, China; College of Intelligence and Computing, Tianjin University, China; College of Cyber Science, Nankai University, China; College of Cyber Science, Nankai University, China; Academy of Military Science, China; Command and Control Engineering College, Army Engineering, University of PLA, China",2023,"Android API-related compatibility issues have be-come a severe problem and significant challenge for app devel-opers due to the well-known Android fragmentation issues. To address this problem, many effective approaches such as app-based and API lifetime-based methods have been proposed to identify incompatible API usages. However, due to the various implementations of API usages and different API invoking paths, there is still a significant weakness of existing approaches, i.e., introducing a massive number of false positives (FP) and false negatives (FN). To this end, in this paper, we propose PSDroid, an automated compatibility detection approach for Android apps, which aims to reduce FPs and FNs by overcoming several technical bottlenecks. Firstly, we make substantial efforts to carry out a preliminary study to summarize a set of novel API usages with diverse checking implementations. Secondly, we construct a refined API lifetime database by leveraging a semantic resolving analysis on all existing Android SDK frameworks. Based on the above two key phases, we design and implement a novel path-sensitive semantic approach to effectively and automatically detect incompatibility issues. To demonstrate the performance, we compared with five existing approaches (i.e., FicFinder, ACRYL, CIDER, IctAPIFinder, and CID) and the results show that PSDroid outperforms existing tools. We also conducted an in-depth root cause analysis to comprehensively explain the ability of PSDroid in reducing FPs and FNs. Finally, 18/30 reported issues have been confirmed and further fixed by app developers.",10.1109/ICSE48619.2023.00033,Compatibility detection;Android app;Path-sensitive analysis;Semantic analysis,Root cause analysis;Databases;Semantics;Software engineering,Android (operating system);application program interfaces;mobile computing;program testing;smart phones,Android API-related compatibility issues;Android apps;Android fragmentation issues;API lifetime-based methods;app devel-opers;app developers;automated compatibility detection approach;compatibility issue detection;different API invoking paths;diverse checking implementations;existing Android SDK frameworks;false negatives;in-depth root cause analysis;incompatibility issues;incompatible API usages;novel API;novel path-sensitive semantic approach;path-sensitive semantic analysis;PSDroid;refined API lifetime database;semantic resolving analysis;significant weakness,
836,APIs and Libraries,OSSFP: Precise and Scalable C/C++ Third-Party Library Detection using Fingerprinting Functions,J. Wu; Z. Xu; W. Tang; L. Zhang; Y. Wu; C. Liu; K. Sun; L. Zhao; Y. Liu,"School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Software, Tsinghua University, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore",2023,"Third-party libraries (TPLs) are frequently used in software to boost efficiency by avoiding repeated developments. However, the massive using TPLs also brings security threats since TPLs may introduce bugs and vulnerabilities. Therefore, software composition analysis (SCA) tools have been proposed to detect and manage TPL usage. Unfortunately, due to the presence of common and trivial functions in the bloated feature dataset, existing tools fail to precisely and rapidly identify TPLs in C/C++ real-world projects. To this end, we propose OSSFP, a novel SCA framework for effective and efficient TPL detection in large-scale real-world projects via generating unique fingerprints for open source software. By removing common and trivial functions and keeping only the core functions to build the fingerprint index for each TPL project, OSSFP significantly reduces the database size and accelerates the detection process. It also improves TPL detection accuracy since noises are excluded from the fingerprints. We applied OSSFP on a large data set containing 23,427 C/C++ repositories, which included 585,683 versions and 90 billion lines of code. The result showed that it could achieve 90.84% of recall and 90.34% of precision, which outperformed the state-of-the-art tool by 35.31% and 3.71%, respectively. OSSFP took only 0.12 seconds on average to identify all TPLs per project, which was 22 times faster than the other tool. OSSFP has proven to be highly scalable on large-scale datasets.",10.1109/ICSE48619.2023.00034,,Codes;Databases;Computer bugs;Fingerprint recognition;Libraries;Security;Indexes,C++ language;fingerprint identification;program debugging;public domain software;security of data;software libraries,bloated feature dataset;bugs;fingerprint index;fingerprinting functions;open source software;OSSFP;precise scalable C-C++ third-party library detection;SCA;security threats;software composition analysis tools;TPL project;vulnerabilities,
837,Blockchain/Smart contracts,Smartmark: Software Watermarking Scheme for Smart Contracts,T. Kim; Y. Jang; C. Lee; H. Koo; H. Kim,"Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Computer Science and Engineering, Sungkyunkwan University, Suwon, South Korea; Department of Electrical and Computer Engineering, Sungkyunkwan University, Suwon, South Korea",2023,"A smart contract is a self-executing program on a blockchain to ensure an immutable and transparent agreement without the involvement of intermediaries. Despite its growing popularity for many blockchain platforms like Ethereum, no technical means is available even when a smart contract requires to be protected from being copied. One promising direction to claim a software ownership is software watermarking. However, applying existing software watermarking techniques is challenging because of the unique properties of a smart contract, such as a code size constraint, non-free execution cost, and no support for dynamic allocation under a virtual machine environment. This paper introduces a novel software watermarking scheme, dubbed Smartmark, aiming to protect the ownership of a smart contract against a pirate activity. Smartmark builds the control flow graph of a target contract runtime bytecode, and locates a collection of bytes that are randomly elected for representing a watermark. We implement a full-fledged prototype for Ethereum, applying Smartmark to 27,824 unique smart contract bytecodes. Our empirical results demonstrate that Smartmark can effectively embed a watermark into a smart contract and verify its presence, meeting the requirements of credibility and imperceptibility while incurring an acceptable performance degradation. Besides, our security analysis shows that Smartmark is resilient against viable watermarking corruption attacks; e.g., a large number of dummy opcodes are needed to disable a watermark effectively, resulting in producing an illegitimate smart contract clone that is not economical.",10.1109/ICSE48619.2023.00035,Smart contract;Software watermarking;Blockchain;Software copyrights,Resistance;Costs;Runtime;Smart contracts;Prototypes;Watermarking;Software,blockchains;computer crime;contracts;flow graphs;watermarking,blockchain;code size constraint;control flow graph;Ethereum;nonfree execution cost;performance degradation;pirate activity;security analysis;self-executing program;smart contract bytecodes;SmartMark;software ownership;software watermarking;target contract runtime bytecode;watermarking corruption attacks,
838,Blockchain/Smart contracts,Turn the Rudder: A Beacon of Reentrancy Detection for Smart Contracts on Ethereum,Z. Zheng; N. Zhang; J. Su; Z. Zhong; M. Ye; J. Chen,"Sun Yat-sen University, China; Sun Yat-sen University, China; Sun Yat-sen University, China; Sun Yat-sen University, China; Sun Yat-sen University, China; Sun Yat-sen University, China",2023,"Smart contracts are programs deployed on a blockchain and are immutable once deployed. Reentrancy, one of the most important vulnerabilities in smart contracts, has caused millions of dollars in financial loss. Many reentrancy detection approaches have been proposed. It is necessary to investigate the performance of these approaches to provide useful guidelines for their application. In this work, we conduct a large-scale empirical study on the capability of five well-known or recent reentrancy detection tools such as Mythril and Sailfish. We collect 230,548 verified smart contracts from Etherscan and use detection tools to analyze 139,424 contracts after deduplication, which results in 21,212 contracts with reentrancy issues. Then, we manually examine the defective functions located by the tools in the contracts. From the examination results, we obtain 34 true positive contracts with reentrancy and 21,178 false positive contracts without reentrancy. We also analyze the causes of the true and false positives. Finally, we evaluate the tools based on the two kinds of contracts. The results show that more than 99.8% of the reentrant contracts detected by the tools are false positives with eight types of causes, and the tools can only detect the reentrancy issues caused by call.value(), 58.8% of which can be revealed by the Ethereum's official IDE, Remix. Furthermore, we collect real-world reentrancy attacks reported in the past two years and find that the tools fail to find any issues in the corresponding contracts. Based on the findings, existing works on reentrancy detection appear to have very limited capability, and researchers should turn the rudder to discover and detect new reentrancy patterns except those related to call.value().",10.1109/ICSE48619.2023.00036,Smart contract;Reentrancy;Empirical study,Smart contracts;Blockchains;Software engineering;Guidelines,blockchains;contracts;cryptocurrencies,178 false positive contracts;21 contracts;230 contracts;34 true positive contracts;548 verified smart contracts;corresponding contracts;real-world reentrancy attacks;recent reentrancy detection tools;reentrancy detection approaches;reentrancy issues;reentrancy patterns;reentrant contracts,
839,Blockchain/Smart contracts,BSHUNTER: Detecting and Tracing Defects of Bitcoin Scripts,P. Zheng; X. Luo; Z. Zheng,Sun Yat-sen University; The Hong Kong Polytechnic University; Sun Yat-sen University,2023,"Supporting the most popular cryptocurrency, the Bitcoin platform allows its transactions to be programmable via its scripts. Defects in Bitcoin scripts will make users lose their bitcoins. However, there are few studies on the defects of Bitcoin scripts. In this paper, we conduct the first systematic investigation on the defects of Bitcoin scripts through three steps, including defect definition, defect detection, and exploitation tracing. First, we define six typical defects of scripts in Bitcoin history, namely unbinded-txid, simple-key, useless-sig, uncertain-sig, impossible-key, and never-true. Three are inspired by the community, and three are new from us. Second, we develop a tool to discover Bitcoin scripts with any of typical defects based on symbolic execution and enhanced by historical exact scripts. By analyzing all Bitcoin transactions from Oct. 2009 to Aug. 2022, we find that 383,544 transaction outputs are paid to the Bitcoin scripts with defects. The total amount of them is 3,115.43 BTC, which is around 60 million dollars at present. Third, in order to trace the exploitation of the defects, we instrument the Bitcoin VM to record the traces of the real-world spending transactions of the buggy scripts. We find that 84,130 output scripts are exploited. The implementation and non-harmful datasets are released.",10.1109/ICSE48619.2023.00037,bitcoin;blockchain;smart contract,Systematics;Instruments;Bitcoin;History;Contracts;Software engineering,blockchains;cryptocurrencies,Bitcoin scripts;Bitcoin transactions;Bitcoin VM;BSHUNTER;buggy scripts;cryptocurrency;defect detection;impossible-key;never-true;simple-key;typical defects;unbinded-txid;uncertain-sig;useless-sig,
840,Cognitive aspects of software development,Do I Belong? Modeling Sense of Virtual Community Among Linux Kernel Contributors,B. Trinkenreich; K. -J. Stol; A. Sarma; D. M. German; M. A. Gerosa; I. Steinmacher,"Northern Arizona University, Flagstaff, AZ, USA; Lero, the SFI Research Centre for Software, University College Cork, Ireland; Oregon State University, Portland, OR, USA; University of Victoria, Victoria, Canada; Northern Arizona University, Flagstaff, AZ, USA; Northern Arizona University, Flagstaff, AZ, USA",2023,"The sense of belonging to a community is a basic human need that impacts an individual's behavior, long-term engagement, and job satisfaction, as revealed by research in disciplines such as psychology, healthcare, and education. Despite much research on how to retain developers in Open Source Software (OSS) projects and other virtual, peer-production communities, there is a paucity of research investigating what might contribute to a sense of belonging in these communities. To that end, we develop a theoretical model that seeks to understand the link between OSS developer motives and a Sense of Virtual Community (SVC). We test the model with a dataset collected in the Linux Kernel developer community (N=225), using structural equation modeling techniques. Our results for this case study show that intrinsic motivations (social or hedonic motives) are positively associated with a sense of virtual community, but living in an authoritative country and being paid to contribute can reduce the sense of virtual community. Based on these results, we offer suggestions for open source projects to foster a sense of virtual community, with a view to retaining contributors and Improving projects' sustainability.",10.1109/ICSE48619.2023.00038,sense of virtual community;belonging;open source;software developers;human factors;survey;PLS-SEM,Surveys;Linux;Static VAr compensators;Psychology;Medical services;Mathematical models;Kernel,human factors;Linux;operating system kernels;psychology;public domain software,hedonic motive;intrinsic motivation;Linux kernel contributors;Linux Kernel developer community;open source software projects;OSS;peer-production communities;Sense of Virtual Community;social motive;SVC,
841,Code smells and clones,Comparison and Evaluation of Clone Detection Techniques with Different Code Representations,Y. Wang; Y. Ye; Y. Wu; W. Zhang; Y. Xue; Y. Liu,"University of Science and Technology of China, China; University of Science and Technology of China, China; Nanyang Technological University, Singapore; University of Science and Technology of China, China; University of Science and Technology of China, China; Nanyang Technological University, Singapore",2023,"As one of bad smells in code, code clones may increase the cost of software maintenance and the risk of vulnerability propagation. In the past two decades, numerous clone detection technologies have been proposed. They can be divided into text-based, token-based, tree-based, and graph-based approaches according to their code representations. Different code representations abstract the code details from different perspectives. However, it is unclear which code representation is more effective in detecting code clones and how to combine different code representations to achieve ideal performance. In this paper, we present an empirical study to compare the clone detection ability of different code representations. Specifically, we reproduce 12 clone detection algorithms and divide them into different groups according to their code representations. After analyzing the empirical results, we find that token and tree representations can perform better than graph representation when detecting simple code clones. However, when the code complexity of a code pair increases, graph representation becomes more effective. To make our findings more practical, we perform manual analysis on open-source projects to seek a possible distribution of different clone types in the open-source community. Through the results, we observe that most clone pairs belong to simple code clones. Based on this observation, we discard heavyweight graph-based clone detection algorithms and conduct combination experiments to find out a suitable combination of token-based and tree-based approaches for achieving scalable and effective code clone detection. We develop the suitable combination into a tool called TACC and evaluate it with other state-of-the-art code clone detectors. Experimental results indicate that TACC performs better and has the ability to detect large-scale code clones.",10.1109/ICSE48619.2023.00039,Clone Detection;Empirical Study;Code Representation;Large Scale,Software maintenance;Codes;Costs;Scalability;Cloning;Manuals;Detectors,program testing;software maintenance;trees (mathematics),clone pairs;clone types;code clone detection;code complexity;code representation;graph representation;graph-based clone detection algorithms;large-scale code clones;software maintenance;TACC tool;token representations;tree-based approaches;vulnerability propagation,
842,Code smells and clones,Learning Graph-based Code Representations for Source-level Functional Similarity Detection,J. Liu; J. Zeng; X. Wang; Z. Liang,National University of Singapore; National University of Singapore; University of Science and Technology of China; University of Science and Technology of China,2023,"Detecting code functional similarity forms the basis of various software engineering tasks. However, the detection is challenging as functionally similar code fragments can be implemented differently, e.g., with irrelevant syntax. Recent studies incorporate program dependencies as semantics to identify syntactically different yet semantically similar programs, but they often focus only on local neighborhoods (e.g., one-hop dependencies), limiting the expressiveness of program semantics in modeling functionalities. In this paper, we present Tailor that explicitly exploits deep graph-structured code features for functional similarity detection. Given source-level programs, Tailor first represents them into code property graphs (CPGs) - which combine abstract syntax trees, control flow graphs, and data flow graphs - to collectively reason about program syntax and semantics. Then, Tailor learns representations of CPGs by applying a CPG-based neural network (CPGNN) to iteratively propagate information on them. It improves over prior work on code representation learning through a new graph neural network (GNN) tailored to CPG structures instead of the off-the-shelf GNNs used previously. We systematically evaluate Tailor on C and Java programs using two public benchmarks. Experimental results show that Tailor outperforms the state-of-the-art approaches, achieving 99.8% and 99.9% F-scores in code clone detection and 98.3% accuracy in source code classification.",10.1109/ICSE48619.2023.00040,,Representation learning;Codes;Source coding;Semantics;Cloning;Syntactics;Graph neural networks,data flow graphs;graph neural networks;Java;learning (artificial intelligence);source code (software),abstract syntax trees;code clone detection;code functional similarity;code property graphs;code representation;control flow graphs;CPG-based neural network;data flow graphs;deep graph-structured code features;functionally similar code fragments;graph neural network;graph-based code representations;irrelevant syntax;modeling functionalities;one-hop dependencies;program dependencies;program semantics;program syntax;software engineering tasks;source code classification;source-level functional similarity detection;source-level programs;Tailor,
843,Code smells and clones,The Smelly Eight: An Empirical Study on the Prevalence of Code Smells in Quantum Computing,Q. Chen; R. CÃ¢mara; J. Campos; A. Souto; I. Ahmed,"University of California, Irvine, USA; LASIGE, Faculdade de CiÃªncias, Universidade de Lisboa, Lisboa, Portugal; Faculty of Engineering, University of Porto, Porto, Portugal; LASIGE, Faculdade de CiÃªncias, Universidade de Lisboa, Lisboa, Portugal; University of California, Irvine, USA",2023,"Quantum Computing (QC) is a fast-growing field that has enhanced the emergence of new programming languages and frameworks. Furthermore, the increased availability of computational resources has also contributed to an influx in the development of quantum programs. Given that classical and QC are significantly different due to the intrinsic nature of quantum programs, several aspects of QC (e.g., performance, bugs) have been investigated, and novel approaches have been proposed. However, from a purely quantum perspective, maintenance, one of the major steps in a software development life-cycle, has not been considered by researchers yet. In this paper, we fill this gap and investigate the prevalence of code smells in quantum programs as an indicator of maintenance issues. We defined eight quantum-specific smells and validated them through a survey with 35 quantum developers. Since no tool specifically aims to detect quantum smells, we developed a tool called QSmell that supports the proposed quantum-specific smells. Finally, we conducted an empirical investigation to analyze the prevalence of quantum-specific smells in 15 open-source quantum programs. Our results showed that 11 programs (73.33%) contain at least one smell and, on average, a program has three smells. Furthermore, the long circuit is the most prevalent smell present in 53.33% of the programs.",10.1109/ICSE48619.2023.00041,Quantum computing;Quantum software engineering;Empirical study;Quantum-specific code smell,Surveys;Computer languages;Quantum computing;Codes;Computer bugs;Maintenance engineering;Software,quantum computing,code smells;computational resources;open-source quantum programs;programming languages;quantum computing;quantum developers;quantum smells;quantum-specific smells;software development life-cycle,1
844,Fuzzing: Techniques and tools,Reachable Coverage: Estimating Saturation in Fuzzing,D. Liyanage; M. BÃ¶hme; C. Tantithamthavorn; S. Lipp,"Monash University, Australia; MPI-SP, Germany; Monash University, Australia; TU Munich, Germany",2023,"Reachable coverage is the number of code elements in the search space of a fuzzer (i.e., an automatic software testing tool). A fuzzer cannot find bugs in code that is unreachable. Hence, reachable coverage quantifies fuzzer effectiveness. Using static program analysis, we can compute an upper bound on the number of reachable coverage elements, e.g., by extracting the call graph. However, we cannot decide whether a coverage element is reachable in general. If we could precisely determine reachable coverage efficiently, we would have solved the software verification problem. Unfortunately, we cannot approach a given degree of accuracy for the static approximation, either. In this paper, we advocate a statistical perspective on the approximation of the number of elements in the fuzzer's search space, where accuracy does improve as a function of the analysis runtime. In applied statistics, corresponding estimators have been developed and well established for more than a quarter century. These estimators hold an exciting promise to finally tackle the long-standing challenge of counting reachability. In this paper, we explore the utility of these estimators in the context of fuzzing. Estimates of reachable coverage can be used to measure (a) the amount of untested code, (b) the effectiveness of the testing technique, and (c) the completeness of the ongoing fuzzing campaign (w.r.t. the asymptotic max. achievable coverage). We make all data and our analysis publicly available.",10.1109/ICSE48619.2023.00042,,Codes;Upper bound;Runtime;Computer bugs;Fuzzing;Software,program diagnostics;program testing;program verification;statistical analysis,counting reachability;coverage element;fuzzer;reachable coverage,1
845,Fuzzing: Techniques and tools,Learning Seed-Adaptive Mutation Strategies for Greybox Fuzzing,M. Lee; S. Cha; H. Oh,Korea University; Sungkyunkwan University; Korea University,2023,"In this paper, we present a technique for learning seed-adaptive mutation strategies for fuzzers. The performance of mutation-based fuzzers highly depends on the mutation strategy that specifies the probability distribution of selecting mutation methods. As a result, developing an effective mutation strategy has received much attention recently, and program-adaptive techniques, which observe the behavior of the target program to learn the optimized mutation strategy per program, have become a trending approach to achieve better performance. They, however, still have a major limitation; they disregard the impacts of different characteristics of seed inputs which can lead to explore deeper program locations. To address this limitation, we present SEAMFUZZ, a novel fuzzing technique that automatically captures the characteristics of individual seed inputs and applies different mutation strategies for different seed inputs. By capturing the syntactic and semantic similarities between seed inputs, SEAMFUZZ clusters them into proper groups and learns effective mutation strategies tailored for each seed cluster by using the customized Thompson sampling algorithm. Experimental results show that SEAMFUZZ improves both the path-discovering and bug-finding abilities of state-of-the-art fuzzers on real-world programs.",10.1109/ICSE48619.2023.00043,Fuzzing;Software Testing,Semantics;Clustering algorithms;Syntactics;Fuzzing;Probability distribution;Behavioral sciences;Software engineering,fuzzy set theory;optimisation;pattern clustering;probability;program debugging;program testing,greybox fuzzing;mutation-based fuzzers;novel fuzzing technique;optimized mutation strategy;probability distribution;program-adaptive techniques;SEAMFUZZ clusters;seed-adaptive mutation strategies;Thompson sampling algorithm,
846,Fuzzing: Techniques and tools,Improving Java Deserialization Gadget Chain Mining via Overriding-Guided Object Generation,S. Cao; X. Sun; X. Wu; L. Bo; B. Li; R. Wu; W. Liu; B. He; Y. Ouyang; J. Li,Yangzhou University; Yangzhou University; Yangzhou University; Yangzhou University; Yangzhou University; Xiamen University; Yangzhou University; Ant Group; Ant Group; Ant Group,2023,"Java (de)serialization is prone to causing security-critical vulnerabilities that attackers can invoke existing methods (gadgets) on the application's classpath to construct a gadget chain to perform malicious behaviors. Several techniques have been proposed to statically identify suspicious gadget chains and dynamically generate injection objects for fuzzing. However, due to their incomplete support for dynamic program features (e.g., Java runtime polymorphism) and ineffective injection object generation for fuzzing, the existing techniques are still far from satisfactory. In this paper, we first performed an empirical study to investigate the characteristics of Java deserialization vulnerabilities based on our manually collected 86 publicly known gadget chains. The empirical results show that 1) Java deserialization gadgets are usually exploited by abusing runtime polymorphism, which enables attackers to reuse serializable overridden methods; and 2) attackers usually invoke exploitable overridden methods (gadgets) via dynamic binding to generate injection objects for gadget chain construction. Based on our empirical findings, we propose a novel gadget chain mining approach, GCMiner, which captures both explicit and implicit method calls to identify more gadget chains, and adopts an overriding-guided object generation approach to generate valid injection objects for fuzzing. The evaluation results show that GCMiner significantly outperforms the state-of-the-art techniques, and discovers 56 unique gadget chains that cannot be identified by the baseline approaches.",10.1109/ICSE48619.2023.00044,Java deserialization vulnerability;gadget chain;method overriding;exploit generation,Java;Runtime;Fuzzing;Behavioral sciences;Object recognition,computer crime;data mining;Java;object-oriented programming;program testing,dynamic program features;exploitable overridden methods;fuzzing;GCMiner;Java deserialization gadget chain mining;Java deserialization vulnerabilities;overriding-guided object generation;security-critical vulnerabilities;serializable overridden methods,2
847,Fuzzing: Techniques and tools,Evaluating and Improving Hybrid Fuzzing,L. Jiang; H. Yuan; M. Wu; L. Zhang; Y. Zhang,"Southern University of Science and Technology, Shenzhen, China; Southern University of Science and Technology, Shenzhen, China; Southern University of Science and Technology, Shenzhen, China; University of Illinois Urbana-Champaign, Champaign, USA; Southern University of Science and Technology, Shenzhen, China",2023,"To date, various hybrid fuzzers have been proposed for maximal program vulnerability exposure by integrating the power of fuzzing strategies and concolic executors. While the existing hybrid fuzzers have shown their superiority over conventional coverage-guided fuzzers, they seldom follow equivalent evaluation setups, e.g., benchmarks and seed corpora. Thus, there is a pressing need for a comprehensive study on the existing hybrid fuzzers to provide implications and guidance for future research in this area. To this end, in this paper, we conduct the first extensive study on state-of-the-art hybrid fuzzers. Surprisingly, our study shows that the performance of existing hybrid fuzzers may not well generalize to other experimental settings. Meanwhile, their performance advantages over conventional coverage-guided fuzzers are overall limited. In addition, instead of simply updating the fuzzing strategies or concolic executors, updating their coordination modes potentially poses crucial performance impact of hybrid fuzzers. Accordingly, we propose CoFuzz to improve the effectiveness of hybrid fuzzers by upgrading their coordination modes. Specifically, based on the baseline hybrid fuzzer QSYM, CoFuzz adopts edge-oriented scheduling to schedule edges for applying concolic execution via an online linear regression model with Stochastic Gradient Descent. It also adopts sampling-augmenting synchronization to derive seeds for applying fuzzing strategies via the interval path abstraction and John walk as well as incrementally updating the model. Our evaluation results indicate that CoFuzz can significantly increase the edge coverage (e.g., 16.31% higher than the best existing hybrid fuzzer in our study) and expose around 2X more unique crashes than all studied hybrid fuzzers. Moreover, CoFuzz successfully detects 37 previously unknown bugs where 30 are confirmed with 8 new CVEs and 20 are fixed.",10.1109/ICSE48619.2023.00045,,Schedules;Image edge detection;Computer bugs;Linear regression;Stochastic processes;Pressing;Fuzzing,gradient methods;program diagnostics;program testing;regression analysis;stochastic processes,concolic executors;coverage-guided fuzzers;edge-oriented scheduling;fuzzing strategies;hybrid fuzzer QSYM;interval path abstraction;John walk;online linear regression model;sampling-augmenting synchronization;stochastic gradient descent,1
848,Software architectures and designs,Robustification of Behavioral Designs against Environmental Deviations,C. Zhang; T. Saluja; R. Meira-GÃ³es; M. Bolton; D. Garlan; E. Kang,"Carnegie Mellon University, Pittsburgh, PA, USA; Swarthmore College, Swarthmore, PA, USA; The Pennsylvania State University, State College, PA, USA; University of Virginia, Charlottesville, VA, USA; Carnegie Mellon University, Pittsburgh, PA, USA; Carnegie Mellon University, Pittsburgh, PA, USA",2023,"Modern software systems are deployed in a highly dynamic, uncertain environment. Ideally, a system that is robust should be capable of establishing its most critical requirements even in the presence of possible deviations in the environment. We propose a technique called behavioral robustification, which involves systematically and rigorously improving the robustness of a design against potential deviations. Given behavioral models of a system and its environment, along with a set of user-specified deviations, our robustification method produces a redesign that is capable of satisfying a desired property even when the environment exhibits those deviations. In particular, we describe how the robustification problem can be formulated as a multi-objective optimization problem, where the goal is to restrict the deviating environment from causing a violation of a desired property, while maximizing the amount of existing functionality and minimizing the cost of changes to the original design. We demonstrate the effectiveness of our approach on case studies involving the robustness of an electronic voting machine and safety-critical interfaces.",10.1109/ICSE48619.2023.00046,robustness;robustification;labeled transition systems,Costs;Software systems;Robustness;Behavioral sciences;Electronic voting;Optimization;Software engineering,government data processing;optimisation;robust control;safety-critical software,behavioral designs;behavioral robustification;desired property;deviating environment;environmental deviations;given behavioral models;highly dynamic environment;modern software systems;multiobjective optimization problem;possible deviations;potential deviations;robustification method;robustification problem;safety-critical interfaces;uncertain environment;user-specified deviations,
849,Software architectures and designs,A Qualitative Study on the Implementation Design Decisions of Developers,J. T. Liang; M. Arab; M. Ko; A. J. Ko; T. D. LaToza,"School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Department of Computer Science, George Mason University, Fairfax, VA, USA; Department of Computer Science, Virginia Tech, Blacksburg, VA, USA; Information School, University of Washington, Seattle, WA, USA; Department of Computer Science, George Mason University, Fairfax, VA, USA",2023,"Decision-making is a key software engineering skill. Developers constantly make choices throughout the software development process, from requirements to implementation. While prior work has studied developer decision-making, the choices made while choosing what solution to write in code remain understudied. In this mixed-methods study, we examine the phenomenon where developers select one specific way to implement a behavior in code, given many potential alternatives. We call these decisions implementation design decisions. Our mixed-methods study includes 46 survey responses and 14 semi-structured interviews with professional developers about their decision types, considerations, processes, and expertise for implementation design decisions. We find that implementation design decisions, rather than being a natural outcome from higher levels of design, require constant monitoring of higher level design choices, such as requirements and architecture. We also show that developers have a consistent general structure to their implementation decision-making process, but no single process is exactly the same. We discuss the implications of our findings on research, education, and practice, including insights on teaching developers how to make implementation design decisions.",10.1109/ICSE48619.2023.00047,implementation design decisions;software design,Surveys;Codes;Decision making;Education;Computer architecture;Software;Interviews,decision making;software engineering;teaching,decision types;higher level design choices;implementation decision-making process;implementation design decisions;mixed-methods study;professional developers;software development process;teaching developers,
850,Software security and privacy,BFTDETECTOR: Automatic Detection of Business Flow Tampering for Digital Content Service,I. L. Kim; W. Wang; Y. Kwon; X. Zhang,"Department of Computer Science, Purdue University, West Lafayette, USA; University of Southern California, Los Angeles, USA; Department of Computer Science, Purdue University, West Lafayette, USA; Department of Computer Science, Purdue University, West Lafayette, USA",2023,"Digital content services provide users with a wide range of content, such as news, articles, or movies, while monetizing their content through various business models and promotional methods. Unfortunately, poorly designed or unpro-tected business logic can be circumvented by malicious users, which is known as business flow tampering. Such flaws can severely harm the businesses of digital content service providers. In this paper, we propose an automated approach that discov-ers business flow tampering flaws. Our technique automatically runs a web service to cover different business flows (e.g., a news website with vs. without a subscription paywall) to collect execution traces. We perform differential analysis on the execution traces to identify divergence points that determine how the business flow begins to differ, and then we test to see if the divergence points can be tampered with. We assess our approach against 352 real-world digital content service providers and discover 315 flaws from 204 websites, including TIME, Fortune, and Forbes. Our evaluation result shows that our technique successfully identifies these flaws with low false-positive and false-negative rates of 0.49% and 1.44%, respectively.",10.1109/ICSE48619.2023.00048,JavaScript;business flow tampering;dynamic analysis;vulnerability detection,Fault diagnosis;Analytical models;Web services;Software algorithms;Motion pictures;Business;Software engineering,computer network security;Internet;Web services;Web sites,automatic detection;business models;different business flows;discov-ers business flow tampering flaws;divergence points;execution traces;malicious users;real-world digital content service providers;unpro-tected business logic;web service,
851,Software security and privacy,FedSlice: Protecting Federated Learning Models from Malicious Participants with Model Slicing,Z. Zhang; Y. Li; B. Liu; Y. Cai; D. Li; Y. Guo; X. Chen,"Key Laboratory of High-Confidence Software Technologies (MOE), School of Computer Science, Peking University; Institute for AI Industry Research (AIR), Tsinghua University; School of Computer Science, Beijing University of Posts and Telecommunications; Key Laboratory of High-Confidence Software Technologies (MOE), School of Computer Science, Peking University; Key Laboratory of High-Confidence Software Technologies (MOE), School of Computer Science, Peking University; Key Laboratory of High-Confidence Software Technologies (MOE), School of Computer Science, Peking University; Key Laboratory of High-Confidence Software Technologies (MOE), School of Computer Science, Peking University",2023,"Crowdsourcing Federated learning (CFL) is a new crowdsourcing development paradigm for the Deep Neural Network (DNN) models, also called â€œsoftware 2.0â€. In practice, the privacy of CFL can be compromised by many attacks, such as free-rider attacks, adversarial attacks, gradient leakage attacks, and inference attacks. Conventional defensive techniques have low efficiency because they deploy heavy encryption techniques or rely on Trusted Execution Environments (TEEs). To improve the efficiency of protecting CFL from these attacks, this paper proposes FedSlice to prevent malicious participants from getting the whole server-side model while keeping the performance goal of CFL. FedSlice breaks the server-side model into several slices and delivers one slice to each participant. Thus, a malicious participant can only get a subset of the server-side model, preventing them from effectively conducting effective attacks. We evaluate FedSlice against these attacks, and results show that FedSlice provides effective defense: the server-side model leakage is reduced from 100% to 43.45%, the success rate of adversarial attacks is reduced from 100% to 11.66%, the average accuracy of membership inference is reduced from 71.91% to 51.58%, and the data leakage from shared gradients is reduced to the level of random guesses. Besides, FedSlice only introduces less than 2% accuracy loss and about 14% computation overhead. To the best of our knowledge, this is the first paper to discuss defense methods against these attacks to the CFL framework.",10.1109/ICSE48619.2023.00049,Deep Neural Networks;Software Engineering;Crowdsourcing;Federated Learning,Crowdsourcing;Privacy;Computational modeling;Artificial neural networks;Software;Data models;Encryption,blockchains;computer network security;crowdsourcing;cryptography;data privacy;deep learning (artificial intelligence);learning (artificial intelligence),adversarial attacks;CFL framework;conventional defensive techniques;crowdsourcing development paradigm;Deep Neural Network models;effective attacks;FedSlice;free-rider attacks;gradient leakage attacks;heavy encryption techniques;inference attacks;malicious participant;model slicing;protecting Federated learning models;server-side model leakage,
852,Software security and privacy,PTPDroid: Detecting Violated User Privacy Disclosures to Third-Parties of Android Apps,Z. Tan; W. Song,"School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China; School of Computer Science and Engineering, Nanjing University of Science and Technology, Nanjing, China",2023,"Android apps frequently access personal information to provide customized services. Since such information is sensitive in general, regulators require Android app vendors to publish privacy policies that describe what information is collected and why it is collected. Existing work mainly focuses on the types of the collected data but seldom considers the entities that collect user privacy, which could falsely classify problematic declarations about user privacy collected by third-parties into clear disclosures. To address this problem, we propose PTPDroid, a flow-to-policy consistency checking approach and an automated tool, to comprehensively uncover from the privacy policy the violated disclosures to third-parties. Our experiments on real-world apps demonstrate the effectiveness and superiority of PTPDroid, and our empirical study on 1,000 popular real-world apps reveals that violated user privacy disclosures to third-parties are prevalent in practice.",10.1109/ICSE48619.2023.00050,Android app;privacy policy;third-party entities;violation detection;taint analysis;empirical study,Privacy;Data privacy;Codes;Regulators;Static analysis;Benchmark testing;Natural language processing,Android (operating system);data privacy;mobile computing,Android app vendors;automated tool;customized services;flow-to-policy consistency checking;personal information access;privacy policy;PTPDroid;real-world apps;third-parties;violated user privacy disclosure detection,
853,Software security and privacy,Adhere: Automated Detection and Repair of Intrusive Ads,Y. Yan; Y. Zheng; X. Liu; N. Medvidovic; W. Wang,"University of Southern California, Los Angeles, CA, USA; IBM T. J. Watson Research Center, Yorktown Heights, NY, USA; University at Buffalo, SUNY, Buffalo, NY, USA; University of Southern California, Los Angeles, CA, USA; University of Southern California, Los Angeles, CA, USA",2023,"Today, more than 3 million websites rely on online advertising revenue. Despite the monetary incentives, ads often frustrate users by disrupting their experience, interrupting content, and slowing browsing. To improve ad experiences, leading media associations define Better Ads Standards for ads that are below user expectations. However, little is known about how well websites comply with these standards and whether existing approaches are sufficient for developers to quickly resolve such issues. In this paper, we propose Adhere, a technique that can detect intrusive ads that do not comply with Better Ads Standards and suggest repair proposals. Adhere works by first parsing the initial web page to a DOM tree to search for potential static ads, and then using mutation observers to monitor and detect intrusive (dynamic/static) ads on the fly. To handle ads' volatile nature, Adhere includes two detection algorithms for desktop and mobile ads to identify different ad violations during three phases of page load events. It recursively applies the detection algorithms to resolve nested layers of DOM elements inserted by ad delegations. We evaluate Adhere on Alexa Top 1 Million Websites. The results show that Adhere is effective in detecting violating ads and suggesting repair proposals. Comparing to the current available alternative, Adhere detected intrusive ads on 4,656 more mobile websites and 3,911 more desktop websites, and improved recall by 16.6% and accuracy by 4.2%.",10.1109/ICSE48619.2023.00051,ad experience;advertising practice;Better Ads Standards,Virtual assistants;Web pages;Maintenance engineering;Observers;Media;Proposals;Detection algorithms,advertising data processing;Web sites,ad delegations;ad experiences;ad violations;ads standards;Alexa Top 1 Million Web sites;desktop ads;desktop Web sites;detection algorithms;DOM elements;initial Web page;intrusive ads;mobile ads;mobile Web sites;monetary incentives;online advertising revenue;potential static ads;repair proposals;user expectations,
854,Software security and privacy,Bad Snakes: Understanding and Improving Python Package Index Malware Scanning,D. -L. Vu; Z. Newman; J. S. Meyers,Chainguard and FPT University; Chainguard; Chainguard,2023,"Open-source, community-driven package repositories see thousands of malware packages each year, but do not currently run automated malware detection systems. In this work, we explore the security goals of the repository administrators and the requirements for deploying such malware scanners via a case study of the Python ecosystem and PyPI repository, including interviews with administrators and maintainers. Further, we evaluate existing malware detection techniques for deployment in this setting by creating a benchmark dataset and comparing several existing tools: the malware checks implemented in PyPI, Bandit4Mal, and OSSGadget's OSS Detect Backdoor. We find that repository administrators have exacting requirements for such malware detection tools. Specifically, they consider a false positive rate of even 0.1% to be unacceptably high, given the large number of package releases that might trigger false alerts. Measured tools have false positive rates between 15% and 97%; increasing thresholds for detection rules to reduce this rate renders the true positive rate useless. While automated tools are far from reaching these demands, we find that a socio-technical malware detection system has emerged to meet these needs: external security researchers perform repository malware scans, filter for useful results, and report the results to repository administrators. These parties face different incentives and constraints on their time and tooling. We conclude with recommendations for improving detection capabilities and strengthening the collaboration between security researchers and software repository administrators.",10.1109/ICSE48619.2023.00052,Open-source software (OSS) Supply Chain;Malware Detection;PyPI;Qualitative Study;Quantitative Study,Ecosystems;Malware;Security;Time factors;Indexes;Interviews;Open source software,invasive software;Python;software packages,automated malware detection systems;bad snakes;Bandit4Mal;community-driven package repositories;false alerts;false positive rate;malware checks;open-source;OSSGadget OSS Detect Backdoor;PyPI;Python ecosystem;Python package index malware scanning;security goals;socio-technical malware detection system;software repository administrators,
855,AI systems engineering,FedDebug: Systematic Debugging for Federated Learning Applications,W. Gill; A. Anwar; M. A. Gulzar,"Computer Science Department, Virginia Tech, Blacksburg, USA; Computer Science and Engineering Department, University of Minnesota Twin Cities, Minneapolis, USA; Computer Science Department, Virginia Tech, Blacksburg, USA",2023,"In Federated Learning (FL), clients independently train local models and share them with a central aggregator to build a global model. Impermissibility to access clients' data and collaborative training make FL appealing for applications with data-privacy concerns, such as medical imaging. However, these FL characteristics pose unprecedented challenges for debugging. When a global model's performance deteriorates, identifying the responsible rounds and clients is a major pain point. Developers resort to trial-and-error debugging with subsets of clients, hoping to increase the global model's accuracy or let future FL rounds retune the model, which are time-consuming and costly. We design a systematic fault localization framework, Fedde-bug,that advances the FL debugging on two novel fronts. First, Feddebug enables interactive debugging of realtime collaborative training in FL by leveraging record and replay techniques to construct a simulation that mirrors live FL. Feddebug'sbreakpoint can help inspect an FL state (round, client, and global model) and move between rounds and clients' models seam-lessly, enabling a fine-grained step-by-step inspection. Second, Feddebug automatically identifies the client(s) responsible for lowering the global model's performance without any testing data and labels-both are essential for existing debugging techniques. Feddebug's strengths come from adapting differential testing in conjunction with neuron activations to determine the client(s) deviating from normal behavior. Feddebug achieves 100% accuracy in finding a single faulty client and 90.3% accuracy in finding multiple faulty clients. Feddebug's interactive de-bugging incurs 1.2% overhead during training, while it localizes a faulty client in only 2.1% of a round's training time. With FedDebug,we bring effective debugging practices to federated learning, improving the quality and productivity of FL application developers.",10.1109/ICSE48619.2023.00053,software debugging;federated learning;testing;client;fault localization;neural networks;CNN,Training;Location awareness;Fault diagnosis;Adaptation models;Systematics;Federated learning;Collaboration,data privacy;inspection;learning (artificial intelligence);program debugging,-error debugging;access clients;data-privacy concerns;debugging techniques;effective debugging practices;FedDebug;Feddebug;Federated Learning applications;FL application developers;FL characteristics;FL debugging;FL state;future FL rounds;global model;interactive debugging;live FL;local models;multiple faulty clients;realtime collaborative training;single faulty client;systematic debugging;systematic fault localization framework,
856,AI systems engineering,Practical and Efficient Model Extraction of Sentiment Analysis APIs,W. Wu; J. Zhang; V. J. Wei; X. Chen; Z. Zheng; I. King; M. R. Lyu,"School of Software Engineering, Sun Yat-sen University; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computing, The Hong Kong Polytechnic University; Tencent; School of Software Engineering, Sun Yat-sen University; Department of Computer Science and Engineering, The Chinese University of Hong Kong; Department of Computer Science and Engineering, The Chinese University of Hong Kong",2023,"Despite their stunning performance, developing deep learning models from scratch is a formidable task. Therefore, it popularizes Machine-Learning-as-a-Service (MLaaS), where general users can access the trained models of MLaaS providers via Application Programming Interfaces (APIs) on a pay-per-query basis. Unfortunately, the success of MLaaS is under threat from model extraction attacks, where attackers intend to extract a local model of equivalent functionality to the target MLaaS model. However, existing studies on model extraction of text analytics APIs frequently assume adversaries have strong knowledge about the victim model, like its architecture and parameters, which hardly holds in practice. Besides, since the attacker's and the victim's training data can be considerably discrepant, it is non-trivial to perform efficient model extraction. In this paper, to advance the understanding of such attacks, we propose a framework, PEEP, for practical and efficient model extraction of sentiment analysis APIs with only query access. Specifically, PEEP features a learning-based scheme, which employs out-of-domain public corpora and a novel query strategy to construct proxy training data for model extraction. Besides, PEEP introduces a greedy search algorithm to settle an appropriate architecture for the extracted model. We conducted extensive experiments with two victim models across three datasets and two real-life commercial sentiment analysis APIs. Experimental results corroborate that PEEP can consistently outperform the state-of-the-art baselines in terms of effectiveness and efficiency.",10.1109/ICSE48619.2023.00054,model extraction;sentiment analysis APIs;active learning;architecture search,Training;Analytical models;Sentiment analysis;Training data;Computer architecture;Feature extraction;Data models,application program interfaces;cloud computing;deep learning (artificial intelligence);learning (artificial intelligence);sentiment analysis,deep learning models;efficient model extraction;extracted model;model extraction attacks;real-life commercial sentiment analysis APIs;target MLaaS model;text analytics APIs;trained models;victim model,
857,AI systems engineering,CrossCodeBench: Benchmarking Cross-Task Generalization of Source Code Models,C. Niu; C. Li; V. Ng; B. Luo,"State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing; Human Language Technology Research Institute, University of Texas at Dallas, Richardson, Texas, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing",2023,"Despite the recent advances showing that a model pre-trained on large-scale source code data is able to gain appreciable generalization capability, it still requires a sizeable amount of data on the target task for fine-tuning. And the effectiveness of the model generalization is largely affected by the size and quality of the fine-tuning data, which is detrimental for target tasks with limited or unavailable resources. Therefore, cross-task generalization, with the goal of improving the generalization of the model to unseen tasks that have not been seen before, is of strong research and application value. In this paper, we propose a large-scale benchmark that includes 216 existing code-related tasks. Then, we annotate each task with the corresponding meta information such as task description and instruction, which contains detailed information about the task and a solution guide. This also helps us to easily create a wide variety of â€œtraining/evaluationâ€ task splits to evaluate the various cross-task generalization capabilities of the model. Then we perform some preliminary experiments to demonstrate that the cross-task generalization of models can be largely improved by in-context learning methods such as few-shot learning and learning from task instructions, which shows the promising prospects of conducting cross-task learning research on our benchmark. We hope that the collection of the datasets and our benchmark will facilitate future work that is not limited to cross-task generalization.",10.1109/ICSE48619.2023.00055,Pre-training of source code;cross-task transfer learning;few-shot learning;AI for SE,Learning systems;Deep learning;Source coding;Benchmark testing;Software;Data models;Task analysis,learning (artificial intelligence);source code (software),benchmarking cross-task generalization;code-related tasks;conducting cross-task learning research;cross-task generalization capabilities;fine-tuning data;large-scale benchmark;large-scale source code data;model generalization;source code models;target task;task description;task instructions,
858,Debugging,ECSTATIC: An Extensible Framework for Testing and Debugging Configurable Static Analysis,A. Mordahl; Z. Zhang; D. Soles; S. Wei,"Department of Computer Science, The University of Texas at Dallas, Richardson, TX, USA; Department of Computer Science, The University of Texas at Dallas, Richardson, TX, USA; Department of Computer Science, The University of Texas at Dallas, Richardson, TX, USA; Department of Computer Science, The University of Texas at Dallas, Richardson, TX, USA",2023,"Testing and debugging the implementation of static analysis is a challenging task, often involving significant manual effort from domain experts in a tedious and unprincipled process. In this work, we propose an approach that greatly improves the automation of this process for static analyzers with configuration options. At the core of our approach is the novel adaptation of the theoretical partial order relations that exist between these options to reason about the correctness of actual results from running the static analyzer with different configurations. This allows for automated testing of static analyzers with clearly defined oracles, followed by automated delta debugging, even in cases where ground truths are not defined over the input programs. To apply this approach to many static analysis tools, we design and implement ECSTATIC, an easy-to-extend, open-source framework. We have integrated four popular static analysis tools, SOOT, WALA, DOOP, and FlowDroid, into ECSTATIC. Our evaluation shows running ECSTATIC detects 74 partial order bugs in the four tools and produces reduced bug-inducing programs to assist debugging. We reported 42 bugs; in all cases where we received responses, the tool developers confirmed the reported tool behavior was unintended. So far, three bugs have been fixed and there are ongoing discussions to fix more.",10.1109/ICSE48619.2023.00056,Program analysis;testing and debugging,Location awareness;Computer bugs;Debugging;Static analysis;Manuals;Benchmark testing;Fuzzing,program debugging;program diagnostics;program testing;public domain software;software tools,automated delta debugging;automated testing;configurable static analysis debugging;configurable static analysis testing;configuration options;DOOP tool;ECSTATIC;FlowDroid tool;open-source framework;oracles;partial order bugs;partial order relations;SOOT tool;static analysis tools;static analyzer;WALA tool,
859,Debugging,Responsibility in Context: On Applicability of Slicing in Semantic Regression Analysis,S. Badihi; K. Ahmed; Y. Li; J. Rubin,"University of British, Columbia, Canada; University of British, Columbia, Canada; Nanyang Technological University, Singapore; University of British, Columbia, Canada",2023,"Numerous program slicing approaches aim to help developers troubleshoot regression failures - one of the most time-consuming development tasks. The main idea behind these approaches is to identify a subset of interdependent program statements relevant to the failure, minimizing the amount of code developers need to inspect. Accuracy and reduction rate achieved by slicing are the key considerations toward their applicability in practice: inspecting only the statements in a slice should be faster and more efficient than inspecting the code in full. In this paper, we report on our experiment applying one of the most recent and accurate slicing approaches, dual slicing, to the task of troubleshooting regression failures. As subjects, we use projects from the popular Defects4J benchmark and a systematically-collected set of eight large, open-source client-library project pairs with at least one library upgrade failure, which we refer to as LibRench. The results of our experiments show that the produced slices, while effective in reducing the scope of manual inspection, are still very large to be comfortably analyzed by a human. When inspecting these slices, we observe that most statements in a slice deal with the propagation of information between changed code blocks; these statements are essential for obtaining the necessary context for the changes but are not responsible for the failure directly. Motivated by this insight, we propose a novel approach, implemented in a tool named INPRESS, for further reducing the size of a slice by accurately identifying and summarizing the propagation-related code blocks. Our evaluation of INPRESS shows that it is able to produce slices that are 76% shorter than the original ones (207 vs. 2,007 execution statements, on average), thus, reducing the amount of information developers need to inspect without losing the necessary contextual information.",10.1109/ICSE48619.2023.00057,Program slicing;slice minimization;regression failures;case study,Codes;Semantics;Manuals;Benchmark testing;Inspection;Minimization;Libraries,program slicing;public domain software;regression analysis;system recovery,code developers;Defects4J benchmark;INPRESS;interdependent program statements;library upgrade failure;LibRench;manual inspection;open-source client-library project pairs;program slicing;propagation-related code blocks;regression failure troubleshooting;semantic regression;time-consuming development tasks,
860,Debugging,Does the Stream API Benefit from Special Debugging Facilities? A Controlled Experiment on Loops and Streams with Specific Debuggers,J. Reichl; S. Hanenberg; V. Gruhn,"NA; Institute for Computer Science and Business Information Systems (ICB), University of Duisburg-Essen, Essen, Germany; Institute for Computer Science and Business Information Systems (ICB), University of Duisburg-Essen, Essen, Germany",2023,"Java's Stream API, that massively makes use of lambda expressions, permits a more declarative way of defining operations on collections in comparison to traditional loops. While experimental results suggest that the use of the Stream API has measurable benefits with respect to code readability (in comparison to loops), a remaining question is whether it has other implications. And one of such implications is, for example, tooling in general and debugging in particular because of the following: While the traditional loop-based approach applies filters one after another to single elements, the Stream API applies filters on whole collections. In the meantime there are dedicated debuggers for the Stream API, but it remains unclear whether such a debugger (on the Stream API) has a measurable benefit in comparison to the traditional stepwise debugger (on loops). The present papers introduces a controlled experiment on the debugging of filter operations using a stepwise debugger versus a stream debugger. The results indicate that under the experiment's settings the stream debugger has a significant ($\mathrm{p} < .001$) and large, positive effect $(\eta_{p}^{2}=.899;\ \frac{M_{stepwise}}{M_{stream}} \sim 204\%)$. However, the experiment reveals that additional factors interact with the debugger treatment such as whether or not the failing object is known upfront. The mentioned factor has a strong and large disordinal interaction effect with the debugger ($\mathrm{p} < .001; \eta_{p}^{2}=.928$): In case an object is known upfront that can be used to identify a failing filter, the stream debugger is even less efficient than the stepwise debugger $(\frac{M_{stepwise}}{M_{stream}}\sim 72\%)$. Hence, while we found overall a positive effect of the stream debugger, the answer whether or not debugging is easier on loops or streams cannot be answered without taking the other variables into account. Consequently, we see a contribution of the present paper not only in the comparison of different debuggers but in the identification of additional factors.",10.1109/ICSE48619.2023.00058,Software Engineering;Programming Techniques;Debugging aids;Usability testing,Java;Codes;Debugging;Software;Object recognition;Testing;Software engineering,application program interfaces;Java;program debugging,debugger treatment;debugging;Java;lambda expressions;loop-based approach;stepwise debugger;Stream API;stream debugger,
861,Debugging,Fonte: Finding Bug Inducing Commits from Failures,G. An; J. Hong; N. Kim; S. Yoo,"School of Computing, KAIST, Daejeon, Republic of Korea; SAP Labs Korea, Seoul, Republic of Korea; School of Computing, KAIST, Daejeon, Republic of Korea; School of Computing, KAIST, Daejeon, Republic of Korea",2023,"A Bug Inducing Commit (BIC) is a commit that introduces a software bug into the codebase. Knowing the relevant BIC for a given bug can provide valuable information for debugging as well as bug triaging. However, existing BIC identification techniques are either too expensive (because they require the failing tests to be executed against previous versions for bisection) or inapplicable at the debugging time (because they require post hoc artefacts such as bug reports or bug fixes). We propose Fonte, an efficient and accurate BIC identification technique that only requires test coverage. Fonte combines Fault Localisation (FL) with BIC identification and ranks commits based on the suspiciousness of the code elements that they modified. Fonte reduces the search space of BICs using failure coverage as well as a filter that detects commits that are merely style changes. Our empirical evaluation using 130 real-world BICs shows that Fonte significantly outperforms state-of-the-art BIC identification techniques based on Information Retrieval as well as neural code embedding models, achieving at least 39% higher MRR. We also report that the ranking scores produced by Fonte can be used to perform weighted bisection, further reducing the cost of BIC identification. Finally, we apply Fonte to a large-scale industry project with over 10M lines of code, and show that it can rank the actual BIC within the top five commits for 87% of the studied real batch-testing failures, and save the BIC inspection cost by 32% on average.",10.1109/ICSE48619.2023.00059,Bug Inducing Commit;Fault Localisation;Git;Weighted Bisection;Batch Testing,Industries;Costs;Codes;Computer bugs;Debugging;Syntactics;Software,information retrieval;inspection;program debugging;program testing,accurate BIC identification technique;actual BIC;batch-testing failures;BIC inspection cost;BICs using failure coverage;bug fixes;Bug Inducing Commit;bug reports;bug triaging;efficient BIC identification technique;existing BIC identification techniques;finding Bug Inducing commits;Fonte;given bug;ranks commits;real-world BICs;relevant BIC;software bug;state-of-the-art BIC identification techniques;test coverage,
862,Defect analysis,RepresentThemAll: A Universal Learning Representation of Bug Reports,S. Fang; T. Zhang; Y. Tan; H. Jiang; X. Xia; X. Sun,"Macau University of Science and Technology, Macau, China; Macau University of Science and Technology, Macau, China; Macau University of Science and Technology, Macau, China; Dalian University of Technology, Dalian, China; Huawei, Hangzhou, China; Yangzhou University, Yangzhou, China",2023,"Deep learning techniques have shown promising performance in automated software maintenance tasks associated with bug reports. Currently, all existing studies learn the customized representation of bug reports for a specific downstream task. Despite early success, training multiple models for multiple downstream tasks faces three issues: complexity, cost, and compatibility, due to the customization, disparity, and uniqueness of these automated approaches. To resolve the above challenges, we propose RepresentThemAll, a pre-trained approach that can learn the universal representation of bug reports and handle multiple downstream tasks. Specifically, RepresentThemAll is a universal bug report framework that is pre-trained with two carefully designed learning objectives: one is the dynamic masked language model and another one is a contrastive learning objective, â€œfind yourselfâ€. We evaluate the performance of RepresentThemAll on four downstream tasks, including duplicate bug report detection, bug report summarization, bug priority prediction, and bug severity prediction. Our experimental results show that RepresentThemAll outperforms all baseline approaches on all considered downstream tasks after well-designed fine-tuning.",10.1109/ICSE48619.2023.00060,,Training;Deep learning;Software maintenance;Costs;Computer bugs;Stacking;Transformers,deep learning (artificial intelligence);learning (artificial intelligence);program debugging;software maintenance,automated software maintenance tasks;bug priority prediction;bug report summarization;bug reports;bug severity prediction;carefully designed learning objectives;considered downstream tasks;contrastive learning objective;customized representation;deep learning techniques;duplicate bug report detection;multiple downstream tasks;pre-trained approach;RepresentThemAll;specific downstream task;training multiple models;universal bug report framework;universal learning representation;universal representation,
863,Defect analysis,Demystifying Exploitable Bugs in Smart Contracts,Z. Zhang; B. Zhang; W. Xu; Z. Lin,Purdue University; Harrison High School; Georgia Institute of Technology; Ohio State University,2023,"Exploitable bugs in smart contracts have caused significant monetary loss. Despite the substantial advances in smart contract bug finding, exploitable bugs and real-world attacks are still trending. In this paper we systematically investigate 516 unique real-world smart contract vulnerabilities in years 2021â€“2022, and study how many can be exploited by malicious users and cannot be detected by existing analysis tools. We further categorize the bugs that cannot be detected by existing tools into seven types and study their root causes, distributions, difficulties to audit, consequences, and repair strategies. For each type, we abstract them to a bug model (if possible), facilitating finding similar bugs in other contracts and future automation. We leverage the findings in auditing real world smart contracts, and so far we have been rewarded with $102,660 bug bounties for identifying 15 critical zero-day exploitable bugs, which could have caused up to $22.52 millions monetary loss if exploited.",10.1109/ICSE48619.2023.00061,Blockchain;Smart Contract;Vulnerability;Security;Empirical Study,Automation;Computer bugs;Smart contracts;Maintenance engineering;Security;Software engineering,blockchains;computer network security;contracts;program debugging,$102 bug bounties;660 bug bounties;bug model;real-world smart contract vulnerabilities;significant monetary loss;similar bugs;smart contract bug finding;world smart contracts;zero-day exploitable bugs,
864,Defect analysis,Understanding and Detecting On-The-Fly Configuration Bugs,T. Wang; Z. Jia; S. Li; S. Zheng; Y. Yu; E. Xu; S. Peng; X. Liao,"National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; National University of Defense Technology, Changsha, China; Hunan University, Changsha, China; National University of Defense Technology, Changsha, China",2023,"Software systems introduce an increasing number of configuration options to provide flexibility, and support updating the options on the fly to provide persistent services. This mechanism, however, may affect the system reliability, leading to unexpected results like software crashes or functional errors. In this paper, we refer to the bugs caused by on-the-fly configuration updates as on-the-fly configuration bugs, or OCBugs for short. In this paper, we conducted the first in-depth study on 75 real-world OCBugs from 5 widely used systems to understand the symptoms, root causes, and triggering conditions of OCBugs. Based on our study, we designed and implemented Parachute, an automated testing framework to detect OCBugs. Our key insight is that the value of one configuration option, either loaded at the startup phase or updated on the fly, should have the same effects on the target program. Parachute generates tests for on-the-fly configuration updates by mutating the existing tests and conducts differential analysis to identify OCBugs. We evaluated Parachute on 7 real-world software systems. The results show that Parachute detected 75% (42/56) of the known OCBugs, and reported 13 unknown bugs, 11 of which have been confirmed or fixed by developers until the time of writing.",10.1109/ICSE48619.2023.00062,on-the-fly configuration updates;bug detection;metamorphic testing,Computer bugs;Writing;Software systems;Reliability;Testing;Software engineering,program debugging;program testing;software reliability,automated testing framework;differential analysis;functional errors;on-the-fly configuration bug detection;on-the-fly configuration updates;Parachute;real-world OCBugs;real-world software systems;software crashes;system reliability,1
865,Defect analysis,Explaining Software Bugs Leveraging Code Structures in Neural Machine Translation,P. Mahbub; O. Shuvo; M. M. Rahman,"Department of Computer Science, Dalhousie University, Nova Scotia, Canada; Department of Computer Science, Dalhousie University, Nova Scotia, Canada; Department of Computer Science, Dalhousie University, Nova Scotia, Canada",2023,"Software bugs claim â‰ˆ 50 % of development time and cost the global economy billions of dollars. Once a bug is reported, the assigned developer attempts to identify and understand the source code responsible for the bug and then corrects the code. Over the last five decades, there has been significant research on automatically finding or correcting software bugs. However, there has been little research on automatically explaining the bugs to the developers, which is essential but a highly challenging task. In this paper, we propose Bugsplainer, a transformer-based generative model, that generates natural language explanations for software bugs by learning from a large corpus of bug-fix commits. Bugsplainer can leverage structural information and buggy patterns from the source code to generate an explanation for a bug. Our evaluation using three performance metrics shows that Bugsplainer can generate understandable and good explanations according to Google's standard, and can outperform multiple baselines from the literature. We also conduct a developer study involving 20 participants where the explanations from Bugsplainer were found to be more accurate, more precise, more concise and more useful than the baselines.",10.1109/ICSE48619.2023.00063,software bug;bug explanation;software engineering;software maintenance;natural language processing;deep learning;transformers,Measurement;Codes;Source coding;Computer bugs;Natural languages;Transformers;Software,language translation;learning (artificial intelligence);natural language processing;program debugging;source code (software),assigned developer attempts;bug-fix commits;buggy patterns;Bugsplainer;leverage structural information;neural machine translation;software bugs leveraging code structures;source code;transformer-based generative model,
866,Developer's behaviors,Is It Enough to Recommend Tasks to Newcomers? Understanding Mentoring on Good First Issues,X. Tan; Y. Chen; H. Wu; M. Zhou; L. Zhang,"State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China; ShenYuan Honors College, Beihang University, Beijing, China; ShenYuan Honors College, Beihang University, Beijing, China; Key Laboratory of High Confidence Software Technologies Ministry of Education, School of Computer Science, Peking University, Beijing, China; State Key Laboratory of Software Development Environment, School of Computer Science and Engineering, Beihang University, Beijing, China",2023,"Newcomers are critical for the success and continuity of open source software (OSS) projects. To attract newcomers and facilitate their onboarding, many OSS projects recommend tasks for newcomers, such as good first issues (GFIs). Previous studies have preliminarily investigated the effects of GFIs and techniques to identify suitable GFIs. However, it is still unclear whether just recommending tasks is enough and how significant mentoring is for newcomers. To better understand mentoring in OSS communities, we analyze the resolution process of 48,402 GFIs from 964 repositories through a mix-method approach. We investigate the extent, the mentorship structures, the discussed topics, and the relevance of expert involvement. We find that ~70% of GFIs have expert participation, with each GFI usually having one expert who makes two comments. Half of GFIs will receive their first expert comment within 8.5 hours after a newcomer comment. Through analysis of the collaboration networks of newcomers and experts, we observe that community mentorship presents four types of structure: centralized mentoring, decentralized mentoring, collaborative mentoring, and distributed mentoring. As for discussed topics, we identify 14 newcomer challenges and 18 expert mentoring content. By fitting the generalized linear models, we find that expert involvement positively correlates with newcomers' successful contributions but negatively correlates with newcomers' retention. Our study manifests the status and significance of mentoring in the OSS projects, which provides rich practical implications for optimizing the mentoring process and helping newcomers contribute smoothly and suecessfully,",10.1109/ICSE48619.2023.00064,newcomer;mentoring;open source;good first issue,Fitting;Collaboration;Mentoring;Task analysis;Open source software;Software engineering,employee welfare;organisational aspects;project management;public domain software;software development management,centralized mentoring;collaboration network analysis;collaborative mentoring;community mentorship;decentralized mentoring;distributed mentoring;expert comment;expert involvement;expert mentoring content;generalized linear model;GFI;good first issues;mix-method approach;newcomer comment;newcomer task recommendation;open source software projects;OSS projects,
867,Developer's behaviors,From Organizations to Individuals: Psychoactive Substance Use By Professional Programmers,K. Newman; M. Endres; W. Weimer; B. Johnson,"Computer Science and Engineering, University of Michigan, Ann Arbor, Michigan, USA; Computer Science and Engineering, University of Michigan, Ann Arbor, Michigan, USA; Computer Science and Engineering, University of Michigan, Ann Arbor, Michigan, USA; Department of Computer Science, George Mason University, Fairfax, Virginia, USA",2023,"Psychoactive substances, which influence the brain to alter perceptions and moods, have the potential to have positive and negative effects on critical software engineering tasks. They are widely used in software, but that use is not well understood. We present the results of the first qualitative investigation of the experiences of, and challenges faced by, psychoactive substance users in professional software communities. We conduct a the-matic analysis of hour-long interviews with 26 professional pro-grammers who use psychoactive substances at work. Our results provide insight into individual motivations and impacts, including mental health and the relationships between various substances and productivity. Our findings elaborate on socialization effects, including soft skills, stigma, and remote work. The analysis also highlights implications for organizational policy, including positive and negative impacts on recruitment and retention. By exploring individual usage motivations, social and cultural ramifications, and organizational policy, we demonstrate how substance use can permeate all levels of software development.",10.1109/ICSE48619.2023.00065,software engineering;mental health;drug use;productivity;qualitative methods,Productivity;Mental health;Debugging;Software;Remote working;Interviews;Task analysis,human factors;organisational aspects;psychology;software engineering,critical software engineering;mental health;motivations;organizational policy;productivity;professional programmers;professional software communities;psychoactive substances;qualitative investigation;social-and-cultural ramifications;socialization effects;software development;thematic analysis,
868,Developer's behaviors,On the Self-Governance and Episodic Changes in Apache Incubator Projects: An Empirical Study,L. Yin; X. Zhang; V. Filkov,"University of California, Davis, Davis, California, USA; University of California, Davis, Davis, California, USA; University of California, Davis, Davis, California, USA",2023,"Sustainable Open Source Software (OSS) projects are characterized by the ability to attract new project members and maintain an energetic project community. Building sustainable OSS projects from a nascent state requires effective project governance and socio-technical structure to be interleaved, in a complex and dynamic process. Although individual disciplines have studied each separately, little is known about how governance and software development work together in practice toward sustainability. Prior work has shown that many OSS projects experience large, episodic changes over short periods of time, which can propel them or drag them down. However, sustainable projects typically manage to come out unscathed from such changes, while others do not. The natural questions arise: Can we identify the back-and-forth between governance and socio-technical structure that lead to sustainability following episodic events? And, how about those that do not lead to sustainability? From a data set of social, technical, and policy digital traces from 262 sustainability-labeled ASF incubator projects, here we employ a large-scale empirical study to characterize episodic changes in socio-technical aspects measured by Change Intervals (CI), governance rules and regulations in a form of Institutional Statements (IS), and the temporal relationships between them. We find that sustainable projects during episodic changes can adapt themselves to institutional statements more efficiently, and that institutional discussions can lead to episodic changes intervals in socio-technical aspects of the projects, and vice versa. In practice, these results can provide timely guidance beyond socio-technical considerations, adding rules and regulations in the mix, toward a unified analytical framework for OSS project sustainability.",10.1109/ICSE48619.2023.00066,Open Source Software;Sustainability;Governance;Apache Software Foundation,Analytical models;Buildings;Collaboration;Propulsion;Regulation;Sustainable development;Open source software,project management;public domain software;sustainable development,"262 sustainability-labeled;apache incubator projects;building sustainable OSS projects;Change Intervals;effective project governance;energetic project community;episodic changes intervals;episodic events;governance rules;large-scale empirical study;OSS project sustainability;project members;self-governance;social policy digital traces;socio-technical aspects;socio-technical considerations;socio-technical structure;software development;Sustainable Open Source Software projects;sustainable projects;technical, policy digital traces",
869,Developer's behaviors,Socio-Technical Anti-Patterns in Building ML-Enabled Software: Insights from Leaders on the Forefront,A. Mailach; N. Siegmund,"ScaDS.AI, Leipzig University, Dresden/Leipzig; ScaDS.AI, Leipzig University, Dresden/Leipzig",2023,"Although machine learning (ML)-enabled software systems seem to be a success story considering their rise in economic power, there are consistent reports from companies and practitioners struggling to bring ML models into production. Many papers have focused on specific, and purely technical aspects, such as testing and pipelines, but only few on socio-technical aspects. Driven by numerous anecdotes and reports from practitioners, our goal is to collect and analyze socio-technical challenges of productionizing ML models centered around and within teams. To this end, we conducted the largest qualitative empirical study in this area, involving the manual analysis of 66 hours of talks that have been recorded by the MLOps community. By analyzing talks from practitioners for practitioners of a community with over 11,000 members in their Slack workspace, we found 17 anti-patterns, often rooted in organizational or management problems. We further list recommendations to overcome these problems, ranging from technical solutions over guidelines to organizational restructuring. Finally, we contextu-alize our findings with previous research, confirming existing results, validating our own, and highlighting new insights.",10.1109/ICSE48619.2023.00067,,Economics;Biological system modeling;Pipelines;Production;Manuals;Machine learning;Software systems,learning (artificial intelligence);organisational aspects;software engineering,building ML-enabled software;economic power;machine learning-enabled software systems;MLOps community;numerous anecdotes;organizational management problems;socio-technical anti-patterns,
870,Developer's behaviors,Moving on from the Software Engineers' Gambit: An Approach to Support the Defense of Software Effort Estimates,P. G. F. Matsubara; I. Steinmacher; B. Gadelha; T. Conte,"Universidade Federal do Amazonas (UFAM), Manaus, AM; Northern Arizona University, Flagstaff, AZ, USA; Universidade Federal do Amazonas (UFAM), Manaus, AM; Universidade Federal do Amazonas (UFAM), Manaus, AM",2023,"Pressure for higher productivity and faster delivery is increasingly pervading software organizations. This can lead software engineers to act like chess players playing a gambitâ€”making sacrifices of their technically sound estimates, thus submitting their teams to time pressure. In turn, time pressure can have varied detrimental effects, such as poor product quality and emotional distress, decreasing productivity, which leads to more time pressure and delays: a hard-to-stop vicious cycle. This reveals a need for moving on from the more passive strategy of yielding to pressure to a more active one of defending software estimates. Therefore, we propose an approach to support software estimators in acquiring knowledge on how to carry out such defense, by introducing negotiation principles encapsulated in a set of defense lenses, presented through a digital simulation. We evaluated the proposed approach through a controlled experiment with software practitioners from different companies. We collected data on participants' attitudes, subjective norms, perceived behavioral control, and intentions to perform the defense of their estimates in light of the Theory of Planned Behavior. We employed a frequentist and a bayesian approach to data analysis. Results show improved scores among experimental group participants after engaging with the digital simulation and learning about the lenses. They were also more inclined to choose a defense action when facing pressure scenarios than a control group exposed to questions to reflect on the reasons and outcomes of pressure over estimates. Qualitative evidence reveals that practitioners perceived the set of lenses as useful in their current work environments. Collectively, these results show the effectiveness of the proposed approach and its perceived relevance for the industry, despite the low amount of time required to engage with it.",10.1109/ICSE48619.2023.00068,Software Effort Estimation;Negotiation;Behavioral Software Engineering;Defense of Estimates,Productivity;Knowledge engineering;Digital simulation;Software;Product design;Behavioral sciences;Quality assessment,Bayes methods;data analysis;product quality;project management;software engineering,bayesian approach;defense action;defense lenses;digital simulation;emotional distress;facing pressure scenarios;gambit-making sacrifices;higher productivity;perceived behavioral control;pervading software organizations;poor product quality;software effort estimates;software engineers;software estimates;software estimators;software practitioners;technically sound estimates;time pressure,
871,Program translation and synthesis,Concrat: An Automatic C-to-Rust Lock API Translator for Concurrent Programs,J. Hong; S. Ryu,"School of Computing KAIST, Daejeon, South Korea; School of Computing KAIST, Daejeon, South Korea",2023,"Concurrent programs suffer from data races. To prevent data races, programmers use locks. However, programs can eliminate data races only when they acquire and release correct locks at correct timing. The lock API of C, in which people have developed a large portion of legacy system programs, does not validate the correct use of locks. On the other hand, Rust, a recently developed system programming language, provides a lock API that guarantees the correct use of locks via type checking. This makes rewriting legacy system programs in Rust a promising way to retrofit safety into them. Unfortunately, manual C-to-Rust translation is extremely laborious due to the discrepancies between their lock APIs. Even the state-of-the-art automatic C-to-Rust translator retains the C lock API, expecting developers to replace them with the Rust lock API. In this work, we propose an automatic tool to replace the C lock API with the Rust lock API. It facilitates C-to-Rust translation of concurrent programs with less human effort than the current practice. Our tool consists of a Rust code transformer that takes a lock summary as an input and a static analyzer that efficiently generates precise lock summaries. We show that the transformer is scalable and widely applicable while preserving the semantics; it transforms 66 KLOC in 2.6 seconds and successfully handles 74% of real-world programs. We also show that the analyzer is scalable and precise; it analyzes 66 KLOC in 4.3 seconds.",10.1109/ICSE48619.2023.00069,,Concurrent computing;Semantics;Transforms;Manuals;Programming;Aging;Transformers,application program interfaces;C language;concurrency (computers);program diagnostics;program verification;software maintenance,automatic C-to-Rust lock API translator;C lock API;concurrent programs;data races;legacy system programs;Rust code transformer;system programming language;type checking,
872,Program translation and synthesis,Triggers for Reactive Synthesis Specifications,G. Amram; D. Ma'ayan; S. Maoz; O. Pistiner; J. O. Ringert,"Tel Aviv University, Israel; Tel Aviv University, Israel; Tel Aviv University, Israel; Tel Aviv University, Israel; Bauhaus University Weimar, Germany",2023,"Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. Two of the main challenges in bringing reactive synthesis to practice are its very high worst-case complexity and the difficulty of writing declarative specifications using basic LTL operators. To address the first challenge, researchers have suggested the GR(1) fragment of LTL, which has an efficient poly-nomial time symbolic synthesis algorithm. To address the second challenge, specification languages include higher-level constructs that aim at allowing engineers to write succinct and readable specifications. One such construct is the triggers operator, as supported, e.g., in the Property Specification Language (PSL). In this work we introduce triggers into specifications for reactive synthesis. The effectiveness of our contribution relies on a novel encoding of regular expressions using symbolic finite automata (SFA) and on a novel semantics for triggers that, in contrast to PSL triggers, admits an efficient translation into GR(1). We show that our triggers are expressive and succinct, and prove that our encoding is optimal. We have implemented our ideas on top of the Spectra language and synthesizer. We demonstrate the usefulness and effectiveness of using triggers in specifications for synthesis, as well as the challenges involved in using them, via a study of more than 300 triggers written by undergraduate students who participated in a project class on writing specifications for synthesis. To the best of our knowledge, our work is the first to introduce triggers into specifications for reactive synthesis.",10.1109/ICSE48619.2023.00070,Reactive synthesis;Formal specifications,Synthesizers;Semantics;Automata;Writing;Encoding;Specification languages;Computational efficiency,computational complexity;finite automata;formal specification;specification languages;temporal logic,basic LTL operators;correct-by-construction reactive system;declarative specifications;higher-level constructs;Property Specification Language;PSL triggers;reactive synthesis specifications;readable specifications;specification languages;succinct specifications;temporal logic specification;triggers operator,1
873,Program translation and synthesis,Using Reactive Synthesis: An End-to-End Exploratory Case Study,D. Ma'ayan; S. Maoz,"Tel Aviv University, Israel; Tel Aviv University, Israel",2023,"Reactive synthesis is an automated procedure to obtain a correct-by-construction reactive system from its temporal logic specification. Despite its attractiveness and major research progress in the past decades, reactive synthesis is still in early-stage and has not gained popularity outside academia. We conducted an exploratory case study in which we followed students in a semester-long university workshop class on their end-to-end use of a reactive synthesizer, from writing the specifications to executing the synthesized controllers. The data we collected includes more than 500 versions of more than 80 specifications, as well as more than 2500 Slack messages, all written by the class participants. Our grounded theory analysis reveals that the use of reactive synthesis has clear benefits for certain tasks and that adequate specification language constructs assist in the specification writing process. However, inherent issues such as unrealizabilty, non-well-separation, the gap of knowledge between the users and the synthesizer, and considerable running times prevent reactive synthesis from fulfilling its promise. Based on our analysis, we propose action items in the directions of language and specification quality, tools for analysis and execution, and process and methodology, all towards making reactive synthesis more applicable for software engineers.",10.1109/ICSE48619.2023.00071,Reactive synthesis;Formal specifications,Synthesizers;Conferences;Metals;Debugging;Writing;Software;Specification languages,formal specification;software engineering;specification languages;temporal logic,correct-by-construction reactive system;end-to-end exploratory case study;end-to-end use;reactive synthesis;reactive synthesizer,
874,Program translation and synthesis,Syntax and Domain Aware Model for Unsupervised Program Translation,F. Liu; J. Li; L. Zhang,"State Key Laboratory of Software Development Environment, Beihang University, Beijing, China; Key Lab of High Confidence Software Technology, MoE, Peking University, Beijing, China; State Key Laboratory of Software Development Environment, Beihang University, Beijing, China",2023,"There is growing interest in software migration as the development of software and society. Manually migrating projects between languages is error-prone and expensive. In recent years, researchers have begun to explore automatic program translation using supervised deep learning techniques by learning from large-scale parallel code corpus. However, parallel resources are scarce in the programming language domain, and it is costly to collect bilingual data manually. To address this issue, several unsupervised programming translation systems are proposed. However, these systems still rely on huge monolingual source code to train, which is very expensive. Besides, these models cannot perform well for translating the languages that are not seen during the pre-training procedure. In this paper, we propose SDA-Trans, a syntax and domain-aware model for program translation, which leverages the syntax structure and domain knowledge to enhance the cross-lingual transfer ability. SDA-Trans adopts unsupervised training on a smaller-scale corpus, including Python and Java monolingual programs. The experimental results on function translation tasks between Python, Java, and C++ show that SDA-Trans outperforms many large-scale pre-trained models, especially for unseen language translation.",10.1109/ICSE48619.2023.00072,program translation;neural networks;syntax structure;unsupervised learning,Training;Deep learning;Java;Source coding;Syntactics;Programming;Software,deep learning (artificial intelligence);Java;language translation;learning (artificial intelligence);natural language processing;Python;supervised learning;unsupervised learning,automatic program translation;bilingual data;cross-lingual transfer ability;domain knowledge;domain-aware model;error-prone;function translation tasks;huge monolingual source code;Java monolingual programs;large-scale parallel code;large-scale pre-trained models;manually migrating projects;parallel resources;pre-training procedure;programming language domain;SDA-Trans;smaller-scale corpus;software migration;supervised deep learning techniques;syntax structure;unseen language translation;unsupervised program translation;unsupervised programming translation systems;unsupervised training,
875,Documentation,Developer-Intent Driven Code Comment Generation,F. Mu; X. Chen; L. Shi; S. Wang; Q. Wang,"State Key Laboratory of Intelligent Game, Beijing, China; State Key Laboratory of Intelligent Game, Beijing, China; State Key Laboratory of Intelligent Game, Beijing, China; Lassonde School of Engineering, York University, Toronto, Canada; State Key Laboratory of Intelligent Game, Beijing, China",2023,"Existing automatic code comment generators mainly focus on producing a general description of functionality for a given code snippet without considering developer intentions. However, in real-world practice, comments are complicated, which often contain information reflecting various intentions of developers, e.g., functionality summarization, design rationale, implementation details, code properties, etc. To bridge the gap between automatic code comment generation and real-world comment practice, we define Developer-Intent Driven Code Comment Generation, which can generate intent-aware comments for the same source code with different intents. To tackle this challenging task, we propose DOME, an approach that utilizes Intent-guided Selective Attention to explicitly select intent-relevant information from the source code, and produces various comments reflecting different intents. Our approach is evaluated on two real-world Java datasets, and the experimental results show that our approach outperforms the state-of-the-art baselines. A human evaluation also confirms the significant potential of applying DOME in practical usage, enabling developers to comment code effectively according to their own needs.",10.1109/ICSE48619.2023.00073,Code Comment Generation;Intent-Controllable Comment Generation;Automated Comment-Intent Labeling,Training;Java;Codes;Source coding;Semantics;Generators;Labeling,Java;software engineering;source code (software),automatic code comment generation;code properties;developer intentions;DOME;intent-guided selective attention;intent-relevant information;Java datasets;source code,
876,Documentation,Data Quality Matters: A Case Study of Obsolete Comment Detection,S. Xu; Y. Yao; F. Xu; T. Gu; J. Xu; X. Ma,"State Key Lab for Novel Software Technology, Nanjing University, China; State Key Lab for Novel Software Technology, Nanjing University, China; State Key Lab for Novel Software Technology, Nanjing University, China; Tiktok, USA; State Key Lab for Novel Software Technology, Nanjing University, China; State Key Lab for Novel Software Technology, Nanjing University, China",2023,"Machine learning methods have achieved great success in many software engineering tasks. However, as a data-driven paradigm, how would the data quality impact the effectiveness of these methods remains largely unexplored. In this paper, we explore this problem under the context of just-in-time obsolete comment detection. Specifically, we first conduct data cleaning on the existing benchmark dataset, and empirically observe that with only 0.22% label corrections and even 15.0% fewer data, the existing obsolete comment detection approaches can achieve up to 10.7% relative accuracy improvement. To further mitigate the data quality issues, we propose an adversarial learning framework to simultaneously estimate the data quality and make the final predictions. Experimental evaluations show that this adversarial learning framework can further improve the relative accuracy by up to 18.1% compared to the state-of-the-art method. Although our current results are from the obsolete comment detection problem, we believe that the proposed two-phase solution, which handles the data quality issues through both the data aspect and the algorithm aspect, is also generalizable and applicable to other machine learning based software engineering tasks.",10.1109/ICSE48619.2023.00074,Obsolete comment detection;machine learning for software engineering;data quality,Data integrity;Software algorithms;Semantics;Training data;Adversarial machine learning;Encoding;Cleaning,data handling;deep learning (artificial intelligence);software engineering,adversarial learning framework;data cleaning;data quality issues;data-driven paradigm;existing obsolete comment detection;just-in-time obsolete comment detection;machine learning methods;software engineering,
877,Documentation,Revisiting Learning-based Commit Message Generation,J. Dong; Y. Lou; D. Hao; L. Tan,"Key Laboratory of High Confidence Software Technologies (Peking University), MoE, School of Computer Science, Peking University, Beijing, China; School of Computer Science, Fudan University, Shanghai, China; Key Laboratory of High Confidence Software Technologies (Peking University), MoE, School of Computer Science, Peking University, Beijing, China; Department of Computer Science, Purdue University, West Lafayette, USA",2023,"Commit messages summarize code changes and help developers understand the intention. To alleviate human efforts in writing commit messages, researchers have proposed various automated commit message generation techniques, among which learning-based techniques have achieved great success in recent years. However, existing evaluation on learning-based commit message generation relies on the automatic metrics (e.g., BLEU) widely used in natural language processing (NLP) tasks, which are aggregated scores calculated based on the similarity between generated commit messages and the ground truth. Therefore, it remains unclear what generated commit messages look like and what kind of commit messages could be precisely generated by existing learning-based techniques. To fill this knowledge gap, this work performs the first study to systematically investigate the detailed commit messages generated by learning-based techniques. In particular, we first investigate the frequent patterns of the commit messages generated by state-of-the-art learning-based techniques. Surprisingly, we find the majority (~90%) of their generated commit messages belong to simple patterns (i.e., addition/removal/fix/avoidance patterns). To further explore the reasons, we then study the impact of datasets, input representations, and model components. We surprisingly find that existing learning-based techniques have competitive performance even when the inputs are only represented by change marks (i.e., â€œ+â€/â€œ-â€/â€œ â€), It indicates that existing learning-based techniques poorly utilize syntax and semantics in the code while mostly focusing on change marks, which could be the major reason for generating so many pattern-matching commit messages. We also find that the pattern ratio in the training set might also positively affect the pattern ratio of generated commit messages; and model components might have different impact on the pattern ratio.",10.1109/ICSE48619.2023.00075,Commit Message Generation;Deep Learning;Pattern-based,Training;Measurement;Codes;Semantics;Focusing;Writing;Syntactics,language translation;learning (artificial intelligence);natural language processing;software maintenance,automated commit message generation techniques;detailed commit messages;existing learning-based techniques;generated commit messages;pattern-matching commit messages;revisiting learning-based commit message generation;state-of-the-art learning-based techniques,
878,Documentation,Commit Message Matters: Investigating Impact and Evolution of Commit Message Quality,J. Li; I. Ahmed,"University of California, Irvine, Irvine, CA, USA; University of California, Irvine, Irvine, CA, USA",2023,"Commit messages play an important role in communication among developers. To measure the quality of commit messages, researchers have defined what semantically constitutes a Good commit message: it should have both the summary of the code change (What) and the motivation/reason behind it (Why). The presence of the issue report/pull request links referenced in a commit message has been treated as a way of providing Why information. In this study, we found several quality issues that could hamper the links' ability to provide Why information. Based on this observation, we developed a machine learning classifier for automatically identifying whether a commit message has What and Why information by considering both the commit messages and the link contents. This classifier outperforms state-of-the-art machine learning classifiers by 12 percentage points improvement in the F1 score. With the improved classifier, we conducted a mixed method empirical analysis and found that: (1) Commit message quality has an impact on software defect proneness, and (2) the overall quality of the commit messages decreases over time, while developers believe they are writing better commit messages. All the research artifacts (i.e., tools, scripts, and data) of this study are available on the accompanying website [2].",10.1109/ICSE48619.2023.00076,Commit message quality;software defect proneness;empirical analysis,Codes;Machine learning;Writing;Software;Software engineering,learning (artificial intelligence);pattern classification;software quality,code change;commit message quality;F1 score;mixed method empirical analysis;software defect proneness,
879,Software logging,PILAR: Studying and Mitigating the Influence of Configurations on Log Parsing,H. Dai; Y. Tang; H. Li; W. Shang,"Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Polytechnique MontrÃ©al, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada",2023,"The significance of logs has been widely acknowledged with the adoption of various log analysis techniques that assist in software engineering tasks. Many log analysis techniques require structured logs as input while raw logs are typically unstructured. Automated log parsing is proposed to convert unstructured raw logs into structured log templates. Some log parsers achieve promising accuracy, yet they rely on significant efforts from the users to tune the parameters to achieve optimal results. In this paper, we first conduct an empirical study to understand the influence of the configurable parameters of six state-of-the-art log parsers on their parsing results on three aspects: 1) varying the parameters while using the same dataset, 2) keeping the same parameters while using different datasets, and 3) using different samples from the same dataset. Our results show that all these parsers are sensitive to the parameters, posing challenges to their adoption in practice. To mitigate such challenges, we propose PILAR (Parameter Insensitive Log Parser), an entropy-based log parsing approach. We compare PILAR with the existing log parsers on the same three aspects and find that PILAR is the most parameter-insensitive one. In addition, PILAR achieves the second highest parsing accuracy and efficiency among all the state-of-the-art log parsers. This paper paves the road for easing the adoption of log analysis in software engineer practices.",10.1109/ICSE48619.2023.00077,,Roads;Software;Task analysis;Software engineering,entropy;grammars;natural language processing;software engineering;system monitoring,automated log parsing;configurable parameters;entropy-based log parsing approach;existing log parsers;highest parsing accuracy;log analysis techniques;Parameter Insensitive Log Parser;parsing results;PILAR;software engineering tasks;state-of-the-art log parsers;structured log templates;unstructured raw logs,
880,Software logging,Did We Miss Something Important? Studying and Exploring Variable-Aware Log Abstraction,Z. Li; C. Luo; T. -H. Chen; W. Shang; S. He; Q. Lin; D. Zhang,"Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Microsoft Research, China; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Microsoft Research, China; Microsoft Research, China; Microsoft Research, China",2023,"Due to the sheer size of software logs, developers rely on automated techniques for log analysis. One of the first and most important steps of automated log analysis is log abstraction, which parses the raw logs into a structured format. Prior log abstraction techniques aim to identify and abstract all the dynamic variables in logs and output a static log template for automated log analysis. However, these abstracted dynamic variables may also contain important information that is useful to different tasks in log analysis. In this paper, we investigate the characteristics of dynamic variables and their importance in practice, and explore the potential of a variable-aware log abstraction technique. Through manual investigations and surveys with practitioners, we find that different categories of dynamic variables record various information that can be important depending on the given tasks, the distinction of dynamic variables in log abstraction can further assist in log analysis. We then propose a deep learning based log abstraction approach, named VALB, which can identify different categories of dynamic variables and preserve the value of specified categories of dynamic variables along with the log templates (i.e., variable-aware log abstraction). Through the evaluation on a widely used log abstraction benchmark, we find that VALB outperforms other state-of-the-art log abstraction techniques on general log abstraction (i.e., when abstracting all the dynamic variables) and also achieves a high variable-aware log abstraction accuracy that further identifies the category of the dynamic variables. Our study highlights the potential of leveraging the important information recorded in the dynamic variables to further improve the process of log analysis.",10.1109/ICSE48619.2023.00078,software logs;log abstraction;deep learning,Surveys;Deep learning;Manuals;Benchmark testing;Software;Task analysis;Software engineering,data mining;deep learning (artificial intelligence);system monitoring,abstracted dynamic variables;abstraction techniques;automated log analysis;dynamic variables record various information;general log abstraction;log templates;raw logs;software logs;state-of-the-art log abstraction;static log template;variable-aware log abstraction accuracy;variable-aware log abstraction technique,
881,Software logging,On the Temporal Relations between Logging and Code,Z. Ding; Y. Tang; Y. Li; H. Li; W. Shang,"Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada; Beijing University of Posts and Telecommunications, Beijing, China; Polytechnique MontrÃ©al, Montreal, Canada; Department of Computer Science and Software Engineering, Concordia University, Montreal, Canada",2023,"Prior work shows that misleading logging texts (i.e., the textual descriptions in logging statements) can be counterproductive for developers during their use of logs. One of the most important types of information provided by logs is the temporal information of the recorded system behavior. For example, a logging text may use a perfective aspect to describe a fact that an important system event has finished. Although prior work has performed extensive studies on automated logging suggestions, few of these studies investigate the temporal relations between logging and code. In this work, we make the first attempt to comprehensively study the temporal relations between logging and its corresponding source code. In particular, we focus on two types of temporal relations: (1) logical temporal relations, which can be inferred from the execution order between the logging statement and the corresponding source code; and (2) semantic temporal relations, which can be inferred based on the semantic meaning of the logging text. We first perform qualitative analyses to study these two types of logging-code temporal relations and the inconsistency between them. As a result, we derive rules to detect these two types of temporal relations and their inconsistencies. Based on these rules, we propose a tool named TempoLo to automatically detect the issues of temporal inconsistencies between logging and code. Through an evaluation of four projects, we find that TempoLo can effectively detect temporal inconsistencies with a small number of false positives. To gather developers' feedback on whether such inconsistencies are worth fixing, we report 15 detected instances from these projects to developers. 13 instances from three projects are confirmed and fixed, while two instances of the remaining project are pending at the time of this writing. Our work lays the foundation for describing temporal relations between logging and code and demonstrates the potential for a deeper understanding of the relationship between logging and code.",10.1109/ICSE48619.2023.00079,software logging;logging text;temporal relations,Codes;Source coding;Semantics;Writing;Software;Behavioral sciences;Task analysis,data mining;inference mechanisms;source code (software);system monitoring;temporal logic,automated logging suggestions;corresponding source code;logging statement;logging text;logging-code temporal relations;semantic temporal relations;temporal inconsistencies;temporal information,1
882,Software logging,How Do Developers' Profiles and Experiences Influence their Logging Practices? An Empirical Study of Industrial Practitioners,G. Rong; S. Gu; H. Shen; H. Zhang; H. Kuang,"State Key Laboratory For Novel Software Technology, Nonjing University, Nanjing, China; State Key Laboratory For Novel Software Technology, Nonjing University, Nanjing, China; HilstLab, Peter Faber Business School, Australian Catholic University, Sydney, Australia; State Key Laboratory For Novel Software Technology, Nonjing University, Nanjing, China; State Key Laboratory For Novel Software Technology, Nonjing University, Nanjing, China",2023,"Logs record the behavioral data of running programs and are typically generated by executing log statements. Software developers generally carry out logging practices with clear intentions and associated concerns (I&Cs). However, I&Cs may not be properly fulfilled in source code as log placement - specifically determination of a log statement's context and content - is often susceptible to an individual's profile and experience. Some industrial studies have been conducted to discern developers' main logging I&Cs and the way I&Cs are fulfilled. However, the findings are only based on the developers from a single company in each individual study and hence have limited generalizability. More importantly, there lacks a comprehensive and deep understanding of the relationships between developers' profiles and experiences and their logging practices from a wider perspective. To fill this significant gap, we conducted an empirical study using mixed methods comprising questionnaire surveys, semi-structured interviews, and code analyses with practitioners from a wide range of companies across a variety of industrial domains. Results reveal that while developers share common logging I&Cs and conduct logging practices mainly in the coding stage, their profiles and experiences profoundly influence their logging I&Cs and the way the I&Cs are fulfilled. These findings pave the way to facilitate the acceptance of important logging I&Cs and the adoption of good logging practices by developers",10.1109/ICSE48619.2023.00080,Logging practice;Intention;Concern;Fulfill,Training;Surveys;Systematics;Source coding;Companies;Software systems;Behavioral sciences,program diagnostics;software engineering;source code (software),behavioral data;developers profiles;industrial practitioners;intentions and associated concerns;log placement;log statements;logging I-and-Cs;logs record;software developers;source code,
883,Software logging,When to Say What: Learning to Find Condition-Message Inconsistencies,I. Bouzenia; M. Pradel,"University of Stuttgart, Germany; University of Stuttgart, Germany",2023,"Programs often emit natural language messages, e.g., in logging statements or exceptions raised on unexpected paths. To be meaningful to users and developers, the message, i.e., what to say, must be consistent with the condition under which it gets triggered, i.e., when to say it. However, checking for inconsistencies between conditions and messages is challenging because the conditions are expressed in the logic of the programming language, while messages are informally expressed in natural language. This paper presents CMI-Finder, an approach for detecting condition-message inconsistencies. CMI-Finder is based on a neural model that takes a condition and a message as its input and then predicts whether the two are consistent. To address the problem of obtaining realistic, diverse, and large-scale training data, we present six techniques to generate large numbers of inconsistent examples to learn from automatically. Moreover, we describe and compare three neural models, which are based on binary classification, triplet loss, and fine-tuning, respectively. Our evaluation applies the approach to 300K condition-message statements extracted from 42 million lines of Python code. The best model achieves a precision of 78% at a recall of 72% on a dataset of past bug fixes. Applying the approach to the newest versions of popular open-source projects reveals 50 previously unknown bugs, 19 of which have been confirmed by the developers so far.",10.1109/ICSE48619.2023.00081,,Codes;Computer bugs;Natural languages;Training data;Predictive models;Data models;Python,learning (artificial intelligence);natural language processing;neural nets;pattern classification;program debugging;public domain software;Python,binary classification;CMI;condition-message inconsistencies;natural language;neural model;programming language;Python code,
884,Software logging,SemParser: A Semantic Parser for Log Analytics,Y. Huo; Y. Su; C. Lee; M. R. Lyu,"Computer Science & Engineering Dept., The Chinese University of Hong Kong, Hong Kong, China; School of Software Engineering, Sun Yat-sen University, Zhuhai, China; Computer Science & Engineering Dept., The Chinese University of Hong Kong, Hong Kong, China; Computer Science & Engineering Dept., The Chinese University of Hong Kong, Hong Kong, China",2023,"Logs, being run-time information automatically generated by software, record system events and activities with their timestamps. Before obtaining more insights into the run-time status of the software, a fundamental step of log analysis, called log parsing, is employed to extract structured templates and parameters from the semi-structured raw log messages. However, current log parsers are all syntax-based and regard each message as a character string, ignoring the semantic information included in parameters and templates. Thus, we propose the first semantic-based parser SemParser to unlock the critical bottleneck of mining semantics from log messages. It contains two steps, an end-to-end semantics miner and a joint parser. Specifically, the first step aims to identify explicit semantics inside a single log, and the second step is responsible for jointly inferring implicit semantics and computing structural outputs according to the contextual knowledge base of the logs. To analyze the effectiveness of our semantic parser, we first demonstrate that it can derive rich semantics from log messages collected from six widely-applied systems with an average F1 score of 0.985. Then, we conduct two representative downstream tasks, showing that current downstream models improve their performance with appropriately extracted semantics by 1.2%-11.7% and 8.65% on two anomaly detection datasets and a failure identification dataset, respectively. We believe these findings provide insights into semantically understanding log messages for the log analysis community.",10.1109/ICSE48619.2023.00082,log parsing;semantic parser;log analytics,Codes;Semantics;Knowledge based systems;Software;Task analysis;Anomaly detection;Software engineering,data handling;data mining;feature extraction;grammars;system monitoring,appropriately extracted semantics;called log parsing;computing structural outputs;contextual knowledge base;current log parsers;end-to-end semantics miner;explicit semantics;implicit semantics;joint parser;log analysis community;log analytics;mining semantics;raw log messages;record system events;rich semantics;run-time information;run-time status;semantic information;semantic parser;semantic-based parser SemParser;semantically understanding;single log;structured templates;syntax-based,
885,Test generation,Badge: Prioritizing UI Events with Hierarchical Multi-Armed Bandits for Automated UI Testing,D. Ran; H. Wang; W. Wang; T. Xie,"School of Computer Science, Peking University, China; School of EECS, Peking University, China; University of Illinois, Urbana-Champaign, USA; School of Computer Science, Peking University, China",2023,"To assure high quality of mobile applications (apps for short), automated UI testing triggers events (associated with UI elements on app UIs) without human intervention, aiming to maximize code coverage and find unique crashes. To achieve high test effectiveness, automated UI testing prioritizes a UI event based on its exploration value (e.g., the increased code coverage of future exploration rooted from the UI event). Various strategies have been proposed to estimate the exploration value of a UI event without considering its exploration diversity (reflecting the variance of covered code entities achieved by explorations rooted from this UI event across its different triggerings), resulting in low test effectiveness, especially on complex mobile apps. To address the preceding problem, in this paper, we propose a new approach named Badge to prioritize UI events considering both their exploration values and exploration diversity for effective automated UI testing. In particular, we design a hierarchical multi-armed bandit model to effectively estimate the exploration value and exploration diversity of a UI event based on its historical explorations along with historical explorations rooted from UI events in the same UI group. We evaluate Badge on 21 highly popular industrial apps widely used by previous related work. Experimental results show that Badge outperforms state-of-the-art/practice tools with 18%-146% relative code coverage improvement and finding 1.19-5.20 Ã— unique crashes, demonstrating the effectiveness of Badge. Further experimental studies confirm the benefits brought by Badge's individual algorithms.",10.1109/ICSE48619.2023.00083,GUI testing;mobile testing;mobile app;Android;multi-armed bandits;reinforcement learning,Codes;Fuzzing;Computer crashes;Mobile applications;Testing;Software engineering;Graphical user interfaces,mobile computing;program testing;user interfaces,app UIs;automated UI testing prioritizes;automated UI testing triggers events;exploration diversity;exploration value;historical explorations;prioritizing UI events;UI event,
886,Test generation,Efficiency Matters: Speeding Up Automated Testing with GUI Rendering Inference,S. Feng; M. Xie; C. Chen,"Monash University, Melbourne, Australia; Australian National University, Canberra, Australia; Monash University, Melbourne, Australia",2023,"Due to the importance of Android app quality assurance, many automated GUI testing tools have been developed. Although the test algorithms have been improved, the impact of GUI rendering has been overlooked. On the one hand, setting a long waiting time to execute events on fully rendered GUIs slows down the testing process. On the other hand, setting a short waiting time will cause the events to execute on partially rendered GUIs, which negatively affects the testing effectiveness. An optimal waiting time should strike a balance between effectiveness and efficiency. We propose AdaT, a lightweight image-based approach to dynamically adjust the inter-event time based on GUI rendering state. Given the real-time streaming on the GUI, AdaT presents a deep learning model to infer the rendering state, and synchronizes with the testing tool to schedule the next event when the GUI is fully rendered. The evaluations demonstrate the accuracy, efficiency, and effectiveness of our approach. We also integrate our approach with the existing automated testing tool to demonstrate the usefulness of AdaT in covering more activities and executing more events on fully rendered GUIs.",10.1109/ICSE48619.2023.00084,Efficient android GUI testing;GUI rendering;Machine Learning,Deep learning;Schedules;Quality assurance;Streaming media;Rendering (computer graphics);Real-time systems;Synchronization,Android (operating system);deep learning (artificial intelligence);graphical user interfaces;mobile computing;program testing;rendering (computer graphics),AdaT;Android app quality assurance;automated GUI testing tools;efficiency matters;existing automated testing tool;fully rendered GUIs;GUI rendering inference;GUI rendering state;inter-event time;lightweight image-based approach;long waiting time;optimal waiting time;partially rendered GUIs;short waiting time;test algorithms;testing effectiveness;testing process,
887,Test generation,CodaMosa: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models,C. Lemieux; J. P. Inala; S. K. Lahiri; S. Sen,"University of British Columbia, Canada; Microsoft Research, USA; Microsoft Research, USA; Microsoft Research, USA",2023,"Search-based software testing (SBST) generates high-coverage test cases for programs under test with a combination of test case generation and mutation. SBST's performance relies on there being a reasonable probability of generating test cases that exercise the core logic of the program under test. Given such test cases, SBST can then explore the space around them to exercise various parts of the program. This paper explores whether Large Language Models (LLMs) of code, such as OpenAI's Codex, can be used to help SBST's exploration. Our proposed algorithm, CodaMosa, conducts SBST until its coverage improvements stall, then asks Codex to provide example test cases for under-covered functions. These examples help SBST redirect its search to more useful areas of the search space. On an evaluation over 486 benchmarks, CodaMosa achieves statistically significantly higher coverage on many more benchmarks (173 and 279) than it reduces coverage on (10 and 4), compared to SBST and LLM-only baselines.",10.1109/ICSE48619.2023.00085,search based software testing;codex;test suite generation;python;large language model;automated testing,Software testing;Codes;Benchmark testing;Software;Space exploration;Test pattern generators;Software engineering,probability;program testing;search problems,CodaMosa;core logic;OpenAIs Codex;pre-trained large language models;probability;programs;SBSTs exploration;search space;search-based software testing;statistical significant;test case generation,
888,SE for security 1,Taintmini: Detecting Flow of Sensitive Data in Mini-Programs with Static Taint Analysis,C. Wang; R. Ko; Y. Zhang; Y. Yang; Z. Lin,The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University; The Ohio State University,2023,"Mini-programs, which are programs running inside mobile super apps such as WeChat, often have access to privacy-sensitive information, such as location data and phone numbers, through APUs provided by the super apps. This access poses a risk of privacy sensitive data leaks, either accidentally from carelessly programmed mini-programs or intentionally from malicious ones. To address this concern, it is crucial to track the flow of sensitive data in mini-programs for either human analysis or automated tools. Although existing taint analysis techniques have been widely studied, they face unique challenges in tracking sensitive data flows in mini-programs, such as cross-language, cross-page, and cross-mini-program data flows. This paper presents a novel framework, Taintmini, which addresses these challenges by using a novel universal data flow graph approach that captures data flows within and across mini-programs. We have evaluated Taintminiwith 238,866 mini-programs and detect 27,184 that contain sensitive data flows. We have also applied Taintminito detect privacy leakage colluding mini-programs and identify 455 such programs from them that clearly violate privacy policy.",10.1109/ICSE48619.2023.00086,Mini-programs;Taint analysis;Privacy leaks detection;Security;Empirical Study,Privacy;Data privacy;Social networking (online);Message services;Flow graphs;Faces;Software engineering,data flow graphs;data privacy;mobile computing;program diagnostics;security of data,866 mini-programs;carelessly programmed mini-programs;cross-mini-program data;privacy sensitive data leaks;sensitive data flows;universal data flow graph approach,
889,SE for security 1,AChecker: Statically Detecting Smart Contract Access Control Vulnerabilities,A. Ghaleb; J. Rubin; K. Pattabiraman,"University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada",2023,"As most smart contracts have a financial nature and handle valuable assets, smart contract developers use access control to protect assets managed by smart contracts from being misused by malicious or unauthorized people. Unfortunately, programming languages used for writing smart contracts, such as Solidity, were not designed with a permission-based security model in mind. Therefore, smart contract developers implement access control checks based on their judgment and in an adhoc manner, which results in several vulnerabilities in smart contracts, called access control vulnerabilities. Further, the in-consistency in implementing access control makes it difficult to reason about whether a contract meets access control needs and is free of access control vulnerabilities. In this work, we propose AChecker - an approach for detecting access control vulnerabilities. Unlike prior work, AChecker does not rely on pre-defined patterns or contract transactions history. Instead, it infers access control implemented in smart contracts via static data-flow analysis. Moreover, the approach performs further symbolic-based analysis to distinguish cases when unauthorized people can obtain control of the contract as intended functionality. We evaluated AChecker on three public datasets of real-world smart contracts, including one which consists of contracts with assigned access control CVEs, and compared its effectiveness with eight analysis tools. The evaluation results showed that AChecker outperforms these tools in terms of both precision and recall. In addition, AChecker flagged vulnerabilities in 21 frequently-used contracts on Ethereum blockchain with 90% precision.",10.1109/ICSE48619.2023.00087,Smart contract;security;access control;data-flow analysis,Access control;Solid modeling;Computer languages;Smart contracts;Static analysis;Writing;Blockchains,authorisation;blockchains;contracts,AChecker;contract transactions history;CVE;Ethereum blockchain;permission-based security model;smart contract access control vulnerabilities;Solidity,1
890,SE for security 1,Fine-grained Commit-level Vulnerability Type Prediction by CWE Tree Structure,S. Pan; L. Bao; X. Xia; D. Lo; S. Li,"College of Computer Science and Technology, Zhejiang University, China; College of Computer Science and Technology, Zhejiang University, China; Huawei, China; School of Information Systems, Singapore Management University, Singapore; College of Computer Science and Technology, Zhejiang University, China",2023,"Identifying security patches via code commits to allow early warnings and timely fixes for Open Source Software (OSS) has received increasing attention. However, the existing detection methods can only identify the presence of a patch (i.e., a binary classification) but fail to pinpoint the vulnerability type. In this work, we take the first step to categorize the security patches into fine-grained vulnerability types. Specifically, we use the Common Weakness Enumeration (CWE) as the label and perform fine-grained classification using categories at the third level of the CWE tree. We first formulate the task as a Hierarchical Multi-label Classification (HMC) problem, i.e., inferring a path (a sequence of CWE nodes) from the root of the CWE tree to the node at the target depth. We then propose an approach named TreeVul with a hierarchical and chained architecture, which manages to utilize the structure information of the CWE tree as prior knowledge of the classification task. We further propose a tree structure aware and beam search based inference algorithm for retrieving the optimal path with the highest merged probability. We collect a large security patch dataset from NVD, consisting of 6,541 commits from 1,560 GitHub OSS repositories. Experimental results show that Tree-vulsignificantly outperforms the best performing baselines, with improvements of 5.9%, 25.0%, and 7.7% in terms of weighted F1-score, macro F1-score, and MCC, respectively. We further conduct a user study and a case study to verify the practical value of TreeVul in enriching the binary patch detection results and improving the data quality of NVD, respectively.",10.1109/ICSE48619.2023.00088,Software Security;Vulnerability Type;CWE,Codes;Data integrity;Computer architecture;Inference algorithms;Classification algorithms;Security;Task analysis,inference mechanisms;pattern classification;program diagnostics;public domain software;search problems;security of data;tree data structures,beam search;binary classification;binary patch detection;code commits;common weakness enumeration;CWE nodes;CWE tree structure;fine grained commit-level vulnerability type prediction;fine-grained classification;hierarchical multilabel classification problem;inference algorithm;Open Source Software;security patch dataset;structure information;tree structure aware;Tree-vulsignificantly;TreeVul,
891,SE for security 1,Silent Vulnerable Dependency Alert Prediction with Vulnerability Key Aspect Explanation,J. Sun; Z. Xing; Q. Lu; X. Xu; L. Zhu; T. Hoang; D. Zhao,"Data61, Eveleigh, CSIRO, Sydney, Australia; Data61, Eveleigh, CSIRO, Sydney, Australia; Data61, Eveleigh, CSIRO, Sydney, Australia; Data61, Eveleigh, CSIRO, Sydney, Australia; Data61, Eveleigh, CSIRO, Sydney, Australia; Data61, Eveleigh, CSIRO, Sydney, Australia; Research School of Computer Science, CECS, Australian National University, Canberra, Australia",2023,"Due to convenience, open-source software is widely used. For beneficial reasons, open-source maintainers often fix the vulnerabilities silently, exposing their users unaware of the updates to threats. Previous works all focus on black-box binary detection of the silent dependency alerts that suffer from high false-positive rates. Open-source software users need to analyze and explain AI prediction themselves. Explainable AI becomes remarkable as a complementary of black-box AI models, providing details in various forms to explain AI decisions. Noticing there is still no technique that can discover silent dependency alert on time, in this work, we propose a framework using an encoder-decoder model with a binary detector to provide explainable silent dependency alert prediction. Our model generates 4 types of vulnerability key aspects including vulnerability type, root cause, attack vector, and impact to enhance the trustworthiness and users' acceptance to alert prediction. By experiments with several models and inputs, we confirm CodeBERT with both commit messages and code changes achieves the best results. Our user study shows that explainable alert predictions can help users find silent dependency alert more easily than black-box predictions. To the best of our knowledge, this is the first research work on the application of Explainable AI in silent dependency alert prediction, which opens the door of the related domains.",10.1109/ICSE48619.2023.00089,,Codes;Soft sensors;Closed box;Detectors;Predictive models;Generators;Data models,artificial intelligence;explanation;natural language processing;neural nets;public domain software;security of data,AI decisions;AI prediction;attack vector;black-box AI models;black-box binary detection;black-box predictions;CodeBERT;encoder-decoder model;explainable AI;explainable silent dependency alert prediction;false-positive rates;open-source maintainers;open-source software users;silent vulnerable dependency alert prediction;vulnerability key aspect explanation;vulnerability type,
892,Development and evolution of AI-intensive systems,Reusing Deep Neural Network Models through Model Re-engineering,B. Qi; H. Sun; X. Gao; H. Zhang; Z. Li; X. Liu,"SKLSDE Lab, Beihang University, Beijing, China; SKLSDE Lab, Beihang University, Beijing, China; SKLSDE Lab, Beihang University, Beijing, China; Chongqing University, Chongqing, China; SKLSDE Lab, Beihang University, Beijing, China; SKLSDE Lab, Beihang University, Beijing, China",2023,"Training deep neural network (DNN) models, which has become an important task in today's software development, is often costly in terms of computational resources and time. With the inspiration of software reuse, building DNN models through reusing existing ones has gained increasing attention recently. Prior approaches to DNN model reuse have two main limitations: 1) reusing the entire model, while only a small part of the model's functionalities (labels) are required, would cause much overhead (e.g., computational and time costs for inference), and 2) model reuse would inherit the defects and weaknesses of the reused model, and hence put the new system under threats of security attack. To solve the above problem, we propose SeaM, a tool that re-engineers a trained DNN model to improve its reusability. Specifically, given a target problem and a trained model, SeaM utilizes a gradient-based search method to search for the model's weights that are relevant to the target problem. The re-engineered model that only retains the relevant weights is then reused to solve the target problem. Evaluation results on widely-used models show that the re-engineered models produced by SeaM only contain 10.11% weights of the original models, resulting 42.41% reduction in terms of inference time. For the target problem, the re-engineered models even outperform the original models in classification accuracy by 5.85%. Moreover, reusing the re-engineered models inherits an average of 57% fewer defects than reusing the entire model. We believe our approach to reducing reuse overhead and defect inheritance is one important step forward for practical model reuse.",10.1109/ICSE48619.2023.00090,model reuse;deep neural network;re-engineering;DNN modularization,Training;Computational modeling;Source coding;Artificial neural networks;Search problems;Data models;Security,deep learning (artificial intelligence);software reusability,computational resources;computational time costs;deep neural network models;DNN model reuse;DNN models;entire model;gradient-based search method;inference time;model reengineering;practical model reuse;reuse overhead;reused model;reusing existing ones;SeaM;software development;software reuse;target problem;trained DNN model;trained model;widely-used models,
893,Development and evolution of AI-intensive systems,PYEVOLVE: Automating Frequent Code Changes in Python ML Systems,M. Dilhara; D. Dig; A. Ketkar,"University of Colorado Boulder, USA; JetBrains Research, University of Colorado Boulder, USA; Uber Technologies Inc., USA",2023,"Because of the naturalness of software and the rapid evolution of Machine Learning (ML) techniques, frequently repeated code change patterns (CPATs) occur often. They range from simple API migrations to changes involving several complex control structures such as for loops. While manually performing CPATs is tedious, the current state-of-the-art techniques for inferring transformation rules are not advanced enough to handle unseen variants of complex CPATs, resulting in a low recall rate. In this paper we present a novel, automated workflow that mines CPATs, infers the transformation rules, and then transplants them automatically to new target sites. We designed, implemented, evaluated and released this in a tool, PYEVOLVE. At its core is a novel data-flow, control-flow aware transformation rule inference engine. Our technique allows us to advance the state-of-the-art for transformation-by-example tools; without it, 70% of the code changes that PYEVOLVE transforms would not be possible to automate. Our thorough empirical evaluation of over 40,000 transformations shows 97% precision and 94% recall. By accepting 90% of CPATs generated by PYEVOLVE in famous open-source projects, developers confirmed its changes are useful.",10.1109/ICSE48619.2023.00091,Python;Machine Learning;Repetitive code changes;Transformation by Example;Program synthesis;Programming by example;Program transformation,Codes;Transforms;Machine learning;Software;Organ transplantation;Engines;Software engineering,application program interfaces;data mining;inference mechanisms;learning (artificial intelligence);public domain software;Python,automated workflow;automating frequent code changes;code change patterns;complex control structures;complex CPATs;control-flow aware transformation rule inference engine;current state-of-the-art techniques;low recall rate;mines CPATs;novel data-flow;PYEVOLVE transforms;python ML systems;simple API migrations;target sites;transformation rules;transformation-by-example tools;unseen variants,1
894,Development and evolution of AI-intensive systems,DeepArc: Modularizing Neural Networks for the Model Maintenance,X. Ren; Y. Lin; Y. Xue; R. Liu; J. Sun; Z. Feng; J. S. Dong,"University of Science and Technology of China, China; Shanghai Jiao Tong University, China; University of Science and Technology of China, China; National University of Singapore, Singapore; Singapore Management University, Singapore; Tianjin University, China; National University of Singapore, Singapore",2023,"Neural networks are an emerging data-driven programming paradigm widely used in many areas. Unlike traditional software systems consisting of decomposable modules, a neural network is usually delivered as a monolithic package, raising challenges for some maintenance tasks such as model restructure and re-adaption. In this work, we propose DeepArc, a novel modularization method for neural networks, to reduce the cost of model maintenance tasks. Specifically, DeepArc decomposes a neural network into several consecutive modules, each of which encapsulates consecutive layers with similar semantics. The network modularization facilitates practical tasks such as refactoring the model to preserve existing features (e.g., model compression) and enhancing the model with new features (e.g., fitting new samples). The modularization and encapsulation allow us to restructure or retrain the model by only pruning and tuning a few localized neurons and layers. Our experiments show that (1) DeepArc can boost the runtime efficiency of the state-of-the-art model compression techniques by 14.8%; (2) compared to the traditional model retraining, DeepArc only needs to train less than 20% of the neurons on average to fit adversarial samples and repair under-performing models, leading to 32.85% faster training performance while achieving similar model prediction performance.",10.1109/ICSE48619.2023.00092,architecture;modularization;neural networks,Training;Runtime;Neurons;Semantics;Predictive models;Maintenance engineering;Software systems,data compression;learning (artificial intelligence);neural nets;software maintenance,emerging data-driven programming paradigm;encapsulation;model maintenance tasks;model restructure;modularizing neural networks;network modularization;neural network;similar model prediction performance;state-of-the-art model compression techniques;traditional model retraining,
895,Development and evolution of AI-intensive systems,Decomposing a Recurrent Neural Network into Modules for Enabling Reusability and Replacement,S. M. Imtiaz; F. Batole; A. Singh; R. Pan; B. D. Cruz; H. Rajan,"Department of Computer Science, Iowa State University, Ames, IA, USA; Department of Computer Science, Iowa State University, Ames, IA, USA; Department of Computer Science, Iowa State University, Ames, IA, USA; IBM Research, Yorktown Heights, NY, USA; Department of Computer Science, Iowa State University, Ames, IA, USA; Department of Computer Science, Iowa State University, Ames, IA, USA",2023,"Can we take a recurrent neural network (RNN) trained to translate between languages and augment it to support a new natural language without retraining the model from scratch? Can we fix the faulty behavior of the RNN by replacing portions associated with the faulty behavior? Recent works on decomposing a fully connected neural network (FCNN) and convolutional neural network (CNN) into modules have shown the value of engineering deep models in this manner, which is standard in traditional SE but foreign for deep learning models. However, prior works focus on the image-based multi-class classification problems and cannot be applied to RNN due to (a) different layer structures, (b) loop structures, (c) different types of input-output architectures, and (d) usage of both non-linear and logistic activation functions. In this work, we propose the first approach to decompose an RNN into modules. We study different types of RNNs, i.e., Vanilla, LSTM, and GRU. Further, we show how such RNN modules can be reused and replaced in various scenarios. We evaluate our approach against 5 canonical datasets (i.e., Math QA, Brown Corpus, Wiki-toxicity, Cline OOS, and Tatoeba) and 4 model variants for each dataset. We found that decomposing a trained model has a small cost (Accuracy: -0.6%, BLEU score: +0.10%). Also, the decomposed modules can be reused and replaced without needing to retrain.",10.1109/ICSE48619.2023.00093,recurrent neural networks;decomposing;modules;modularity,Deep learning;Recurrent neural networks;Costs;Natural languages;Computer architecture;Behavioral sciences;Convolutional neural networks,convolutional neural nets;deep learning (artificial intelligence);language translation;learning (artificial intelligence);natural language processing;neural nets;pattern classification;recurrent neural nets;text analysis,4 model variants;convolutional neural network;decomposed modules;deep learning models;different layer structures;enabling reusability;engineering deep models;faulty behavior;fully connected neural network;image-based multiclass classification problems;natural language;recurrent neural network;RNN modules;trained model,
896,Vulnerability analysis and assessment,CHRONOS: Time-Aware Zero-Shot Identification of Libraries from Vulnerability Reports,Y. Lyu; T. Le-Cong; H. J. Kang; R. Widyasari; Z. Zhao; X. -B. D. Le; M. Li; D. Lo,Singapore Management University; Singapore Management University; Singapore Management University; Singapore Management University; Singapore Management University; The University of Melbourne; Nanjing University; Singapore Management University,2023,"Tools that alert developers about library vulnerabilities depend on accurate, up-to-date vulnerability databases which are maintained by security researchers. These databases record the libraries related to each vulnerability. However, the vulnerability reports may not explicitly list every library and human analysis is required to determine all the relevant libraries. Human analysis may be slow and expensive, which motivates the need for automated approaches. Researchers and practitioners have proposed to automatically identify libraries from vulnerability reports using extreme multi-label learning (XML). While state-of-the-art XML techniques showed promising performance, their experimental settings do not practically fit what happens in reality. Previous studies randomly split the vulnerability reports data for training and testing their models without considering the chronological order of the reports. This may unduly train the models on chronologically newer reports while testing the models on chronologically older ones. However, in practice, one often receives chronologically new reports, which may be related to previously unseen libraries. Under this practical setting, we observe that the performance of current XML techniques declines substantially, e.g., F1 decreased from 0.7 to 0.24 under experiments without and with consideration of chronological order of vulnerability reports. We propose a practical library identification approach, namely Chronos, based on zero-shot learning. The novelty of Chronos is three-fold. First, Chronos fits into the practical pipeline by considering the chronological order of vulnerability reports. Second, Chronos enriches the data of the vulnerability descriptions and labels using a carefully designed data enhancement step. Third, Chronos exploits the temporal ordering of the vulnerability reports using a cache to prioritize prediction of versions of libraries that recently had reports of vulnerabilities. In our experiments, Chronos achieves an average F1-score of 0.75, 3x better than the best XML-based approach. Data enhancement and the time-aware adjustment improve Chronos over the vanilla zero-shot learning model by 27% in average F1.",10.1109/ICSE48619.2023.00094,zero-shot learning;library identification;unseen labels;extreme multi-label classification;vulnerability reports,Training;Databases;Pipelines;XML;Libraries;Data models;Security,learning (artificial intelligence);security of data;software engineering;XML,chronological order;Chronos;data enhancement;extreme multilabel learning;human analysis;library identification approach;library vulnerabilities;time-aware adjustment;time-aware zero-shot identification;vulnerability databases;XML;zero-shot learning,1
897,Vulnerability analysis and assessment,Understanding the Threats of Upstream Vulnerabilities to Downstream Projects in the Maven Ecosystem,Y. Wu; Z. Yu; M. Wen; Q. Li; D. Zou; H. Jin,"School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Computer Science and Technology, Huazhong University of Science and Technology, China",2023,"Modern software systems are increasingly relying on dependencies from the ecosystem. A recent estimation shows that around 35% of an open-source project's code come from its depended libraries. Unfortunately, open-source libraries are often threatened by various vulnerability issues, and the number of disclosed vulnerabilities is increasing steadily over the years. Such vulnerabilities can pose significant security threats to the whole ecosystem, not only to the vulnerable libraries themselves, but also to the corresponding downstream projects. Many Software Composition Analysis (SCA) tools have been proposed, aiming to detect vulnerable libraries or components referring to existing vulnerability databases. However, recent studies report that such tools often generate a large number of false alerts. Particularly, up to 73.3% of the projects depending on vulnerable libraries are actually safe. Aiming to devise more precise tools, understanding the threats of vulnerabilities holistically in the ecosystem is significant, as already performed by a number of existing studies. However, previous researches either analyze at a very coarse granularity (e.g., without analyzing the source code) or are limited by the study scales. This study aims to bridge such gaps. In particular, we collect 44,450 instances of (CVE, upstream, downstream) relations and analyze around 50 million invocations made from downstream to upstream projects to understand the potential threats of upstream vulnerabilities to downstream projects in the Maven ecosystem. Our investigation makes interesting yet significant findings with respect to multiple aspects, including the reach-ability of vulnerabilities, the complexities of the reachable paths as well as how downstream projects and developers perceive upstream vulnerabilities. We believe such findings can not only provide a holistic understanding towards the threats of upstream vulnerabilities in the Maven ecosystem, but also can guide future researches in this field.",10.1109/ICSE48619.2023.00095,Maven;Ecosystem Security;Vulnerability,Codes;Databases;Source coding;Ecosystems;Estimation;Software systems;Libraries,security of data;software libraries;software reusability,depended libraries;downstream projects;Maven ecosystem;open-source libraries;open-source project;SCA;security threats;software composition analysis tools;upstream vulnerabilities;vulnerability databases;vulnerable libraries,
898,Vulnerability analysis and assessment,SecBench.js: An Executable Security Benchmark Suite for Server-Side JavaScript,M. H. M. Bhuiyan; A. S. Parthasarathy; N. Vasilakis; M. Pradel; C. -A. Staicu,CISPA Helmholtz Center for Information Security; IIITDM Kancheepuram; Brown University & MIT; University of Stuttgart; CISPA Helmholtz Center for Information Security,2023,"NPM is the largest software ecosystem in the world, offering millions of free, reusable packages. In recent years, various security threats to packages published on npm have been reported, including vulnerabilities that affect millions of users. To continuously improve techniques for detecting vulnerabilities and mitigating attacks that exploit them, a reusable benchmark of vulnerabilities would be highly desirable. Ideally, such a benchmark should be realistic, come with executable exploits, and include fixes of vulnerabilities. Unfortunately, there currently is no such benchmark, forcing researchers to repeatedly develop their own evaluation datasets and making it difficult to compare techniques with each other. This paper presents SecBench.js,, the first comprehensive benchmark suite of vulnerabilities and executable exploits for npm. The benchmark comprises 600 vulnerabilities, which cover the five most common vulnerability classes for server-side JavaScript. Each vulnerability comes with a payload that exploits the vulnerability and an oracle that validates successful exploitation. SecBench.js, enables various applications, of which we explore three in this paper: (i) cross-checking SecBench.js, against public security advisories reveals 168 vulnerable versions in 19 packages that are mislabeled in the advisories; (ii) applying simple code transformations to the exploits in our suite helps identify flawed fixes of vulnerabilities; (iii) dynamically analyzing calls to common sink APIs, e.g., exec(), yields a ground truth of code locations for evaluating vulnerability detectors. Beyond providing a reusable benchmark to the community, our work identified 20 zero-day vulnerabilities, most of which are already acknowledged by practitioners.",10.1109/ICSE48619.2023.00096,,Fault diagnosis;Codes;Benchmark testing;Software;Safety;Security;Public policy,application program interfaces;authoring languages;security of data,common sink API;common vulnerability classes;executable exploits;executable security benchmark suite;NPM;public security advisories;SecBench.js cross-checking;server-side JavaScript;simple code transformation;vulnerability detectors;vulnerability reusable benchmark;zero-day vulnerabilities,
899,Vulnerability analysis and assessment,On Privacy Weaknesses and Vulnerabilities in Software Systems,P. Sangaroonsilp; H. K. Dam; A. Ghose,"University of Wollongong, New South Wales, Australia; University of Wollongong, New South Wales, Australia; University of Wollongong, New South Wales, Australia",2023,"In this digital era, our privacy is under constant threat as our personal data and traceable online/offline activities are frequently collected, processed and transferred by many software applications. Privacy attacks are often formed by exploiting vulnerabilities found in those software applications. The Common Weakness Enumeration (CWE) and Common Vulnerabilities and Exposures (CVE) systems are currently the main sources that software engineers rely on for understanding and preventing publicly disclosed software vulnerabilities. However, our study on all 922 weaknesses in the CWE and 156,537 vulnerabilities registered in the CVE to date has found a very small coverage of privacy-related vulnerabilities in both systems, only 4.45% in CWE and 0.1% in CVE. These also cover only a small number of areas of privacy threats that have been raised in existing privacy software engineering research, privacy regulations and frameworks, and relevant reputable organisations. The actionable insights generated from our study led to the introduction of 11 new common privacy weaknesses to supplement the CWE system, making it become a source for both security and privacy vulnerabilities.",10.1109/ICSE48619.2023.00097,Privacy;Vulnerabilities;Threats;CWE;CVE;Software,Privacy;Data privacy;Software systems;Regulation;Security;Software engineering,data privacy;safety-critical software,common vulnerabilities and exposures;common weakness enumeration;constant threat;CVE;CWE system;personal data;privacy attacks;privacy regulations;privacy software engineering research;privacy threats;privacy weaknesses;privacy-related vulnerabilities;publicly disclosed software vulnerabilities;software applications;software systems,
900,Defect detection and prediction,Detecting Exception Handling Bugs in C++ Programs,H. Zhang; J. Luo; M. Hu; J. Yan; J. Zhang; Z. Qiu,"State Key Lab. of Computer Science, Institute of Software, CAS, China, Univ. of Chinese Academy of Sciences, Beijing, China; Tech. Center of Softw. Eng., Institute of Software, CAS, China, Univ. of Chinese Academy of Sciences, Beijing, China; State Key Lab. of Computer Science, Institute of Software, CAS, China, Hangzhou Institute for Advanced Study, Univ. of Chinese Academy of Sciences, Beijing, China; State Key Lab. of Computer Science, Institute of Software, CAS, China, Univ. of Chinese Academy of Sciences, Beijing, China; State Key Lab. of Computer Science, Institute of Software, CAS, China, Univ. of Chinese Academy of Sciences, Beijing, China; School of Mathematical Sciences, Peking University, Beijing, China",2023,"Exception handling is a mechanism in modern programming languages. Studies have shown that the exception handling code is error-prone. However, there is still limited research on detecting exception handling bugs, especially for C++ programs. To tackle the issue, we try to precisely represent the exception control flow in C++ programs and propose an analysis method that makes use of the control flow to detect such bugs. More specifically, we first extend control flow graph by introducing the concepts of five different kinds of basic blocks, and then modify the classic symbolic execution framework by extending the program state to a quadruple and properly processing try, throw and catch statements. Based on the above techniques, we develop a static analysis tool on the top of Clang Static Analyzer to detect exception handling bugs. We run our tool on projects with high stars from GitHub and find 36 exception handling bugs in 8 projects, with a precision of 84%. We compare our tool with four state-of-the-art static analysis tools (Cppcheck, Clang Static Analyzer, Facebook Infer and IKOS) on projects from GitHub and handmade benchmarks. On the GitHub projects, other tools are not able to detect any exception handling bugs found by our tool. On the handmade benchmarks, our tool has a significant higher recall.",10.1109/ICSE48619.2023.00098,static analysis;exception handling;bug finding,Computer languages;Computer bugs;Process control;Stars;C++ languages;Static analysis;Benchmark testing,C++ language;exception handling;flow graphs;program compilers;program debugging;program diagnostics;program testing;public domain software,C++ programs;Clang Static Analyzer;control flow graph;exception control flow;exception handling bugs;modern programming languages;program state;state-of-the-art static analysis tools;static analysis tool,
901,Defect detection and prediction,Learning to Boost Disjunctive Static Bug-Finders,Y. Ko; H. Oh,Meta; Korea University,2023,"We present a new learning-based approach for accel-erating disjunctive static bug-finders. Industrial static bug-finders usually perform disjunctive analysis, differentiating program states along different execution paths of a program. Such path-sensitivity is essential for reducing false positives but it also increases analysis costs exponentially. Therefore, practical bug-finders use a state-selection heuristic to keep track of a small number of beneficial states only. However, designing a good heuristic for real-world programs is challenging; as a result, modern static bug-finders still suffer from low cost/bug-finding efficiency. In this paper, we aim to address this problem by learning effective state-selection heuristics from data. To this end, we present a novel data-driven technique that efficiently collects alarm-triggering traces, learns multiple candidate models, and adaptively chooses the best model tailored for each target program. We evaluate our approach with Infer and show that our technique significantly improves Infer's bug-finding efficiency for a range of open-source C programs.",10.1109/ICSE48619.2023.00099,machine learning;static analysis,Adaptation models;Costs;Software engineering,C language;learning (artificial intelligence);program debugging;program diagnostics,alarm-triggering traces;data-driven technique;disjunctive static bug-finders;execution paths;industrial static bug-finders;Infer's bug-finding efficiency;learning-based approach;open-source C programs;path-sensitivity;program states;real-world programs;state-selection heuristics;target program,
902,Defect detection and prediction,Predicting Bugs by Monitoring Developers During Task Execution,G. Laudato; S. Scalabrino; N. Novielli; F. Lanubile; R. Oliveto,"STAKE Lab, University of Molise, Italy; STAKE Lab, University of Molise, Italy; University of Bari, Italy; University of Bari, Italy; STAKE Lab, University of Molise, Italy",2023,"Knowing which parts of the source code will be defective can allow practitioners to better allocate testing resources. For this reason, many approaches have been proposed to achieve this goal. Most state-of-the-art predictive models rely on product and process metrics, i.e., they predict the defectiveness of a component by considering what developers did. However, there is still limited evidence of the benefits that can be achieved in this context by monitoring how developers complete a development task. In this paper, we present an empirical study in which we aim at understanding whether measuring human aspects on developers while they write code can help predict the introduction of defects. First, we introduce a new developer-based model which relies on behavioral, psychophysical, and control factors that can be measured during the execution of development tasks. Then, we run a controlled experiment involving 20 software developers to understand if our developer-based model is able to predict the introduction of bugs. Our results show that a developer-based model is able to achieve a similar accuracy compared to a state-of-the-art code-based model, i.e., a model that uses only features measured from the source code. We also observed that by combining the models it is possible to obtain the best results (84% accuracy).",10.1109/ICSE48619.2023.00100,bug prediction;human aspects of software engineering;biometric sensors;empirical software engineering,Training;Codes;Source coding;Computer bugs;Predictive models;Software;Behavioral sciences,program debugging;program testing;software engineering;software quality,20 software developers;allocate testing resources;developer-based model;development task;monitoring developers;predicting bugs;source code;state-of-the-art code-based model;state-of-the-art predictive models;task execution,
903,Defect detection and prediction,Detecting Isolation Bugs via Transaction Oracle Construction,W. Dou; Z. Cui; Q. Dai; J. Song; D. Wang; Y. Gao; W. Wang; J. Wei; L. Chen; H. Wang; H. Zhong; T. Huang,"State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences; Inspur Software Group Co., Ltd.; Inspur Software Group Co., Ltd.; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Lab of Computer Science, Institute of Software, Chinese Academy of Sciences",2023,"Transactions are used to maintain the data integrity of databases, and have become an indispensable feature in modern Database Management Systems (DBMSs). Despite extensive efforts in testing DBMSs and verifying transaction processing mechanisms, isolation bugs still exist in widely-used DBMSs when these DBMSs violate their claimed transaction isolation levels. Isolation bugs can cause severe consequences, e.g., incorrect query results and database states. In this paper, we propose a novel transaction testing approach, Transaction oracle construction (Troc), to automatically detect isolation bugs in DBMSs. The core idea of Troc is to decouple a transaction into independent statements, and execute them on their own database views, which are constructed under the guidance of the claimed transaction isolation level. Any divergence between the actual transaction execution and the independent statement execution indicates an isolation bug. We implement and evaluate Troc on three widely-used DBMSs, i.e., MySQL, MariaDB, and TiDB. We have detected 5 previously-unknown isolation bugs in the latest versions of these DBMSs.",10.1109/ICSE48619.2023.00101,Database system;transaction;isolation;oracle,Data integrity;Computer bugs;Database systems;Testing;Software engineering;IEEE transactions,data integrity;database management systems;distributed databases;program debugging;query processing;transaction processing,actual transaction execution;claimed transaction isolation level;isolation bug;modern Database Management Systems;previously-unknown isolation bugs;Transaction oracle construction;transaction testing approach;verifying transaction;widely-used DBMSs,
904,Defect detection and prediction,SmallRace: Static Race Detection for Dynamic Languages - A Case on Smalltalk,S. Cui; Y. Gao; R. Unterguggenberger; W. Pichler; S. Livingstone; J. Huang,"Texas A&M University College Station, Texas; Texas A&M University College Station, Texas; Lam Research; Lam Research; Texas A&M University College Station, Texas; Texas A&M University College Station, Texas",2023,"Smalltalk, one of the first object-oriented programming languages, has had a tremendous influence on the evolution of computer technology. Due to the simplicity and productivity provided by the language, Smalltalk is still in active use today by many companies with large legacy codebases and with new code written every day. A crucial problem in Smalltalk programming is the race condition. Like in any other parallel language, debugging race conditions is inherently challenging, but in Smalltalk, it is even more challenging due to its dynamic nature. Being a purely dynamically-typed language, Smalltalk allows assigning any object to any variable without type restrictions, and allows forking new threads to execute arbitrary anonymous code blocks passed as objects. In Smalltalk, race conditions can be introduced easily, but are difficult to prevent at runtime. We present SmallRace, a novel static race detection framework designed for multithreaded dynamic languages, with a focus on Smalltalk. A key component of SmallRace is SmallIR, a subset of LLVM IR, in which all variables are declared with the same type-a generic pointer 18âœ¶. This allows SmallRace to design an effective interprocedural thread-sensitive pointer analysis to infer the concrete types of dynamic variables. SmallRace automatically translates Smalltalk source code into SmallIR, supports most of the modern Smalltalk syntax in Visual Works, and generates actionable race reports with detailed debugging information. Importantly, SmallRace has been used to analyze a production codebase in a large company with over a million lines of code, and it has found tens of complex race conditions in the production code.",10.1109/ICSE48619.2023.00102,,Productivity;Visualization;Parallel languages;Codes;Instruction sets;Source coding;Debugging,C++ language;Java;multi-threading;object-oriented programming;parallel languages;program compilers;program debugging;program diagnostics;program verification;Smalltalk;source code (software),complex race conditions;modern Smalltalk syntax;multithreaded dynamic languages;novel static race detection framework;object-oriented programming languages;parallel language;race condition;SmallRace;Smalltalk programming;Smalltalk source code,
905,Studies on gender in SE,â€œSTILL AROUNDâ€: Experiences and Survival Strategies of Veteran Women Software Developers,S. Van Breukelen; A. Barcombt; S. Baltes; A. Serebrenik,"Eindhoven University of Technology, The Netherlands; University of Calgary, Canada; University of Adelaide, Australia; Eindhoven University of Technology, The Netherlands",2023,"The intersection of ageism and sexism can create a hostile environment for veteran software developers belonging to marginalized genders. In this study, we conducted 14 interviews to examine the experiences of people at this intersection, primar-ily women, in order to discover the strategies they employed in order to successfully remain in the field. We identified 283 codes, which fell into three main categories: Strategies, Experiences, and Perception. Several strategies we identified, such as (Deliberately) Not Trying to Look Younger, were not previously described in the software engineering literature. We found that, in some compa-nies, older women developers are recognized as having particular value, further strengthening the known benefits of diversity in the workforce. Based on the experiences and strategies, we suggest organizations employing software developers to consider the benefits of hiring veteran women software developers. For example, companies can draw upon the life experiences of older women developers in order to better understand the needs of customers from a similar demographic. While we recognize that many of the strategies employed by our study participants are a response to systemic issues, we still consider that, in the short-term, there is benefit in describing these strategies for developers who are experiencing such issues today.",10.1109/ICSE48619.2023.00103,age;gender;intersectionality;software development;interview study;qualitative research,Industries;Codes;Engineering profession;Companies;Software;Distance measurement;Interviews,employment;gender issues;organisational aspects;personnel;software engineering,life experiences;older women developers;primar-ily women;software engineering literature;survival Strategies;veteran software developers;veteran women software developers,
906,AI testing 1,When and Why Test Generators for Deep Learning Produce Invalid Inputs: an Empirical Study,V. Riccio; P. Tonella,"UniversitÃ della Svizzera italiana, Lugano, Switzerland; UniversitÃ della Svizzera italiana, Lugano, Switzerland",2023,"Testing Deep Learning (DL) based systems inherently requires large and representative test sets to evaluate whether DL systems generalise beyond their training datasets. Diverse Test Input Generators (TIGs) have been proposed to produce artificial inputs that expose issues of the DL systems by triggering misbehaviours. Unfortunately, such generated inputs may be invalid, i.e., not recognisable as part of the input domain, thus providing an unreliable quality assessment. Automated validators can ease the burden of manually checking the validity of inputs for human testers, although input validity is a concept difficult to formalise and, thus, automate. In this paper, we investigate to what extent TIGs can generate valid inputs, according to both automated and human validators. We conduct a large empirical study, involving 2 different automated validators, 220 human assessors, 5 different TIGs and 3 classification tasks. Our results show that 84% artificially generated inputs are valid, according to automated validators, but their expected label is not always preserved. Automated validators reach a good consensus with humans (78% accuracy), but still have limitations when dealing with feature-rich datasets.",10.1109/ICSE48619.2023.00104,software testing;deep learning,Deep learning;Training;Generators;Software;Quality assessment;Task analysis;Testing,deep learning (artificial intelligence);pattern classification;program testing,artificial inputs;deep learning;DL systems;human testers;quality assessment;representative test sets;test input generators;TIG,
907,AI testing 1,Fuzzing Automatic Differentiation in Deep-Learning Libraries,C. Yang; Y. Deng; J. Yao; Y. Tu; H. Li; L. Zhang,"University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; The Chinese University of Hong Kong, Shenzhen; Huazhong University of Science and Technology; University of Science and Technology of China; University of Illinois, Urbana-Champaign",2023,"Deep learning (DL) has attracted wide attention and has been widely deployed in recent years. As a result, more and more research efforts have been dedicated to testing DL libraries and frameworks. However, existing work largely overlooked one crucial component of any DL system, automatic differentiation (AD), which is the basis for the recent development of DL. To this end, we propose âˆ‡Fuzz, the first general and practical approach specifically targeting the critical AD component in DL libraries. Our key insight is that each DL library API can be abstracted into a function processing tensors/vectors, which can be differentially tested under various execution scenarios (for computing outputs/gradients with different implementations). We have implemented $\nabla \text{Fuzz}$ as a fully automated API-level fuzzer targeting AD in DL libraries, which utilizes differential testing on different execution scenarios to test both first-order and high-order gradients, and also includes automated filtering strategies to remove false positives caused by numerical instability. We have performed an extensive study on four of the most popular and actively-maintained DL libraries, PyTorch, TensorFlow, JAX, and OneFlow. The result shows that $\nabla \text{Fuzz}$ substantially outperforms state-of-the-art fuzzers in terms of both code coverage and bug detection. To date, $\nabla \text{Fuzz}$ has detected 173 bugs for the studied DL libraries, with 144 already confirmed by developers (117 of which are previously unknown bugs and 107 are related to AD). Remarkably, $\nabla \text{Fuzz}$ contributed 58.3% (7/12) of all high-priority AD bugs for PyTorch and JAX during a two-month period. None of the confirmed AD bugs were detected by existing fuzzers.",10.1109/ICSE48619.2023.00105,,Deep learning;Codes;Filtering;Computer bugs;Fuzzing;Libraries;Engines,application program interfaces;computer network security;deep learning (artificial intelligence);learning (artificial intelligence);program debugging;program testing;security of data;tensors,bugdetection;code coverage;critical AD component;deep learning;deep-learning libraries;differential testingon different execution scenarios;DL libraries;DL library API;DL system;DLlibraries;existing fuzzers;false positives;fully automated API-level fuzzertargeting AD;general practicalapproach;high-order gradients;high-priorityAD bugs;JAX;numerical instability;outperformsstate-of-the-art fuzzers;overlooked onecrucial component;proposeâˆ‡Fuzz;PyTorch;remarkably âˆ‡Fuzz;studiedDL libraries;testing DL librariesand frameworks;thatâˆ‡Fuzz;wehave implementedâˆ‡Fuzz;wide attention andhas,1
908,AI testing 1,Lightweight Approaches to DNN Regression Error Reduction: An Uncertainty Alignment Perspective,Z. Li; M. Zhang; J. Xu; Y. Yao; C. Cao; T. Chen; X. Ma; J. LÃ¼,"Department of Computer Science and Technology, State Key Lab of Novel Software Technology, Nanjing University, China; Department of Computer Science and Technology, State Key Lab of Novel Software Technology, Nanjing University, China; Department of Computer Science and Technology, State Key Lab of Novel Software Technology, Nanjing University, China; Department of Computer Science and Technology, State Key Lab of Novel Software Technology, Nanjing University, China; Department of Computer Science and Technology, State Key Lab of Novel Software Technology, Nanjing University, China; Department of Computer Science, Birkbeck, University of London, UK; Department of Computer Science and Technology, State Key Lab of Novel Software Technology, Nanjing University, China; Department of Computer Science and Technology, State Key Lab of Novel Software Technology, Nanjing University, China",2023,"Regression errors of Deep Neural Network (DNN) models refer to the case that predictions were correct by the old-version model but wrong by the new-version model. They frequently occur when upgrading DNN models in production systems, causing disproportionate user experience degradation. In this paper, we propose a lightweight regression error reduction approach with two goals: 1) requiring no model retraining and even data, and 2) not sacrificing the accuracy. The proposed approach is built upon the key insight rooted in the unmanaged model uncertainty, which is intrinsic to DNN models, but has not been thoroughly explored especially in the context of quality assurance of DNN models. Specifically, we propose a simple yet effective ensemble strategy that estimates and aligns the two models' uncertainty. We show that a Pareto improvement that reduces the regression errors without compromising the overall accuracy can be guaranteed in theory and largely achieved in practice. Comprehensive experiments with various representative models and datasets confirm that our approaches significantly outperform the state-of-the-art alternatives.",10.1109/ICSE48619.2023.00106,Software regression;deep neural networks;uncertainty alignment;model ensemble,Degradation;Production systems;Uncertainty;Quality assurance;Artificial neural networks;Predictive models;Software,deep learning (artificial intelligence);regression analysis;user experience,deep neural network models;disproportionate user experience degradation;DNN regression error reduction;lightweight regression error reduction approach;new-version model;old-version model;uncertainty alignment perspective;unmanaged model uncertainty,
909,AI testing 1,Revisiting Neuron Coverage for DNN Testing: A Layer-Wise and Distribution-Aware Criterion,Y. Yuan; Q. Pang; S. Wang,"The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China",2023,"Various deep neural network (DNN) coverage criteria have been proposed to assess DNN test inputs and steer input mutations. The coverage is characterized via neurons having certain outputs, or the discrepancy between neuron outputs. Nevertheless, recent research indicates that neuron coverage criteria show little correlation with test suite quality. In general, DNNs approximate distributions, by incorporating hierarchical layers, to make predictions for inputs. Thus, we champion to deduce DNN behaviors based on its approximated distributions from a layer perspective. A test suite should be assessed using its induced layer output distributions. Accordingly, to fully examine DNN behaviors, input mutation should be directed toward diversifying the approximated distributions. This paper summarizes eight design requirements for DNN coverage criteria, taking into account distribution properties and practical concerns. We then propose a new criterion, Neural Coverage (nlc),that satisfies all design requirements. NLC treats a single DNN layer as the basic computational unit (rather than a single neuron) and captures four critical properties of neuron output distributions. Thus, NL C accurately describes how DNNs comprehend inputs via approximated distributions. We demonstrate that NLC is significantly correlated with the diversity of a test suite across a number of tasks (classification and generation) and data formats (image and text). Its capacity to discover DNN prediction errors is promising. Test input mutation guided by NLC results in a greater quality and diversity of exposed erroneous behaviors.",10.1109/ICSE48619.2023.00107,machine learning testing;coverage,Correlation;Neurons;Artificial neural networks;Fuzzing;Behavioral sciences;Task analysis;Optimization,approximation theory;deep learning (artificial intelligence);program testing;statistical distributions,approximated distributions;data formats;deep neural network coverage criteria;design requirements;distribution properties;distribution-aware criterion;DNN behaviors;DNN coverage criteria;DNN layer;DNN prediction errors;DNN test inputs;DNN testing;hierarchical layers;induced layer output distributions;input mutation;layer-wise criterion;neuron coverage criteria;neuron output distributions;neuron outputs;NLC;software testing;test suite quality,1
910,Code review,"Code Review of Build System Specifications: Prevalence, Purposes, Patterns, and Perceptions",M. Nejati; M. Alfadel; S. McIntosh,"Software REBELs, University of Waterloo, Canada; Software REBELs, University of Waterloo, Canada; Software REBELs, University of Waterloo, Canada",2023,"Build systems automate the integration of source code into executables. Maintaining build systems is known to be challenging. Lax build maintenance can lead to costly build breakages or unexpected software behaviour. Code review is a broadly adopted practice to improve software quality. Yet, little is known about how code review is applied to build specifications. In this paper, we present the first empirical study of how code review is practiced in the context of build specifications. Through quantitative analysis of 502,931 change sets from the Qt and Eclipse communities, we observe that changes to build specifications are at least two times less frequently discussed during code review when compared to production and test code changes. A qualitative analysis of 500 change sets reveals that (i) comments on changes to build specifications are more likely to point out defects than rates reported in the literature for production and test code, and (ii) evolvability and dependency-related issues are the most frequently raised patterns of issues. Follow-up interviews with nine developers with 1â€“40 years of experience point out social and technical factors that hinder rigorous review of build specifications, such as a prevailing lack of understanding of and interest in build systems among developers, and the lack of dedicated tooling to support the code review of build specifications.",10.1109/ICSE48619.2023.00108,build systems;build specifications;code review,Codes;Statistical analysis;Source coding;Production;Software quality;Maintenance engineering;Interviews,program testing;reviews;software maintenance;software quality;source code (software),build specifications;build system specifications;build systems automate;code review;software behaviour;software quality;source code;test code changes,1
911,Program repair techniques and applications,Better Automatic Program Repair by Using Bug Reports and Tests Together,M. Motwani; Y. Brun,"University of Massachusetts, Amherst, Massachusetts, USA; University of Massachusetts, Amherst, Massachusetts, USA",2023,"Automated program repair is already deployed in industry, but concerns remain about repair quality. Recent research has shown that one of the main reasons repair tools produce incorrect (but seemingly correct) patches is imperfect fault localization (FL). This paper demonstrates that combining information from natural-language bug reports and test executions when localizing faults can have a significant positive impact on repair quality. For example, existing repair tools with such FL are able to correctly repair 7 defects in the Defects4J benchmark that no prior tools have repaired correctly. We develop, Blues, the first information-retrieval-based, statement-level FL technique that requires no training data. We further develop RAFL, the first unsupervised method for combining multiple FL techniques, which outperforms a supervised method. Using RAFL, we create SBIR by combining Blues with a spectrum-based (SBFL) technique. Evaluated on 815 real-world defects, SBIR consistently ranks buggy statements higher than its underlying techniques. We then modify three state-of-the-art repair tools, Arja, SequenceR, and SimFix, to use SBIR, SBFL, and Blues as their internal FL. We evaluate the quality of the produced patches on 689 real-world defects. Arja and SequenceR significantly benefit from SBIR: Arja using SBIR correctly repairs 28 defects, but only 21 using SBFL, and only 15 using Blues; SequenceR using SBIR correctly repairs 12 defects, but only 10 using SBFL, and only 4 using Blues. SimFix, (which has internal mechanisms to overcome poor FL), correctly repairs 30 defects using SBIR and SBFL, but only 13 using Blues. Our work is the first investigation of simultaneously using multiple software artifacts for automated program repair, and our promising findings suggest future research in this directions is likely to be fruitful.",10.1109/ICSE48619.2023.00109,Automatic Program repair;Information retrieval based fault localization;Debugging;fault localization,Location awareness;Industries;Computer bugs;Training data;Maintenance engineering;Benchmark testing;Software,information retrieval;learning (artificial intelligence);maintenance engineering;program debugging;program diagnostics;program testing;software fault tolerance;software maintenance,Arja;automated program repair;better automatic program repair;Blues;but seemingly correct;Defects4J benchmark;imperfect fault localization;incorrect patches;information-retrieval-based;internal FL;localizing faults;main reasons repair tools;multiple FL techniques;natural-language bug reports;poor FL;prior tools;real-world defects;repair 7 defects;repair quality;repairs 30 defects;SBFL;SBIR correctly repairs;SequenceR;spectrum-based technique;state-of-the-art repair tools;statement-level FL technique,
912,Program repair techniques and applications,CCTEST: Testing and Repairing Code Completion Systems,Z. Li; C. Wang; Z. Liu; H. Wang; D. Chen; S. Wang; C. Gao,"The Hong Kong University of Science and Technology, Hong Kong SAR; Harbin Institute of Technology, Shenzhen, China; The Hong Kong University of Science and Technology, Hong Kong SAR; Swiss Federal Institute of Technology Lausanne, Switzerland; The Hong Kong University of Science and Technology, Hong Kong SAR; The Hong Kong University of Science and Technology, Hong Kong SAR; Harbin Institute of Technology, Shenzhen, China",2023,"Code completion, a highly valuable topic in the software development domain, has been increasingly promoted for use by recent advances in large language models (LLMs). To date, visible LLM-based code completion frameworks such as GitHub Copilot and GPT are trained using deep learning over vast quantities of unstructured text and open source code. As the paramount component and the cornerstone in daily programming tasks, code completion has largely boosted professionals' efficiency in building real-world software systems. In contrast to this flourishing market, we find that code completion systems often output suspicious results, and to date, an automated testing and enhancement framework for code completion systems is not available. This research proposes CCTEST, a framework to test and repair code completion systems in black-box settings. CCTEST features a set of novel mutation strategies, namely program structure-consistent (PSC) mutations, to generate mutated code completion inputs. Then, it detects inconsistent outputs, representing possibly erroneous cases, from all the completed code cases. Moreover, CCTEST repairs the code completion outputs by selecting the output that mostly reflects the â€œaverageâ€ appearance of all output cases, as the final output of the code completion systems. With around 18K test inputs, we detected 33,540 inputs that can trigger erroneous cases (with a true positive rate of 86%) from eight popular LLM-based code completion systems. With repairing, we show that the accuracy of code completion systems is notably increased by 40% and 67% with respect to BLEU score and Levenshtein edit similarity.",10.1109/ICSE48619.2023.00110,,Deep learning;Codes;Source coding;Closed box;Maintenance engineering;Task analysis;Programming profession,deep learning (artificial intelligence);natural language processing;program testing;software engineering;source code (software),black-box settings;BLEU score;CCTEST features;code completion outputs;completed code cases;GitHub copilot;GPT;large language models;Levenshtein edit similarity;LLM-based code completion systems;mutated code completion inputs;open source code;program structure-consistent mutations;PSC mutations;repairing,
913,Program repair techniques and applications,KNOD: Domain Knowledge Distilled Tree Decoder for Automated Program Repair,N. Jiang; T. Lutellier; Y. Lou; L. Tan; D. Goldwasser; X. Zhang,"Purdue University, West Lafayette, USA; University of Alberta, Alberta, Canada; Fudan University, Shanghai, China; Purdue University, West Lafayette, USA; Purdue University, West Lafayette, USA; Purdue University, West Lafayette, USA",2023,"Automated Program Repair (APR) improves soft-ware reliability by generating patches for a buggy program automatically. Recent APR techniques leverage deep learning (DL) to build models to learn to generate patches from existing patches and code corpora. While promising, DL-based APR techniques suffer from the abundant syntactically or semantically incorrect patches in the patch space. These patches often disobey the syntactic and semantic domain knowledge of source code and thus cannot be the correct patches to fix a bug. We propose a DL-based APR approach KNOD, which in-corporates domain knowledge to guide patch generation in a direct and comprehensive way. KNOD has two major novelties, including (1) a novel three-stage tree decoder, which directly generates Abstract Syntax Trees of patched code according to the inherent tree structure, and (2) a novel domain-rule distillation, which leverages syntactic and semantic rules and teacher-student distributions to explicitly inject the domain knowledge into the decoding procedure during both the training and inference phases. We evaluate KNOD on three widely-used benchmarks. KNOD fixes 72 bugs on the Defects4J v1.2, 25 bugs on the QuixBugs, and 50 bugs on the additional Defects4J v2.0 benchmarks, outperforming all existing APR tools.",10.1109/ICSE48619.2023.00111,Automated Program Repair;Abstract Syntax Tree;Deep Learning,Training;Codes;Source coding;Computer bugs;Semantics;Syntactics;Benchmark testing,authoring languages;C++ language;deep learning (artificial intelligence);learning (artificial intelligence);program debugging;program diagnostics;program testing;software maintenance;source code (software);trees (mathematics),25 bugs;abstract Syntax Trees;abundant syntactically;APR approach KNOD;automated program repair;buggy program;code corpora;correct patches;decoding procedure;DL-based APR techniques;domain knowledge distilled tree decoder;existing APR tools;generate patches;in-corporates domain knowledge;inherent tree structure;KNOD fixes 72 bugs;novel domain-rule distillation;patch generation;patch space;patched code;recent APR techniques leverage deep learning;semantic domain knowledge;semantic rules;semantically incorrect patches;soft-ware reliability;source code;syntactic rules;three-stage tree decoder,2
914,Program repair techniques and applications,Rete: Learning Namespace Representation for Program Repair,N. Parasaram; E. T. Barr; S. Mechtaev,"University College London, London, United Kingdom; University College London, London, United Kingdom; University College London, London, United Kingdom",2023,"A key challenge of automated program repair is finding correct patches in the vast search space of candidate patches. Real-world programs define large namespaces of variables that considerably contributes to the search space explosion. Existing program repair approaches neglect information about the program namespace, which makes them inefficient and increases the chance of test-overfitting. We propose Rete, a new program repair technique, that learns project-independent information about program namespace and uses it to navigate the search space of patches. Rete uses a neural network to extract project-independent information about variable CDU chains, def-use chains augmented with control flow. Then, it ranks patches by jointly ranking variables and the patch templates into which the variables are inserted. We evaluated Rete on 142 bugs extracted from two datasets, ManyBugs and BugsInPy. Our experiments demonstrate that ReTe generates six new correct patches that fix bugs that previous tools did not repair, an improvement of 31% and 59% over the existing state of the art.",10.1109/ICSE48619.2023.00112,Program Repair;Deep Learning;Patch Prioritisation;Variable Representation,Navigation;Computer bugs;Neural networks;Maintenance engineering;Aerospace electronics;Explosions;Data mining,program debugging;program testing;software maintenance,automated program repair;candidate patches;namespace representation;patch templates;program namespace;program repair technique;project-independent information;real-world programs;ReTe;search space explosion;variable CDU chains;vast search space,
915,Requirements elicitation and understanding,AI-based Question Answering Assistance for Analyzing Natural-language Requirements,S. Ezzini; S. Abualhaija; C. Arora; M. Sabetzadeh,"SnT Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; SnT Centre for Security, Reliability and Trust, University of Luxembourg, Luxembourg; Deakin University, Geelong, Australia; School of Electrical Engineering and Computer Science, University of Ottawa, Canada",2023,"By virtue of being prevalently written in natural language (NL), requirements are prone to various defects, e.g., inconsistency and incompleteness. As such, requirements are frequently subject to quality assurance processes. These processes, when carried out entirely manually, are tedious and may further overlook important quality issues due to time and budget pressures. In this paper, we propose QAssist - a question-answering (QA) approach that provides automated assistance to stakeholders, including requirements engineers, during the analysis of NL requirements. Posing a question and getting an instant answer is beneficial in various quality-assurance scenarios, e.g., incompleteness detection. Answering requirements-related questions automatically is challenging since the scope of the search for answers can go beyond the given requirements specification. To that end, QAssist provides support for mining external domain-knowledge resources. Our work is one of the first initiatives to bring together QA and external domain knowledge for addressing requirements engineering challenges. We evaluate QAssist on a dataset covering three application domains and containing a total of 387 question-answer pairs. We experiment with state-of-the-art QA methods, based primarily on recent large-scale language models. In our empirical study, QAssist localizes the answer to a question to three passages within the requirements specification and within the external domain-knowledge resource with an average recall of 90.1% and 96.5%, respectively. QAssist extracts the actual answer to the posed question with an average accuracy of 84.2%.",10.1109/ICSE48619.2023.00113,Natural-language Requirements;Question Answering (QA);Language Models;Natural Language Processing (NLP);Natural Language Generation (NLG);BERT;T5,Knowledge engineering;Quality assurance;Terminology;Natural languages;Question answering (information retrieval);Internet;Stakeholders,data mining;formal specification;natural language processing;quality assurance;question answering (information retrieval);systems analysis,387 question-answer pairs;actual answer;analyzing natural-language requirements;answering requirements-related questions;application domains;automated assistance;budget pressures;external domain knowledge;external domain-knowledge resource;given requirements specification;important quality issues;incompleteness detection;instant answer;large-scale language models;mining external domain-knowledge resources;natural language;NL requirements;posed question;QAssist;quality assurance processes;question answering assistance;question-answering approach;requirements engineering challenges;requirements engineers;state-of-the-art QA methods,
916,Requirements elicitation and understanding,"Strategies, Benefits and Challenges of App Store-inspired Requirements Elicitation",A. Ferrari; P. Spoletini,"ISTI-CNR; Kennesaw State University, USA",2023,"App store-inspired elicitation is the practice of exploring competitors' apps, to get inspiration for requirements. This activity is common among developers, but little insight is available on its practical use, advantages and possible issues. This paper aims to empirically analyse this technique in a realistic scenario, in which it is used to extend the requirements of a product that were initially captured by means of more traditional requirements elicitation interviews. Considering this scenario, we conduct an experimental simulation with 58 analysts and collect qualitative data. We perform thematic analysis of the data to identify strategies, benefits, and challenges of app store-inspired elicitation, as well as differences with respect to interviews in the considered elicitation setting. Our results show that: (1) specific guidelines and procedures are required to better conduct app store-inspired elicitation; (2) current search features made available by app stores are not suitable for this practice, and more tool support is required to help analysts in the retrieval and evaluation of competing products; (3) while interviews focus on the why dimension of requirements engineering (i.e., goals), app store-inspired elicitation focuses on how (i.e., solutions), offering indications for implementation and improved usability. Our study provides a framework for researchers to address existing challenges and suggests possible benefits to fostering app store-inspired elicitation among practitioners.",10.1109/ICSE48619.2023.00114,app store inspired elicitation;app store analysis;requirements elicitation;interviews;qualitative study;experimental simulation,Surveys;Industries;Analytical models;Systematics;Data models;Requirements engineering;Interviews,formal specification;mobile computing;systems analysis,app store-inspired requirements elicitation;competitor apps;requirements elicitation interviews;requirements engineering;usability,
917,Software verification,Data-driven Recurrent Set Learning For Non-termination Analysis,Z. Han; F. He,"Key Laboratory for Information System Security, MoE Beijing National Research Center for Information Science and Technology, School of Software, Tsinghua University, Beijing, China; Key Laboratory for Information System Security, MoE Beijing National Research Center for Information Science and Technology, School of Software, Tsinghua University, Beijing, China",2023,"Termination is a fundamental liveness property for program verification. In this paper, we revisit the problem of non-termination analysis and propose the first data-driven learning algorithm for synthesizing recurrent sets, where the non-terminating samples are effectively speculated by a novel method. To ensure convergence of learning, we develop a learning algorithm which is guaranteed to converge to a valid recurrent set if one exists, and thus establish its relative completeness. The methods are implemented in a prototype tool, and experimental results on public benchmarks show its efficacy in proving non-termination as it outperforms state-of-the-art tools, both in terms of cases solved and performance. Evaluation on non-linear programs also demonstrates its ability to handle complex programs.",10.1109/ICSE48619.2023.00115,program termination;recurrent set;data-driven approach;black-box learning,Closed box;Prototypes;Benchmark testing;Decision trees;Convergence;Software engineering,learning (artificial intelligence);linear programming;program diagnostics;program verification,data-driven recurrent set learning;fundamental liveness property;learning algorithm;nonlinear programs;nonterminating samples;nontermination analysis;program verification;recurrent sets;valid recurrent,
918,Software verification,Compiling Parallel Symbolic Execution with Continuations,G. Wei; S. Jia; R. Gao; H. Deng; S. Tan; O. BraÄevac; T. Rompf,"Department of Computer Science, Purdue University, West Lafayette, IN, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA; Department of EECS, UC Berkeley, Purdue University, Berkeley, CA, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA; Department of Computer Science, Purdue University, West Lafayette, IN, USA",2023,"Symbolic execution is a powerful program analysis and testing technique. Symbolic execution engines are usually implemented as interpreters, and the induced interpretation over-head can dramatically inhibit performance. Alternatively, implementation choices based on instrumentation provide a limited ability to transform programs. However, the use of compilation and code generation techniques beyond simple instrumentation remains underexplored for engine construction, leaving potential performance gains untapped. In this paper, we show how to tap some of these gains using sophisticated compilation techniques: We present Gensym, an optimizing symbolic-execution compiler that generates symbolic code which explores paths and generates tests in parallel. The key insight of GensYmis to compile symbolic execution tasks into cooperative concurrency via continuation-passing style, which further enables efficient parallelism. The design and implementation of Gensym is based on partial evaluation and generative programming techniques, which make it high-level and performant at the same time. We compare the performance of Gensym against the prior symbolic-execution compiler LLSC and the state-of-the-art symbolic interpreter KLEE. The results show an average 4.6Ã— speedup for sequential execution and 9.4Ã— speedup for parallel execution on 20 benchmark programs.",10.1109/ICSE48619.2023.00116,symbolic execution;compiler;code generation;metaprogramming;continuation,Concurrent computing;Codes;Instruments;Transforms;Programming;Performance gain;Parallel processing,program compilers;program diagnostics;program testing,20 benchmark programs;code generation techniques;continuation-passing style;continuations;efficient parallelism;engine construction;generative programming techniques;Gensym;implementation choices;induced interpretation over-head;interpreters;optimizing symbolic-execution compiler;parallel execution;parallel symbolic execution;performant;potential performance gains;powerful program analysis;simple instrumentation;sophisticated compilation techniques;state-of-the-art symbolic interpreter KLEE;symbolic code;symbolic execution engines;symbolic execution tasks;symbolic-execution compiler LLSC;testing technique,
919,Software verification,Verifying Data Constraint Equivalence in FinTech Systems,C. Wang; G. Fan; P. Yao; F. Pan; C. Zhang,"Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China; Ant Group, Shenzhen, China; Zhejiang University, Hangzhou, China; Ant Group, Shenzhen, China; Department of Computer Science and Engineering, The Hong Kong University of Science and Technology, Hong Kong, China",2023,"Data constraints are widely used in FinTech systems for monitoring data consistency and diagnosing anomalous data manipulations. However, many equivalent data constraints are created redundantly during the development cycle, slowing down the FinTech systems and causing unnecessary alerts. We present EQDAC, an efficient decision procedure to determine the data constraint equivalence. We first propose the symbolic representation for semantic encoding and then introduce two light-weighted analyses to refute and prove the equivalence, respectively, which are proved to achieve in polynomial time. We evaluate EQDAC upon 30,801 data constraints in a FinTech system. It is shown that EQDAC detects 11,538 equivalent data constraints in three hours. It also supports efficient equivalence searching with an average time cost of 1.22 seconds, enabling the system to check new data constraints upon submission.",10.1109/ICSE48619.2023.00117,Equivalence Verification;Data Constraints;Fin-Tech Systems,Costs;Semantics;Production;Companies;Maintenance engineering;Encoding;Monitoring,computational complexity;constraint handling;data handling;financial data processing,anomalous data manipulations;data consistency monitoring;data constraint equivalence;decision procedure;EQDAC;FinTech system;polynomial time;semantic encoding,
920,Software verification,Tolerate Control-Flow Changes for Sound Data Race Prediction,S. Zhu; Y. Guo; L. Zhang; Y. Cai,"State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences, University of Chinese Academy of Sciences, Beijing, China",2023,"Data races seriously threaten the correctness of concurrent programs. Earlier works can report false positives. Recently, trace-based predictive analysis has achieved sound results by inferring feasible traces based on sound partial orders or constraint solvers. However, they hold the same assumption: any read event may affect the control-flow of a predicted trace. Thus, being control-flow sensitive, they have to enforce any read event (in an inferred trace) to either read the same value or a value from the same event as that in the original trace, albeit some slightly relax this. This (even with relaxation) severely limits their predictive ability and many true data races can be missed. We introduce the concept of Fix-Point Event and propose a new partial order model. This allows us to not only predict races with witness traces (like existing works with no control-flow changes) but also soundly infer existences of witness traces with potential control-flow changes. Thus, we can achieve a higher concurrency coverage and detect more data races soundly. We have implemented above as a tool ToccRACE and conducted a set of experiments on a benchmark of seven real-world programs and a large-scale software MySQL, where MySQL produced 427 traces with a total size of 3.4TB. Compared with the state-of-the-art sound data race detector SeqCheck,ToccRACE is significantly more effective by detecting 84.4%/200% more unique/dynamic races on the benchmark programs and 52.22%/49.8% more unique/dynamic races on MySQL, incurring reasonable time and memory costs (about 1.1xÃ—43.5x on the benchmark programs and 10x/1.03x on MySQL). Furthermore, ToccRACE is sound and is comnlcte on two threads.",10.1109/ICSE48619.2023.00118,Concurrency bugs;data races;control flow;static information,Concurrent computing;Costs;Instruction sets;Detectors;Benchmark testing;Programming;Predictive models,program diagnostics,concurrent programs;constraint solvers;fix-point event;large-scale software MySQL;sound data race prediction;sound partial orders model;ToccRACE;tolerate control-flow changes;trace-based predictive analysis;witness traces,
921,"Testing of mobile, web and games",Fill in the Blank: Context-aware Automated Text Input Generation for Mobile GUI Testing,Z. Liu; C. Chen; J. Wang; X. Che; Y. Huang; J. Hu; Q. Wang,"State Key Laboratory of Intelligent Game, Beijing, China; Monash University, Melbourne, Australia; State Key Laboratory of Intelligent Game, Beijing, China; State Key Laboratory of Intelligent Game, Beijing, China; State Key Laboratory of Intelligent Game, Beijing, China; State Key Laboratory of Intelligent Game, Beijing, China; State Key Laboratory of Intelligent Game, Beijing, China",2023,"Automated GUI testing is widely used to help ensure the quality of mobile apps. However, many GUIs require appropriate text inputs to proceed to the next page, which remains a prominent obstacle for testing coverage. Considering the diversity and semantic requirement of valid inputs (e.g., flight departure, movie name), it is challenging to automate the text input generation. Inspired by the fact that the pre-trained Large Language Model (LLM) has made outstanding progress in text generation, we propose an approach named QTypist based on LLM for intelligently generating semantic input text according to the GUI context. To boost the performance of LLM in the mobile testing scenario, we develop a prompt-based data construction and tuning method which automatically extracts the prompts and answers for model tuning. We evaluate QTypist on 106 apps from Google Play, and the result shows that the passing rate of QTypist is 87%, which is 93% higher than the best baseline. We also integrate QTypist with the automated GUI testing tools and it can cover 42% more app activities, 52% more pages, and subsequently help reveal 122% more bugs compared with the raw tool.",10.1109/ICSE48619.2023.00119,Text input generation;GUI testing;Android app;Large language model;Prompt-tuning,Semantics;Computer bugs;Motion pictures;Data models;Internet;Data mining;Tuning,graphical user interfaces;mobile computing;program debugging;program testing;text analysis,appropriate text inputs;automated GUI testing tools;context-aware automated text input generation;GUI context;GUIs;LLM;mobile apps;mobile GUI testing;mobile testing scenario;model tuning;movie name;page;pre-trained Large Language Model;prominent obstacle;prompt-based data construction;QTypist;semantic input text;semantic requirement;testing coverage;text generation;tuning method;valid inputs,
922,"Testing of mobile, web and games",Detecting Dialog-Related Keyboard Navigation Failures in Web Applications,P. T. Chiou; A. S. Alotaibi; W. G. J. Halfond,"University of Southern California, USA; University of Southern California, USA; University of Southern California, USA",2023,"The ability to navigate the Web via the keyboard interface is critical to people with various types of disabilities. However, modern websites often violate web accessibility guidelines for keyboard navigability with respect to web dialogs. In this paper, we present a novel approach for automatically detecting web accessibility bugs that prevent or hinder keyboard users' ability to navigate dialogs in web pages. An extensive evaluation of our technique on real-world subjects showed that our technique is effective in detecting these dialog-related keyboard navigation failures.",10.1109/ICSE48619.2023.00120,Web Accessibility;WCAG;Software Testing;Keyboard Navigation;Dialog;Keyboard Accessibility;Web Dialog;Accessible Dialog;Dialog Accessibility,Navigation;Computer bugs;Keyboards;Web pages;Debugging;Software engineering;Guidelines,handicapped aids;interactive systems;Internet;keyboards;Web sites,dialog-related keyboard navigation failures;keyboard interface;keyboard navigability;keyboard users;Web accessibility bugs;web accessibility guidelines;Web applications;Web dialogs;Web pages,
923,"Testing of mobile, web and games",Columbus: Android App Testing Through Systematic Callback Exploration,P. Bose; D. Das; S. Vasan; S. Mariani; I. Grishchenko; A. Continella; A. Bianchi; C. Kruegel; G. Vigna,"University of California, Santa Barbara; University of California, Santa Barbara; University of California, Santa Barbara; VMware, Inc.; University of California, Santa Barbara; University of Twente; Purdue University; University of California, Santa Barbara; University of California, Santa Barbara",2023,"With the continuous rise in the popularity of Android mobile devices, automated testing of apps has become more important than ever. Android apps are event-driven programs. Unfortunately, generating all possible types of events by interacting with an app's interface is challenging for an automated testing approach. Callback-driven testing eliminates the need for event generation by directly invoking app callbacks. However, existing callback-driven testing techniques assume prior knowledge of Android callbacks, and they rely on a human expert, who is familiar with the Android API, to write stub code that prepares callback arguments before invocation. Since the Android API is very large and keeps evolving, prior techniques could only support a small fraction of callbacks present in the Android framework. In this work, we introduce Columbus, a callback-driven testing technique that employs two strategies to eliminate the need for human involvement: (i) it automatically identifies callbacks by simultaneously analyzing both the Android framework and the app under test; (ii) it uses a combination of under-constrained symbolic execution (primitive arguments), and type-guided dynamic heap introspection (object arguments) to generate valid and effective inputs. Lastly, Columbus integrates two novel feedback mechanisms-data dependency and crash-guidance- during testing to increase the likelihood of triggering crashes and maximizing coverage. In our evaluation, Columbus outperforms state-of-the-art model-driven, checkpoint-based, and callback-driven testing tools both in terms of crashes and coverage.",10.1109/ICSE48619.2023.00121,Android;app testing;callback,Systematics;Codes;Computer bugs;Computer crashes;Object recognition;Smart phones;Testing,Android (operating system);application program interfaces;checkpointing;mobile computing;program testing,Android API;Android app testing;Android apps;Android callbacks;Android framework;Android mobile devices;app callbacks;automated testing approach;callback arguments;callback-driven testing technique;Columbus;continuous rise;event generation;event-driven programs;existing callback-driven testing techniques;state-of-the-art model-driven;systematic callback exploration;testing tools;type-guided dynamic heap introspection;under-constrained symbolic execution,
924,"Testing of mobile, web and games",GameRTS: A Regression Testing Framework for Video Games,J. Yu; Y. Wu; X. Xie; W. Le; L. Ma; Y. Chen; J. Hu; F. Zhang,"Singapore Management University, Singapore; NetEase Fuxi AI Lab, China; Singapore Management University, Singapore; Iowa State University, USA; University of Alberta, Canada; NetEase Fuxi AI Lab, China; NetEase Fuxi AI Lab, China; Zhejiang University, China",2023,"Continuous game quality assurance is of great importance to satisfy the increasing demands of users. To respond to game issues reported by users timely, game com-panies often create and maintain a large number of releases, updates, and tweaks in a short time. Regression testing is an essential technique adopted to detect regression issues during the evolution of the game software. However, due to the special characteristics of game software (e.g., frequent updates and long-running tests), traditional regression testing techniques are not directly applicable. To bridge this gap, in this paper, we perform an early exploratory study to investigate the challenges in regression testing of video games. We first performed empirical studies to better understand the game development process, bugs introduced during game evolution, and the context sensitivity. Based on the results of the study, we proposed the first regression test selection (RTS) technique for game software, which is a compromise between safety and practicality. In particular, we model the test suite of game software as a State Transition Graph (STG) and then perform the RTS on the STG. We establish the dependencies between the states/actions of STG and game files, including game art resources, game design files, and source code, and perform change impact analysis to identify the states/actions (in the STG) that potentially execute such changes. We implemented our framework in a tool, named GameRTS, and evaluated its usefulness on 10 tasks of a large-scale commercial game, including a total of 1,429 commits over three versions. The experimental results demonstrate the usefulness and effectiveness of GameRTS in game RTS. For most tasks, GameRTS only selected one trace from STG, which can significantly reduce the testing time. Furthermore, GameRTS detects all the regression bugs from the test evaluation suites. Compared with the file-level RTS, GameRTS selected fewer states/actions/traces (i.e., 13.77%, 23.97%, 6.85%). In addition, GameRTS identified 2 new critical regression bugs in the game.",10.1109/ICSE48619.2023.00122,Game Testing;Regression Testing;Testing Cases Selection;State Transition Graph,Video games;Sensitivity;Source coding;Computer bugs;Games;Software;Safety,computer games;program testing;quality assurance;regression analysis,2 new critical regression bugs;continuous game quality assurance;game art resources;game design files;game development;game evolution;game files;game issues;game RTS;game software;GameRTS;large-scale commercial game;long-running tests;regression issues;regression test selection technique;regression testing;STG;test evaluation suites;test suite;video games,
925,Recommender systems,Autonomy Is An Acquired Taste: Exploring Developer Preferences for GitHub Bots,A. Ghorbani; N. Cassee; D. Robinson; A. Alami; N. A. Ernst; A. Serebrenik; A. WÄ…sowski,"University of Victoria, Canada; University of Victoria, Canada; University of Victoria, Canada; Aalborg University, Denmark; University of Victoria, Canada; Eindhoven University of Technology, The Netherlands; IT University of Copenhagen, Denmark",2023,"Software bots fulfill an important role in collective software development, and their adoption by developers promises increased productivity. Past research has identified that bots that communicate too often can irritate developers, which affects the utility of the bot. However, it is not clear what other properties of human-bot collaboration affect developers' preferences, or what impact these properties might have. The main idea of this paper is to explore characteristics affecting developer preferences for interactions between humans and bots, in the context of GitHub pull requests. We carried out an exploratory sequential study with interviews and a subsequent vignette-based survey. We find developers generally prefer bots that are personable but show little autonomy, however, more experienced developers tend to prefer more autonomous bots. Based on this empirical evidence, we recommend bot developers increase configuration options for bots so that individual developers and projects can configure bots to best align with their own preferences and project cultures.",10.1109/ICSE48619.2023.00123,Software Bot;Pull Request;Human Aspects,Surveys;Productivity;Collaboration;Chatbots;Software;Interviews;Software development management,public domain software;software quality,autonomous bots;bot developers;GitHub bots;GitHub pull requests;human-bot collaboration;software bots;software development,
926,Recommender systems,Flexible and Optimal Dependency Management via Max-SMT,D. Pinckney; F. Cassano; A. Guha; J. Bell; M. Culpo; T. Gamblin,"Northeastern University, Boston, USA; Northeastern University, Boston, USA; Northeastern University, Boston, USA; Northeastern University, Boston, USA; np-complete, S.r.l., Mantova, Italy; Lawrence Livermore National Laboratory, Livermore, USA",2023,"Package managers such as NPM have become essential for software development. The NPM repository hosts over 2 million packages and serves over 43 billion downloads every week. Unfortunately, the NPM dependency solver has several shortcomings. 1) NPM is greedy and often fails to install the newest versions of dependencies; 2) NPM's algorithm leads to duplicated dependencies and bloated code, which is particularly bad for web applications that need to minimize code size; 3) NPM's vulnerability fixing algorithm is also greedy, and can even introduce new vulnerabilities; and 4) NPM's ability to duplicate dependencies can break stateful frameworks and requires a lot of care to workaround. Although existing tools try to address these problems they are either brittle, rely on post hoc changes to the dependency tree, do not guarantee optimality, or are not composable. We present Pacsolve, a unifying framework and implementation for dependency solving which allows for customizable constraints and optimization goals. We use Pacsolve to build Maxnpm, a complete, drop-in replacement for NPM, which empowers developers to combine multiple objectives when installing dependencies. We evaluate Maxnpm with a large sample of packages from the NPM ecosystem and show that it can: 1) reduce more vulnerabilities in dependencies than NPM's auditing tool in 33% of cases; 2) chooses newer dependencies than NPM in 14% of cases; and 3) chooses fewer dependencies than NPM in 21% of cases. All our code and data is open and available.",10.1109/ICSE48619.2023.00124,package-management;Max-SMT;NPM;Rosette;dependency-management;JavaScript,Codes;Ecosystems;Software algorithms;Semantics;Prototypes;Software;Optimization,optimisation;safety-critical software;software packages;trees (mathematics),customizable constraints;dependency solving;dependency tree;drop-in replacement;duplicated dependencies;flexible-optimal dependency management;installing dependencies;Max-SMT;NPM auditing tool;NPM dependency solver;NPM repository hosts;optimization goals;package managers;Pacsolve;vulnerability fixing algorithm,
927,Program repair with and for AI,Impact of Code Language Models on Automated Program Repair,N. Jiang; K. Liu; T. Lutellier; L. Tan,"Purdue University, West Lafayette, USA; Lynbrook High School, San Jose, USA; University of Alberta, Alberta, Canada; Purdue University, West Lafayette, USA",2023,"Automated program repair (APR) aims to help developers improve software reliability by generating patches for buggy programs. Although many code language models (CLM) are developed and effective in many software tasks such as code completion, there has been little comprehensive, in-depth work to evaluate CLMs' fixing capabilities and to fine-tune CLMs for the APR task. Firstly, this work is the first to evaluate ten CLMs on four APR benchmarks, which shows that surprisingly, the best CLM, as is, fixes 72% more bugs than the state-of-the-art deep-learning (DL)-based APR techniques. Secondly, one of the four APR benchmarks was created by us in this paper to avoid data leaking for a fair evaluation. Thirdly, it is the first work to fine-tune CLMs with APR training data, which shows that fine-tuning brings 31%-1,267% improvement to CLMs and enables them to fix 46%-164 % more bugs than existing DL-based APR techniques. Fourthly, this work studies the impact of buggy lines, showing that CLMs, as is, cannot make good use of the buggy lines to fix bugs, yet fine-tuned CLMs could potentially over-rely on buggy lines. Lastly, this work analyzes the size, time, and memory efficiency of different CLMs. This work shows promising directions for the APR domain, such as fine-tuning CLMs with APR-specific designs, and also raises awareness of fair and comprehensive evaluations of CLMs and calls for more transparent reporting of open-source repositories used in the pre-training data to address the data leaking problem.",10.1109/ICSE48619.2023.00125,Automated Program Repair;Code Language Model;Fine-Tuning;Deep Learning,Codes;Computer bugs;Memory management;Training data;Benchmark testing;Maintenance engineering;Software,deep learning (artificial intelligence);learning (artificial intelligence);program debugging;software maintenance;software reliability,APR benchmarks;APR domain;APR task;APR training data;APR-specific designs;automated program repair;buggy lines;buggy programs;bugs;CLM;CLMs' fixing capabilities;code completion;code language models;comprehensive evaluations;different CLMs;fair evaluation;fair evaluations;fine-tuned CLMs;fine-tuning CLMs;in-depth work;software reliability;software tasks;state-of-the-art deep-learning-based APR techniques,
928,Program repair with and for AI,Tare: Type-Aware Neural Program Repair,Q. Zhu; Z. Sun; W. Zhang; Y. Xiong; L. Zhang,"Key Laboratory of High Confidence Software Technologies, Ministry of Education, (Peking University); School of Computer Science, Peking University, P. R. China; Zhongguancun Laboratory, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, (Peking University); School of Computer Science, Peking University, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, (Peking University); School of Computer Science, Peking University, P. R. China; Key Laboratory of High Confidence Software Technologies, Ministry of Education, (Peking University); School of Computer Science, Peking University, P. R. China",2023,"Automated program repair (APR) aims to reduce the effort of software development. With the development of deep learning, lots of DL-based APR approaches have been proposed using an encoder-decoder architecture. Despite the promising performance, these models share the same limitation: generating lots of untypable patches. The main reason for this phenomenon is that the existing models do not consider the constraints of code captured by a set of typing rules. In this paper, we propose, Tare, a type-aware model for neural program repair to learn the typing rules. To encode an individual typing rule, we introduce three novel components: (1) a novel type of grammars, T-Grammar, that integrates the type information into a standard grammar, (2) a novel representation of code, T-Graph, that integrates the key information needed for type checking an AST, and (3) a novel type-aware neural program repair approach, Tare, that encodes the T-Graph and generates the patches guided by T-Grammar. The experiment was conducted on three benchmarks, 393 bugs from Defects4J v1.2, 444 additional bugs from Defects4J v2.0, and 40 bugs from QuixBugs. Our results show that Tare repairs 62, 32, and 27 bugs on these benchmarks respectively, and outperforms the existing APR approaches on all benchmarks. Further analysis also shows that Tare tends to generate more compilable patches than the existing DL-based APR approaches with the typing rule information.",10.1109/ICSE48619.2023.00126,program repair;neural networks,Deep learning;Codes;Computer bugs;Maintenance engineering;Benchmark testing;Software;Generators,deep learning (artificial intelligence);learning (artificial intelligence);program compilers;program debugging;program diagnostics;program testing;software maintenance,automated program repair;deep learning;encoder-decoder architecture;existing APR approaches;existing DL-based APR approaches;grammars;individual typing rule;novel type-aware neural program repair approach;software development;standard grammar;T-Grammar;Tare repairs;type information;type-aware model;typing rule information;typing rules;untypable patches,
929,Program repair with and for AI,Template-based Neural Program Repair,X. Meng; X. Wang; H. Zhang; H. Sun; X. Liu; C. Hu,"SKLSDE Lab, Beihang University, Beijing, China; SKLSDE Lab, Beihang University, Beijing, China; Chongqing University, Chongqing, China; SKLSDE Lab, Beihang University, Beijing, China; SKLSDE Lab, Beihang University, Beijing, China; SKLSDE Lab, Beihang University, Beijing, China",2023,"In recent years, template-based and NMT-based automated program repair methods have been widely studied and achieved promising results. However, there are still disadvantages in both methods. The template-based methods cannot fix the bugs whose types are beyond the capabilities of the templates and only use the syntax information to guide the patch synthesis, while the NMT-based methods intend to generate the small range of fixed code for better performance and may suffer from the OOV (Out-of-vocabulary) problem. To solve these problems, we propose a novel template-based neural program repair approach called TENURE to combine the template-based and NMT- based methods. First, we build two large-scale datasets for 35 fix templates from template-based method and one special fix template (single-line code generation) from NMT-based method, respectively. Second, the encoder-decoder models are adopted to learn deep semantic features for generating patch intermediate representations (IRs) for different templates. The optimized copy mechanism is also used to alleviate the OOV problem. Third, based on the combined patch IRs for different templates, three tools are developed to recover real patches from the patch IRs, replace the unknown tokens, and filter the patch candidates with compilation errors by leveraging the project-specific information. On Defects4J-vl.2, TENURE can fix 79 bugs and 52 bugs with perfect and Ochiai fault localization, respectively. It is able to repair 50 and 32 bugs as well on Defects4J-v2.0. Compared with the existing template-based and NMT-based studies, TENURE achieves the best performance in all experiments.",10.1109/ICSE48619.2023.00127,automated program repair;fix templates;neural machine translation;deep learning,Location awareness;Codes;Source coding;Computer bugs;Semantics;Maintenance engineering;Benchmark testing,deep learning (artificial intelligence);learning (artificial intelligence);program compilers;program debugging;program diagnostics;program testing,35 fix templates;combined patch IRs;different templates;existing template-based;fixed code;NMT-based automated program repair methods;NMT-based method;NMT-based studies;novel template-based neural program repair approach;special fix template;template-based method;template-based NMT,
930,Program repair with and for AI,Automated Repair of Programs from Large Language Models,Z. Fan; X. Gao; M. Mirchev; A. Roychoudhury; S. H. Tan,"National University of Singapore, Singapore; Beihang University, Beijing, China; National University of Singapore, Singapore; National University of Singapore, Singapore; Southern University of Science and Technology, Shenzhen, China",2023,"Large language models such as Codex, have shown the capability to produce code for many programming tasks. However, the success rate of existing models is low, especially for complex programming tasks. One of the reasons is that language models lack awareness of program semantics, resulting in incorrect programs, or even programs which do not compile. In this paper, we systematically study whether automated program repair (APR) techniques can fix the incorrect solutions produced by language models in LeetCode contests. The goal is to study whether APR techniques can enhance reliability in the code produced by large language models. Our study revealed that: (1) automatically generated code shares common programming mistakes with human-crafted solutions, indicating APR techniques may have potential to fix auto-generated code; (2) given bug location information provided by a statistical fault localization approach, the newly released Codex edit mode, which supports editing code, is similar to or better than existing Java repair tools TBar and Recoder in fixing incorrect solutions. By analyzing the experimental results generated by these tools, we provide several suggestions: (1) enhancing APR tools to surpass limitations in patch space (e.g., introducing more flexible fault localization) is desirable; (2) as large language models can derive more fix patterns by training on more data, future APR tools could shift focus from adding more fix patterns to synthesis/semantics based approaches, (3) combination of language models with APR to curate patch ingredients, is worth studying.",10.1109/ICSE48619.2023.00128,Large Language Model;Program Repair,Location awareness;Training;Analytical models;Codes;Semantics;Maintenance engineering;Programming,Java;program compilers;program debugging;program diagnostics;software maintenance,APR techniques;automated program repair techniques;code shares common programming mistakes;complex programming tasks;incorrect programs;incorrect solutions;language models;program semantics,
931,Program repair with and for AI,Automated Program Repair in the Era of Large Pre-trained Language Models,C. S. Xia; Y. Wei; L. Zhang,"University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign",2023,"Automated Program Repair (APR) aims to help developers automatically patch software bugs. However, current state-of-the-art traditional and learning-based APR techniques face the problem of limited patch variety, failing to fix complicated bugs. This is mainly due to the reliance on bug-fixing datasets to craft fix templates (traditional) or directly predict potential patches (learning-based). Large Pre-Trained Language Models (LLMs), trained using billions of text/code tokens, can potentially help avoid this issue. Very recently, researchers have directly leveraged LLMs for APR without relying on any bug-fixing datasets. Meanwhile, such existing work either failed to include state-of-the-art LLMs or was not evaluated on realistic datasets. Thus, the true power of modern LLMs on the important APR problem is yet to be revealed. In this work, we perform the first extensive study on directly applying LLMs for APR. We select 9 recent state-of-the-art LLMs, including both generative and infilling models, ranging from 125M to 20B in size. We designed 3 different repair settings to evaluate the different ways we can use LLMs to generate patches: 1) generate the entire patch function, 2) fill in a chunk of code given the prefix and suffix 3) output a single line fix. We apply the LLMs under these repair settings on 5 datasets across 3 different languages and compare different LLMs in the number of bugs fixed, generation speed and compilation rate. We also compare the LLMs against recent state-of-the-art APR tools. Our study demonstrates that directly applying state-of-the-art LLMs can already substantially outperform all existing APR techniques on all our datasets. Among the studied LLMs, the scaling effect exists for APR where larger models tend to achieve better performance. Also, we show for the first time that suffix code after the buggy line (adopted in infilling-style APR) is important in not only generating more fixes but more patches with higher compilation rate. Besides patch generation, the LLMs consider correct patches to be more natural than other ones, and can even be leveraged for effective patch ranking or patch correctness checking. Lastly, we show that LLM-based APR can be further substantially boosted via: 1) increasing the sample size, and 2) incorporating fix template information.",10.1109/ICSE48619.2023.00129,Automated Program Repair;Machine Learning,Codes;Computer bugs;Maintenance engineering;Software;Distance measurement;Task analysis;Faces,learning (artificial intelligence);natural language processing;program compilers;program debugging;program diagnostics;software maintenance,3 different languages;3 different repair settings;9 recent state-of-the-art LLMs;automated Program Repair;Automated Program Repair;bug-fixing datasets;complicated bugs;correct patches;developers automatically patch software bugs;different LLMs;directly leveraged LLMs;effective patch ranking;entire patch function;existing APR techniques;fix templates;fixes;generative models;important APR problem;infilling models;LLM-based APR;modern LLMs;patch generation;patch variety;potential patches;Pre-trained Language Models;Pre-Trained Language Models;recent state-of-the-art APR tools;single line fix;studied LLMs,1
932,Programming Languages,Faster or Slower? Performance Mystery of Python Idioms Unveiled with Empirical Evidence,Z. Zhang; Z. Xing; X. Xia; X. Xu; L. Zhu; Q. Lu,"Australian National University, Australia; Australian National University, Australia; Software Engineering Application Technology Lab, Huawei, China; Data61, CSIRO, Australia; Data61, CSIRO, Australia; Data61, CSIRO, Australia",2023,"The usage of Python idioms is popular among Python developers in a formative study of 101 Python idiom performance related questions on Stack Overflow, we find that developers often get confused about the performance impact of Python idioms and use anecdotal toy code or rely on personal project experience which is often contradictory in performance outcomes. There has been no large-scale, systematic empirical evidence to reconcile these performance debates. In the paper, we create a large synthetic dataset with 24,126 pairs of non-idiomatic and functionally-equivalent idiomatic code for the nine unique Python idioms identified in [1], and reuse a large real-project dataset of 54,879 such code pairs provided in [1]. We develop a reliable performance measurement method to compare the speedup or slowdown by idiomatic code against non-idiomatic counterpart, and analyze the performance discrepancies between the synthetic and real-project code, the relationships between code features and performance changes, and the root causes of performance changes at the bytecode level. We summarize our findings as some actionable suggestions for using Python idioms.",10.1109/ICSE48619.2023.00130,,Measurement;Codes;Systematics;Toy manufacturing industry;Reliability;Python;Synthetic data,Java;Python,anecdotal toy code;code pairs;functionally-equivalent idiomatic code;performance changes;performance debates;performance discrepancies;performance mystery;performance outcomes;Python developers;Python idiom performance;real-project code;reliable performance measurement method;systematic empirical evidence;unique Python idioms,
933,Programming Languages,Testability Refactoring in Pull Requests: Patterns and Trends,P. Reich; W. Maalei,"Applied Software Technology, UniversitÃ¤t Hamburg, Hamburg, Germany; Applied Software Technology, UniversitÃ¤t Hamburg, Hamburg, Germany",2023,"To create unit tests, it may be necessary to refactor the production code, e.g. by widening access to specific methods or by decomposing classes into smaller units that are easier to test independently. We report on an extensive study to understand such composite refactoring procedures for the purpose of improving testability. We collected and studied 346,841 java pull requests from 621 GitHub projects. First, we compared the atomic refactorings in two populations: pull requests with changed test-pairs (i.e. with co-changes in production and test code and thus potentially including testability refactoring) and pull requests without test-pairs. We found significantly more atomic refactorings in test-pairs pull requests, such as Change Variable Type Operation or Change Parameter Type. Second, we manually analyzed the code changes of 200 pull requests, where developers explicitly mention the terms â€œtestabilityâ€ or â€œrefactor + testâ€. We identified ten composite refactoring procedures for the purpose of testability, which we call testability refactoring patterns. Third, we manually analyzed additional 524 test-pairs pull requests: both randomly selected and where we assumed to find testability refactorings, e.g. in pull requests about dependency or concurrency issues. About 25% of all analyzed pull requests actually included testability refactoring patterns. The most frequent were extract a method for override or for invocation, widen access to a method for invocation, and extract a class for invocation. We also report on frequent atomic refactorings which co-occur with the patterns and discuss the implications of our findings for research, practice, and education.",10.1109/ICSE48619.2023.00131,Pull request mining;software quality;refactoring patterns;software testability;mining software repositories,Codes;Quality assurance;Sociology;Production;Market research;Software;Statistics,Java;program testing;software maintenance,atomic refactorings;change parameter type;change variable type operation;composite refactoring procedures;GitHub projects;invocation;Java pull requests;production code refactoring;testability refactoring patterns;unit tests,
934,Programming Languages,Usability-Oriented Design of Liquid Types for Java,C. Gamboa; P. Canelas; C. Timperley; A. Fonseca,"School of Computer Science, Carnegie Mellon University, USA; School of Computer Science, Carnegie Mellon University, USA; School of Computer Science, Carnegie Mellon University, USA; LASIGE, Faculdade de CiÃªncias da, Universidade de Lisboa, Portugal",2023,"Developers want to detect bugs as early in the development lifecycle as possible, as the effort and cost to fix them increases with the incremental development of features. Ultimately, bugs that are only found in production can have catastrophic consequences. Type systems are effective at detecting many classes of bugs during development, often providing immediate feedback both at compile-time and while typing due to editor integration. Unfortunately, more powerful static and dynamic analysis tools do not have the same success due to providing false positives, not being immediate, or not being integrated into the language. Liquid Types extend the language type system with predicates, augmenting the classes of bugs that the compiler or IDE can catch compared to the simpler type systems available in mainstream programming languages. However, previous implementations of Liquid Types have not used human-centered methods for designing or evaluating their extensions. Therefore, this paper investigates how Liquid Types can be integrated into a mainstream programming language, Java, by proposing a new design that aims to lower the barriers to entry and adapts to problems that Java developers commonly encounter at runtime. Following a participatory design methodology, we conducted a developer survey to design the syntax of LiquidJava, our prototype. To evaluate if the added effort to writing Liquid Types in Java would convince users to adopt them, we conducted a user study with 30 Java developers. The results show that LiquidJava helped users detect and fix more bugs and that Liquid Types are easy to interpret and learn with few resources. At the end of the study, all users reported interest in adopting LiquidJava for their projects.",10.1109/ICSE48619.2023.00132,Usability;Java;Refinement Types;Liquid Types,Surveys;Java;Computer languages;Liquids;Runtime;Program processors;Computer bugs,Java;program debugging,bugs;development lifecycle;incremental development;Java developers;language type system;liquid types;LiquidJava;mainstream programming language;simpler type systems;usability-oriented design,
935,AI bias and fairness,Towards Understanding Fairness and its Composition in Ensemble Machine Learning,U. Gohar; S. Biswas; H. Rajan,"Dept. of Computer Science, Iowa State University, Ames, IA, USA; School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA",2023,"Machine Learning (ML) software has been widely adopted in modern society, with reported fairness implications for minority groups based on race, sex, age, etc. Many recent works have proposed methods to measure and mitigate algorithmic bias in ML models. The existing approaches focus on single classifier-based ML models. However, real-world ML models are often composed of multiple independent or dependent learners in an ensemble (e.g., Random Forest), where the fairness composes in a non-trivial way. How does fairness compose in ensembles? What are the fairness impacts of the learners on the ultimate fairness of the ensemble? Can fair learners result in an unfair ensemble? Furthermore, studies have shown that hyperparameters influence the fairness of ML models. Ensemble hyperparameters are more complex since they affect how learners are combined in different categories of ensembles. Understanding the impact of ensemble hyperparameters on fairness will help programmers design fair ensembles. Today, we do not understand these fully for different ensemble algorithms. In this paper, we comprehensively study popular real-world ensembles: Bagging, Boosting, Stacking, and Voting. We have developed a benchmark of 168 ensemble models collected from Kaggle on four popular fairness datasets. We use existing fairness metrics to understand the composition of fairness. Our results show that ensembles can be designed to be fairer without using mitigation techniques. We also identify the interplay between fairness composition and data characteristics to guide fair ensemble design. Finally, our benchmark can be leveraged for further research on fair ensembles. To the best of our knowledge, this is one of the first and largest studies on fairness composition in ensembles yet presented in the literature.",10.1109/ICSE48619.2023.00133,fairness;ensemble;machine learning;models,Training;Machine learning algorithms;Stacking;Software algorithms;Benchmark testing;Software;Software measurement,learning (artificial intelligence);pattern classification;random forests,168 ensemble models;different ensemble algorithms;ensemble hyperparameters;ensemble machine Learning;fair ensemble design;fair ensembles;fair learners;fairness compose;fairness composition;fairness impacts;fairness metrics;multiple independent learners;popular fairness datasets;real-world ensembles;real-world ML models;reported fairness implications;single classifier-based ML models;towards understanding fairness;ultimate fairness;unfair ensemble,1
936,AI bias and fairness,Fairify: Fairness Verification of Neural Networks,S. Biswas; H. Rajan,"School of Computer Science, Carnegie Mellon University, Pittsburgh, PA, USA; Dept. of Computer Science, Iowa State University, Ames, IA, USA",2023,"Fairness of machine learning (ML) software has become a major concern in the recent past. Although recent research on testing and improving fairness have demonstrated impact on real-world software, providing fairness guarantee in practice is still lacking. Certification of ML models is challenging because of the complex decision-making process of the models. In this paper, we proposed Fairify, an SMT-based approach to verify individual fairness property in neural network (NN) models. Individual fairness ensures that any two similar individuals get similar treatment irrespective of their protected attributes e.g., race, sex, age. Verifying this fairness property is hard because of the global checking and non-linear computation nodes in NN. We proposed sound approach to make individual fairness verification tractable for the developers. The key idea is that many neurons in the NN always remain inactive when a smaller part of the input domain is considered. So, Fairify leverages white-box access to the models in production and then apply formal analysis based pruning. Our approach adopts input partitioning and then prunes the NN for each partition to provide fairness certification or counterexample. We leveraged interval arithmetic and activation heuristic of the neurons to perform the pruning as necessary. We evaluated Fairify on 25 real-world neural networks collected from four different sources, and demonstrated the effectiveness, scalability and performance over baseline and closely related work. Fairify is also configurable based on the domain and size of the NN. Our novel formulation of the problem can answer targeted verification queries with relaxations and counterexamples, which have practical implications.",10.1109/ICSE48619.2023.00134,fairness;verification;machine learning,Computational modeling;Scalability;Neurons;Artificial neural networks;Production;Machine learning;Biological neural networks,decision making;formal verification;learning (artificial intelligence);neural nets;program verification,activation heuristic;complex decision-making process;Fairify;fairness certification;fairness guarantee;formal analysis based pruning;global checking;individual fairness property;individual fairness verification tractable;input domain;input partitioning;leveraged interval arithmetic;machine learning software;ML models;neural network models;neurons;NN;protected attributes;real-world neural networks;real-world software;SMT-based approach;sound approach;verification queries;white-box access,
937,AI bias and fairness,Leveraging Feature Bias for Scalable Misprediction Explanation of Machine Learning Models,J. Gesi; X. Shen; Y. Geng; Q. Chen; I. Ahmed,"Donald Bren School of ICS, University of California, Irvine, Irvine, USA; Donald Bren School of ICS, University of California, Irvine, Irvine, USA; Donald Bren School of ICS, University of California, Irvine, Irvine, USA; Donald Bren School of ICS, University of California, Irvine, Irvine, USA; Donald Bren School of ICS, University of California, Irvine, Irvine, USA",2023,"Interpreting and debugging machine learning models is necessary to ensure the robustness of the machine learning models. Explaining mispredictions can help significantly in doing so. While recent works on misprediction explanation have proven promising in generating interpretable explanations for mispredictions, the state-of-the-art techniques â€œblindlyâ€ deduce misprediction explanation rules from all data features, which may not be scalable depending on the number of features. To alleviate this problem, we propose an efficient misprediction explanation technique named Bias Guided Misprediction Diagnoser (BGMD), which leverages two prior knowledge about data: a) data often exhibit highly-skewed feature distributions and b) trained models in many cases perform poorly on subdataset with under-represented features. Next, we propose a technique named MAPS (Mispredicted Area UPweight Sampling). MAPS increases the weights of subdataset during model retraining that belong to the group that is prone to be mispredicted because of containing under-represented features. Thus, MAPS make retrained model pay more attention to the under-represented features. Our empirical study shows that our proposed BGMD outperformed the state-of-the-art misprediction diagnoser and reduces diagnosis time by 92%. Furthermore, MAPS outperformed two state-of-the-art techniques on fixing the machine learning model's performance on mispredicted data without compromising performance on all data. All the research artifacts (i.e., tools, scripts, and data) of this study are available in the accompanying website [1].",10.1109/ICSE48619.2023.00135,machine learning;data imbalance;rule induction;misprediction explanation,Machine learning algorithms;Machine learning;Debugging;Predictive models;Prediction algorithms;Robustness;Data models,data handling;explanation;learning (artificial intelligence),BGMD;bias guided misprediction diagnoser;blindly deduce misprediction explanation rules;data features;feature bias;highly-skewed feature distributions;interpretable explanations;machine learning models;MAPS;mispredicted area upweight sampling;scalable misprediction explanation;underrepresented features,
938,AI bias and fairness,Information-Theoretic Testing and Debugging of Fairness Defects in Deep Neural Networks,V. Monjezi; A. Trivedi; G. Tan; S. Tizpaz-Niari,University of Texas at El Paso; University of Colorado Boulder; Pennsylvania State University; University of Texas at El Paso,2023,"The deep feedforward neural networks (DNNs) are increasingly deployed in socioeconomic critical decision support software systems. DNNs are exceptionally good at finding min-imal, sufficient statistical patterns within their training data. Consequently, DNNs may learn to encode decisions-amplifying existing biases or introducing new ones-that may disadvantage protected individuals/groups and may stand to violate legal protections. While the existing search based software testing approaches have been effective in discovering fairness defects, they do not supplement these defects with debugging aids-such as severity and causal explanations-crucial to help developers triage and decide on the next course of action. Can we measure the severity of fairness defects in DNNs? Are these defects symptomatic of improper training or they merely reflect biases present in the training data? To answer such questions, we present Dice: an information-theoretic testing and debugging framework to discover and localize fairness defects in DNNs. The key goal of Dice is to assist software developers in triaging fairness defects by ordering them by their severity. Towards this goal, we quantify fairness in terms of protected information (in bits) used in decision making. A quantitative view of fairness defects not only helps in ordering these defects, our empirical evaluation shows that it improves the search efficiency due to resulting smoothness of the search space. Guided by the quan-titative fairness, we present a causal debugging framework to localize inadequately trained layers and neurons responsible for fairness defects. Our experiments over ten DNNs, developed for socially critical tasks, show that Dice efficiently characterizes the amounts of discrimination, effectively generates discriminatory instances (vis-a-vis the state-of-the-art techniques), and localizes layers/neurons with significant biases.",10.1109/ICSE48619.2023.00136,Algorithmic Fairness;Information Theory;Software Testing;Fairness Defect Localization;Bias Mitigation,Training;Software testing;Software algorithms;Decision making;Training data;Debugging;Software systems,decision making;decision support systems;deep learning (artificial intelligence);feedforward neural nets;learning (artificial intelligence);program debugging;program testing,debugging framework;decisions-amplifying existing biases;deep feedforward neural networks;deep neural networks;DNNs;existing search based software testing approaches;socioeconomic critical decision support software systems;training data;triaging fairness defects,
939,Requirements engineering,Demystifying Privacy Policy of Third-Party Libraries in Mobile Apps,K. Zhao; X. Zhan; L. Yu; S. Zhou; H. Zhou; X. Luo; H. Wang; Y. Liu,The Hong Kong Polytechnic University; Southern University of Science and Technology; The Hong Kong Polytechnic University; The Hong Kong Polytechnic University; The Hong Kong Polytechnic University; The Hong Kong Polytechnic University; Huazhong University of Science and Technology; Southern University of Science and Technology,2023,"The privacy of personal information has received significant attention in mobile software. Although researchers have designed methods to identify the conflict between app behavior and privacy policies, little is known about the privacy compliance issues relevant to third-party libraries (TPLs). The regulators enacted articles to regulate the usage of personal information for TPLs (e.g., the CCPA requires businesses clearly notify consumers if they share consumers' data with third parties or not). However, it remains challenging to investigate the privacy compliance issues of TPLs due to three reasons: 1) Difficulties in collecting TPLs' privacy policies. In contrast to Android apps, which are distributed through markets like Google Play and must provide privacy policies, there is no unique platform for collecting privacy policies of TPLs. 2) Difficulties in analyzing TPL's user privacy access behaviors. TPLs are mainly provided in binary files, such as jar or aar, and their whole functionalities usually cannot be executed independently without host apps. 3) Difficulties in identifying consistency between TPL's functionalities and privacy policies, and host app's privacy policy and data sharing with TPLs. This requires analyzing not only the privacy policies of TPLs and host apps but also their functionalities. In this paper, we propose an automated system named ATPChecker to analyze whether Android TPLs comply with the privacy-related regulations. We construct a data set that contains a list of 458 TPLs, 247 TPL's privacy policies, 187 TPL's binary files and 641 host apps and their privacy policies. Then, we analyze the bytecode of TPLs and host apps, design natural language processing systems to analyze privacy policies, and implement an expert system to identify TPL usage-related regulation compliance. The experimental results show that 23% TPLs violate regulation requirements for providing privacy policies. Over 47% TPLs miss disclosing data usage in their privacy policies. Over 65% host apps share user data with TPLs while 65% of them miss disclosing interactions with TPLs. Our findings remind developers to be mindful of TPL usage when developing apps or writing privacy policies to avoid violating regulations,",10.1109/ICSE48619.2023.00137,Privacy policy;third-party library;Android,Privacy;Data privacy;Regulators;Operating systems;Writing;Regulation;Libraries,Android (operating system);data privacy;mobile computing;natural language processing;smart phones,23% TPLs;247 TPL's privacy policies;458 TPLs;47% TPLs;641 host apps;collecting privacy policies;host app;privacy compliance issues;privacy policy;TPL's user privacy access behaviors;TPLs' privacy policies;writing privacy policies,
940,Requirements engineering,Cross-Domain Requirements Linking via Adversarial-based Domain Adaptation,Z. Chang; M. Li; Q. Wang; S. Li; J. Wang,"Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China; Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China; Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China; Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China; Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China",2023,"Requirements linking is the core of software system maintenance and evolution, and it is critical to assuring software quality. In practice, however, the requirements links are frequently absent or incorrectly labeled, and reconstructing such ties is time-consuming and error-prone. Numerous learning-based approaches have been put forth to address the problem. However, these approaches will lose effectiveness for the Cold-Start projects with few labeled samples. To this end, we propose RADIATION, an adversarial-based domain adaptation approach for cross-domain requirements linking. Generally, RADIATION firstly adopts an IDF-based Masking strategy to filter the domain-specific features. Then it pre-trains a linking model in the source domain with sufficient labeled samples and adapts the model to target domains using a distance-enhanced adversarial technique without using any labeled target samples. Evaluation on five public datasets shows that RADIATION could achieve 66.4% precision, 89.2% recall, and significantly outperform state-of-the-art baselines by 13.4% -42.9% F1. In addition, the designed components, i.e., IDF-based Masking and Distance-enhanced Loss, could significantly improve performance.",10.1109/ICSE48619.2023.00138,Cross-Domain Requirements Linking;Domain Adaptation;Adversarial Learning,Adaptation models;Source coding;Unified modeling language;Software quality;Maintenance engineering;Software systems;Usability,deep learning (artificial intelligence);learning (artificial intelligence);software quality;unsupervised learning,adversarial-based domain adaptation;Cold-Start projects;cross-domain requirements;distance-enhanced adversarial technique;domain adaptation approach;domain-specific features;IDF-based Masking strategy;labeled target samples;linking model;numerous learning-based approaches;RADIATION;requirements linking;requirements links;software quality;software system maintenance;source domain;sufficient labeled samples,
941,SE for security 2,On-Demand Security Requirements Synthesis with Relational Generative Adversarial Networks,V. Koscinski; S. Hashemi; M. Mirakhorli,"Rochester Institute of Technology, Rochester, NY, USA; Rochester Institute of Technology, Rochester, NY, USA; Rochester Institute of Technology, Rochester, NY, USA",2023,"Security requirements engineering is a manual and error-prone activity that is often neglected due to the knowledge gap between cybersecurity professionals and software requirements engineers. In this paper, we aim to automate the process of recommending and synthesizing security requirements specifications and therefore supporting requirements engineers in soliciting and specifying security requirements. We investigate the use of Relational Generative Adversarial Networks (GANs) in automatically synthesizing security requirements specifications. We evaluate our approach using a real case study of the Court Case Management System (CCMS) developed for the Indiana Supreme Court's Division of State Court Administration. We present an approach based on RelGAN to generate security requirements specifications for the CCMS. We show that RelGAN is practical for synthesizing security requirements specifications as indicated by subject matter experts. Based on this study, we demonstrate promising results for the use of GANs in the software requirements synthesis domain. We also provide a baseline for synthesizing requirements, highlight limitations and weaknesses of RelGAN and define opportunities for further investigations.",10.1109/ICSE48619.2023.00139,Software Security Requirements;Requirements Engineering;Generative Adversarial Networks,Knowledge engineering;Education;Medical services;Manuals;Generative adversarial networks;Software;Security,computer network security;formal specification;security of data;systems analysis,cybersecurity professionals;demand security requirements synthesis;recommending synthesizing security requirements specifications;Relational Generative Adversarial Networks;security requirements engineering;software requirements engineers;software requirements synthesis domain;soliciting specifying security requirements;supporting requirements engineers;synthesizing requirements,
942,SE for security 2,Measuring Secure Coding Practice and Culture: A Finger Pointing at the Moon is not the Moon,I. Ryan; U. Roedig; K. -J. Stol,"ADVANCE Centre for Research Training, School of Computer Science and IT, University College Cork, Cork, Ireland; Connect Research Centre, School of Computer Science and IT, University College Cork, Cork, Ireland; Lero, the SFI Research Centre for Software, School of Computer Science and IT, University College Cork, Cork, Ireland",2023,"Software security research has a core problem: it is impossible to prove the security of complex software. A low number of known defects may simply indicate that the software has not been attacked yet, or that successful attacks have not been detected. A high defect count may be the result of white-hat hacker targeting, or of a successful bug bounty program which prevented insecurities from persisting in the wild. This makes it difficult to measure the security of non-trivial software. Researchers instead usually measure effort directed towards ensuring software security. However, different researchers use their own tailored measures, usually devised from industry secure coding guidelines. Not only is there no agreed way to measure effort, there is also no agreement on what effort entails. Qualitative studies emphasise the importance of security culture in an organisation. Where software security practices are introduced solely to ensure compliance with legislative or industry standards, a box-ticking attitude to security may result. The security culture may be weak or non-existent, making it likely that precautions not explicitly mentioned in the standards will be missed. Thus, researchers need both a way to assess software security practice and a way to measure software security culture. To assess security practice, we converted the empirically-established 12 most common software security activities into questions. To assess security culture, we devised a number of questions grounded in prior literature. We ran a secure development survey with both sets of questions, obtaining organic responses from 1,100 software coders in 59 countries. We used proven common activities to assess security practice, and made a first attempt to quantitatively assess aspects of security culture in the broad developer population. Our results show that some coders still work in environments where there is little to no attempt to ensure code security. Security practice and culture do not always correlate, and some organisations with strong secure coding practice have weak secure coding culture. This may lead to problems in defect prevention and sustained software security effort.",10.1109/ICSE48619.2023.00140,Security;secure coding;security compliance,Industries;Surveys;Moon;Fingers;Software;Encoding;Time measurement,computer crime;legislation;program debugging;security of data,1 software coders;100 software coders;12 most common software security activities;code security;complex software;industry secure coding guidelines;measuring secure coding practice;nontrivial software;secure development survey;software security culture;software security effort;software security practice;software security research;strong secure coding practice;weak secure coding culture,1
943,SE for security 2,What Challenges Do Developers Face About Checked-in Secrets in Software Artifacts?,S. K. Basak; L. Neil; B. Reaves; L. Williams,"North Carolina State University, USA; North Carolina State University, USA; North Carolina State University, USA; North Carolina State University, USA",2023,"Throughout 2021, GitGuardian's monitoring of public GitHub repositories revealed a two-fold increase in the number of secrets (database credentials, API keys, and other credentials) exposed compared to 2020, accumulating more than six million secrets. To our knowledge, the challenges developers face to avoid checked-in secrets are not yet characterized. The goal of our paper is to aid researchers and tool developers in understanding and prioritizing opportunities for future research and tool automation for mitigating checked-in secrets through an empirical investigation of challenges and solutions related to checked-in secrets. We extract 779 questions related to checked-in secrets on Stack Exchange and apply qualitative analysis to determine the challenges and the solutions posed by others for each of the challenges. We identify 27 challenges and 13 solutions. The four most common challenges, in ranked order, are: (i) store/version of secrets during deployment; (ii) store/version of secrets in source code; (iii) ignore/hide of secrets in source code; and (iv) sanitize VCS history. The three most common solutions, in ranked order, are: (i) move secrets out of source code/version control and use template config file; (ii) secret management in deployment; and (iii) use local environment variables. Our findings indicate that the same solution has been mentioned to mitigate multiple challenges. However, our findings also identify an increasing trend in questions lacking accepted solutions substantiating the need for future research and tool automation on managing secrets.",10.1109/ICSE48619.2023.00141,secrets;credentials;developers;software secret management;challenges;empirical study;stack exchange,Automation;Databases;Source coding;Market research;Software;History;Faces,application program interfaces;configuration management;source code (software),challenges developers;checked-in secrets;common challenges;developers face;managing secrets;million secrets;multiple challenges;secret management;tool automation,
944,SE for security 2,Lejacon: A Lightweight and Efficient Approach to Java Confidential Computing on SGX,X. Miao; Z. Lin; S. Wang; L. Yu; S. Li; Z. Wang; P. Nie; Y. Chen; B. Shen; H. Jiang,"Shanghai Jiao Tong University, Shanghai, China; Alibaba Group, Shanghai, China; Alibaba Group, Shanghai, China; Alibaba Group, Shanghai, China; Alibaba Group, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Shanghai Jiao Tong University, Shanghai, China; Dalian University of Technology, Dalian, China",2023,"Intel's SGX is a confidential computing technique. It allows key functionalities of C/C++/native applications to be confidentially executed in hardware enclaves. However, numerous cloud applications are written in Java. For supporting their confidential computing, state-of-the-art approaches deploy Java Virtual Machines (JVMs) in enclaves and perform confidential computing on JVMs. Meanwhile, these JVM-in-enclave solutions still suffer from serious limitations, such as heavy overheads of running JVMs in enclaves, large attack surfaces, and deep computation stacks. To mitigate the above limitations, we for-malize a Secure Closed-World (SCW) principle and then propose Lejacon, a lightweight and efficient approach to Java confidential computing. The key idea is, given a Java application, to (1) separately compile its confidential computing tasks into a bundle of Native Confidential Computing (NCC) services; (2) run the NCC services in enclaves on the Trusted Execution Environment (TEE) side, and meanwhile run the non-confidential code on a JVM on the Rich Execution Environment (REE) side. The two sides interact with each other, protecting confidential computing tasks and as well keeping the Trusted Computing Base (TCB) size small. We implement Lejacon and evaluate it against OcclumJ (a state-of-the-art JVM-in-enclave solution) on a set of benchmarks using the BouncyCastle cryptography library. The evaluation results clearly show the strengths of Lejacon: it achieves compet-itive performance in running Java confidential code in enclaves; compared with OcclumJ, Lejacon achieves speedups by up to 16.2x in running confidential code and also reduces the TCB sizes by 90+% on average.",10.1109/ICSE48619.2023.00142,Software Guard Extensions;Separation Compilation;Native Confidential Computing Service;Runtime;Secure Closed-World,Java;Trusted computing;Codes;Runtime;Programming;Virtual machining;Software,cloud computing;computer network security;cryptography;Java;program compilers;software libraries;storage management;trusted computing;virtual machines,confidential computing tasks;confidential computing technique;deep computation stacks;hardware enclaves;Java application;Java confidential code;Java confidential computing;Java Confidential Computing;Java Virtual Machines;JVM-in-enclave solutions;JVMs;Lejacon;lightweight approach;Native Confidential Computing services;nonconfidential code;numerous cloud applications;Rich Execution Environment side;state-of-the-art JVM-in-enclave solution;Trusted Computing Base;Trusted Execution Environment side,
945,SE for security 2,Keyword Extraction From Specification Documents for Planning Security Mechanisms,J. J. Poozhithara; H. U. Asuncion; B. Lagesse,"Computer & Software Systems University of Washington, Bothell, USA; Computer & Software Systems University of Washington, Bothell, USA; Computer & Software Systems University of Washington, Bothell, USA",2023,"Software development companies heavily invest both time and money to provide post-production support to fix security vulnerabilities in their products. Current techniques identify vulnerabilities from source code using static and dynamic analyses. However, this does not help integrate security mechanisms early in the architectural design phase. We develop VDocScan, a technique for predicting vulnerabilities based on specification documents, even before the development stage. We evaluate VDocScan using an extensive dataset of CVE vulnerability reports mapped to over 3600 product documentations. An evaluation of 8 CWE vulnerability pillars shows that even interpretable whitebox classifiers predict vulnerabilities with up to 61.1% precision and 78% recall. Further, using strategies to improve the relevance of extracted keywords, addressing class imbalance, segregating products into categories such as Operating Systems, Web applications, and Hardware, and using blackbox ensemble models such as the random forest classifier improves the performance to 96% precision and 91.1% recall. The high precision and recall shows that VDocScan can anticipate vulnerabilities detected in a product's lifetime ahead of time during the Design phase to incorporate necessary security mechanisms. The performance is consistently high for vulnerabilities with the mode of introduction: architecture and design.",10.1109/ICSE48619.2023.00143,Security;Vulnerability Prediction;CVE;CWE;Keyword Extraction;Documentation,Correlation;Source coding;Operating systems;Companies;Watches;Predictive models;Planning,computer network security;pattern classification;random forests;security of data,3600 product documentations;8 CWE vulnerability pillars;architectural design phase;CVE vulnerability reports;dynamic analyses;extracted keywords;incorporate necessary security mechanisms;keyword extraction;planning security mechanisms;post-production support;security vulnerabilities;software development companies;specification documents;static analyses;VDocScan,
946,Software Evolution,Dependency Facade: The Coupling and Conflicts between Android Framework and Its Customization,W. Jin; Y. Dai; J. Zheng; Y. Qu; M. Fan; Z. Huang; D. Huang; T. Liu,"Xi'an Jiaotong University, Xi'an, China; Xi'an Jiaotong University, Xi'an, China; Xi'an Jiaotong University, Xi'an, China; University of California, Riverside, California, United States; Xi'an Jiaotong University, Xi'an, China; Honor Device Co., Ltd, Xi'an, China; Honor Device Co., Ltd, Xi'an, China; Xi'an Jiaotong University, Xi'an, China",2023,"Mobile device vendors develop their customized Android OS (termed downstream) based on Google Android (termed upstream) to support new features. During daily independent development, the downstream also periodically merges changes of a new release from the upstream into its development branches, keeping in sync with the upstream. Due to a large number of commits to be merged, heavy code conflicts would be reported if auto-merge operations failed. Prior work has studied conflicts in this scenario. However, it is still unclear about the coupling between the downstream and the upstream (We term this coupling as the dependency facade), as well as how merge conflicts are related to this coupling. To address this issue, we first propose the DepFCD to reveal the dependency facade from three aspects, including interface-level dependencies that indicate a clear design boundary, intrusion-level dependencies which blur the boundary, and dependency constraints imposed by the upstream non-SDK restrictions. We then empirically investigate these three aspects (RQ1, RQ2, RQ3) and merge conflicts (RQ4) on the dependency facade. To support the study, we collect four open-source downstream projects and one industrial project, with 15 downstream and 15 corresponding upstream versions. Our study reveals interesting observations and suggests earlier mitigation of merge conflicts through a well-managed dependency facade. Our study will benefit the research about the coupling between upstream and downstream as well as the downstream maintenance practice.",10.1109/ICSE48619.2023.00144,Android;downstream;dependencies;merge conflict,Couplings;Codes;Ecosystems;Maintenance engineering;Internet;Synchronization;Smart phones,Android (operating system);mobile computing;smart phones,Android framework;auto-merge operations;customized Android OS;daily independent development;dependency constraints;dependency facade;Google Android;heavy code conflicts;interface-level dependencies;intrusion-level dependencies;mobile device vendors;open-source downstream projects;upstream versions,1
947,Test quality and improvement,Test Selection for Unified Regression Testing,S. Wang; X. Lian; D. Marinov; T. Xu,"University of Illinois at Urbana-Champaign, Urbana, IL, USA; University of Illinois at Urbana-Champaign, Urbana, IL, USA; University of Illinois at Urbana-Champaign, Urbana, IL, USA; University of Illinois at Urbana-Champaign, Urbana, IL, USA",2023,"Today's software failures have two dominating root causes: code bugs and misconfigurations. To combat failure-inducing software changes, unified regression testing (URT) is needed to synergistically test the changed code and all changed production configurations for deployment reliability. However, URT could incur high cost, as it needs to run a large number of tests under multiple configurations. Regression test selection (RTS) can reduce regression testing cost. Unfortunately, no existing RTS technique reasons about code and configuration changes collectively. We introduce Unified Regression Test Selection (uRTS) to effectively reduce the cost of URT. uRTS supports project changes on 1) code only, 2) configurations only, and 3) both code and configurations. It selects regular tests and configuration tests with a unified selection algorithm. The uRTS algorithm analyzes code and configuration dependencies of each test across runs and across configurations. uRTS provides the same safety guarantee as the state-of-the-art RTS while selecting fewer tests and, more importantly, reducing the end-to-end testing time. We implemented uRTS on top of Ekstazi (a RTS tool for code changes) and Ctest (a configuration testing framework). We evaluate uRTS on hundreds of code revisions and dozens of configurations of five large projects. The results show that uRTS reduces the end-to-end testing time, on average, by 3.64X compared to executing all tests and 1.87X compared to a competitive reference solution that directly extends RTS for URT.",10.1109/ICSE48619.2023.00145,,Codes;Costs;Computer bugs;Production;Reliability engineering;Software;Software reliability,program debugging;program testing;regression analysis;software reliability;system recovery,analyzes code;changed code;code bugs;code changes;configuration dependencies;configuration testing framework;deployment reliability;end-to-end testing time;failure-inducing software changes;software failures;unified regression test selection;unified selection algorithm;URT;uRTS,
948,Test quality and improvement,ATM: Black-box Test Case Minimization based on Test Code Similarity and Evolutionary Search,R. Pan; T. A. Ghaleb; L. Briand,"School of EECS University of Ottawa, Ottawa, Canada; School of EECS University of Ottawa, Ottawa, Canada; School of EECS University of Ottawa, Ottawa, Canada",2023,"Executing large test suites is time and resource consuming, sometimes impossible, and such test suites typically contain many redundant test cases. Hence, test case (suite) minimization is used to remove redundant test cases that are unlikely to detect new faults. However, most test case minimization techniques rely on code coverage (white-box), model-based features, or requirements specifications, which are not always (entirely) accessible by test engineers. Code coverage analysis also leads to scalability issues, especially when applied to large industrial systems. Recently, a set of novel techniques was proposed, called FAST-R, relying solely on test case code for test case minimization, which appeared to be much more efficient than white-box techniques. However, it achieved a comparable low fault detection capability for Java projects, thus making its application challenging in practice. In this paper, we propose ATM (AST-based Test case Minimizer), a similarity-based, search-based test case minimization technique, taking a specific budget as input, that also relies exclusively on the source code of test cases but attempts to achieve higher fault detection through finer-grained similarity analysis and a dedicated search algorithm. ATM transforms test case code into Abstract Syntax Trees (AST) and relies on four tree-based similarity measures to apply evolutionary search, specifically genetic algorithms, to minimize test cases. We evaluated the effectiveness and efficiency of ATM on a large dataset of 16 Java projects with 661 faulty versions using three budgets ranging from 25% to 75% of test suites. ATM achieved significantly higher fault detection rates (0.82 on average), compared to FAST-R (0.61 on average) and random minimization (0.52 on average), when running only 50% of the test cases, within practically acceptable time (1.1 - 4.3 hours, on average, per project version), given that minimization is only occasionally applied when many new test cases are created (major releases). Results achieved for other budgets were consistent.",10.1109/ICSE48619.2023.00146,Test case minimization;Test suite reduction;Tree-based similarity;AST;Genetic algorithm;Black-box testing,Java;Codes;Fault detection;Closed box;Transforms;Syntactics;Minimization,fault diagnosis;genetic algorithms;minimisation;program testing;search problems;source code (software);tree data structures,abstract syntax trees;AST-based test case minimizer;ATM;black-box test case minimization;evolutionary search;FAST-R;fault detection;genetic algorithms;random minimization;search algorithm;search-based test case minimization technique;similarity-based test case minimization technique;test case code;test code similarity;test engineers;test suites;tree-based similarity measures,
949,Test quality and improvement,Measuring and Mitigating Gaps in Structural Testing,S. B. Hossain; M. B. Dwyer; S. Elbaum; A. Nguyen-Tuong,"Department of Computer Science, University of Virginia; Department of Computer Science, University of Virginia; Department of Computer Science, University of Virginia; Department of Computer Science, University of Virginia",2023,"Structural code coverage is a popular test adequacy metric that measures the percentage of program structure (e.g., statement, branch, decision) executed by a test suite. While structural coverage has several benefits, previous studies suggested that code coverage is not a good indicator of a test suite's fault-detection effectiveness as coverage computation does not consider test oracle quality. In this research, we formally define the coverage gap in structural testing as the percentage of program structure that is executed but not observed by any test oracles. Our large-scale empirical study of 13 Java applications, 16K test cases and 51.6K test assertions shows that even for mature test suites, the gap can be as high as 51 percentage points (pp) and 34pp on average. Our study reveals that the coverage gap strongly and negatively correlates with a test suite's fault-detection effectiveness. To mitigate gaps, we propose a lightweight static analysis of program dependencies to produce a ranked recommendation of test focus methods that can reduce the gap and improve test suite quality. When considering 34.8K assertions in the test suite as ground truth, the recommender suggests two-thirds of the focus methods written by developers within the top five recommendations.",10.1109/ICSE48619.2023.00147,code coverage;checked coverage;test oracles;mutation testing;fault-detection effectiveness,Java;Codes;Static analysis;Software measurement;Testing;Software engineering,Java;program diagnostics;program testing;software metrics;software quality,coverage computation;fault-detection;program dependencies;program structure;structural code coverage;structural coverage;structural testing;test oracles;test suite quality,
950,Runtime analysis and self-adaptation,Heterogeneous Anomaly Detection for Software Systems via Semi-supervised Cross-modal Attention,C. Lee; T. Yang; Z. Chen; Y. Su; Y. Yang; M. R. Lyu,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; Sun Yat-sen University, Guangzhou, China; Sun Yat-sen University, Guangzhou, China; Computing and Networking Innovation Lab, Cloud BU, Huawei; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China",2023,"Prompt and accurate detection of system anomalies is essential to ensure the reliability of software systems. Unlike manual efforts that exploit all available run-time information, existing approaches usually leverage only a single type of monitoring data (often logs or metrics) or fail to make effective use of the joint information among different types of data. Consequently, many false predictions occur. To better understand the manifestations of system anomalies, we conduct a systematical study on a large amount of heterogeneous data, i.e., logs and metrics. Our study demonstrates that logs and metrics can manifest system anomalies collaboratively and complementarily, and neither of them only is sufficient. Thus, integrating heterogeneous data can help recover the complete picture of a system's health status. In this context, we propose Hades, the first end-to-end semi-supervised approach to effectively identify system anomalies based on heterogeneous data. Our approach employs a hierarchical architecture to learn a global representation of the system status by fusing log semantics and metric patterns. It captures discriminative features and meaningful interactions from heterogeneous data via a cross-modal attention module, trained in a semi-supervised manner. We evaluate Hades extensively on large-scale simulated data and datasets from Huawei Cloud. The experimental results present the effectiveness of our model in detecting system anomalies. We also release the code and the annotated dataset for replication and future research.",10.1109/ICSE48619.2023.00148,Software System;Anomaly Detection;Cross-modal Learning,Measurement;Codes;Costs;Semantics;Manuals;Semisupervised learning;Software systems,feature extraction;learning (artificial intelligence);supervised learning,available run-time information;cross-modal attention module;end-to-end semisupervised approach;heterogeneous anomaly detection;heterogeneous data;large-scale simulated data;manifest system anomalies;semisupervised cross-modal attention;software systems,
951,Runtime analysis and self-adaptation,Recommending Root-Cause and Mitigation Steps for Cloud Incidents using Large Language Models,T. Ahmed; S. Ghosh; C. Bansal; T. Zimmermann; X. Zhang; S. Rajmohan,UC Davis; Microsoft; Microsoft; Microsoft Research; Microsoft; Microsoft,2023,"Incident management for cloud services is a complex process involving several steps and has a huge impact on both service health and developer productivity. On-call engineers require significant amount of domain knowledge and manual effort for root causing and mitigation of production incidents. Recent advances in artificial intelligence has resulted in state-of-the-art large language models like GPT-3.x (both GPT-3.0 and GPT-3.5), which have been used to solve a variety of problems ranging from question answering to text summarization. In this work, we do the first large-scale study to evaluate the effectiveness of these models for helping engineers root cause and mitigate production incidents. We do a rigorous study at Microsoft, on more than 40,000 incidents and compare several large language models in zero-shot, fine-tuned and multi-task setting using semantic and lexical metrics. Lastly, our human evaluation with actual incident owners show the efficacy and future potential of using artificial intelligence for resolving cloud incidents.",10.1109/ICSE48619.2023.00149,Incident Management;Service Quality;GPT-3.x;Large Language Models,Productivity;Knowledge engineering;Semantics;Manuals;Multitasking;Question answering (information retrieval);Distance measurement,artificial intelligence;cloud computing;natural language processing;question answering (information retrieval);text analysis,actual incident owners;artificial intelligence;cloud incidents;cloud services;complex process;developer productivity;domain knowledge;GPT-3.x;incident management;language models;manual effort;mitigation steps;production incidents;root-cause;service health,
952,Runtime analysis and self-adaptation,Eadro: An End-to-End Troubleshooting Framework for Microservices on Multi-source Data,C. Lee; T. Yang; Z. Chen; Y. Su; M. R. Lyu,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; Sun Yat-sen University, Guangzhou, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China",2023,"The complexity and dynamism of microservices pose significant challenges to system reliability, and thereby, automated troubleshooting is crucial. Effective root cause localization after anomaly detection is crucial for ensuring the reliability of microservice systems. However, two significant issues rest in existing approaches: (1) Microservices generate traces, system logs, and key performance indicators (KPIs), but existing approaches usually consider traces only, failing to understand the system fully as traces cannot depict all anomalies; (2) Troubleshooting microservices generally contains two main phases, i.e., anomaly detection and root cause localization. Existing studies regard these two phases as independent, ignoring their close correlation. Even worse, inaccurate detection results can deeply affect localization effectiveness. To overcome these limitations, we propose Eadro, the first end-to-end framework to integrate anomaly detection and root cause localization based on multi-source data for troubleshooting large-scale microservices. The key insights of Eadro are the anomaly manifestations on different data sources and the close connection between detection and localization. Thus, Eadro models intra-service behaviors and inter-service dependencies from traces, logs, and KPIs, all the while leveraging the shared knowledge of the two phases via multi-task learning. Experiments on two widely-used benchmark microservices demonstrate that Eadro outperforms state-of-the-art approaches by a large margin. The results also show the usefulness of integrating multi-source data. We also release our code and data to facilitate future research.",10.1109/ICSE48619.2023.00150,Microservices;Root Cause Localization;Anomaly Detection;Traces,Location awareness;Codes;Soft sensors;Microservice architectures;Multitasking;Behavioral sciences;Reliability,data handling;learning (artificial intelligence);service-oriented architecture;software reliability,anomaly detection;anomaly manifestations;automated troubleshooting;Eadro;end-to-end troubleshooting framework;intra-service behaviors;key performance indicators;large-scale microservices;localization effectiveness;microservice systems;multisource data;multitask learning;root cause localization;system logs;system reliability,
953,Runtime analysis and self-adaptation,LogReducer: Identify and Reduce Log Hotspots in Kernel on the Fly,G. Yu; P. Chen; P. Li; T. Weng; H. Zheng; Y. Deng; Z. Zheng,"Sun Yat-sen University, Tencent Inc.; Sun Yat-sen University; Tencent Inc.; Tencent Inc.; Tencent Inc.; Tencent Inc.; Sun Yat-sen University",2023,"Modern systems generate a massive amount of logs to detect and diagnose system faults, which incurs expensive storage costs and runtime overhead. After investigating real-world production logs, we observe that most of the logging overhead is due to a small number of log templates, referred to as log hotspots. Therefore, we conduct a systematical study about log hotspots in an industrial system WeChat, which motivates us to identify log hotspots and reduce them on the fly. In this paper, we propose LogReducer, a non-intrusive and language-independent log reduction framework based on eBPF (Extended Berkeley Packet Filter), consisting of both online and offline processes. After two months of serving the offline process of LogReducer in WeChat, the log storage overhead has dropped from 19.7 PB per day to 12.0 PB (i.e., about a 39.08% decrease). Practical implementation and experimental evaluations in the test environment demonstrate that the online process of LogReducer can control the logging overhead of hotspots while preserving logging effectiveness. Moreover, the log hotspot handling time can be reduced from an average of 9 days in production to 10 minutes in the test with the help of LogReducer,",10.1109/ICSE48619.2023.00151,Log Hotspot;eBPF;Log Reduction;Log Parsing,Runtime;Costs;Social networking (online);Process control;Production;Message services;Kernel,mobile computing;program diagnostics;program testing,eBPF;extended Berkeley packet filter;industrial system;language-independent log reduction framework;log hotspot handling time;log storage overhead;log templates;logging effectiveness;logging overhead;LogReducer;non-intrusive log reduction framework;runtime overhead;system fault detection;system fault diagnosis;WeChat,
954,AI testing 2,Aries: Efficient Testing of Deep Neural Networks via Labeling-Free Accuracy Estimation,Q. Hu; Y. Guo; X. Xie; M. Cordy; M. Papadakis; L. Ma; Y. L. Traon,"University of Luxembourg, Luxembourg; Luxembourg Institute of Science and Technology, Luxembourg; Singapore Management University, Singapore; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; University of Alberta, Canada; University of Luxembourg, Luxembourg",2023,"Deep learning (DL) plays a more and more important role in our daily life due to its competitive performance in industrial application domains. As the core of DL-enabled systems, deep neural networks (DNNs) need to be carefully evaluated to ensure the produced models match the expected requirements. In practice, the de facto standard to assess the quality of DNNs in the industry is to check their performance (accuracy) on a collected set of labeled test data. However, preparing such labeled data is often not easy partly because of the huge labeling effort, i.e., data labeling is labor-intensive, especially with the massive new incoming unlabeled data every day. Recent studies show that test selection for DNN is a promising direction that tackles this issue by selecting minimal representative data to label and using these data to assess the model. However, it still requires human effort and cannot be automatic. In this paper, we propose a novel technique, named Aries, that can estimate the performance of DNNs on new unlabeled data using only the information obtained from the original test data. The key insight behind our technique is that the model should have similar prediction accuracy on the data which have similar distances to the decision boundary. We performed a large-scale evaluation of our technique on two famous datasets, CIFAR-10 and Tiny-ImageNet, four widely studied DNN models including ResNetl0l and DenseNetl21, and 13 types of data transformation methods. Results show that the estimated accuracy by Aries is only 0.03% - 2.60% off the true accuracy. Besides, Aries also outperforms the state-of-the-art labeling-free methods in 50 out of 52 cases and selection-labeling-based methods in 96 out of 128 cases.",10.1109/ICSE48619.2023.00152,deep learning testing;performance estimation;distribution shift,Deep learning;Uncertainty;Estimation;Artificial neural networks;Predictive models;Data models;Labeling,deep learning (artificial intelligence);image recognition,Aries;CIFAR-10;collected set;competitive performance;data labeling;data transformation methods;deep learning;deep neural networks;DenseNetl21;DL-enabled systems;DNN;DNN models;industrial application domains;labeled test data;labeling-free accuracy estimation;labeling-free methods;large-scale evaluation;minimal representative data;ResNetl0l;selection-labeling-based methods;test selection;Tiny-ImageNet;unlabeled data,1
955,AI testing 2,CC: Causality-Aware Coverage Criterion for Deep Neural Networks,Z. Ji; P. Ma; Y. Yuan; S. Wang,"The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China",2023,"Deep neural network (DNN) testing approaches have grown fast in recent years to test the correctness and robustness of DNNs. In particular, DNN coverage criteria are frequently used to evaluate the quality of a test suite, and a number of coverage criteria based on neuron-wise, layer-wise, and path-/trace-wise coverage patterns have been published to date. However, we see that existing criteria are insufficient to represent how one neuron would influence subsequent neurons; hence, we lack a concept of how neurons, when functioning as causes and effects, might jointly make a DNN prediction. Given recent advances in interpreting DNN internals using causal inference, we present the first causality-aware DNN coverage criterion, which evaluates a test suite by quantifying the extent to which the suite provides new causal relations for testing DNNs. Performing standard causal inference on DNNs presents both theoretical and practical hurdles. We introduce CC (causal coverage), a practical and efficient coverage criterion that integrates a set of optimizations using DNN domain-specific knowledge. We illustrate the efficacy of CC using diverse, real-world inputs and adversarial inputs, such as adversarial examples (AEs) and backdoor inputs. We demonstrate that CC outperforms previous DNN criteria under various settings with moderate cost.",10.1109/ICSE48619.2023.00153,machine learning testing;Causality Analysis;Software Engineering,Costs;Neurons;Artificial neural networks;Robustness;Optimization;Standards;Testing,deep learning (artificial intelligence);program testing;software quality,causal coverage;causal inference;causal relations;causality-aware DNN coverage criterion;CC;deep neural network testing approaches;DNN coverage criteria;DNN domain-specific knowledge;DNN internals;DNN prediction;DNN testing;particular coverage criteria;test suite quality,
956,AI testing 2,Balancing Effectiveness and Flakiness of Non-Deterministic Machine Learning Tests,C. S. Xia; S. Dutta; S. Misailovic; D. Marinov; L. Zhang,"University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign; University of Illinois, Urbana-Champaign",2023,"Testing Machine Learning (ML) projects is challenging due to inherent non-determinism of various ML algorithms and the lack of reliable ways to compute reference results. Developers typically rely on their intuition when writing tests to check whether ML algorithms produce accurate results. However, this approach leads to conservative choices in selecting assertion bounds for comparing actual and expected results in test assertions. Because developers want to avoid false positive failures in tests, they often set the bounds to be too loose, potentially leading to missing critical bugs. We present FASER - the first systematic approach for balancing the trade-off between the fault-detection effectiveness and flakiness of non-deterministic tests by computing optimal assertion bounds. FASER frames this trade-off as an optimization problem between these competing objectives by varying the assertion bound. FASER leverages 1) statistical methods to estimate the flakiness rate, and 2) mutation testing to estimate the fault-detection effectiveness. We evaluate FASER on 87 non-deterministic tests collected from 22 popular ML projects. FASER finds that 23 out of 87 studied tests have conservative bounds and proposes tighter assertion bounds that maximizes the fault-detection effectiveness of the tests while limiting flakiness. We have sent 19 pull requests to developers, each fixing one test, out of which 14 pull requests have already been accepted.",10.1109/ICSE48619.2023.00154,,Machine learning algorithms;Systematics;Limiting;Statistical analysis;Machine learning;Writing;Reliability,learning (artificial intelligence);optimisation;program debugging;program testing;statistical analysis,assertion bounds;balancing effectiveness;conservative bounds;FASER frames;fault-detection effectiveness;flakiness rate;ML algorithms;ML projects;nondeterminism;nondeterministic machine;nondeterministic tests;optimal assertion bounds;test assertions;testing machine,
957,AI testing 2,Many-Objective Reinforcement Learning for Online Testing of DNN-Enabled Systems,F. Ul Haq; D. Shin; L. C. Briand,"University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg; University of Luxembourg, Luxembourg",2023,"Deep Neural Networks (DNNs) have been widely used to perform real-world tasks in cyber-physical systems such as Autonomous Driving Systems (ADS). Ensuring the correct behavior of such DNN-Enabled Systems (DES) is a crucial topic. Online testing is one of the promising modes for testing such systems with their application environments (simulated or real) in a closed loop, taking into account the continuous interaction between the systems and their environments. However, the environmental variables (e.g., lighting conditions) that might change during the systems' operation in the real world, causing the DES to violate requirements (safety, functional), are often kept constant during the execution of an online test scenario due to the two major challenges: (1) the space of all possible scenarios to explore would become even larger if they changed and (2) there are typically many requirements to test simultaneously. In this paper, we present MORLOT (Many-Objective Rein-forcement Learning for Online Testing), a novel online testing approach to address these challenges by combining Reinforcement Learning (RL) and many-objective search. MORLOT leverages RL to incrementally generate sequences of environmental changes while relying on many-objective search to determine the changes so that they are more likely to achieve any of the uncovered objectives. We empirically evaluate MORLOT using CARLA, a high-fidelity simulator widely used for autonomous driving research, integrated with Transfuser, a DNN-enabled ADS for end-to-end driving. The evaluation results show that MORLOT is significantly more effective and efficient than alternatives with a large effect size. In other words, MORLOT is a good option to test DES with dynamically changing environments while accounting for multiple safety requirements.",10.1109/ICSE48619.2023.00155,DNN Testing;Reinforcement learning;Many objective search;Self-driving cars;Online testing,Q-learning;Systems operation;Scalability;Lighting;Search problems;Safety;Task analysis,control engineering computing;cyber-physical systems;deep learning (artificial intelligence);learning (artificial intelligence);reinforcement learning;telecommunication scheduling;traffic engineering computing,application environments;autonomous driving research;Autonomous Driving Systems;cyber-physical systems;Deep Neural Networks;DES;DNN-enabled ADS;DNN-Enabled Systems;environmental changes;Many-Objective Rein-forcement Learning;many-objective search;MORLOT;novel online testing approach;Objective Reinforcement;online test scenario;real-world tasks;uncovered objectives,
958,AI testing 2,Reliability Assurance for Deep Neural Network Architectures Against Numerical Defects,L. Li; Y. Zhang; L. Ren; Y. Xiong; T. Xie,"Department of Computer Science, University of Illinois Urbana-Champaign; Department of Computer Sciences, University of Wisconsin-Madison; School of Computer Science, Peking University; School of Computer Science, Peking University; School of Computer Science, Peking University",2023,"With the widespread deployment of deep neural networks (DNNs), ensuring the reliability of DNN-based systems is of great importance. Serious reliability issues such as system failures can be caused by numerical defects, one of the most frequent defects in DNNs. To assure high reliability against numerical defects, in this paper, we propose the RANUM approach including novel techniques for three reliability assurance tasks: detection of potential numerical defects, confirmation of potential-defect feasibility, and suggestion of defect fixes. To the best of our knowledge, RANUM is the first approach that confirms potential-defect feasibility with failure-exhibiting tests and suggests fixes automatically. Extensive experiments on the benchmarks of 63 real-world DNN architectures show that RANUM outperforms state-of-the-art approaches across the three reliability assurance tasks. In addition, when the RANUM-generated fixes are compared with developers' fixes on open-source projects, in 37 out of 40 cases, RANUM-generated fixes are equivalent to or even better than human fixes.",10.1109/ICSE48619.2023.00156,neural network;numerical defect;testing;fix,Source coding;Artificial neural networks;Computer architecture;Benchmark testing;Reliability;Task analysis;Optimization,deep learning (artificial intelligence);program diagnostics;program testing;software reliability;system recovery,deep neural network architectures;defect fixes;DNN architectures;DNNs;failure-exhibiting tests;numerical defects;potential-defect feasibility;RANUM-generated fixes;reliability assurance;system failures,
959,Developer's forums,"Demystifying Issues, Challenges, and Solutions for Multilingual Software Development",H. Yang; W. Lian; S. Wang; H. Cai,"Washington State University, Pullman, WA, USA; Washington State University, Pullman, WA, USA; University of Manitoba, Winnipeg, Canada; Washington State University, Pullman, WA, USA",2023,"Developing a software project using multiple languages together has been a dominant practice for years. Yet it remains unclear what issues developers encounter during the development, which challenges cause the issues, and what solutions developers receive. In this paper, we aim to answer these questions via a study on developer discussions on Stack Overflow. By manually analyzing 586 highly relevant posts spanning 14 years, we observed a large variety (11 categories) of issues, dominated by those with interfacing and data handling among different languages. Behind these issues, we found that a major challenge developers faced is the diversity and complexity in multilingual code building and interoperability. Another key challenge lies in developers' lack of particular technical background on the diverse features of various languages (e.g., threading and memory management mechanisms). Meanwhile, Stack Overflow itself served as a key source of solutions to these challenges-the majority (73%) of the posts received accepted answers eventually, and most in a week (36.5% within 24 hours and 25% in the next 6 days). Based on our findings on these issues, challenges, and solutions, we provide actionable insights and suggestions for both multi-language software researchers and developers.",10.1109/ICSE48619.2023.00157,Multilingual software;development issues;language interfacing;software build;data format;interoperability,Codes;Instruction sets;Data handling;Memory management;Buildings;Software;Complexity theory,data handling;open systems;software engineering;storage management,data handling;demystifying issues;interoperability;memory management mechanisms;multilingual code building;multilingual software development;software project development;Stack Overflow;threading management mechanisms,
960,Developer's forums,Automated Summarization of Stack Overflow Posts,B. Kou; M. Chen; T. Zhang,"Purdue University, West Lafayette, USA; University of Southern California, Los Angeles, USA; Purdue University, West Lafayette, USA",2023,"Software developers often resort to Stack Overflow (SO) to fill their programming needs. Given the abundance of relevant posts, navigating them and comparing different solutions is tedious and time-consuming. Recent work has proposed to automatically summarize SO posts to concise text to facilitate the navigation of SO posts. However, these techniques rely only on information retrieval methods or heuristics for text summarization, which is insufficient to handle the ambiguity and sophistication of natural language. This paper presents a deep learning based framework called Assortfor SO post summarization. Assortincludes two complementary learning methods, $\mathbf{Assort}_{S}$ and $\mathbf{Assort}_{IS}$, to address the lack of labeled training data for SO post summarization. $\mathbf{Assort}_{S}$ is designed to directly train a novel ensemble learning model with BERT embeddings and domain-specific features to account for the unique characteristics of SO posts. By contrast, $\mathbf{Assort}_{IS}$ is designed to reuse pre-trained models while addressing the domain shift challenge when no training data is present (i.e., zero-shot learning). Both $\mathbf{Assort}_{S}$ and $\mathbf{Assort}_{IS}$ outperform six existing techniques by at least 13% and 7% respectively in terms of the F1 score. Furthermore, a human study shows that participants significantly preferred summaries generated by $\mathbf{Assort}_{S}$ and $\mathbf{Assort}_{IS}$ over the best baseline, while the preference difference between $\mathbf{Assort}_{S}$ and $\mathbf{Assort}_{IS}$ was small.",10.1109/ICSE48619.2023.00158,Stack Overflow;Text Summarization;Deep Learning,Learning systems;Navigation;Natural languages;Training data;Programming;Information retrieval;Data models,deep learning (artificial intelligence);information retrieval;learning (artificial intelligence);natural language processing;text analysis,assort;assortincludes two complementary learning methods;deep learning based framework;information retrieval methods;labeled training data;learning model;relevant posts;SO post summarization;SO posts;Stack Overflow posts;text summarization,
961,Developer's forums,"Semi-Automatic, Inline and Collaborative Web Page Code Curations",R. Rutishauser; A. A. Meyer; R. Holmes; T. Fritz,"Department of Informatics, University of Zurich, Zurich, Switzerland; Department of Informatics, University of Zurich, Zurich, Switzerland; Department of Computer Science, University of British Columbia, Vancouver, Canada; Department of Informatics, University of Zurich, Zurich, Switzerland",2023,"Software developers spend about a quarter of their workday using the web to fulfill various information needs. Searching for relevant information online can be time-consuming, yet acquired information is rarely systematically persisted for later reference. In this work, we introduce SALI, an approach for semi-automated inline linking of web pages to source code locations. SALI helps developers naturally capture high-quality, explicit links between web pages and specific source code lo-cations by recommending links for curation within the IDE. Through two laboratory studies, we examined the developer's ability to both curate and consume links between web pages and specific source code locations while performing software development tasks. The studies were performed with 20 subjects working on realistic software change tasks from widely-used open-source projects. Results show that developers continuously and concisely curate web pages at meaningful locations in the code with little effort. Additionally, we found that other developers could use these curations while performing new and different change tasks to speed up relevant information gathering within unfamiliar codebases by a factor of 2.4.",10.1109/ICSE48619.2023.00159,Semi-automated link curation;knowledge management;web browsing;collaboration,Costs;Codes;Source coding;Web pages;Rendering (computer graphics);Software;Recording,groupware;information retrieval;Internet;programming environments;public domain software;software engineering;source code (software),collaborative Web page code curations;explicit links;IDE;open-source projects;SALI;semiautomated inline linking;software developers;software development tasks;specific source code locations,
962,Program Comprehension,Identifying Key Classes for Initial Software Comprehension: Can We Do It Better?,W. Pan; X. Du; H. Ming; D. -K. Kim; Z. Yang,"School of Computer Science and Information Engineering, Zhejiang Gongshang University, Hangzhou, China; School of Computer Science and Information Engineering, Zhejiang Gongshang University, Hangzhou, China; School of Engineering and Computer Science, Oakland University, Rochester, USA; School of Engineering and Computer Science, Oakland University, Rochester, USA; School of Computer Science, Xi'an Jiaotong University, Shaanxi, China",2023,"Key classes are excellent starting points for developers, especially newcomers, to comprehend an unknown software system. Though many unsupervised key class identification approaches have been proposed in the literature by representing software as class dependency networks (aka software networks) and using some network metrics (e.g., h-index, a-index, and coreness), they are never aware of the field where the nodes exist and the effect of the field on the importance of the nodes in it. According to the classic field theory in physics, every material particle is in a field through which they exert an impact on other particles in the field via non-contact interactions (e.g., electromagnetic force, gravity, and nuclear force). Similarly, every node in a software network might also exist in a field, which might affect the importance of class nodes in it. In this paper, we propose an approach, iFit, to identify key classes in object-oriented software systems. First, we represent software as a CSNWD (Weighted Directed Class-level Software Network) to capture the topological structure of software, including classes, their couplings, and the direction and strength of couplings. Second, we assume that the nodes in the CSNWD exist in a gravitation-like field and propose a new metric, CG (Cumulative Gravitation-like importance), to measure the importance of classes. CG is inspired by Newton's gravitational formula and uses the PageRank value computed by a biased-PageRank algorithm as the masses of classes. Finally, classes in the system are sorted in descending order according to their CG values, and a cutoff is utilized, that is, the top-ranked classes are recommended as key classes. The experiments were performed on a data set composed of six open-source Java systems from the literature. The results show that iFit is superior to the baseline approaches on 93.75% of the total cases, and is scalable to large-scale software systems. Besides, we find that iFit is neutral to the weighting mechanisms used to assign the weights for different coupling types in the CSNWD, that is, when applying iFit to identify key classes, we can use any one of the weighting mechanisms.",10.1109/ICSE48619.2023.00160,complex networks;field theory;key classes;PageRank;program comprehension,Couplings;Measurement;Java;Electromagnetic forces;Software systems;Object recognition;Physics,Java;object-oriented programming;public domain software;search engines,aka software networks;class dependency networks;class nodes;classic field theory;initial Software comprehension;key classes;large-scale software systems;object-oriented software systems;unknown software system;unsupervised key class identification approaches;Weighted Directed Class-level Software Network,
963,Program Comprehension,Improving API Knowledge Discovery with ML: A Case Study of Comparable API Methods,D. Nam; B. Myers; B. Vasilescu; V. Hellendoorn,"Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA; Carnegie Mellon University, USA",2023,"Developers constantly learn new APIs, but often lack necessary information from documentation, resorting instead to popular question-and-answer platforms such as Stack Overflow. In this paper, we investigate how to use recent machine-Iearning-based knowledge extraction techniques to automatically identify pairs of comparable API methods and the sentences describing the comparison from Stack Overflow answers. We first built a prototype that can be stocked with a dataset of comparable API methods and provides tool-tips to users in search results and in API documentation. We conducted a user study with this tool based on a dataset of TensorFlow comparable API methods spanning 198 hand-annotated facts from Stack Overflow posts. This study confirmed that providing comparable API methods can be useful for helping developers understand the design space of APIs: developers using our tool were significantly more aware of the comparable API methods and better understood the differences between them. We then created SOREL, an comparable API methods knowledge extraction tool trained on our hand-annotated corpus, which achieves a 71% precision and 55% recall at discovering our manually extracted facts and discovers 433 pairs of comparable API methods from thousands of unseen Stack Overflow posts. This work highlights the merit of jointly studying programming assistance tools and constructing machine learning techniques to power them.",10.1109/ICSE48619.2023.00161,API;Knowledge Discovery;Pre trained Language Models;Stack Overflow,Prototypes;Documentation;Machine learning;Programming;Knowledge discovery;Software engineering,application program interfaces;data mining;knowledge acquisition;learning (artificial intelligence);natural language processing;question answering (information retrieval),API knowledge discovery;APIs;comparable API methods knowledge extraction tool;TensorFlow comparable API methods,
964,Program Comprehension,Evidence Profiles for Validity Threats in Program Comprehension Experiments,M. M. BarÃ³n; M. Wyrich; D. Graziotin; S. Wagner,"Institute of Software Engineering, University of Stuttgart, Stuttgart, Germany; Institute of Software Engineering, University of Stuttgart, Stuttgart, Germany; Institute of Software Engineering, University of Stuttgart, Stuttgart, Germany; Institute of Software Engineering, University of Stuttgart, Stuttgart, Germany",2023,"Searching for clues, gathering evidence, and reviewing case files are all techniques used by criminal investigators to draw sound conclusions and avoid wrongful convictions. Medicine, too, has a long tradition of evidence-based practice, in which administering a treatment without evidence of its efficacy is considered malpractice. Similarly, in software engineering (SE) research, we can develop sound methodologies and mitigate threats to validity by basing study design decisions on evidence. Echoing a recent call for the empirical evaluation of design decisions in program comprehension experiments, we conducted a 2-phases study consisting of systematic literature searches, snowballing, and thematic synthesis. We found out (1) which validity threat categories are most often discussed in primary studies of code comprehension, and we collected evidence to build (2) the evidence profiles for the three most commonly reported threats to validity. We discovered that few mentions of validity threats in primary studies (31 of 409) included a reference to supporting evidence. For the three most commonly mentioned threats, namely the influence of programming experience, program length, and the selected comprehension measures, almost all cited studies (17 of 18) did not meet our criteria for evidence. We show that for many threats to validity that are currently assumed to be influential across all studies, their actual impact may depend on the design and context of each specific study. Researchers should discuss threats to validity within the context of their particular study and support their discussions with evidence. The present paper can be one resource for evidence, and we call for more meta-studies of this type to be conducted, which will then inform design decisions in primary studies. Further, although we have applied our methodology in the context of program comprehension, our approach can also be used in other SE research areas to enable evidence-based experiment design decisions and meaningful discussions of threats to validity.",10.1109/ICSE48619.2023.00162,program comprehension;threats to validity;empirical software engineering,Road transportation;Codes;Systematics;Sociology;Programming;Reproducibility of results;Task analysis,software engineering,code comprehension;comprehension measures;criminal investigators;evidence profiles;evidence-based experiment design decisions;evidence-based practice;meta-studies;program comprehension experiments;program length;programming experience;SE research areas;snowballing;software engineering research;sound conclusions;study design decisions;supporting evidence;systematic literature searches;thematic synthesis;validity threat categories,
965,Program Comprehension,Developers' Visuo-spatial Mental Model and Program Comprehension,A. Bouraffa; G. -L. Fuhrmann; W. Maalej,"Applied Software Technology, UniversitÃ¤t Hamburg, Hamburg, Germany; Applied Software Technology, UniversitÃ¤t Hamburg, Hamburg, Germany; Applied Software Technology, UniversitÃ¤t Hamburg, Hamburg, Germany",2023,"Previous works from research and industry have proposed a spatial representation of code in a canvas, arguing that a navigational code space confers developers the freedom to organise elements according to their understanding. By allowing developers to translate logical relatedness into spatial proximity, this code representation could aid in code navigation and comprehension. However, the association between developers' code comprehension and their visuo-spatial mental model of the code is not yet well understood. This mental model is affected on the one hand by the spatial code representation and on the other by the visuo-spatial working memory of developers. We address this knowledge gap by conducting an online experiment with 20 developers following a between-subject design. The control group used a conventional tab-based code visualization, while the experimental group used a code canvas to complete three code comprehension tasks. Furthermore, we measure the participants' visuo-spatial working memory using a Corsi Block test at the end of the tasks. Our results suggest that, overall, neither the spatial representation of code nor the visuo-spatial working memory of developers has a significant impact on comprehension performance. However, we identified significant differences in the time dedicated to different comprehension activities such as navigation, annotation, and UI interactions.",10.1109/ICSE48619.2023.00163,Code comprehension;code navigation;developer productivity;IDE design;code visualization;cognitive studies,Industries;Visualization;Codes;Correlation;Navigation;Layout;Particle measurements,data visualisation;software engineering,code canvas;code comprehension tasks;code navigation;Corsi Block test;navigational code space;spatial code representation;spatial proximity;spatial representation;tab-based code visualization;UI interactions;visuo-spatial mental model;visuo-spatial working memory,
966,Program Comprehension,Two Sides of the Same Coin: Exploiting the Impact of Identifiers in Neural Code Comprehension,S. Gao; C. Gao; C. Wang; J. Sun; D. Lo; Y. Yu,"School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; Singapore Management University, Singapore; Singapore Management University, Singapore; National University of Defense Technology",2023,"Previous studies have demonstrated that neural code comprehension models are vulnerable to identifier naming. By renaming as few as one identifier in the source code, the models would output completely irrelevant results, indicating that identifiers can be misleading for model prediction. However, identifiers are not completely detrimental to code comprehension, since the semantics of identifier names can be related to the program semantics. Well exploiting the two opposite impacts of identifiers is essential for enhancing the robustness and accuracy of neural code comprehension, and still remains under-explored. In this work, we propose to model the impact of identifiers from a novel causal perspective, and propose a counterfactual reasoning-based framework named CREAM. CREAM explicitly captures the misleading information of identifiers through multi-task learning in the training stage, and reduces the misleading impact by counterfactual inference in the inference stage. We evaluate CREAM on three popular neural code comprehension tasks, including function naming, defect detection and code classification. Experiment results show that CREAM not only significantly outperforms baselines in terms of robustness (e.g., +37.9% on the function naming task at F1 score), but also achieve improved results on the original datasets (e.g., +0.5% on the function naming task at F1 score).",10.1109/ICSE48619.2023.00164,,Training;Codes;Source coding;Semantics;Predictive models;Multitasking;Robustness,inference mechanisms;learning (artificial intelligence);source code (software),code classification;CREAM;defect detection;function naming task;identifier names;identifier naming;neural code comprehension models;popular neural code comprehension tasks;source code,
967,Reverse Engineering,SeeHow: Workflow Extraction from Programming Screencasts through Action-Aware Video Analytics,D. Zhao; Z. Xing; X. Xia; D. Ye; X. Xu; L. Zhu,"Australian National University and CSIRO's data61, Australia; Australian National University and CSIRO's data61, Australia; Software Engineering Application Technology Lab, Huawei, China; Tencent AI Lab, China; CSIRO's data61, Australia; CSIRO's data61, Australia",2023,"Programming screencasts (e.g., video tutorials on Youtube or live coding stream on Twitch) are important knowledge source for developers to learn programming knowledge, especially the workflow of completing a programming task. Nonetheless, the image nature of programming screencasts limits the accessibility of screencast content and the workflow embedded in it, resulting in a gap to access and interact with the content and workflow in programming screencasts. Existing non-intrusive methods are limited to extract either primitive human-computer interaction (HCI) actions or coarse-grained video fragments. In this work, we leverage Computer Vision (CV) techniques to build a programming screencast analysis tool which can automatically extract code-line editing steps (enter text, delete text, edit text and select text) from screencasts. Given a programming screencast, our approach outputs a sequence of coding steps and code snippets involved in each step, which we refer to as programming workflow. The proposed method is evaluated on 41 hours of tutorial videos and live coding screencasts with diverse programming environments. The results demonstrate our tool can extract code-line editing steps accurately and the extracted workflow steps can be intuitively understood by developers.",10.1109/ICSE48619.2023.00165,Screencast;Computer vision;Workflow extraction;Action recognition,Human computer interaction;Computer vision;Video on demand;Codes;Visual analytics;Tutorials;Programming,computer vision;human computer interaction;programming;video signal processing,action-aware video analytics;code-line editing steps;computer vision;CV;diverse programming environments;HCI;human-computer interaction;live coding screencasts;programming knowledge;programming screencast analysis tool;programming task;programming workflow;screencast content;SeeHow;workflow extraction,
968,Reverse Engineering,AidUI: Toward Automated Recognition of Dark Patterns in User Interfaces,S. M. Hasan Mansur; S. Salma; D. Awofisayo; K. Moran,"Department of Computer Science, George Mason University, Fairfax, VA; Department of Computer Science, George Mason University, Fairfax, VA; Department of Computer, Science Duke University, Durham, NC; Department of Computer Science, George Mason University, Fairfax, VA",2023,"Past studies have illustrated the prevalence of UI dark patterns, or user interfaces that can lead end-users toward (unknowingly) taking actions that they may not have intended. Such deceptive UI designs can be either intentional (to benefit an online service) or unintentional (through complicit design practices) and can result in adverse effects on end users, such as oversharing personal information or financial loss. While significant research progress has been made toward the development of dark pattern taxonomies across different software domains, developers and users currently lack guidance to help recognize, avoid, and navigate these often subtle design motifs. However, automated recognition of dark patterns is a challenging task, as the instantiation of a single type of pattern can take many forms, leading to significant variability. In this paper, we take the first step toward understanding the extent to which common UI dark patterns can be automatically recognized in modern software applications. To do this, we introduce AidUI, a novel automated approach that uses computer vision and natural language processing techniques to recognize a set of visual and textual cues in application screenshots that signify the presence of ten unique UI dark patterns, allowing for their detection, classification, and localization. To evaluate our approach, we have constructed ContextDP, the current largest dataset of fully-localized UI dark patterns that spans 175 mobile and 83 web UI screenshots containing 301 dark pattern instances. The results of our evaluation illustrate that AidUI achieves an overall precision of 0.66, recall of 0.67, F1-score of 0.65 in detecting dark pattern instances, reports few false positives, and is able to localize detected patterns with an IoU score of 0.84. Furthermore, a significant subset of our studied dark patterns can be detected quite reliably (F1 score of over 0.82), and future research directions may allow for improved detection of additional patterns. This work demonstrates the plausibility of developing tools to aid developers in recognizing and appropriately rectifying deceptive UI patterns.",10.1109/ICSE48619.2023.00166,Dark Pattern;UI Analysis;UI Design,Location awareness;Visualization;Navigation;Taxonomy;User interfaces;Software;Pattern recognition,computer vision;feature extraction;natural language processing;pattern classification;user interfaces,301 dark pattern instances;additional patterns;AidUI;complicit design practices;dark pattern taxonomies;deceptive UI designs;deceptive UI patterns;detected patterns;end users;fully-localized UI dark patterns;novel automated approach;personal information;significant research progress;studied dark patterns;subtle design motifs;toward automated recognition;unique UI dark patterns;user interfaces,
969,Reverse Engineering,Carving UI Tests to Generate API Tests and API Specification,R. Yandrapally; S. Sinha; R. Tzoref-Brill; A. Mesbah,"University of British Columbia, Vancouver, BC, Canada; IBM Research, NY, USA; IBM Research, Haifa, Israel; University of British Columbia, Vancouver, BC, Canada",2023,"Modern web applications make extensive use of API calls to update the UI state in response to user events or server-side changes. For such applications, API-level testing can play an important role, in-between unit-level testing and UI-level (or end-to-end) testing. Existing API testing tools require API specifications (e.g., OpenAPI), which often may not be available or, when available, be inconsistent with the API implementation, thus limiting the applicability of automated API testing to web applications. In this paper, we present an approach that leverages UI testing to enable API-level testing for web applications. Our technique navigates the web application under test and automatically generates an API-level test suite, along with an OpenAPI specification that describes the application's server-side APIs (for REST-based web applications). A key element of our solution is a dynamic approach for inferring API endpoints with path parameters via UI navigation and directed API probing. We evaluated the technique for its accuracy in inferring API specifications and the effectiveness of the â€œcarvedâ€ API tests. Our results on seven open-source web applications show that the technique achieves 98% precision and 56% recall in inferring endpoints. The carved API tests, when added to test suites generated by two automated REST API testing tools, increase statement coverage by 52% and 29% and branch coverage by 99% and 75%, on average. The main benefits of our technique are: (1) it enables API-level testing of web applications in cases where existing API testing tools are inapplicable and (2) it creates API-level test suites that cover server-side code efficiently while exercising APIs as they would be invoked from an application's web UI, and that can augment existing API test suites.",10.1109/ICSE48619.2023.00167,Web Application Testing;API Testing;Test Generation;UI Testing;End-to-end Testing;Test Carving;API Specification Inference,Limiting;Codes;Navigation;Testing;Software engineering,application program interfaces;Internet;program testing,API specification;API-level test suite;application program interfaces;automated REST API testing tools;carved API tests;open-source Web applications;OpenAPI specification;REST-based Web applications;UI navigation;UI testing;unit-level testing,
970,Reverse Engineering,Ex pede Herculem: Augmenting Activity Transition Graph for Apps via Graph Convolution Network,Z. Liu; C. Chen; J. Wang; Y. Su; Y. Huang; J. Hu; Q. Wang,"State Key Laboratory of Intelligent Game, Beijing, China; Monash University, Melbourne, Australia; State Key Laboratory of Intelligent Game, Beijing, China; State Key Laboratory of Intelligent Game, Beijing, China; State Key Laboratory of Intelligent Game, Beijing, China; State Key Laboratory of Intelligent Game, Beijing, China; State Key Laboratory of Intelligent Game, Beijing, China",2023,"Mobile apps are indispensable for people's daily life. With the increase of GUI functions, apps have become more complex and diverse. As the Android app is event-driven, Activity Transition Graph (ATG) becomes an important way of app abstract and graphical user interface (GUI) modeling. Although existing works provide static and dynamic analysis to build ATG for applications, the completeness of ATG obtained is poor due to the low coverage of these techniques. To tackle this challenge, we propose a novel approach, ArchiDroid, to automatically augment the ATG via graph convolution network. It models both the semantics of activities and the graph structure of activity transitions to predict the transition between activities based on the seed ATG extracted by static analysis. The evaluation demonstrates that ArchiDroid can achieve 86% precision and 94% recall in predicting the transition between activities for augmenting ATG. We further apply the augmented ATG in two downstream tasks, i.e., guidance in automated GUI testing and assistance in app function design. Results show that the automated GUI testing tool integrated with ArchiDroid achieves 43% more activity coverage and detects 208% more bugs. Besides, ArchiDroid can predict the missing transition with 85% accuracy in real-world apps for assisting the app function design, and an interview case study further demonstrates its usefulness.",10.1109/ICSE48619.2023.00168,GUI testing;deep learning;program analysis;empirical study,Convolution;Buildings;Semantics;Static analysis;Predictive models;Mobile applications;Task analysis,Android (operating system);graph theory;graphical user interfaces;mobile computing;program diagnostics;program testing,activity coverage;activity transitions;Android app;app function design;ArchiDroid;augmented ATG;augmenting Activity Transition Graph;automated GUI testing tool;ex pede herculem;graph convolution network;graph structure;GUI functions;missing transition;mobile apps;people;real-world apps;seed ATG;static analysis,
971,Software Processes,Sustainability is Stratified: Toward a Better Theory of Sustainable Software Engineering,S. McGuire; E. Schultz; B. Ayoola; P. Ralph,"Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada; Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada; Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada; Faculty of Computer Science, Dalhousie University, Halifax, NS, Canada",2023,"Background: Sustainable software engineering (SSE) means creating software in a way that meets present needs without undermining our collective capacity to meet our future needs. It is typically conceptualized as several intersecting dimensions or â€œpillarsâ€-environmental, social, economic, technical and in-dividual. However; these pillars are theoretically underdeveloped and require refinement. Objectives: The objective of this paper is to generate a better theory of SSE. Method: First, a scoping review was conducted to understand the state of research on SSE and identify existing models thereof. Next, a meta-synthesis of qualitative research on SSE was conducted to critique and improve the existing models identified. Results: 961 potentially relevant articles were extracted from five article databases. These articles were de-duplicated and then screened independently by two screeners, leaving 243 articles to examine. Of these, 109 were non-empirical, the most common empirical method was systematic review, and no randomized controlled experiments were found. Most papers focus on ecological sustainability (158) and the sustainability of software products (148) rather than processes. A meta-synthesis of 36 qualitative studies produced several key propositions, most notably, that sustainability is stratified (has different meanings at different levels of abstraction) and multisystemic (emerges from interactions among multiple social, technical, and sociotechnical systems). Conclusion: The academic literature on SSE is surprisingly non-empirical. More empirical evaluations of specific sustainability interventions are needed. The sustainability of software development products and processes should be conceptualized as multisystemic and stratified, and assessed accordingly.",10.1109/ICSE48619.2023.00169,Sustainable development;software engineering;sustainable software engineering;scoping review;meta-synthesis,Economics;Sociotechnical systems;Systematics;Databases;Biological system modeling;Instruments;Software,ecology;software engineering;sustainable development,article databases;ecological sustainability;meta-synthesis;qualitative research;social systems;sociotechnical systems;software development products;SSE;sustainability interventions;sustainable software engineering,
972,Static analysis,DLInfer: Deep Learning with Static Slicing for Python Type Inference,Y. Yan; Y. Feng; H. Fan; B. Xu,"State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",2023,"Python programming language has gained enor-mous popularity in the past decades. While its flexibility signifi-cantly improves software development productivity, the dynamic typing feature challenges software maintenance and quality assurance. To facilitate programming and type error checking, the Python programming language has provided a type hint mechanism enabling developers to annotate type information for variables. However, this manual annotation process often requires plenty of resources and may introduce errors. In this paper, we propose a deep learning type inference technique, namely DLInfer, to automatically infer the type infor-mation for Python programs. DLInfer collects slice statements for variables through static analysis and then vectorizes them with the Unigram Language Model algorithm. Based on the vectorized slicing features, we designed a bi-directional gated recurrent unit model to learn the type propagation information for inference. To validate the effectiveness of DLInfer, we conduct an extensive empirical study on 700 open-source projects. We evaluate its accuracy in inferring three kinds of fundamental types, including built-in, library, and user-defined types. By training with a large-scale dataset, DLInfer achieves an average of 98.79% Top-1 accuracy for the variables that can get type information through static analysis and manual annotation. Further, DLInfer achieves 83.03% type inference accuracy on average for the variables that can only obtain the type information through dynamic analysis. The results indicate DLInfer is highly effective in inferring types. It is promising to apply it to assist in various software engineering tasks for Python programs.",10.1109/ICSE48619.2023.00170,type inference;Python;static slicing,Deep learning;Training;Software maintenance;Quality assurance;Annotations;Static analysis;Manuals,deep learning (artificial intelligence);inference mechanisms;learning (artificial intelligence);program diagnostics;Python;reasoning about programs;recurrent neural nets;software engineering;software maintenance;type theory,83.03% type inference accuracy;bi-directional gated recurrent unit model;deep learning type inference technique;DLInfer;dynamic typing feature;enor-mous popularity;flexibility signifi-cantly;fundamental types;inferring types;manual annotation process;Python programming language;Python programs;Python type inference;quality assurance;slice statements;software development productivity;software maintenance;static analysis;static slicing;type error checking;type hint mechanism;type infor-mation;type information;type propagation information;Unigram Language Model algorithm;user-defined types;vectorized slicing features,
973,Static analysis,ViolationTracker: Building Precise Histories for Static Analysis Violations,P. Yu; Y. Wu; X. Peng; J. Peng; J. Zhang; P. Xie; W. Zhao,"School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China; School of Computer Science, Fudan University, Shanghai, China",2023,"Automatic static analysis tools (ASATs) detect source code violations to static analysis rules and are usually used as a guard for source code quality. The adoption of ASATs, however, is often challenged because of several problems such as a large number of false alarms, invalid rule priorities, and inappropriate rule configurations. Research has shown that tracking the history of the violations is a promising way to solve the above problems because the facts of violation fixing may reflect the developers' subjective expectations on the violation detection results. Precisely identifying the revisions that induce or fix a violation is however challenging because of the imprecise matching of violations between code revisions and ignorance of merge commits in the maintenance history. In this paper, we propose ViolationTracker, an approach to precisely matching the violation instances between adjacent revisions and building the life cycle of violations with the identification of inducing, fixing, deleting, and reopening of each violation case. The approach employs code entity anchoring heuristics for violation matching and considers merge commits that used to be ignored in existing research. We evaluate ViolationTracker with a manually-validated dataset that consists of 500 violation instances and 158 threads of 30 violation cases with detailed evolution history from open-source projects. Violation Tracker achieves over 93 % precision and 98 % recall on violation matching, outperforming the state-of-the-art approach, and 99.4 % precision on rebuilding the histories of violation cases. We also show that ViolationTracker is useful to identify actionable violations. A preliminary empirical study reveals the possibility to prioritize static analysis rules according to further analysis on the actionable rates of the rules.",10.1109/ICSE48619.2023.00171,,Codes;Source coding;Buildings;Static analysis;Manuals;Maintenance engineering;History,software performance evaluation;software quality;source code (software),actionable violations;ASATs;automatic static analysis tools;building precise histories;code revisions;rule configurations;source code quality;source code violations;static analysis rules;static analysis violations;violation case;violation detection;violation fixing;violation instances;violation matching;violation tracker;ViolationTracker,
974,Testing of database and low-level software,Compiler Test-Program Generation via Memoized Configuration Search,J. Chen; C. Suo; J. Jiang; P. Chen; X. Li,"College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China; College of Intelligence and Computing, Tianjin University, Tianjin, China",2023,"To ensure compilers' quality, compiler testing has received more and more attention, and test-program generation is the core task. In recent years, some approaches have been proposed to explore test configurations for generating more effective test programs, but they either are restricted by historical bugs or suffer from the cost-effectiveness issue. Here, we propose a novel test-program generation approach (called MCS) to further improving the performance of compiler testing. MCS conducts memoized search via multi-agent reinforcement learning (RL) for guiding the construction of effective test configurations based on the memoization for the explored test configurations during the on-the-fly compiler-testing process. During the process, the elaborate coordination among configuration options can be also well learned by multi-agent RL, which is required for generating bug-triggering test programs. Specifically, MCS considers the diversity among test configurations to efficiently explore the input space and the testing results under each explored configuration to learn which portions of space are more bug-triggering. Our extensive experiments on GCC and LLVM demonstrate the performance of MCS, significantly outperforming the state-of-the-art test-program generation approaches in bug detection. Also, MCS detects 16 new bugs on the latest trunk revisions of GCC and LLVM, and all of them have been confirmed or fixed by developers. MCS has been deployed by a global IT company (i.e., Huawei) for testing their in-house compiler, and detects 10 new bugs (covering all the 5 bugs detected by the compared approaches), all of which have been confirmed.",10.1109/ICSE48619.2023.00172,Compiler Testing;Test Program Generation;Reinforcement Learning;Configuration,Program processors;Atmospheric measurements;Computer bugs;Reinforcement learning;Companies;Particle measurements;Space exploration,learning (artificial intelligence);multi-agent systems;program compilers;program debugging;program testing;reinforcement learning,compiler test-program generation;compiler testing;compiler-testing process;compilers;configuration options;cost-effectiveness issue;effective test configurations;effective test programs;explored configuration;explored test configurations;generating bug-triggering test programs;historical bugs;in-house compiler;MCS conducts;memoized configuration search;multiagent reinforcement learning;multiagent RL;novel test-program generation approach;state-of-the-art test-program generation approaches,
975,Testing of database and low-level software,Generating Test Databases for Database-Backed Applications,C. Yan; S. Nath; S. Lu,"Microsoft Research, Redmond, USA; Microsoft Research, Redmond, USA; University of Chicago, Chicago, USA",2023,"Database-backed applications are widely used. To effectively test these applications, one needs to design not only user inputs but also database states, which imposes unique challenges. First, valid database states have to satisfy complicated constraints determined by application semantics, and hence are difficult to synthesize. Second, the state space of a database is huge, as an application can contain tens to hundreds of tables with up to tens of fields per table. Making things worse, each test involving database operations takes significant time to run. Consequently, unhelpful database states and running tests on them can severely waste testing resources. We propose DBGRILLER, a tool that generates database states to facilitate thorough testing of database-backed applications. To effectively generate valid database states, DBGRILLER strategically injects minor mutation into existing database states and transforms part of the application-under-test into a stand-alone validity checker. To tackle the huge database state space and save testing time, DBGRILLER uses program analysis to identify a novel branch-projected DB view that can be used to filter out database states that are unlikely to increase the testing branch coverage. Our evaluation on 9 popular open-source database applications shows that DBGRILLER can effectively increase branch coverage of existing tests and expose previously unknown bugs.",10.1109/ICSE48619.2023.00173,Automated testing;Test data generation;database-backed application;database-state generation,Databases;Computer bugs;Semantics;Transforms;Testing;Software engineering,database management systems;program diagnostics;program testing,application semantics;application-under-test;branch coverage;branch-projected DB view;database state space;database-backed applications;DBGRILLER;open-source database applications;program analysis;unhelpful database states,
976,Testing of database and low-level software,Testing Database Engines via Query Plan Guidance,J. Ba; M. Rigger,National University of Singapore; National University of Singapore,2023,"Database systems are widely used to store and query data. Test oracles have been proposed to find logic bugs in such systems, that is, bugs that cause the database system to compute an incorrect result. To realize a fully automated testing approach, such test oracles are paired with a test case generation technique; a test case refers to a database state and a query on which the test oracle can be applied. In this work, we propose the concept of Query Plan Guidance (QPG) for guiding automated testing towards â€œinterestingâ€ test cases. SQL and other query languages are declarative. Thus, to execute a query, the database system translates every operator in the source language to one of the potentially many so-called physical operators that can be executed; the tree of physical operators is referred to as the query plan. Our intuition is that by steering testing towards exploring a variety of unique query plans, we also explore more interesting behaviors-some of which are potentially incorrect. To this end, we propose a mutation technique that gradually applies promising mutations to the database state, causing the DBMS to create potentially unseen query plans for subsequent queries. We applied our method to three mature, widely-used, and extensively-tested database systems-SQLite, TiDB, and CockroachDB-and found 53 unique, previously unknown bugs. Our method exercises $4.85-408.48\times$ more unique query plans than a naive random generation method and $7.46\times$ more than a code coverage guidance method. Since most database systems-including commercial ones-expose query plans to the user, we consider QPG a generally applicable, black-box approach and believe that the core idea could also be applied in other contexts (e.g., to measure the quality of a test suite).",10.1109/ICSE48619.2023.00174,automated testing;test case generation,Structured Query Language;Codes;Computer bugs;Closed box;Database systems;Behavioral sciences;Software measurement,program testing;query processing;SQL,black-box approach;code coverage guidance method;data querying;data storage;database engine testing;database state;database system;DBMS;fully automated testing approach;logic bugs;mutation technique;naive random generation method;ockroachDB;physical operators;QPG;query languages;query plan guidance;source language;SQLite;test case generation technique;test oracles;TiDB;unique query plans,2
977,Testing of database and low-level software,Testing Database Systems via Differential Query Execution,J. Song; W. Dou; Z. Cui; Q. Dai; W. Wang; J. Wei; H. Zhong; T. Huang,"State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences",2023,"Database Management Systems (DBMSs) provide efficient data retrieval and manipulation for many applications through Structured Query Language (SQL). Incorrect implementations of DBMSs can result in logic bugs, which cause SELECT queries to fetch incorrect results, or UPDATE and DELETE queries to generate incorrect database states. Existing approaches mainly focus on detecting logic bugs in SELECT queries. However, logic bugs in UPDATE and DELETE queries have not been tackled. In this paper, we propose a novel and general approach, which we have termed Differential Query Execution (DQE), to detect logic bugs in SELECT, UPDATE and DELETE queries of DBMSs. The core idea of DQE is that different SQL queries with the same predicate usually access the same rows in a database. For example, a row updated by an UPDATE query with a predicate Ï† should also be fetched by a SELECT query with the same predicate Ï†, If not, a logic bug is revealed in the target DBMS. To evaluate the effectiveness and generality of DQE, we apply DQE on five production-level DBMSs, i.e., MySQL, MariaDB, TiDB, CockroachDB and SQLite. In total, we have detected 50 unique bugs in these DBMSs, 41 of which have been confirmed, and 11 have been fixed. We expect that the simplicity and generality of DQE can greatly improve the reliability of DBMSs.",10.1109/ICSE48619.2023.00175,Database system;DBMS testing;logic bug,Structured Query Language;Computer bugs;Reliability engineering;Database systems;Software reliability;Testing;Software engineering,database management systems;distributed databases;query processing;relational databases;SQL;transaction processing,50 unique bugs;Database Management Systems;database Systems;DBMSs;different SQL queries;Differential Query Execution;DQE;efficient data retrieval;incorrect database states;incorrect implementations;incorrect results;logic bug;predicate Ï†;SELECT query;Structured Query Language;UPDATE query,1
978,Software performance,Analysing the Impact of Workloads on Modeling the Performance of Configurable Software Systems,S. MÃ¼hlbauer; F. Sattler; C. Kaltenecker; J. Dorn; S. Apel; N. Siegmund,"Leipzig University; Saarland Informatics Campus, Saarland University; Saarland Informatics Campus, Saarland University; Leipzig University; Saarland Informatics Campus, Saarland University; ScaDS.AI Dresden, Leipzig University, Leipzig",2023,"Modern software systems often exhibit numerous configuration options to tailor them to user requirements, including the system's performance behavior. Performance models derived via machine learning are an established approach for estimating and optimizing configuration-dependent software performance. Most existing approaches in this area rely on software performance measurements conducted with a single workload (i.e., input fed to a system). This single workload, however, is often not representative of a software system's real-world application scenarios. Understanding to what extent configuration and workload-individually and combined-cause a software system's performance to vary is key to understand whether performance models are generalizable across different configurations and workloads. Yet, so far, this aspect has not been systematically studied. To fill this gap, we conducted a systematic empirical study across 25 258 configurations from nine real-world configurable software systems to investigate the effects of workload variation at system-level performance and for individual configuration options. We explore driving causes for workload-configuration interactions by enriching performance observations with option-specific code coverage information. Our results demonstrate that workloads can induce substantial performance variation and interact with configuration options, often in non-monotonous ways. This limits not only the generalizability of single-workload models, but also challenges assumptions for existing transfer-learning techniques. As a result, workloads should be considered when building performance prediction models to maintain and improve representativeness and reliability.",10.1109/ICSE48619.2023.00176,,Codes;Systematics;Sensitivity;System performance;Software performance;Predictive models;Software systems,learning (artificial intelligence);program testing;software performance evaluation,building performance prediction models;configuration options;configuration-dependent software performance;extent configuration;modern software systems;option-specific code coverage information;performance models;performance observations;performance variation;real-world configurable software systems;single-workload models;software performance measurements;system-level performance;transfer-learning techniques;workload variation;workload-configuration interactions,
979,Software performance,Twins or False Friends? A Study on Energy Consumption and Performance of Configurable Software,M. Weber; C. Kaltenecker; F. Sattler; S. Apel; N. Siegmund,"Leipzig University, Germany; Saarland University, Germany; Saarland University, Germany; Saarland University, Germany; Leipzig University, Germany",2023,"Reducing energy consumption of software is an increasingly important objective, and there has been extensive research for data centers, smartphones, and embedded systems. However, when it comes to software, we lack working tools and methods to directly reduce energy consumption. For performance, we can resort to configuration options for tuning response time or throughput of a software system. For energy, it is still unclear whether the underlying assumption that runtime performance correlates with energy consumption holds, especially when it comes to optimization via configuration. To evaluate whether and to what extent this assumption is valid for configurable software systems, we conducted the largest empirical study of this kind to date. First, we searched the literature for reports on whether and why runtime performance correlates with energy consumption. We obtained a mixed, even contradicting picture from positive to negative correlation, and that configurability has not been considered yet as a factor for this variance. Second, we measured and analyzed both the runtime performance and energy consumption of 14 real-world software systems. We found that, in many cases, it depends on the software system's configuration whether runtime performance and energy consumption correlate and that, typically, only few configuration options influence the degree of correlation. A fine-grained analysis at the function level revealed that only few functions are relevant to obtain an accurate proxy for energy consumption and that, knowing them, allows one to infer individual transfer factors between runtime performance and energy consumption.",10.1109/ICSE48619.2023.00177,energy consumption;performance;configurability;correlation,Energy consumption;Runtime;Correlation;Energy measurement;Software systems;Time measurement;Software measurement,embedded systems;energy consumption;smart phones,configurable software systems;configuration options;energy consumption;runtime performance,
980,Code generation,Learning Deep Semantics for Test Completion,P. Nie; R. Banerjee; J. J. Li; R. J. Mooney; M. Gligoric,"UT, Austin, USA; UT, Austin, USA; UT, Austin, USA; UT, Austin, USA; UT, Austin, USA",2023,"Writing tests is a time-consuming yet essential task during software development. We propose to leverage recent advances in deep learning for text and code generation to assist developers in writing tests. We formalize the novel task of test completion to automatically complete the next statement in a test method based on the context of prior statements and the code under test. We develop TECo-a deep learning model using code semantics for test completion. The key insight underlying TECO is that predicting the next statement in a test method requires reasoning about code execution, which is hard to do with only syntax-level data that existing code completion models use. Teco extracts and uses six kinds of code semantics data, including the execution result of prior statements and the execution context of the test method. To provide a testbed for this new task, as well as to evaluate TECO, we collect a corpus of 130,934 test methods from 1,270 open-source Java projects. Our results show that Teco achieves an exact-match accuracy of 18, which is 29% higher than the best baseline using syntax-level data only. When measuring functional correctness of generated next statement, Teco can generate runnable code in 29% of the cases compared to 18% obtained by the best baseline. Moreover, Teco is sianificantly better than prior work on test oracle generation.",10.1109/ICSE48619.2023.00178,test completion;deep neural networks;programming language semantics,Deep learning;Measurement;Java;Codes;Semantics;Writing;Predictive models,deep learning (artificial intelligence);Java;program compilers;program testing,130 test methods;934 test methods;code completion models;code execution;code semantics data;deep learning;deep semantics;prior statements;runnable code;syntax-level data;TECO;Teco;test completion;test method;test oracle generation;time-consuming yet essential task;writing tests,
981,Code generation,SkCoder: A Sketch-based Approach for Automatic Code Generation,J. Li; Y. Li; G. Li; Z. Jin; Y. Hao; X. Hu,"Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China; Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China; Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China; Key Lab of High Confidence Software Technology, MoE (Peking University), Beijing, China; aiXcoder, Beijing, China; Zhejiang University, Ningbo, China",2023,"Recently, deep learning techniques have shown great success in automatic code generation. Inspired by the code reuse, some researchers propose copy-based approaches that can copy the content from similar code snippets to obtain better performance. Practically, human developers recognize the content in the similar code that is relevant to their needs, which can be viewed as a code sketch. The sketch is further edited to the desired code. However, existing copy-based approaches ignore the code sketches and tend to repeat the similar code without necessary modifications, which leads to generating wrong results. In this paper, we propose a sketch-based code generation approach named Skcoderto mimic developers' code reuse behavior. Given a natural language requirement, Skcoderretrieves a similar code snippet, extracts relevant parts as a code sketch, and edits the sketch into the desired code. Our motivations are that the extracted sketch provides a well-formed pattern for telling models â€œhow to writeâ€. The post-editing further adds requirement-specific details into the sketch and outputs the complete code. We conduct experiments on two public datasets and a new dataset collected by this work. We compare our approach to 20 baselines using 5 widely used metrics. Experimental results show that (1) Skcodercan generate more correct programs, and outperforms the state-of-the-art -CodeT5-base by 30.30%, 35.39%, and 29.62% on three datasets. (2) Our approach is effective to multiple code generation models and improves them by up to 120.1% in Pass@l. (3) We investigate three plausible code sketches and discuss the importance of sketches. (4) We manually evaluate the generated code and prove the superiority of our Skcoderin three aspects.",10.1109/ICSE48619.2023.00179,Code Generation;Deep Learning,Measurement;Deep learning;Codes;Natural languages;Behavioral sciences;Software engineering,deep learning (artificial intelligence);program compilers,automatic code generation;code reuse;code sketch;CodeT5-base;complete code;copy-based approaches;extracted sketch;multiple code generation models;plausible code sketches;similar code snippet;SkCoder;sketch-based approach;sketch-based code generation approach,
982,Code generation,An Empirical Comparison of Pre-Trained Models of Source Code,C. Niu; C. Li; V. Ng; D. Chen; J. Ge; B. Luo,"State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; Human Language Technology Research Institute, University of Texas at Dallas, Richardson, Texas, USA; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China; State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing, China",2023,"While a large number of pre-trained models of source code have been successfully developed and applied to a variety of software engineering (SE) tasks in recent years, our understanding of these pre-trained models is arguably fairly limited. With the goal of advancing our understanding of these models, we perform the first systematic empirical comparison of 19 recently-developed pre-trained models of source code on 13 SE tasks. To gain additional insights into these models, we adopt a recently -developed 4-dimensional categorization of pre-trained models, and subsequently investigate whether there are correlations between different categories of pre-trained models and their performances on different SE tasks.",10.1109/ICSE48619.2023.00180,Pre-training of Source Code;AI for SE,Systematics;Codes;Correlation;Source coding;Task analysis;Software engineering,software engineering;source code (software),4-dimensional categorization;SE tasks;software engineering tasks;source code,1
983,Code generation,On the Robustness of Code Generation Techniques: An Empirical Study on GitHub Copilot,A. Mastropaolo; L. Pascarella; E. Guglielmi; M. Ciniselli; S. Scalabrino; R. Oliveto; G. Bavota,"SEART @ Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland; SEART @ Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland; STAKE Lab, University of Molise, Italy; SEART @ Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland; STAKE Lab, University of Molise, Italy; STAKE Lab, University of Molise, Italy; SEART @ Software Institute, UniversitÃ della Svizzera italiana (USI), Switzerland",2023,"Software engineering research has always being concerned with the improvement of code completion approaches, which suggest the next tokens a developer will likely type while coding. The release of GitHub Copilot constitutes a big step forward, also because of its unprecedented ability to automatically generate even entire functions from their natural language description. While the usefulness of Copilot is evident, it is still unclear to what extent it is robust. Specifically, we do not know the extent to which semantic-preserving changes in the natural language description provided to the model have an effect on the generated code function. In this paper we present an empirical study in which we aim at understanding whether different but semantically equivalent natural language descriptions result in the same recommended function. A negative answer would pose questions on the robustness of deep learning (DL)-based code generators since it would imply that developers using different wordings to describe the same code would obtain different recommendations. We asked Copilot to automatically generate 892 Java methods starting from their original Javadoc description. Then, we generated different semantically equivalent descriptions for each method both manually and automatically, and we analyzed the extent to which predictions generated by Copilot changed. Our results show that modifying the description results in different code recommendations in âˆ¼46% of cases. Also, differences in the semantically equivalent descriptions might impact the correctness of the generated code (Â±28%).",10.1109/ICSE48619.2023.00181,Empirical Study;Recommender Systems,Deep learning;Java;Codes;Natural languages;Robustness;Generators;Encoding,deep learning (artificial intelligence);Java;natural language processing;program compilers;software engineering,big step;code completion approaches;code generation techniques;deep learning-based code generators;description results;different but semantically equivalent natural language descriptions;different code recommendations;different recommendations;different wordings;entire functions;generated code function;github copilot;natural language description;original Javadoc description;recommended function;semantically equivalent descriptions;software engineering research;unprecedented ability,1
984,Code generation,Source Code Recommender Systems: The Practitioners' Perspective,M. Ciniselli; L. Pascarella; E. Aghajani; S. Scalabrino; R. Oliveto; G. Bavota,"SEART @ Software Institute, UniversitÃ¡ della Svizzera italiana (USI), Switzerland; SEART @ Software Institute, UniversitÃ¡ della Svizzera italiana (USI), Switzerland; SEART @ Software Institute, UniversitÃ¡ della Svizzera italiana (USI), Switzerland; STAKE Lab @ University of Molise, Italy; STAKE Lab @ University of Molise, Italy; SEART @ Software Institute, UniversitÃ¡ della Svizzera italiana (USI), Switzerland",2023,"The automatic generation of source code is one of the long-lasting dreams in software engineering research. Several techniques have been proposed to speed up the writing of new code. For example, code completion techniques can recommend to developers the next few tokens they are likely to type, while retrieval-based approaches can suggest code snippets relevant for the task at hand. Also, deep learning has been used to automatically generate code statements starting from a natural language description. While research in this field is very active, there is no study investigating what the users of code recommender systems (i.e., software practitioners) actually need from these tools. We present a study involving 80 software developers to investigate the characteristics of code recommender systems they consider important. The output of our study is a taxonomy of 70 â€œrequirementsâ€ that should be considered when designing code recommender systems. For example, developers would like the recommended code to use the same coding style of the code under development. Also, code recommenders being â€œawareâ€ of the developers' knowledge (e.g., what are the framework/libraries they already used in the past) and able to customize the recommendations based on this knowledge would be appreciated by practitioners. The taxonomy output of our study points to a wide set of future research directions for code recommenders.",10.1109/ICSE48619.2023.00182,Code Recommender Systems;Empirical Study;Practitioners' Survey,Surveys;Deep learning;Codes;Source coding;Taxonomy;Natural languages;Software,deep learning (artificial intelligence);natural language processing;recommender systems;software engineering;source code (software),80 software developers;automatic generation;code completion techniques;code recommenders;code snippets;code statements;coding style;designing code recommender systems;long-lasting dreams;recommended code;software engineering research;software practitioners;source code recommender systems,
985,Software development tools,Safe Low-Level Code Without Overhead is Practical,S. Pirelli; G. Candea,EPFL; EPFL,2023,"Developers write low-level systems code in unsafe programming languages due to performance concerns. The lack of safety causes bugs and vulnerabilities that safe languages avoid. We argue that safety without run-time overhead is possible through type invariants that prove the safety of potentially unsafe operations. We empirically show that Rust and C# can be extended with such features to implement safe network device drivers without run-time overhead, and that Ada has these features already.",10.1109/ICSE48619.2023.00183,programming languages;safety;performance,Performance evaluation;Computer languages;Codes;Computer bugs;Programming;Safety;C# languages,device drivers;program debugging;source code (software),Ada;bugs;potentially unsafe operations;run-time overhead;safe languages;safe network device drivers;safe-low-level system code;type invariants;unsafe programming languages,
986,Software development tools,Sibyl: Improving Software Engineering Tools with SMT Selection,W. Leeson; M. B. Dwyer; A. Filieri,"Department of Computer Science, University of Virginia, Charlottesville, USA; Department of Computer Science, University of Virginia, Charlottesville, USA; Department of Computing, Imperial College London, London, UK",2023,"SMT solvers are often used in the back end of different software engineering toolsâ”€e.g., program verifiers, test generators, or program synthesizers. There are a plethora of algorithmic techniques for solving SMT queries. Among the available SMT solvers, each employs its own combination of algorithmic techniques that are optimized for different fragments of logics and problem types. The most efficient solver can change with small changes in the SMT query, which makes it nontrivial to decide which solver to use. Consequently, designers of software engineering tools often select a single solver, based on familiarity or convenience, and tailor their tool towards it. Choosing an SMT solver at design time misses the opportunity to optimize query solve times and, for tools where SMT solving is a bottleneck, the performance loss can be significant. In this work, we present Sibyl, an automated SMT selector based on graph neural networks (GNNs). Sibyl creates a graph representation of a given SMT query and uses GNNs to predict how each solver in a suite of SMT solvers would perform on said query. Sibyl learns to predict based on features of SMT queries that are specific to the population on which it is trained - avoiding the need for manual feature engineering. Once trained, Sibyl makes fast and accurate predictions which can substantially reduce the time needed to solve a set of SMT queries. We evaluate Sibyl in four scenarios in which SMT solvers are used: in competition, in a symbolic execution engine, in a bounded model checker, and in a program synthesis tool. We find that Sibyl improves upon the state of the art in nearly every case and provide evidence that it generalizes better than existing techniques. Further, we evaluate Sibyl's overhead and demonstrate that it has the potential to speedup a variety of different software engineering tools.",10.1109/ICSE48619.2023.00184,graph neural networks;satisfiable modulo theories;algorithm selection,Synthesizers;Software algorithms;Buildings;Sociology;Prototypes;Predictive models;Graph neural networks,formal specification;formal verification;graph neural networks;graph theory;program diagnostics;program verification;query processing;software engineering,algorithmic techniques;automated SMT selector;available SMT solvers;different software engineering tools-e.g;efficient solver;given SMT query;improving software engineering tools;program synthesis tool;Sibyl;single solver;SMT selection;SMT solver;SMT solving,
987,Software development tools,CoCoSoDa: Effective Contrastive Learning for Code Search,E. Shi; Y. Wang; W. Gu; L. Du; H. Zhang; S. Han; D. Zhang; H. Sun,"Xi'an Jiaotong University; School of Software Engineering, Sun Yat-sen University; The Chinese University of Hong Kong; Microsoft Research; Chongqing University; Microsoft Research; Microsoft Research; Xi'an Jiaotong University",2023,"Code search aims to retrieve semantically relevant code snippets for a given natural language query. Recently, many approaches employing contrastive learning have shown promising results on code representation learning and greatly improved the performance of code search. However, there is still a lot of room for improvement in using contrastive learning for code search. In this paper, we propose CoCoSoDa to effectively utilize contrastive learning for code search via two key factors in contrastive learning: data augmentation and negative samples. Specifically, soft data augmentation is to dynamically masking or replacing some tokens with their types for input sequences to generate positive samples. Momentum mechanism is used to generate large and consistent representations of negative samples in a mini-batch through maintaining a queue and a momentum encoder. In addition, multimodal contrastive learning is used to pull together representations of code-query pairs and push apart the unpaired code snippets and queries. We conduct extensive experiments to evaluate the effectiveness of our approach on a large-scale dataset with six programming languages. Experimental results show that: (1) CoCoSoDa outperforms 18 baselines and especially exceeds CodeBERT, GraphCodeBERT, and UniXcoder by 13.3%, 10.5%, and 5.9% on average MRR scores, respectively. (2) The ablation studies show the effectiveness of each component of our approach. (3) We adapt our techniques to several different pre-trained models such as RoBERTa, CodeBERT, and GraphCodeBERT and observe a significant boost in their performance in code search. (4) Our model performs robustly under different hyper-parameters. Furthermore, we perform qualitative and quantitative analyses to explore reasons behind the good performance of our model.",10.1109/ICSE48619.2023.00185,code search;contrastive learning;soft data augmentation;momentum mechanism,Representation learning;Adaptation models;Computer languages;Codes;Statistical analysis;Source coding;Natural languages,data augmentation;learning (artificial intelligence);natural language processing;query processing;supervised learning,code representation;code search;code-query pairs;effective contrastive learning;multimodal contrastive learning;queries;semantically relevant code snippets;unpaired code snippets,
988,Fault injection and mutation,Coverage Guided Fault Injection for Cloud Systems,Y. Gao; W. Dou; D. Wang; W. Feng; J. Wei; H. Zhong; T. Huang,"State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences; State Key Laboratory of Computer Science, Institute of Software, Chinese Academy of Sciences",2023,"To support high reliability and availability, modern cloud systems are designed to be resilient to node crashes and reboots. That is, a cloud system should gracefully recover from node crashes/reboots and continue to function. However, node crashes/reboots that occur under special timing can trigger crash recovery bugs that lie in incorrect crash recovery protocols and their implementations. To ensure that a cloud system is free from crash recovery bugs, some fault injection approaches have been proposed to test whether a cloud system can correctly recover from various crash scenarios. These approaches are not effective in exploring the huge crash scenario space without developers' knowledge. In this paper, we propose Crash Fuzz, a fault injection testing approach that can effectively test crash recovery behaviors and reveal crash recovery bugs in cloud systems. CrashFuzz mutates the combinations of possible node crashes and reboots according to runtime feedbacks, and prioritizes the combinations that are prone to increase code coverage and trigger crash recovery bugs for smart exploration. We have implemented CrashFuzz and evaluated it on three popular open-source cloud systems, i.e., ZooKeeper, HDFS and HBase. CrashFuzz has detected 4 unknown bugs and 1 known bug. Compared with other fault injection approaches, CrashFuzz can detect more crash recovery bugs and achieve higher code coverage.",10.1109/ICSE48619.2023.00186,cloud system;crash recovery bug;fault injection;bug detection;fuzzing,Codes;Runtime;Protocols;Computer bugs;Reliability engineering;Timing;Behavioral sciences,cloud computing;program testing,cloud system;code coverage;coverage guided fault injection;crash recovery behaviors;CrashFuzz;fault injection approaches;HBase;HDFS;incorrect crash recovery protocols;node crashes;open-source cloud systems;trigger crash recovery bugs;ZooKeeper,
989,Fault injection and mutation,Diver: Oracle-Guided SMT Solver Testing with Unrestricted Random Mutations,J. Kim; S. So; H. Oh,"Korea University, Republic of Korea; Korea University, Republic of Korea; Korea University, Republic of Korea",2023,"We present Diver, a novel technique for effectively finding critical bugs in SMT solvers. Ensuring the correctness of SMT solvers is becoming increasingly important as many applications use solvers as a foundational basis. In response, several approaches for testing SMT solvers, which are classified into differential testing and oracle-guided approaches, have been proposed until recently. However, they are still unsatisfactory in that (1) differential testing approaches cannot validate unique yet important features of solvers, and (2) oracle-guided approaches cannot generate diverse tests due to their reliance on limited mutation rules. Diver aims to complement these shortcomings, particularly focusing on finding bugs that are missed by existing approaches. To this end, we present a new testing technique that performs oracle-guided yet unrestricted random mutations. We have used Diver to validate the most recent versions of three popular SMT solvers: CVC5, Z3 and dReal. In total, Diver found 25 new bugs, of which 21 are critical and directly affect the reliability of the solvers. We also empirically prove DIVER's own strength by showing that existing tools are unlikely to find the bugs discovered by Diver.",10.1109/ICSE48619.2023.00187,software testing;fuzzing;SMT solver,Computer bugs;Focusing;Software;Reliability;Testing;Software engineering,computability;program debugging;program testing,critical bug finding;CVC5;differential testing;Diver;dReal;oracle-guided SMT solver testing;satisfiability modulo theories;unrestricted random mutations;Z3,
990,Vulnerability detection,An Empirical Study of Deep Learning Models for Vulnerability Detection,B. Steenhoek; M. M. Rahman; R. Jiles; W. Le,"Iowa State University, Ames, Iowa, USA; Iowa State University, Ames, Iowa, USA; Iowa State University, Ames, Iowa, USA; Iowa State University, Ames, Iowa, USA",2023,"Deep learning (DL) models of code have recently reported great progress for vulnerability detection. In some cases, DL-based models have outperformed static analysis tools. Although many great models have been proposed, we do not yet have a good understanding of these models. This limits the further advancement of model robustness, debugging, and deployment for the vulnerability detection. In this paper, we surveyed and reproduced 9 state-of-the-art (SOTA) deep learning models on 2 widely used vulnerability detection datasets: Devign and MSR. We investigated 6 research questions in three areas, namely model capabilities, training data, and model interpretation. We experimentally demonstrated the variability between different runs of a model and the low agreement among different models' outputs. We investigated models trained for specific types of vulnerabilities compared to a model that is trained on all the vulnerabilities at once. We explored the types of programs DL may consider â€œhardâ€ to handle. We investigated the relations of training data sizes and training data composition with model performance. Finally, we studied model interpretations and analyzed important features that the models used to make predictions. We believe that our findings can help better understand model results, provide guidance on preparing training data, and improve the robustness of the models. All of our datasets, code, and results are available at https://doi.org/10.6084/m9.figshare.20791240.",10.1109/ICSE48619.2023.00188,deep learning;vulnerability detection;empirical study,Deep learning;Analytical models;Codes;Training data;Static analysis;Debugging;Predictive models,deep learning (artificial intelligence);learning (artificial intelligence);meta data;program diagnostics,9 state-of-the-art deep learning models;different models;DL-based models;great models;model capabilities;model interpretation;model robustness;training data composition;training data sizes;vulnerability detection,
991,Vulnerability detection,DeepVD: Toward Class-Separation Features for Neural Network Vulnerability Detection,W. Wang; T. N. Nguyen; S. Wang; Y. Li; J. Zhang; A. Yadavally,"Department of Informatics, New Jersey Institute of Technology, New Jersey, USA; Computer Science Department, The University of Texas at Dallas, Texas, USA; Department of Informatics, New Jersey Institute of Technology, New Jersey, USA; Department of Informatics, New Jersey Institute of Technology, New Jersey, USA; Computer Science Department, University of Illinois Urbana-Champaign, Illinois, USA; Computer Science Department, The University of Texas at Dallas, Texas, USA",2023,"The advances of machine learning (ML) including deep learning (DL) have enabled several approaches to implicitly learn vulnerable code patterns to automatically detect software vulnerabilities. A recent study showed that despite successes, the existing ML/DL-based vulnerability detection (VD) models are limited in the ability to distinguish between the two classes of vulnerability and benign code. We propose DeepVD, a graph-based neural network VD model that emphasizes on class-separation features between vulnerability and benign code. DeepVDleverages three types of class-separation features at different levels of abstraction: statement types (similar to Part-of-Speech tagging), Post-Dominator Tree (covering regular flows of execution), and Exception Flow Graph (covering the exception and error-handling flows). We conducted several experiments to evaluate DeepVD in a real-world vulnerability dataset of 303 projects with 13,130 vulnerable methods. Our results show that DeepVD relatively improves over the state-of-the-art ML/DL-based VD approaches 13%â€“29.6% in precision, 15.6%â€“28.9% in recall, and 16.4%â€“25.8% in F-score. Our ablation study confirms that our designed features and components help DeepVDachieve high class-separability for vulnerability and benign code.",10.1109/ICSE48619.2023.00189,neural vulnerability detection;graph neural network;class separation,Deep learning;Codes;Neural networks;Tagging;Feature extraction;Software;Flow graphs,deep learning (artificial intelligence);graph theory;learning (artificial intelligence);security of data,13 methods;130 vulnerable methods;benign code;deep learning;DeepVD;DeepVDachieve high class-separability;designed features;error-handling flows;Exception Flow Graph;graph-based neural network VD model;machine learning;neural network vulnerability detection;real-world vulnerability dataset;regular flows;software vulnerabilities;toward class-separation features;vulnerability detection models;vulnerable code patterns,
992,Vulnerability detection,Enhancing Deep Learning-based Vulnerability Detection by Building Behavior Graph Model,B. Yuan; Y. Lu; Y. Fang; Y. Wu; D. Zou; Z. Li; Z. Li; H. Jin,"School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; Nanyang Technological University, Singapore; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Cyber Science and Engineering, Huazhong University of Science and Technology, China; School of Computer Science and Technology, Huazhong University of Science and Technology, China",2023,"Software vulnerabilities have posed huge threats to the cyberspace security, and there is an increasing demand for automated vulnerability detection (VD). In recent years, deep learning-based (DL-based) vulnerability detection systems have been proposed for the purpose of automatic feature extraction from source code. Although these methods can achieve ideal performance on synthetic datasets, the accuracy drops a lot when detecting real-world vulnerability datasets. Moreover, these approaches limit their scopes within a single function, being not able to leverage the information between functions. In this paper, we attempt to extract the function's abstract behaviors, figure out the relationships between functions, and use this global information to assist DL-based VD to achieve higher performance. To this end, we build a Behavior Graph Model and use it to design a novel framework, namely VulBG. To examine the ability of our constructed Behavior Graph Model, we choose several existing DL-based VD models (e.g., TextCNN, ASTGRU, CodeBERT, Devign, and VulCNN) as our baseline models and conduct evaluations on two real-world datasets: the balanced $\text{FFMpeg}+\text{Qemu}$ dataset and the unbalanced $\text{Chrome} +\text{Debian}$ dataset. Experimental results indicate that VulBG enables all baseline models to detect more real vulnerabilities, thus improving the overall detection performance.",10.1109/ICSE48619.2023.00190,Vulnerability Detection;Behavior Graph;Deep Learning,Source coding;Buildings;Cyberspace;Feature extraction;Software;Behavioral sciences;Security,computer network security;convolutional neural nets;deep learning (artificial intelligence);feature extraction;graph theory;security of data,automated vulnerability detection;automatic feature extraction;Behavior Graph Model;cyberspace security;deep learning-based vulnerability detection;detecting real-world vulnerability datasets;detection performance;existing DL-based VD models;huge threats;ideal performance;single function;software vulnerabilities;source code;synthetic datasets;vulnerability detection systems,
993,Vulnerability detection,Vulnerability Detection with Graph Simplification and Enhanced Graph Representation Learning,X. -C. Wen; Y. Chen; C. Gao; H. Zhang; J. M. Zhang; Q. Liao,"School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China; School of Big Data and Software Engineering, Chongqing University, China; Department of Informatics, King's College, London, UK; School of Computer Science and Technology, Harbin Institute of Technology, Shenzhen, China",2023,"Prior studies have demonstrated the effectiveness of Deep Learning (DL) in automated software vulnerability detection. Graph Neural Networks (GNNs) have proven effective in learning the graph representations of source code and are commonly adopted by existing DL-based vulnerability detection methods. However, the existing methods are still limited by the fact that GNNs are essentially difficult to handle the connections between long-distance nodes in a code structure graph. Besides, they do not well exploit the multiple types of edges in a code structure graph (such as edges representing data flow and control flow). Consequently, despite achieving state-of-the-art performance, the existing GNN-based methods tend to fail to capture global information (i.e., long-range dependencies among nodes) of code graphs. To mitigate these issues, in this paper, we propose a novel vulnerability detection framework with grAph siMplification and enhanced graph rePresentation LEarning, named AMPLE. AMPLE mainly contains two parts: 1) graph simplification, which aims at reducing the distances between nodes by shrinking the node sizes of code structure graphs; 2) enhanced graph representation learning, which involves one edge-aware graph convolutional network module for fusing heterogeneous edge information into node representations and one kernel-scaled representation module for well capturing the relations between distant graph nodes. Experiments on three public benchmark datasets show that AMPLE outperforms the state-of-the-art methods by 0.39%-35.32% and 7.64%-199.81% with respect to the accuracy and F1 score metrics, respectively. The results demonstrate the effectiveness of AMPLE in learning global information of code graphs for vulnerability detection.",10.1109/ICSE48619.2023.00191,Software vulnerability;graph simplification;graph representation learning,Representation learning;Measurement;Learning systems;Deep learning;Codes;Image edge detection;Source coding,convolutional neural nets;deep learning (artificial intelligence);graph neural networks;software reliability;source code (software),AMPLE;automated software vulnerability detection;code graphs;code structure graph;distant graph nodes;DL-based vulnerability detection methods;edge-aware graph convolutional network module;enhanced graph representation learning;existing GNN-based methods;graph neural networks;graph representations;graph simplification;kernel-scaled representation module;learning global informatio;long-distance nodes;node representations;novel vulnerability detection framework;source code,1
994,Vulnerability detection,Does data sampling improve deep learning-based vulnerability detection? Yeas! and Nays!,X. Yang; S. Wang; Y. Li; S. Wang,"University of Manitoba, Canada; University of Manitoba, Canada; New Jersey Institute of Technology, USA; New Jersey Institute of Technology, USA",2023,"Recent progress in Deep Learning (DL) has sparked interest in using DL to detect software vulnerabilities automatically and it has been demonstrated promising results at detecting vulnerabilities. However, one prominent and practical issue for vulnerability detection is data imbalance. Prior study observed that the performance of state-of-the-art (SOTA) DL-based vulnerability detection (DLVD) approaches drops precipitously in real world imbalanced data and a 73% drop of F1-score on average across studied approaches. Such a significant performance drop can disable the practical usage of any DLVD approaches. Data sampling is effective in alleviating data imbalance for machine learning models and has been demonstrated in various software engineering tasks. Therefore, in this study, we conducted a systematical and extensive study to assess the impact of data sampling for data imbalance problem in DLVD from two aspects: i) the effectiveness of DLVD, and ii) the ability of DLVD to reason correctly (making a decision based on real vulnerable statements). We found that in general, oversampling outperforms undersampling, and sampling on raw data outperforms sampling on latent space, typically random oversampling on raw data performs the best among all studied ones (including advanced one SMOTE and OSS). Surprisingly, OSS does not help alleviate the data imbalance issue in DLVD. If the recall is pursued, random undersampling is the best choice. Random oversampling on raw data also improves the ability of DLVD approaches for learning real vulnerable patterns. However, for a significant portion of cases (at least 33% in our datasets), DVLD approach cannot reason their prediction based on real vulnerable statements. We provide actionable suggestions and a roadmap to practitioners and researchers.",10.1109/ICSE48619.2023.00192,Vulnerability detection;deep learning;data sampling;interpretable AI,Deep learning;Systematics;Software;Data models;Task analysis;Software engineering,deep learning (artificial intelligence);sampling methods;software engineering,data imbalance problem;data sampling;deep learning-based vulnerability detection;DL-based vulnerability detection;DLVD approaches;DVLD approach;machine learning models;random oversampling;raw data outperforms;software engineering tasks;software vulnerabilities;vulnerable patterns,
995,Issue reporting and reproduction,Incident-aware Duplicate Ticket Aggregation for Cloud Systems,J. Liu; S. He; Z. Chen; L. Li; Y. Kang; X. Zhang; P. He; H. Zhang; Q. Lin; Z. Xu; S. Rajmohan; D. Zhang; M. R. Lyu,"The Chinese University of Hong Kong, Hong Kong SAR, China; Microsoft Research, Beijing, China; The Chinese University of Hong Kong, Hong Kong SAR, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; Microsoft Research, Beijing, China; The Chinese University of Hong Kong, Shenzhen, China; Chongqing University, Chongqing, China; Microsoft Research, Beijing, China; Microsoft Azure, Redmond, USA; Microsoft 365, Redmond, USA; Microsoft Research, Beijing, China; The Chinese University of Hong Kong, Hong Kong SAR, China",2023,"In cloud systems, incidents are potential threats to customer satisfaction and business revenue. When customers are affected by incidents, they often request customer support service (CSS) from the cloud provider by submitting a support ticket. Many tickets could be duplicate as they are reported in a distributed and uncoordinated manner. Thus, aggregating such duplicate tickets is essential for efficient ticket management. Previous studies mainly rely on tickets' textual similarity to detect duplication; however, duplicate tickets in a cloud system could carry semantically different descriptions due to the complex service dependency of the cloud system. To tackle this problem, we propose iPACK, an incident-aware method for aggregating duplicate tickets by fusing the failure information between the customer side (i.e., tickets) and the cloud side (i.e., incidents). We extensively evaluate iPACK on three datasets collected from the production environment of a large-scale cloud platform, Azure. The experimental results show that iPACK can precisely and comprehensively aggregate duplicate tickets, achieving an F1 score of 0.871~0.935 and outperforming state-of-the-art methods by 12.4%~31.2%.",10.1109/ICSE48619.2023.00193,duplicate tickets;incidents;cloud systems;reliability,Root cause analysis;Aggregates;Semantics;Customer satisfaction;Production;Complexity theory;Software engineering,cloud computing;customer satisfaction,business revenue;cloud provider;cloud system;CSS;customer satisfaction;customer support service;incident-aware duplicate ticket aggregation;iPACK;large-scale cloud platform;production environment;ticket management,
996,Issue reporting and reproduction,Large Language Models are Few-shot Testers: Exploring LLM-based General Bug Reproduction,S. Kang; J. Yoon; S. Yoo,"School of Computing, KAIST, Daejeon, Republic of Korea; School of Computing, KAIST, Daejeon, Republic of Korea; School of Computing, KAIST, Daejeon, Republic of Korea",2023,"Many automated test generation techniques have been developed to aid developers with writing tests. To facilitate full automation, most existing techniques aim to either increase coverage, or generate exploratory inputs. However, existing test generation techniques largely fall short of achieving more semantic objectives, such as generating tests to reproduce a given bug report. Reproducing bugs is nonetheless important, as our empirical study shows that the number of tests added in open source repositories due to issues was about 28% of the corresponding project test suite size. Meanwhile, due to the difficulties of transforming the expected program semantics in bug reports into test oracles, existing failure reproduction techniques tend to deal exclusively with program crashes, a small subset of all bug reports. To automate test generation from general bug reports, we propose Libro, a framework that uses Large Language Models (LLMs), which have been shown to be capable of performing code-related tasks. Since LLMs themselves cannot execute the target buggy code, we focus on post-processing steps that help us discern when LLMs are effective, and rank the produced tests according to their validity. Our evaluation of Libro shows that, on the widely studied Defects4J benchmark, Libro can generate failure reproducing test cases for 33% of all studied cases (251 out of 750), while suggesting a bug reproducing test in first place for 149 bugs. To mitigate data contamination (i.e., the possibility of the LLM simply remembering the test code either partially or in whole), we also evaluate Libro against 31 bug reports submitted after the collection of the LLM training data terminated: Libro produces bug reproducing tests for 32% of the studied bug reports. Overall, our results show Libro has the potential to significantly enhance developer efficiency by automatically generating tests from bug reports.",10.1109/ICSE48619.2023.00194,test generation;natural language processing;software engineering,Codes;Computer bugs;Semantics;Training data;Benchmark testing;Writing;Test pattern generators,program debugging;program testing;software fault tolerance,automated test generation;bug report;bug reproducing test;Defects4J benchmarkdata contamination;failure reproduction techniques;Large Language Models;Libro;LLM-based general bug reproduction;program semantics;test code;test oracles,
997,Issue reporting and reproduction,On the Reproducibility of Software Defect Datasets,H. -N. Zhu; C. Rubio-GonzÃ¡lez,"University of California, Davis, United States of America; University of California, Davis, United States of America",2023,"Software defect datasets are crucial to facilitating the evaluation and comparison of techniques in fields such as fault localization, test generation, and automated program repair. However, the reproducibility of software defect artifacts is not immune to breakage. In this paper, we conduct a study on the reproducibility of software defect artifacts. First, we study five state-of-the-art Java defect datasets. Despite the multiple strategies applied by dataset maintainers to ensure reproducibility, all datasets are prone to breakages. Second, we conduct a case study in which we systematically test the reproducibility of 1,795 software artifacts during a 13-month period. We find that 62.6% of the artifacts break at least once, and 15.3% artifacts break multiple times. We manually investigate the root causes of breakages and handcraft 10 patches, which are automatically applied to 1,055 distinct artifacts in 2,948 fixes. Based on the nature of the root causes, we propose automated dependency caching and artifact isolation to prevent further breakage. In particular, we show that isolating artifacts to eliminate external dependencies increases reproducibility to 95% or higher, which is on par with the level of reproducibility exhibited by the most reliable manually curated dataset.",10.1109/ICSE48619.2023.00195,software reproducibility;software defects;software maintenance;software quality,Location awareness;Java;Maintenance engineering;Reproducibility of results;Software;Software reliability;Test pattern generators,Java;program debugging;program diagnostics;program testing;software maintenance;software quality,055 distinct artifacts;1 artifacts;1 software artifacts;795 software artifacts;artifact isolation;automated program repair;breakage;dataset maintainers;dependency caching;external dependencies increases reproducibility;reliable manually curated dataset;software defect artifacts;software defect datasets;state-of-the-art Java defect datasets,
998,Issue reporting and reproduction,Context-aware Bug Reproduction for Mobile Apps,Y. Huang; J. Wang; Z. Liu; S. Wang; C. Chen; M. Li; Q. Wang,"Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China; Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China; Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China; Electrical Engineering and Computer Science, York University, Canada; Monash University, Melbourne, Australia; Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China; Science and Technology on Integrated Information System Laboratory, Institute of Software Chinese Academy of Sciences, Beijing, China",2023,"Bug reports are vital for software maintenance that allow the developers being informed of the problems encountered in the software. Before bug fixing, developers need to reproduce the bugs which is an extremely time-consuming and tedious task, and it is highly expected to automate this process. However, it is challenging to do so considering the imprecise or incomplete natural language described in reproducing steps, and the missing or ambiguous single source of information in GUI components. In this paper, we propose a context-aware bug reproduction approach ScopeDroid which automatically reproduces crashes from textual bug reports for mobile apps. It first constructs a state transition graph (STG) and extracts the contextual information of components. We then design a multi-modal neural matching network to derive the fuzzy matching matrix between all candidate GUI events and reproducing steps. With the STG and matching information, it plans the exploration path for reproducing the bug, and enriches the initial STG iteratively. We evaluate the approach on 102 bug reports from 69 popular Android apps, and it successfully reproduces 63.7% of the crashes, outper-forming the state-of-the-art baselines by 32.6% and 38.3%. We also evaluate the usefulness and robustness of ScopeDroid with promising results. Furthermore, to train the neural matching network, we develop a heuristic-based automated training data generation method, which can potentially motivate and facilitate other activities as user interface operations.",10.1109/ICSE48619.2023.00196,,Software maintenance;Computer bugs;Neural networks;Training data;Robustness;Mobile applications;Data mining,Android (operating system);graphical user interfaces;mobile computing;program debugging;program testing;software maintenance;user interfaces,102 bug reports;69 popular Android apps;ambiguous single source;bug fixing;context-aware bug reproduction approach ScopeDroid;contextual information;crashes;fuzzy matching matrix;GUI components;imprecise;initial STG;matching information;missing source;mobile apps;multimodal neural matching network;reproducing steps;software maintenance;state transition graph;textual bug reports,
999,Issue reporting and reproduction,"Read It, Don't Watch It: Captioning Bug Recordings Automatically",S. Feng; M. Xie; Y. Xue; C. Chen,Monash University; Australian National University; University of Science and Technology of China; Monash University,2023,"Screen recordings of mobile applications are easy to capture and include a wealth of information, making them a popular mechanism for users to inform developers of the problems encountered in the bug reports. However, watching the bug recordings and efficiently understanding the semantics of user actions can be time-consuming and tedious for developers. Inspired by the conception of the video subtitle in movie industry, we present a lightweight approach CAPdroid to caption bug recordings automatically. CAPdroid is a purely image-based and non-intrusive approach by using image processing and convolutional deep learning models to segment bug recordings, infer user action attributes, and generate subtitle descriptions. The automated experiments demonstrate the good performance of CAPdroid in inferring user actions from the recordings, and a user study confirms the usefulness of our generated step descriptions in assisting developers with bug replay.",10.1109/ICSE48619.2023.00197,bug recording;video captioning;android app,Industries;Image segmentation;Computer bugs;Semantics;Natural languages;Motion pictures;Recording,convolutional neural nets;deep learning (artificial intelligence);feature extraction;image processing;program debugging;video signal processing,bug replay;bug reports;captioning bug recordings;convolutional deep learning models;infer user action;lightweight approach CAPdroid;mobile applications;nonintrusive approach;popular mechanism;purely image-based;screen recordings;segment bug recordings;user actions,
1000,Software quality,Duetcs: Code Style Transfer through Generation and Retrieval,B. Chen; Z. Abedjan,"TU Berlin, Berlin, Germany; Leibniz UniversitÃ¤t Hannover & L3S Research Center, Hannover, Germany",2023,"Coding style has direct impact on code comprehension. Automatically transferring code style to user's preference or consistency can facilitate project cooperation and maintenance, as well as maximize the value of open-source code. Existing work on automating code stylization is either limited to code formatting or requires human supervision in pre-defining style checking and transformation rules. In this paper, we present unsupervised methods to assist automatic code style transfer for arbitrary code styles. The main idea is to leverage Big Code database to learn style and content embedding separately to generate or retrieve a piece of code with the same functionality and the desired target style. We carefully encode style and content features, so that a style embedding can be learned from arbitrary code. We explored the capabilities of novel attention-based style generation models and meta-learning and implemented our ideas in DUETCS. We complement the learning-based approach with a retrieval mode, which uses the same embeddings to directly search for the desired piece of code in Big Code. Our experiments show that DUETCS captures more style aspects than existing baselines.",10.1109/ICSE48619.2023.00198,,Symbiosis;Metalearning;Codes;Databases;Instruments;Maintenance engineering;Encoding,learning (artificial intelligence);source code (software);unsupervised learning,arbitrary code styles;attention-based style generation models;automatic code style;automating code stylization;code comprehension;code formatting;Code style transfer;coding style;content features;desired target style;DUETCS captures more style aspects;leverage Big Code database;open-source code;pre-defining style checking;style embedding;transformation rules,
1001,SE education methods and tools,On the Applicability of Language Models to Block-Based Programs,E. Griebl; B. Fein; F. ObermÃ¼ller; G. Fraser; R. Just,"University of Passau, Passau, Germany; University of Passau, Passau, Germany; University of Passau, Passau, Germany; University of Passau, Passau, Germany; University of Washington, Seattle, USA",2023,"Block-based programming languages like Scratch are increasingly popular for programming education and end-user programming. Recent program analyses build on the insight that source code can be modelled using techniques from natural language processing. Many of the regularities of source code that support this approach are due to the syntactic overhead imposed by textual programming languages. This syntactic overhead, however, is precisely what block-based languages remove in order to simplify programming. Consequently, it is unclear how well this modelling approach performs on block-based programming languages. In this paper, we investigate the applicability of language models for the popular block-based programming language Scratch. We model Scratch programs using n-gram models, the most essential type of language model, and transformers, a popular deep learning model. Evaluation on the example tasks of code completion and bug finding confirm that blocks inhibit predictability, but the use of language models is nevertheless feasible. Our findings serve as foundation for improving tooling and analyses for block-based languages.",10.1109/ICSE48619.2023.00199,Block-Based Programs;Scratch;Natural Language Model;Code Completion;Bugram,Computer languages;Analytical models;Codes;Source coding;Computational modeling;Predictive models;Transformers,computer science education;deep learning (artificial intelligence);natural language processing;program debugging;program diagnostics;programming;programming languages,block-based languages;block-based programming languages;block-based programs;end-user programming;language model;language Scratch;model Scratch programs;modelled using techniques;modelling approach performs;n-gram models;natural language processing;popular block-based programming;popular deep learning model;programming education;recent program analyses;source code;syntactic overhead;textual programming languages,
1002,Metamorphic testing,MTTM: Metamorphic Testing for Textual Content Moderation Software,W. Wang; J. -t. Huang; W. Wu; J. Zhang; Y. Huang; S. Li; P. He; M. R. Lyu,"Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; School of Software Engineering, Sun Yat-sen University, Zhuhai, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China; School of Data Science, The Chinese University of Hong Kong, Shenzhen, Shenzhen, China; Department of Computer Science and Engineering, The Chinese University of Hong Kong, Hong Kong, China",2023,"The exponential growth of social media platforms such as Twitter and Facebook has revolutionized textual communication and textual content publication in human society. However, they have been increasingly exploited to propagate toxic content, such as hate speech, malicious advertisement, and pornography, which can lead to highly negative impacts (e.g., harmful effects on teen mental health). Researchers and practitioners have been enthusiastically developing and extensively deploying textual content moderation software to address this problem. However, we find that malicious users can evade moderation by changing only a few words in the toxic content. Moreover, modern content moderation software's performance against malicious inputs remains underexplored. To this end, we propose MTTM, a Metamorphic Testing framework for Textual content Moderation software. Specifically, we conduct a pilot study on 2, 000 text messages collected from real users and summarize eleven metamorphic relations across three perturbation levels: character, word, and sentence. MTTM employs these metamorphic relations on toxic textual contents to generate test cases, which are still toxic yet likely to evade moderation. In our evaluation, we employ MTTM to test three commercial textual content moderation software and two state-of-the-art moderation algorithms against three kinds of toxic content. The results show that MTTM achieves up to 83.9%, 51%, and 82.5% error finding rates (EFR) when testing commercial moderation software provided by Google, Baidu, and Huawei, respectively, and it obtains up to 91.2% EFR when testing the state-of-the-art algorithms from the academy. In addition, we leverage the test cases generated by MTTM to retrain the model we explored, which largely improves model robustness 0% ~ 5.9% EFR) while maintaining the accuracy on the original test set. A demo can be found in this link1.",10.1109/ICSE48619.2023.00200,Software testing;metamorphic relations;NLP software;textual content moderation,Systematics;Social networking (online);Perturbation methods;Software algorithms;Web and internet services;Software performance;Software,program testing;social networking (online),commercial moderation software;error finding rates;metamorphic relations;metamorphic testing framework;modern content moderation software;MTTM;social media platform;textual content moderation software;textual content publication;toxic textual contents,
1003,Metamorphic testing,Metamorphic Shader Fusion for Testing Graphics Shader Compilers,D. Xiao; Z. Liu; S. Wang,"The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China; The Hong Kong University of Science and Technology, Hong Kong, China",2023,"Computer graphics are powered by graphics APIs (e.g., OpenGL, Direct3D) and their associated shader compilers, which render high-quality images by compiling and optimizing user-written high-level shader programs into GPU machine code. Graphics rendering is extensively used in production scenarios like virtual reality (VR), gaming, autonomous driving, and robotics. Despite the development by industrial manufacturers such as Intel, Nvidia, and AMD, shader compilers - like traditional software - may produce ill-rendered outputs. In turn, these errors may result in negative results, from poor user experience in entertainment to accidents in driving assistance systems. This paper introduces FSHADER, a metamorphic testing (MT) framework designed specifically for shader compilers to uncover erroneous compilations and optimizations. FSHADER tests shader compilers by mutating input shader programs via four carefully-designed metamorphic relations (MRs). In particular, FSHADER fuses two shader programs via an MR and checks the visual consistency between the image rendered from the fused shader program with the output of fusing individually rendered images. Our study of 12 shader compilers covers five mainstream GPU vendors, including Intel, AMD, Nvidia, ARM, and Apple. We successfully uncover over 16K error-triggering inputs that generate incorrect rendering outputs. We manually locate and characterize buggy optimization places, and developers have confirmed representative bugs.",10.1109/ICSE48619.2023.00201,Compiler testing;Metamorphic testing;Shader compilers;Graphics compilers;GPU driver;Virtual reality,Visualization;Computer bugs;Graphics processing units;Virtual reality;Rendering (computer graphics);User experience;Software,application program interfaces;program compilers;program debugging;program testing;rendering (computer graphics),associated shader compilers;autonomous driving;computer graphics;erroneous compilations;FSHADER;fused shader program;GPU machine code;graphics API;graphics rendering;graphics shader compilers;high-quality images;input shader programs;metamorphic shader fusion;metamorphic testing framework;optimizations;rendered images;representative bugs;user-written high-level shader programs,
1004,Metamorphic testing,MorphQ: Metamorphic Testing of the Qiskit Quantum Computing Platform,M. Paltenghi; M. Pradel,"Department of Computer Science, University of Stuttgart, Stuttgart, Germany; Department of Computer Science, University of Stuttgart, Stuttgart, Germany",2023,"As quantum computing is becoming increasingly popular, the underlying quantum computing platforms are growing both in ability and complexity. Unfortunately, testing these platforms is challenging due to the relatively small number of existing quantum programs and because of the oracle problem, i.e., a lack of specifications of the expected behavior of programs. This paper presents MorphQ, the first metamorphic testing approach for quantum computing platforms. Our two key contributions are (i) a program generator that creates a large and diverse set of valid (i.e., non-crashing) quantum programs, and (ii) a set of program transformations that exploit quantum-specific metamorphic relationships to alleviate the oracle problem. Evaluating the approach by testing the popular Qiskit platform shows that the approach creates over 8k program pairs within two days, many of which expose crashes. Inspecting the crashes, we find 13 bugs, nine of which have already been confirmed. MorphQ widens the slim portfolio of testing techniques of quantum computing platforms, helping to create a reliable software stack for this increasingly important field.",10.1109/ICSE48619.2023.00202,quantum computing;metamorphic testing;software engineering;compiler testing;quantum computing platforms;quantum bugs;Qiskit;MorphQ;quantum software reliability;quality assurance;quantum program generator;differential testing;fuzz testing,Quantum computing;Automatic programming;Computer bugs;Full stack;Fuzzing;Reliability engineering;Software,program debugging;program testing;quantum computing,8k program pairs;metamorphic testing approach;MorphQ;oracle problem;popular Qiskit platform;program generator;program transformations;Qiskit quantum computing platform;quantum programs;quantum-specific metamorphic relationships;underlying quantum computing platforms,1
1005,Pre-trained and few shot learning for SE,Automating Code-Related Tasks Through Transformers: The Impact of Pre-training,R. Tufano; L. Pascarella; G. Bavota,"Software Institute - USI Universita della Svizzera italiana, Switzerland; Software Institute - USI Universita della Svizzera italiana, Switzerland; Software Institute - USI Universita della Svizzera italiana, Switzerland",2023,"Transformers have gained popularity in the software engineering (SE) literature. These deep learning models are usually pre-trained through a self-supervised objective, meant to provide the model with basic knowledge about a language of interest (e.g., Java). A classic pre-training objective is the masked language model (MLM), in which a percentage of tokens from the input (e.g., a Java method) is masked, with the model in charge of predicting them. Once pre-trained, the model is then fine-tuned to support the specific downstream task of interest (e.g., code summarization). While there is evidence suggesting the boost in performance provided by pre-training, little is known about the impact of the specific pre-training objective(s) used. Indeed, MLM is just one of the possible pre-training objectives and recent work from the natural language processing field suggest that pre-training objectives tailored for the specific downstream task of interest may substantially boost the model's performance. For example, in the case of code summarization, a tailored pre-training objective could be the identification of an appropriate name for a given method, considering the method name to generate as an extreme summary. In this study, we focus on the impact of pre-training objectives on the performance of trans-formers when automating code-related tasks. We start with a systematic literature review aimed at identifying the pre-training objectives used in SE. Then, we pre-train 32 transformers using both (i) generic pre-training objectives usually adopted in SE; and (ii) pre-training objectives tailored to specific code-related tasks subject of our experimentation, namely bug-fixing, code summarization, and code completion. We also compare the pre-trained models with non pre-trained ones and show the advantage brought by pre-training in different scenarios, in which more or less fine-tuning data are available. Our results show that: (i) pre-training helps in boosting performance only if the amount of fine-tuning data available is small; (ii) the MLM objective is usually sufficient to maximize the prediction performance of the model, even when comparing it with pre-training objectives specialized for the downstream task at hand.",10.1109/ICSE48619.2023.00203,Pre-training;Code Recommenders,Java;Codes;Systematics;Training data;Predictive models;Transformers;Data models,deep learning (artificial intelligence);Java;learning (artificial intelligence);natural language processing;software engineering;supervised learning,classic pre-training objective;code summarization;nonpre-trained ones;possible pre-training objectives;pre-train 32 transformers;specific pre-training objective;tailored pre-training objective,
1006,Pre-trained and few shot learning for SE,Log Parsing with Prompt-based Few-shot Learning,V. -H. Le; H. Zhang,"School of Information and Physical Sciences, The University of Newcastle, Australia; School of Big Data and Software Engineering, Chongqing University, China",2023,"Logs generated by large-scale software systems provide crucial information for engineers to understand the system status and diagnose problems of the systems. Log parsing, which converts raw log messages into structured data, is the first step to enabling automated log analytics. Existing log parsers extract the common part as log templates using statistical features. However, these log parsers often fail to identify the correct templates and parameters because: 1) they often overlook the semantic meaning of log messages, and 2) they require domain-specific knowledge for different log datasets. To address the limitations of existing methods, in this paper, we propose LogPPT to capture the patterns of templates using prompt-based few-shot learning. LogPPT utilises a novel prompt tuning method to recognise keywords and parameters based on a few labelled log data. In addition, an adaptive random sampling algorithm is designed to select a small yet diverse training set. We have conducted extensive experiments on 16 public log datasets. The experimental results show that LogPPT is effective and efficient for log parsing.",10.1109/ICSE48619.2023.00204,log parsing;few-shot learning;prompt-tuning;deep learning,Training;Scalability;Software algorithms;Semantics;Production;Feature extraction;Software systems,data mining;grammars;learning (artificial intelligence);system monitoring,16 public log datasets;automated log analytics;correct templates;diagnose problems;different log datasets;labelled log data;large-scale software systems;log parsing;log templates;prompt-based few-shot learning;raw log messages,2
1007,Pre-trained and few shot learning for SE,Retrieval-Based Prompt Selection for Code-Related Few-Shot Learning,N. Nashid; M. Sintaha; A. Mesbah,"University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada; University of British Columbia, Vancouver, Canada",2023,"Large language models trained on massive code corpora can generalize to new tasks without the need for task-specific fine-tuning. In few-shot learning, these models take as input a prompt, composed of natural language instructions, a few instances of task demonstration, and a query and generate an output. However, the creation of an effective prompt for code-related tasks in few-shot learning has received little attention. We present a technique for prompt creation that automatically retrieves code demonstrations similar to the developer task, based on embedding or frequency analysis. We apply our approach, Cedar, to two different programming languages, statically and dynamically typed, and two different tasks, namely, test assertion generation and program repair. For each task, we compare Cedar with state-of-the-art task-specific and fine-tuned models. The empirical results show that, with only a few relevant code demonstrations, our prompt creation technique is effective in both tasks with an accuracy of 76% and 52% for exact matches in test assertion generation and program repair tasks, respectively. For assertion generation, Cedar outperforms existing task-specific and fine-tuned models by 333% and 11%, respectively. For program repair, Cedar yields 189% better accuracy than task-specific models and is competitive with recent fine-tuned models. These findings have practical implications for practitioners, as Cedar could potentially be applied to multilingual and multitask settings without task or language-specific training with minimal examples and effort.",10.1109/ICSE48619.2023.00205,Large Language Models;Transformers;Few-shot learning;Program repair;Test assertion generation,Training;Computer languages;Codes;Source coding;Natural languages;Maintenance engineering;Chatbots,learning (artificial intelligence);natural language processing;software maintenance,Cedar;code demonstrations;code-related few-shot learning;code-related tasks;developer task;fine-tuned models;language models;language-specific training;massive code corpora;natural language instructions;program repair tasks;prompt creation technique;relevant code demonstrations;retrieval-based prompt selection;task demonstration;task-specific fine-tuning;task-specific models;test assertion generation,
1008,Pre-trained and few shot learning for SE,An Empirical Study of Pre-Trained Model Reuse in the Hugging Face Deep Learning Model Registry,W. Jiang; N. Synovic; M. Hyatt; T. R. Schorlemmer; R. Sethi; Y. -H. Lu; G. K. Thiruvathukal; J. C. Davis,Purdue University; Loyola University Chicago; Loyola University Chicago; Purdue University; Loyola University Chicago; Purdue University; Loyola University Chicago; Purdue University,2023,"Deep Neural Networks (DNNs) are being adopted as components in software systems. Creating and specializing DNNs from scratch has grown increasingly difficult as state-of-the-art architectures grow more complex. Following the path of traditional software engineering, machine learning engineers have begun to reuse large-scale pre-trained models (PTMs) and fine-tune these models for downstream tasks. Prior works have studied reuse practices for traditional software packages to guide software engineers towards better package maintenance and dependency management. We lack a similar foundation of knowledge to guide behaviors in pre-trained model ecosystems. In this work, we present the first empirical investigation of PTM reuse. We interviewed 12 practitioners from the most popular PTM ecosystem, Hugging Face, to learn the practices and challenges of PTM reuse. From this data, we model the decision-making process for PTM reuse. Based on the identified practices, we describe useful attributes for model reuse, including provenance, reproducibility, and portability. Three challenges for PTM reuse are missing attributes, discrepancies between claimed and actual performance, and model risks. We substantiate these identified challenges with systematic measurements in the Hugging Face ecosystem. Our work informs future directions on optimizing deep learning ecosystems by automated measuring useful attributes and potential attacks, and envision future research on infrastructure and standardization for model registries.",10.1109/ICSE48619.2023.00206,Software reuse;Empirical software engineering;Machine learning;Deep learning;Software supply chain;Engineering decision making;Cybersecurity;Trust,Deep learning;Systematics;Biological system modeling;Ecosystems;Decision making;Supply chains;Standardization,decision making;deep learning (artificial intelligence);learning (artificial intelligence);software packages,deep learning ecosystems;deep neural networks;dependency management;Hugging Face deep learning model registry;Hugging face ecosystem;machine learning engineers;model registries;package maintenance;pre-trained model reuse;pretrained model ecosystems;PTM ecosystem;PTM reuse;reuse practices;software engineers;software packages;software systems;traditional software engineering,2
1009,Pre-trained and few shot learning for SE,ContraBERT: Enhancing Code Pre-trained Models via Contrastive Learning,S. Liu; B. Wu; X. Xie; G. Meng; Y. Liu,"Nanyang Technological University, Singapore; Nanyang Technological University, Singapore; Singapore Management University, Singapore; SKLOIS, Institute of Information Engineering, Chinese Academy of Sciences, China; Nanyang Technological University, Singapore",2023,"Large-scale pre-trained models such as CodeBERT, GraphCodeBERT have earned widespread attention from both academia and industry. Attributed to the superior ability in code representation, they have been further applied in multiple downstream tasks such as clone detection, code search and code translation. However, it is also observed that these state-of-the-art pre-trained models are susceptible to adversarial attacks. The performance of these pre-trained models drops significantly with simple perturbations such as renaming variable names. This weakness may be inherited by their downstream models and thereby amplified at an unprecedented scale. To this end, we propose an approach namely ContraBERT that aims to improve the robustness of pre-trained models via contrastive learning. Specifically, we design nine kinds of simple and complex data augmentation operators on the programming language (PL) and natural language (NL) data to construct different variants. Furthermore, we continue to train the existing pre-trained models by masked language modeling (MLM) and contrastive pre-training task on the original samples with their augmented variants to enhance the robustness of the model. The extensive ex-periments demonstrate that ContraBERT can effectively improve the robustness of the existing pre-trained models. Further study also confirms that these robustness-enhanced models provide improvements as compared to original models over four popular downstream tasks.",10.1109/ICSE48619.2023.00207,Code Pre-trained Models;Contrastive Learning;Model Robustness,Industries;Computer languages;Codes;Perturbation methods;Natural languages;Cloning;Data augmentation,data augmentation;learning (artificial intelligence);natural language processing;supervised learning,code representation;code search;code translation;complex data augmentation operators;ContraBERT;contrastive learning;downstream models;enhancing code pre-trained models;existing pre-trained models;large-scale pre-trained models;masked language modeling;multiple downstream tasks;natural language;pre-training task;robustness-enhanced models;simple data augmentation operators;state-of-the-art pre-trained models,
1010,Program analysis,DStream: A Streaming-Based Highly Parallel IFDS Framework,X. Wang; Z. Zuo; L. Bu; J. Zhao,"State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China; State Key Laboratory for Novel Software Technology, Nanjing University, China",2023,"The IFDS framework supports interprocedural dataflow analysis with distributive flow functions over finite domains. A large class of interprocedural dataflow analysis problems can be formulated as IFDS problems and thus can be solved with the IFDS framework precisely. Unfortunately, scaling IFDS analysis to large-scale programs is challenging in terms of both massive memory consumption and low analysis efficiency. This paper presents DStream, a scalable system dedicated to precise and highly parallel IFDS analysis for large-scale programs. DStream leverages a streaming-based out-of-core computation model to reduce memory footprint significantly and adopts fine-grained data parallelism to achieve efficiency. We implemented a taint analysis as a DStream instance analysis and compared DStream with three state-of-the-art tools. Our exper-iments validate that DStream outperforms all other tools with average speedups from 4.37x to 14.46x on a commodity PC with limited available memory. Meanwhile, the experiments confirm that DStream successfully scales to large-scale programs which the state-of-the-art tools (e.g., FlowDroid and/or DiskDroid) fail to analyze.",10.1109/ICSE48619.2023.00208,interprocedural static analysis;IFDS analysis;streaming;data-parallel computation,Costs;Scalability;Computational modeling;Memory management;Software algorithms;Parallel processing;Data models,data flow analysis;parallel processing;program diagnostics,distributive flow functions;DStream instance analysis;DStream leverages;fine-grained data parallelism;finite domains;highly parallel IFDS analysis;interprocedural dataflow analysis problems;large-scale programs;memory footprint;streaming-based highly parallel IFDS framework;streaming-based out-of-core computation model;taint analysis,
1011,Program analysis,(Partial) Program Dependence Learning,A. Yadavally; T. N. Nguyen; W. Wang; S. Wang,"Computer Science Department, The University of Texas at Dallas, Texas, USA; Computer Science Department, The University of Texas at Dallas, Texas, USA; Department of Informatics, New Jersey Institute of Technology, New Jersey, USA; Department of Informatics, New Jersey Institute of Technology, New Jersey, USA",2023,"Code fragments from developer forums often migrate to applications due to the code reuse practice. Owing to the incomplete nature of such programs, analyzing them to early determine the presence of potential vulnerabilities is challenging. In this work, we introduce NeuralPDA, a neural network-based program dependence analysis tool for both complete and partial programs. Our tool efficiently incorporates intra-statement and inter-statement contextual features into statement representations, thereby modeling program dependence analysis as a statement-pair dependence decoding task. In the empirical evaluation, we report that NeuralPDA predicts the CFG and PDG edges in complete Java and C/C++ code with combined F-scores of 94.29% and 92.46%, respectively. The F-score values for partial Java and C/C++ code range from 94.29%-97.17% and 92.46%-96.01%, respectively. We also test the usefulness of the PDGs predicted by NeuralPDA (i.e., PDG*) on the downstream task of method-level vulnerability detection. We discover that the performance of the vulnerability detection tool utilizing PDG* is only 1.1% less than that utilizing the PDGs generated by a program analysis tool. We also report the detection of 14 real-world vulnerable code snippets from StackOverflow by a machine learning-based vulnerability detection tool that employs the PDGs predicted by NeuralPDA for these code snippets.",10.1109/ICSE48619.2023.00209,neural partial program analysis;neural program dependence analysis;neural networks;deep learning,Java;Analytical models;Codes;Image edge detection;Neural networks;Decoding;Task analysis,Java;learning (artificial intelligence);program diagnostics;security of data,code fragments;code reuse practice;combined F-scores;complete programs;developer forums;F-score values;incomplete nature;inter-statement contextual features;intra-statement;machine learning-based vulnerability detection tool;method-level vulnerability detection;neural network-based program dependence analysis tool;NeuralPDA;partial programs;PDGs;potential vulnerabilities;program analysis tool;real-world vulnerable code snippets;statement representations;statement-pair dependence,
1012,Program analysis,MirrorTaint: Practical Non-intrusive Dynamic Taint Tracking for JVM-based Microservice Systems,Y. Ouyang; K. Shao; K. Chen; R. Shen; C. Chen; M. Xu; Y. Zhang; L. Zhang,"University of Illinois, Urbana-Champaign, USA; Ant Group, Shanghai, China; Southern University of Science and Technology, Shenzhen, China; Peking University, Beijing, China; Ant Group, Shanghai, China; Ant Group, Shanghai, China; Southern University of Science and Technology, Shenzhen, China; University of Illinois, Urbana-Champaign, USA",2023,"Taint analysis, i.e., labeling data and propagating the labels through data flows, has been widely used for analyzing program information flows and ensuring system/data security. Due to its important applications, various taint analysis techniques have been proposed, including static and dynamic taint analysis. However, existing taint analysis techniques can be hardly applied to the rising microservice systems for industrial applications. To address such a problem, in this paper, we proposed the first practical non-intrusive dynamic taint analysis technique MirrorTaint for extensively supporting microservice systems on JVMs. In particular, by instrumenting the microservice systems, MirrorTaint constructs a set of data structures with their respective policies for labeling/propagating taints in its mirrored space. Such data structures are essentially non-intrusive, i.e., modifying no program meta-data or runtime system. Then, during program execution, MirrorTaint replicates the stack-based JVM instruction execution in its mirrored space on-the-fly for dynamic taint tracking. We have evaluated MirrorTaint against state-of-the-art dynamic and static taint analysis systems on various popular open-source microservice systems. The results demonstrate that MirrorTaint can achieve better compatibility, quite close precision and higher recall (97.9%/100.0%) than state-of-the-art Phosphor (100.0%/9.9%) and FlowDroid (100%/28.2%). Also, MirrorTaint incurs lower runtime overhead than Phosphor (although both are dynamic techniques). Moreover, we have performed a case study in Ant Group, a global billion-user FinTech company, to compare MirrorTaint and their mature developer-experience-based data checking system for automatically generated fund documents. The result shows that the developer experience can be incomplete, causing the data checking system to only cover 84.0% total data relations, while MirrorTaint can automatically find 99.0% relations with 100.0% precision. Lastly, we also applied MirrorTaint to successfully detect a recently wide-spread Log4j2 security vulnerability.",10.1109/ICSE48619.2023.00210,dynamic taint analysis;microservice;JVM,Runtime;Instruments;Phosphors;Microservice architectures;Companies;Aerospace electronics;Benchmark testing,computer network security;data privacy;data structures;Internet of Things;Java;mobile computing;program diagnostics;security of data;virtual machines,84.0% total data relations;data flows;data structures;dynamic taint analysis;dynamic techniques;existing taint analysis techniques;JVM-based microservice systems;mature developer-experience-based data;mirrored space;MirrorTaint constructs;MirrorTaint incurs lower runtime overhead;nonintrusive dynamic taint tracking;popular open-source microservice systems;practical nonintrusive;program meta-data;rising microservice systems;runtime system;state-of-the-art dynamic;static taint analysis systems;taints,
1013,Vulnerability testing and patching,VULGEN: Realistic Vulnerability Generation Via Pattern Mining and Deep Learning,Y. Nong; Y. Ou; M. Pradel; F. Chen; H. Cai,"Washington State University, Pullman, WA, USA; The University of Texas at Dallas, Richardson, TX, USA; University of Stuttgart, Stuttgart, Germany; The University of Texas at Dallas, Richardson, TX, USA; Washington State University, Pullman, WA, USA",2023,"Building new, powerful data-driven defenses against prevalent software vulnerabilities needs sizable, quality vulnerability datasets, so does large-scale benchmarking of existing defense solutions. Automatic data generation would promisingly meet the need, yet there is little work aimed to generate much-needed quality vulnerable samples. Meanwhile, existing similar and adaptable techniques suffer critical limitations for that purpose. In this paper, we present VULGEN, the first injection-based vulnerability-generation technique that is not limited to a particular class of vulnerabilities. VULGEN combines the strengths of deterministic (pattern-based) and probabilistic (deep-learning/DL-based) program transformation approaches while mutually overcoming respective weaknesses. This is achieved through close collaborations between pattern mining/application and DL-based injection localization, which separates the concerns with how and where to inject. By leveraging large, pretrained programming language modeling and only learning locations, VULGEN mitigates its own needs for quality vulnerability data (for training the localization model). Extensive evaluations show that VULGEN significantly outperforms a state-of-the-art (SOTA) pattern-based peer technique as well as both Transformer- and GNN-based approaches in terms of the percentages of generated samples that are vulnerable and those also exactly matching the ground truth (by 38.0-430.1% and 16.3-158.2%, respectively). The VULGEN-generated samples led to substantial performance improvements for two SOTA DL-based vulnerability detectors (by up to 31.8% higher in F1), close to those brought by the ground-truth real-world samples and much higher than those by the same numbers of existing synthetic samples.",10.1109/ICSE48619.2023.00211,Software vulnerability;data generation;bug injection;pattern mining;deep learning;vulnerability detection,Training;Location awareness;Detectors;Benchmark testing;Probabilistic logic;Transformers;Software,data mining;deep learning (artificial intelligence);learning (artificial intelligence);security of data,adaptable techniques;automatic data generation;deep learning;defense solutions;GNN-based approaches;ground-truth real-world samples;injection-based vulnerability-generation technique;large-scale benchmarking;powerful data-driven defenses;pretrained programming language modeling;prevalent software vulnerabilities;quality vulnerability data;quality vulnerability datasets;quality vulnerable samples;realistic vulnerability generation via pattern mining;similar techniques;SOTA DL-based vulnerability detectors;state-of-the-art pattern-based peer technique;transformation approaches;VULGEN-generated samples,
1014,Vulnerability testing and patching,Compatible Remediation on Vulnerabilities from Third-Party Libraries for Java Projects,L. Zhang; C. Liu; Z. Xu; S. Chen; L. Fan; L. Zhao; J. Wu; Y. Liu,"School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; College of Intelligence and Computing, Tianjin University, China; College of Cyber Science, Nankai University, China; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore; School of Computer Science and Engineering, Nanyang Technological University, Singapore",2023,"With the increasing disclosure of vulnerabilities in open-source software, software composition analysis (SCA) has been widely applied to reveal third-party libraries and the associated vulnerabilities in software projects. Beyond the revelation, SCA tools adopt various remediation strategies to fix vulnerabilities, the quality of which varies substantially. However, ineffective remediation could induce side effects, such as compi-lation failures, which impede acceptance by users. According to our studies, existing SCA tools could not correctly handle the concerns of users regarding the compatibility of remediated projects. To this end, we propose Compatible Remediation of Third-party libraries (CORAL) for Maven projects to fix vulnerabilities without breaking the projects. The evaluation proved that Coralnot only fixed 87.56% of vulnerabilities which outperformed other tools (best 75.32%) and achieved a 98.67% successful compilation rate and a 92.96% successful unit test rate. Furthermore, we found that 78.45% of vulnerabilities in popular Maven projects could be fixed without breaking the compilation, and the rest of the vulnerabilities (21.55%) could either be fixed by upgrades that break the compilations or even be impossible to fix by upgrading.",10.1109/ICSE48619.2023.00212,Remediation;Compatibility;Java;Open-source software,Java;Libraries;Security;Open source software;Software engineering,Java;public domain software;security of data;software libraries,Compatible Remediation of Third-party libraries;CORAL;Java projects;Maven projects;open-source software;SCA tools;software composition analysis;software projects,
1015,Vulnerability testing and patching,Automated Black-Box Testing of Mass Assignment Vulnerabilities in RESTful APIs,D. Corradini; M. Pasqua; M. Ceccato,"Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy; Department of Computer Science, University of Verona, Verona, Italy",2023,"Mass assignment is one of the most prominent vulnerabilities in RESTful APIs that originates from a misconfiguration in common web frameworks. This allows attackers to exploit naming convention and automatic binding to craft malicious requests that (massively) override data supposed to be read-only. In this paper, we adopt a black-box testing perspective to automatically detect mass assignment vulnerabilities in RESTful APIs. Indeed, execution scenarios are generated purely based on the OpenAPI specification, that lists the available operations and their message format. Clustering is used to group similar operations and reveal read-only fields, the latter are candidates for mass assignment. Then, test interaction sequences are automatically generated by instantiating abstract testing templates, with the aim of trying to use the found read-only fields to carry out a mass assignment attack. Test interactions are run, and their execution is assessed by a specific oracle, in order to reveal whether the vulnerability could be successfully exploited. The proposed novel approach has been implemented and evaluated on a set of case studies written in different programming languages. The evaluation highlights that the approach is quite effective in detecting seeded vulnerabilities, with a remarkably high accuracy.",10.1109/ICSE48619.2023.00213,REST API;Security testing;Black-box testing;Automated software testing;Mass assignment,Knowledge engineering;Authorization;Computer languages;Databases;Closed box;Restful API;Programming,application program interfaces;computer crime;formal specification;pattern clustering;program testing,abstract testing templates;automated black-box testing;automatic binding;clustering;malicious requests;mass assignment attack;mass assignment vulnerabilities;OpenAPI specification;read-only fields;RESTful APIs;seeded vulnerabilities;test interaction sequences;Web frameworks,1
1016,Vulnerability testing and patching,CoLeFunDa: Explainable Silent Vulnerability Fix Identification,J. Zhou; M. Pacheco; J. Chen; X. Hu; X. Xia; D. Lo; A. E. Hassan,"Centre for Software Excellence, Huawei, Toronto, Canada; Centre for Software Excellence, Huawei, Toronto, Canada; Centre for Software Excellence, Huawei, Toronto, Canada; School of Software Technology, Zhejiang University, Ningbo, China; Huawei, China; School of Information Systems, Singapore Management University, Singapore; Software Analysis and Intelligence Lab (SAIL), Queen's University",2023,"It is common practice for OSS users to leverage and monitor security advisories to discover newly disclosed OSS vulnerabilities and their corresponding patches for vulnerability remediation. It is common for vulnerability fixes to be publicly available one week earlier than their disclosure. This gap in time provides an opportunity for attackers to exploit the vulnerability. Hence, OSS users need to sense the fix as early as possible so that the vulnerability can be remediated before it is exploited. However, it is common for OSS to adopt a vulnerability disclosure policy which causes the majority of vulnerabilities to be fixed silently, meaning the commit with the fix does not indicate any vulnerability information. In this case even if a fix is identified, it is hard for OSS users to understand the vulnerability and evaluate its potential impact. To improve early sensing of vulnerabilities, the identification of silent fixes and their corresponding explanations (e.g., the corresponding common weakness enumeration (CWE) and exploitability rating) are equally important. However, it is challenging to identify silent fixes and provide explanations due to the limited and diverse data. To tackle this challenge, we propose CoLeFunDa: a framework consisting of a Contrastive Learner and FunDa, which is a novel approach for Function change Data augmentation. FunDa first increases the fix data (i.e., code changes) at the function level with unsupervised and supervised strategies. Then the contrastive learner leverages contrastive learning to effectively train a function change encoder, FCBERT, from diverse fix data. Finally, we leverage FCBERT to further fine-tune three downstream tasks, i.e., silent fix identification, CWE category classification, and exploitability rating classification, respectively. Our result shows that CoLeFunDa outperforms all the state-of-art baselines in all downstream tasks. We also conduct a survey to verify the effectiveness of CoLeFunDa in practical usage. The result shows that CoLeFunDa can categorize 62.5% (25 out of 40) CVEs with correct CWE categories within the top 2 recommendations.",10.1109/ICSE48619.2023.00214,OSS Vulnerabilities;Contrastive Learning,Surveys;Codes;Data augmentation;Sensors;Security;Task analysis;Monitoring,data augmentation;learning (artificial intelligence);public domain software;security of data;software engineering,CoLeFunDa;common weakness enumeration;contrastive learner;CVEs;CWE category classification;explainable silent vulnerability fix identification;exploitability rating classification;FCBERT;function change data augmentation;function change encoder;OSS vulnerabilities;security advisories;unsupervised strategies;vulnerability disclosure policy;vulnerability remediation,
1017,Cyber-physical systems testing,Finding Causally Different Tests for an Industrial Control System,C. M. Poskitt; Y. Chen; J. Sun; Y. Jiang,"Singapore Management University, Singapore; ShanghaiTech University, China; Singapore Management University, Singapore; Tsinghua University, China",2023,"Industrial control systems (ICSs) are types of cyber-physical systems in which programs, written in languages such as ladder logic or structured text, control industrial processes through sensing and actuating. Given the use of ICSs in critical infrastructure, it is important to test their resilience against manipulations of sensor/actuator inputs. Unfortunately, existing methods fail to test them comprehensively, as they typically focus on finding the simplest-to-craft manipulations for a testing goal, and are also unable to determine when a test is simply a minor permutation of another, i.e. based on the same causal events. In this work, we propose a guided fuzzing approach for finding 'meaningfully differentâ€™ tests for an ICS via a general formalisation of sensor/actuator-manipulation strategies. Our algorithm identifies the causal events in a test, generalises them to an equivalence class, and then updates the fuzzing strategy so as to find new tests that are causally different from those already identified. An evaluation of our approach on a real-world water treatment system shows that it is able to find 106% more causally different tests than the most comparable fuzzer. While we focus on diversifying the test suite of an ICS, our formalisation may be useful for other fuzzers that intercept communication channels.",10.1109/ICSE48619.2023.00215,Cyber-physical systems;fuzzing;test diversity;equivalence classes;causality,Integrated circuits;Process control;Communication channels;Fuzzing;Model checking;Mathematical models;Sensors,critical infrastructures;cyber-physical systems;industrial control,causally different tests;critical infrastructure;cyber-physical systems;equivalence class;guided fuzzing approach;ICSs;industrial control system;industrial processes;sensor-actuator-manipulation strategies;test suite;testing goal;water treatment system,
1018,Cyber-physical systems testing,DoppelgÃ¤nger Test Generation for Revealing Bugs in Autonomous Driving Software,Y. Huai; Y. Chen; S. Almanee; T. Ngo; X. Liao; Z. Wan; Q. A. Chen; J. Garcia,"University of California, Irvine; University of California, Irvine; University of California, Irvine; VNU University of Engineering and Technology; University of California, Irvine; University of California, Irvine; University of California, Irvine; University of California, Irvine",2023,"Vehicles controlled by autonomous driving software (ADS) are expected to bring many social and economic benefits, but at the current stage not being broadly used due to concerns with regard to their safety. Virtual tests, where autonomous vehicles are tested in software simulation, are common practices because they are more efficient and safer compared to field operational tests. Specifically, search-based approaches are used to find particularly critical situations. These approaches provide an opportunity to automatically generate tests; however, system-atically producing bug-revealing tests for ADS remains a major challenge. To address this challenge, we introduce DoppelTest, a test generation approach for ADSes that utilizes a genetic algorithm to discover bug-revealing violations by generating scenarios with multiple autonomous vehicles that account for traffic control (e.g., traffic signals and stop signs). Our extensive evaluation shows that DoppelTest can efficiently discover 123 bug-revealing violations for a production-grade ADS (Baidu Apollo) which we then classify into 8 unique bug categories.",10.1109/ICSE48619.2023.00216,cyber-physical systems;autonomous driving systems;search-based software testing,Software testing;Computer bugs;Web and internet services;Traffic control;Software systems;Safety;Test pattern generators,control engineering computing;genetic algorithms;mobile robots;program debugging;program testing;road traffic control;road vehicles;search problems;traffic engineering computing,123 bug-revealing violations;8 unique bug categories;autonomous driving software;bug-revealing tests;bugs;current stage;doppelgÃ¤nger test generation;economic benefits;multiple autonomous vehicles;operational tests;particularly critical situations;production-grade ADS;search-based approaches;social benefits;software simulation;test generation approach;traffic control;virtual tests,
1019,Cyber-physical systems testing,Generating Realistic and Diverse Tests for LiDAR-Based Perception Systems,G. Christian; T. Woodlief; S. Elbaum,"University of Virginia, Charlottesville, Virginia, USA; University of Virginia, Charlottesville, Virginia, USA; University of Virginia, Charlottesville, Virginia, USA",2023,"Autonomous systems rely on a perception component to interpret their surroundings, and when misinterpretations occur, they can and have led to serious and fatal system-level failures. Yet, existing methods for testing perception software remain limited in both their capacity to efficiently generate test data that translates to real-world performance and in their diversity to capture the long tail of rare but safety-critical scenarios. These limitations are particularly evident for perception systems based on LiDAR sensors, which have emerged as a crucial component in modern autonomous systems due to their ability to provide a 3D scan of the world and operate in all lighting conditions. To address these limitations, we introduce a novel approach for testing LiDAR-based perception systems by leveraging existing real-world data as a basis to generate realistic and diverse test cases through mutations that preserve realism invariants while generating inputs rarely found in existing data sets, and automatically crafting oracles that identify potentially safety-critical issues in perception performance. We implemented our approach to assess its ability to identify perception failures, generating over 50,000 test inputs for five state-of-the-art LiDAR-based perception systems. We found that it efficiently generated test cases that yield errors in perception that could result in real consequences if these systems were deployed and does so at a low rate of false positives.",10.1109/ICSE48619.2023.00217,Software Testing and Validation;Machine Learning,Software testing;Three-dimensional displays;Laser radar;Autonomous systems;Lighting;Tail;Software,failure analysis;mobile robots;optical radar;program testing,000 test inputs;50 test inputs;diverse test cases;fatal system-level failures;generating realistic;leveraging existing real-world data;LiDAR sensors;modern autonomous systems;perception component;perception failures;perception performance;realistic test cases;serious system-level failures;state-of-the-art LiDAR-based perception systems;test data;testing perception software,1
1020,Software ecosystems,Rules of Engagement: Why and How Companies Participate in OSS,M. Guizani; A. A. Castro-Guzman; A. Sarma; I. Steinmacher,"EECS. Oregon State University, Oregon, USA; Oregon State University, Oregon, USA; EECS. Oregon State University, Oregon, USA; Northern Arizona University, Arizona, USA",2023,"Company engagement in open source (OSS) is now the new norm. From large technology companies to startups, companies are participating in the OSS ecosystem by open-sourcing their technology, sponsoring projects through funding or paid developer time. However, our understanding of the OSS ecosystem is rooted in the â€œold worldâ€ model where individual contributors sustain OSS projects. In this work, we create a more comprehensive understanding of the hybrid OSS landscape by investigating what motivates companies to contribute and how they contribute to OSS. We conducted interviews with 20 participants who have different roles (e.g., CEO, OSPO Lead, Ecosystem Strategist) at 17 different companies of different sizes from large companies (e.g. Microsoft, RedHat, Google, Spotify) to startups. Data from semi-structured interviews reveal that company motivations can be categorized into four levels (Founders' Vision, Reputation, Business Advantage, and Reciprocity) and companies participate through different mechanisms (e.g., Developers' Time, Mentoring Time, Advocacy & Promotion Time), each of which tie to the different types of motivations. We hope our findings nudge more companies to participate in the OSS ecosystem, helping make it robust, diverse, and sustainable.",10.1109/ICSE48619.2023.00218,Open Source;OSS;companies in open source;motivations;diversity,Ecosystems;Companies;Lead;Internet;Mentoring;Interviews;Software engineering,business data processing;public domain software,company engagement;company motivations;large companies;open source;OSS ecosystem;OSS projects;startups;technology companies,
1021,Software ecosystems,An Empirical Study on Software Bill of Materials: Where We Stand and the Road Ahead,B. Xia; T. Bi; Z. Xing; Q. Lu; L. Zhu,"CSIRO's Data61, Sydney, Australia; CSIRO's Data61, Sydney, Australia; CSIRO's Data61, Sydney, Australia; CSIRO's Data61, Sydney, Australia; CSIRO's Data61, Sydney, Australia",2023,"The rapid growth of software supply chain attacks has attracted considerable attention to software bill of materials (SBOM). SBOMs are a crucial building block to ensure the transparency of software supply chains that helps improve software supply chain security. Although there are significant efforts from academia and industry to facilitate SBOM development, it is still unclear how practitioners perceive SBOMs and what are the challenges of adopting SBOMs in practice. Furthermore, existing SBOM-related studies tend to be ad-hoc and lack software engineering focuses. To bridge this gap, we conducted the first empirical study to interview and survey SBOM practitioners. We applied a mixed qualitative and quantitative method for gathering data from 17 interviewees and 65 survey respondents from 15 countries across five continents to understand how practitioners perceive the SBOM field. We summarized 26 statements and grouped them into three topics on SBOM's states of practice. Based on the study results, we derived a goal model and highlighted future directions where practitioners can put in their effort.",10.1109/ICSE48619.2023.00219,software bill of materials;SBOM;bill of materials;responsible AI;empirical study,Surveys;Industries;Roads;Supply chains;Bills of materials;Software;Security,bills of materials;software engineering;supply chain management,qualitative method;quantitative method;SBOM development;software bill of materials;software supply chain attacks;software supply chain security,1